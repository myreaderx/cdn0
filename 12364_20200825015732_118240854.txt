{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Pushy robots learn the fundamentals of object manipulation ","link":"https://news.mit.edu/2019/pushy-robots-learn-fundamentals-object-manipulation-1022","description":"<p>MIT researchers have compiled a dataset that captures the detailed behavior of a robotic system physically pushing hundreds of different objects. Using the dataset — the largest and most diverse of its kind — researchers can train robots to “learn” pushing dynamics that are fundamental to many complex object-manipulation tasks, including reorienting and inspecting objects, and uncluttering scenes.</p>\n<p>To capture the data, the researchers designed an automated system consisting of an industrial robotic arm with precise control, a 3D motion-tracking system, depth and traditional cameras, and software that stitches everything together. The arm pushes around modular objects that can be adjusted for weight, shape, and mass distribution. For each push, the system captures how those characteristics affect the robot’s push.</p>\n<p>The dataset, called “Omnipush,” contains 250 different pushes of 250 objects, totaling roughly 62,500 unique pushes. It’s already being used by researchers to, for instance, build models that help robots predict where objects will land when they’re pushed.</p>\n<p>“We need a lot of rich data to make sure our robots can learn,” says Maria Bauza, a graduate student in the Department of Mechanical Engineering (MechE) and first author of a paper describing Omnipush that’s being presented at the upcoming International Conference on Intelligent Robots and Systems. “Here, we’re collecting data from a real robotic system, [and] the objects are varied enough to capture the richness of the pushing phenomena. This is important to help robots understand how pushing works, and to translate that information to other similar objects in the real world.”</p>\n<p>Joining Bauza on the paper are: Ferran Alet and Yen-Chen Lin, graduate students in the Computer Science and Artificial Intelligence Laboratory and the Department of Electrical Engineering and Computer Science (EECS); Tomas Lozano-Perez, the School of Engineering Professor of Teaching Excellence; Leslie P. Kaelbling, the Panasonic Professor of Computer Science and Engineering; Phillip Isola, an assistant professor in EECS; and Alberto Rodriguez, an associate professor in MechE.</p>\n<p><strong>Diversifying data</strong></p>\n<p>Why focus on pushing behavior? Modeling pushing dynamics that involve friction between objects and surfaces, Rodriguez explains, is critical in higher-level robotic tasks. Consider the visually and technically impressive robot that can play Jenga, which Rodriguez recently co-designed. “The robot is performing a complex task, but the core of the mechanics driving that task is still that of pushing an object affected by, for instance, the friction between blocks,” Rodriguez says.</p>\n<p>Omnipush builds on a similar dataset built in the Manipulation and Mechanisms Laboratory (MCube) by Rodriguez, Bauza, and other researchers that captured pushing data on only 10 objects. After making the dataset public in 2016, they gathered feedback from researchers. One complaint was lack of object diversity: Robots trained on the dataset struggled to generalize information to new objects. There was also no video, which is important for computer vision, video prediction, and other tasks.</p>\n<p>For their new dataset, the researchers leverage an industrial robotic arm with precision control of the velocity and position of a pusher, basically a vertical steel rod. As the arm pushes the objects, a “Vicon”&nbsp;motion-tracking system —&nbsp;which has been used in films, virtual reality, and for research —&nbsp;follows the objects. There’s also an RGB-D camera, which adds depth information to captured video.</p>\n<p>The key was building modular objects. The uniform central pieces, made from aluminum, look like four-pointed stars and weigh about 100 grams. Each central piece contains markers on its center and points, so the Vicon system can detect its pose within a millimeter.</p>\n<p>Smaller pieces in four shapes — concave, triangular, rectangular, and circular — can be magnetically attached to any side of the central piece. Each piece weighs between 31 to 94 grams, but extra weights, ranging from 60 to 150 grams, can be dropped into little holes in the pieces. All pieces of the puzzle-like objects align both horizontally and vertically, which helps emulate the friction a single object with the same shape and mass distribution would have. All combinations of different sides, weights, and mass distributions added up to 250 unique objects.</p>\n<p>For each push, the arm automatically moves to a random position several centimeters from the object. Then, it selects a random direction and pushes the object for one second. Starting from where it stopped, it then chooses another random direction and repeats the process 250 times. Each push records &nbsp;the pose of the object and RGB-D video, which can be used for various video-prediction purposes. Collecting the data took 12 hours a day, for two weeks, totaling more than 150 hours. Humans intervention was only needed when manually reconfiguring the objects.</p>\n<p>The objects don’t specifically mimic any real-life items. Instead, they’re designed to capture the diversity of “kinematics” and “mass asymetries” expected of real-world objects, which model the physics of the motion of real-world objects. Robots can then extrapolate, say, the physics model of an Omnipush object with uneven mass distribution to any real-world object with similar uneven weight distributions.</p>\n<p>“Imagine pushing a table with four legs, where most weight is over one of the legs. When you push the table, you see that it rotates on the heavy leg and have to readjust. Understanding that mass distribution, and its effect on the outcome of a push, is something robots can learn with this set of objects,” Rodriguez says.</p>\n<p><strong>Powering new research</strong></p>\n<p>In one experiment, the researchers used Omnipush to train a model to predict the final pose of pushed objects, given only the initial pose and description of the push. They trained the model on 150 Omnipush objects, and tested it on a held-out portion of objects. Results showed that the Omnipush-trained model was twice as accurate as models trained on a few similar datasets. In their paper, the researchers also recorded benchmarks in accuracy that other researchers can use for comparison.&nbsp;</p>\n<p>Because Omnipush captures video of the pushes, one potential application is video prediction. A collaborator, for instance, is now using the dataset to train a robot to essentially “imagine” pushing objects between two points. After training on Omnipush, the robot is given as input two video frames, showing an object in its starting position and ending position. Using the starting position, the robot predicts all future video frames that ensure the object reaches its ending position. Then, it pushes the object in a way that matches each predicted video frame, until it gets to the frame with the ending position.</p>\n<p>“The robot is asking, ‘If I do this action, where will the object be in this frame?’ Then, it selects the action that maximizes the likelihood of getting the object in the position it wants,” Bauza says. “It decides how to move objects by first imagining how the pixels in the image will change after a push.”</p>\n<p>“Omnipush includes precise measurements of object motion, as well as visual data, for an important class of interactions between robot and objects in the world,” says Matthew T. Mason, a professor of computer science and robotics at Carnegie Melon University. “Robotics researchers can use this data to develop and test new robot learning approaches … that will fuel continuing advances in robotic manipulation.”</p>","descriptionType":"html","publishedDate":"Tue, 22 Oct 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"ed5afa0d47cb3d1735d9536561b3b0ae","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Two-legged robot mimics human balance while running and jumping ","link":"https://news.mit.edu/2019/two-legged-robot-mimics-human-balance-while-running-jumping-1030","description":"<p>Rescuing victims from a burning building, a chemical spill, or any disaster that is inaccessible to human responders could one day be a mission for resilient, adaptable robots. Imagine, for instance, rescue-bots that can bound through rubble on all fours, then rise up on two legs to push aside a heavy obstacle or break through a locked door.</p>\n<p>Engineers are making strides on the design of four-legged robots and their ability to <a href=\"http://news.mit.edu/2014/mit-cheetah-robot-runs-jumps-0915\">run</a>, <a href=\"http://news.mit.edu/2015/cheetah-robot-lands-running-jump-0529\">jump</a> and even do <a href=\"http://news.mit.edu/2019/mit-mini-cheetah-first-four-legged-robot-to-backflip-0304\">backflips</a>. But getting two-legged, humanoid robots to exert force or push against something without falling has been a significant stumbling block.</p>\n<p>Now engineers at MIT and the University of Illinois at Urbana-Champaign have developed a method to control balance in a two-legged, teleoperated robot — an essential step toward enabling a humanoid to carry out high-impact tasks in challenging environments.</p>\n<p>The team’s robot, physically resembling a machined torso and two legs, is controlled remotely by a human operator wearing a vest that transmits information about the human’s motion and ground reaction forces to the robot.</p>\n<p>Through the vest, the human operator can both direct the robot’s locomotion and feel the robot’s motions. If the robot is starting to tip over, the human feels a corresponding pull on the vest and can adjust in a way to rebalance both herself and, synchronously, the robot.</p>\n<p>In experiments with the robot to test this new “balance feedback” approach, the researchers were able to remotely maintain the robot’s balance as it jumped and walked in place in sync with its human operator.</p>\n<p>“It’s like running with a heavy backpack — you can feel how the dynamics of the backpack move around you, and you can compensate properly,” says Joao Ramos, who developed the approach as an MIT postdoc. “Now if you want to open a heavy door, the human can command the robot to throw its body at the door and push it open, without losing balance.”</p>\n<p>Ramos, who is now an assistant professor at the University of Illinois at Urbana-Champaign, has detailed the approach in a study appearing today in <em>Science Robotics</em>. His co-author on the study is Sangbae Kim, associate professor of mechanical engineering at MIT.</p>\n<p><strong>More than motion</strong></p>\n<p>Previously, Kim and Ramos built the two-legged robot <a href=\"https://web.mit.edu/jlramos/www/Hermes.html\">HERMES</a> (for Highly Efficient Robotic Mechanisms and Electromechanical System) and developed methods for it to mimic the motions of an operator via teleoperation, an approach that the researchers say comes with certain humanistic advantages.</p>\n<p>“Because you have a person who can learn and adapt on the fly, a robot can perform motions that it’s never practiced before [via teleoperation],” Ramos says.</p>\n<p>In demonstrations, HERMES has poured coffee into a cup, wielded an ax to chop wood, and handled an extinguisher to put out a fire.</p>\n<p>All these tasks have involved the robot’s upper body and algorithms to match the robot’s limb positioning with that of its operator’s. HERMES was able to carry out high-impact motions because the robot &nbsp;was rooted in place. Balance, in these cases, was much simpler to maintain. If the robot were required to take any steps, however, it would have likely tipped over in attempting to mimic the operator’s motions.</p>\n<p>“We realized in order to generate high forces or move heavy objects, just copying motions wouldn’t be enough, because the robot would fall easily,” Kim says. “We needed to copy the operator’s dynamic balance.”</p>\n<p>Enter Little HERMES, a miniature version of HERMES that is about a third the size of an average human adult. The team engineered the robot as simply a torso and two legs, and designed the system specifically to test lower-body tasks, such as locomotion and balance. As with its full-body counterpart, Little HERMES is designed for teleoperation, with an operator suited up in a vest to control the robot’s actions.</p>\n<p>For the robot to copy the operator’s balance rather than just their motions, the team had to first find a simple way to represent balance. Ramos eventually realized that balance could be stripped down to two main ingredients: a person’s center of mass and their center of pressure — basically, a point on the ground where a force equivalent to all supporting forces is exerted.</p>\n<p>The location of the center of mass in relation to the center of pressure, Ramos found, relates directly to how balanced a person is at any given time. He also found that the position of these two ingredients could be physically represented as an inverted &nbsp;pendulum. Imagine swaying from side to side while staying rooted to the same spot. The effect is similar to the swaying of an upside-down pendulum, the top end representing a human’s center of mass (usually in the torso) and the bottom representing their center of pressure on the ground.&nbsp;</p>\n<p><strong>Heavy lifting</strong></p>\n<p>To define how center of mass relates to center of pressure, Ramos gathered human motion data, including measurements in the lab, where he swayed back and forth, walked in place, and jumped on a force plate that measured the forces he exerted on the ground, as the position of his feet and torso were recorded. He then condensed this data into measurements of the center of mass and the center of pressure, and developed a model to represent each in relation to the other, as an inverted pendulum.</p>\n<p>He then developed a second model, similar to the model for human balance but scaled to the dimensions of the smaller, lighter robot, and he developed a control algorithm to link and enable feedback between the two models.</p>\n<p>The researchers tested this balance feedback model, first on a simple inverted pendulum that they built in the lab, in the form of a beam about the same height as Little HERMES. They connected the beam to their teleoperation system, and it swayed back and forth along a track in response to an operator’s movements. As the operator swayed to one side, the beam did likewise — a movement that the operator could also feel through the vest. If the beam swayed too far, the operator, feeling the pull, could lean the other way to compensate, and keep the beam balanced.</p>\n<p>The experiments showed that the new feedback model could work to maintain balance on the beam, so the researchers then tried the model on Little HERMES. They also developed an algorithm for the robot to automatically translate the simple model of balance to the forces that each of its feet would have to generate, to copy the operator’s feet.</p>\n<p>In the lab, Ramos found that as he wore the vest, he could not only control the robot’s motions and balance, but he also could feel the robot’s movements. When the robot was struck with a hammer from various directions, Ramos felt the vest jerk in the direction the robot moved. Ramos instinctively resisted the tug, which the robot registered as a subtle shift in the center of mass in relation to center of pressure, which it in turn mimicked. The result was that the robot was able to keep from tipping over, even amidst repeated blows to its body.</p>\n<p>Little HERMES also mimicked Ramos in other exercises, including running and jumping in place, and walking on uneven ground, all while maintaining its balance without the aid of tethers or supports.</p>\n<p>“Balance feedback is a difficult thing to define because it’s something we do without thinking,” Kim says. “This is the first time balance feedback is properly defined for the dynamic actions. This will change how we control a teleoperated humanoid.”</p>\n<p>Kim and Ramos will continue to work on developing a full-body humanoid with similar balance control, to one day be able to gallop through a disaster zone and rise up to push away barriers as part of rescue or salvage missions.</p>\n<p>“Now we can do heavy door opening or lifting or throwing heavy objects, with proper balance communication,” Kim says.</p>\n<p>This research was supported, in part, by Hon Hai Precision Industry Co. Ltd. and Naver Labs Corporation.</p>","descriptionType":"html","publishedDate":"Wed, 30 Oct 2019 18:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"eb36f13df580a0b33f1cfe05b1f932fb","bgimgJsdelivr":"","metaImg":"","author":"Jennifer Chu | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"An interdisciplinary approach to accelerating human-machine collaboration","link":"https://news.mit.edu/2019/humatics-david-mindell-machine-1002","description":"<p>David Mindell has spent his career defying traditional distinctions between disciplines. His work has explored the ways humans interact with machines, drive innovation, and maintain societal well-being as technology transforms our economy.</p>\n<p>And, Mindell says, he couldn’t have done it anywhere but MIT. He joined MIT’s faculty 23 years ago after completing his PhD in the Program in Science, Technology, and Society, and he currently holds a dual appointment in engineering and humanities as the Frances and David Dibner Professor of the History of Engineering and Manufacturing in the School of Humanities, Arts, and Social Sciences and professor of aeronautics and astronautics.</p>\n<p>Mindell’s experience combining fields of study has shaped his ideas about the relationship between humans and machines. Those ideas are what led him to found Humatics — a startup named from the merger of “human” and “robotics.”</p>\n<p>Humatics is trying to change the way humans work alongside machines, by enabling location tracking and navigation indoors, underground, and in other areas where technologies like GPS are limited. It accomplishes this by using radio frequencies to track things at the millimeter scale — unlocking what Mindell calls microlocation technology.</p>\n<p>The company’s solution is already being used in places like shipping ports and factories, where humans work alongside cranes, industrial tools, automated guided vehicles (AGVs), and other machines. These businesses often lack consistent location data for their machines and are forced to adopt inflexible routes for their mobile robots.</p>\n<p>“One of the holy grails is to have humans and robots share the same space and collaborate, and we’re enabling mobile robots to work in human environments safely and on a large scale,” Mindell says. “Safety is a critical first form of collaboration, but beyond that, we’re just beginning to learn how to work [in settings] where robots and people are exquisitely aware of where they are.”</p>\n<p><strong>A company decades in the making</strong></p>\n<p>MIT has a long history of transcending research fields to improve our understanding of the world. Take, for example, Norbert Wiener, who served on MIT’s faculty in the Department of Mathematics between 1919 and his death in 1964.</p>\n<p>Wiener is credited with formalizing the field of cybernetics, which is an approach to understanding feedback systems he defined as “the scientific study of control and communication in the animal and the machine.\" Cybernetics can be applied to mechanical, biological, cognitive, and social systems, among others, and it sparked a frenzy of interdisciplinary study and scientific collaboration.</p>\n<p>In 2002, Mindell wrote a book exploring the history of cybernetics before Wiener and its emergence at the intersection of a range of disciplines during World War II. It is one of several books Mindell has written that deal with interdisciplinary responses to complex problems, particularly in extreme environments like lunar landings and the deep sea.</p>\n<p>The interdisciplinary perspective Mindell forged at MIT has helped him identify the limitations of technology that prevent machines and humans from working together seamlessly.</p>\n<p>One particular shortcoming that Mindell has thought about for years is the lack of precise location data in places like warehouses, subway systems, and shipping ports.</p>\n<p>“In five years, we’ll look back at 2019 and say, ‘I can’t believe we didn’t know where anything was,’” Mindell says. “We’ve got so much data floating around, but the link between the actual physical world we all inhabit and move around in and the digital world that’s exploding is really still very poor.”</p>\n<p>In 2014, Mindell partnered with Humatics co-founder Gary Cohen, who has worked as an intellectual property strategist for biotech companies in the Kendall Square area, to solve the problem.</p>\n<p>In the beginning of 2015, Mindell collaborated with Lincoln Laboratory alumnus and radar expert Greg Charvat; the two built a prototype navigation system and started the company two weeks later. Charvat became Humatics’ CTO and first employee.</p>\n<p>“It was clear there was about to be this huge flowering of robotics and autonomous systems and AI, and I thought the things we learned in extreme environments, notably under sea and in aviation, had an enormous amount of application to industrial environments,” Mindell says. “The company is about bringing insights from years of experience with remote and autonomous systems in extreme environments into transit, logistics, e-commerce, and manufacturing.”</p>\n<p><strong>Bringing microlocation to industry</strong></p>\n<p>Factories, ports, and other locations where GPS data is unworkable or insufficient adopt a variety of solutions to meet their tracking and navigation needs. But each workaround has its drawbacks.</p>\n<p>RFID and Bluetooth technologies, for instance, can track assets but have short ranges and are expensive to deploy across large areas.</p>\n<p>Cameras and sensing methods like LIDAR can be used to help machines see their environment, but they struggle with things like rain and different lighting conditions. Floor tape embedded with wires or magnets is also often used to guide machines through fixed routes, but it isn’t well-suited for today’s increasingly dynamic warehouses and production lines.</p>\n<p>Humatics has focused on making the capabilities of its microlocation location system as easy to leverage as possible. The location and tracking data it collects can be integrated into whatever warehouse management system or “internet of things” (IoT) platforms customers are already using.</p>\n<p>Its radio frequency beacons have a range of up to 500 meters and, when installed as part of a constellation, can pinpoint three dimensional locations to within 2 centimeters, creating a virtual grid of the surrounding environment.</p>\n<p>The beacons can be combined with an onboard navigation hub that helps mobile robots move around dynamic environments. Humatics’ system also gathers location data from multiple points at once, monitoring the speed of a forklift, helping a crane operator place a shipping crate, and guiding a robot around obstacles simultaneously.</p>\n<p>The data Humatics collects don’t just help customers improve their processes; they can also transform the way workers and machines share space and work together. Indeed, with a new chip just emerging from its labs, Mindell says Humatics is moving industries such as manufacturing and logistics into “the world of ubiquitous, millimeter-accurate positioning.”</p>\n<p>It’s all possible because of the company’s holistic approach to the age-old problem of human-machine interaction.</p>\n<p>“Humatics is an example of what can happen when we think about technology in a unique, broader context,” Mindell says. “It’s an example of what MIT can accomplish when it pays serious attention to these two ways [from humanities and engineering] of looking at the world.”</p>","descriptionType":"html","publishedDate":"Wed, 02 Oct 2019 04:00:01 +0000","feedId":12364,"bgimg":"","linkMd5":"aa1044fe9f13f552fecc775f36bc6977","bgimgJsdelivr":"","metaImg":"","author":"Zach Winn | MIT News Office","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"CSAIL robot disinfects Greater Boston Food Bank","link":"https://news.mit.edu/2020/csail-robot-disinfects-greater-boston-food-bank-covid-19-0629","description":"<p>With every droplet that we can’t see, touch, or feel dispersed into the air, the threat of spreading Covid-19 persists. It’s become increasingly critical to keep these heavy droplets from lingering — especially on surfaces, which are welcoming and generous hosts.&nbsp;</p> \n<p>Thankfully, our chemical cleaning products are effective, but using them to disinfect larger settings can be expensive, dangerous, and time-consuming. Across the globe there are thousands of warehouses, grocery stores, schools, and other spaces where cleaning workers are at risk.</p> \n<p>With that in mind, a team from MIT’s <a href=\"http://csail.mit.edu\">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL), in collaboration with <a href=\"https://www.avarobotics.com/\">Ava Robotics</a> and the <a href=\"https://www.gbfb.org/\">Greater Boston Food Bank</a> (GBFB), designed a new robotic system that powerfully disinfects surfaces and neutralizes aerosolized forms of the coronavirus.</p> \n<p>The approach uses a custom UV-C light fixture designed at CSAIL that is integrated with Ava Robotics’ mobile robot base. The results were encouraging enough that researchers say that the approach could be useful for autonomous UV disinfection in other environments, such as factories, restaurants, and supermarkets.&nbsp;</p> \n<p>UV-C light has proven to be effective at killing viruses and bacteria on surfaces and aerosols, but it’s unsafe for humans to be exposed. Fortunately, Ava’s telepresence robot doesn’t require any human supervision. Instead of the telepresence top, the team subbed in a UV-C array for disinfecting surfaces. Specifically, the array uses short-wavelength ultraviolet light to kill microorganisms and disrupt their DNA in a process called ultraviolet germicidal irradiation.</p> \n<p>The complete robot system is capable of mapping the space — in this case, GBFB’s warehouse — and navigating between waypoints and other specified areas. In testing the system, the team used a UV-C dosimeter, which confirmed that the robot was delivering the expected dosage of UV-C light predicted by the model.</p> \n<p>“Food banks provide an essential service to our communities, so it is critical to help keep these operations running,” says Alyssa Pierson, CSAIL research scientist and technical lead of the UV-C lamp assembly. “Here, there was a unique opportunity to provide additional disinfecting power to their current workflow, and help reduce the risks of Covid-19 exposure.”&nbsp;</p> \n<p>Food banks are also facing a particular demand due to the stress of Covid-19. The United Nations projected that, because of the virus, the number of people facing severe food insecurity worldwide <a href=\"https://www.wfp.org/news/covid-19-will-double-number-people-facing-food-crises-unless-swift-action-taken\">could double to 265 million</a>. In the United States alone, the five-week total of job losses has risen to 26 million, potentially pushing millions more into food insecurity.&nbsp;</p> \n<p>During tests at GBFB, the robot was able to drive by the pallets and storage aisles at a speed of roughly 0.22 miles per hour. At this speed, the robot could cover a 4,000-square-foot space in GBFB’s warehouse in just half an hour. The UV-C dosage delivered during this time can neutralize approximately 90 percent of coronaviruses on surfaces. For many surfaces, this dose will be higher, resulting in more of the virus neutralized.</p> \n<p>Typically, this method of ultraviolet germicidal irradiation is used largely in hospitals and medical settings, to sterilize patient rooms and stop the spread of microorganisms like <span>methicillin-resistant <em>staphylococcus aureus</em></span> and <span class=\"st\"><em>Clostridium difficile</em></span>, and the UV-C light also works against airborne pathogens. While it’s most effective in the direct “line of sight,” it can get to nooks and crannies as the light bounces off surfaces and onto other surfaces.&nbsp;</p> \n<p>\"Our 10-year-old warehouse is a relatively new food distribution facility with AIB-certified, state-of-the-art cleanliness and food safety standards,” says Catherine D’Amato, president and CEO of the Greater Boston Food Bank. “Covid-19 is a new pathogen that GBFB, and the rest of the world, was not designed to handle. We are pleased to have this opportunity to work with MIT CSAIL and Ava Robotics to innovate and advance our sanitation techniques to defeat this menace.\"&nbsp;</p> \n<p>As a first step, the team teleoperated the robot to teach it the path around the warehouse — meaning it’s equipped with autonomy to move around, without the team needing to navigate it remotely.&nbsp;</p> \n<p>It can go to defined waypoints on its map, such as going to the loading dock, then the warehouse shipping floor, then returning to base. They define those waypoints from the expert human user in teleop mode, and then can add new waypoints to the map as needed.&nbsp;</p> \n<p>Within GBFB, the team identified the warehouse shipping floor as a “high-importance area” for the robot to disinfect. Each day, workers stage aisles of products and arrange them for up to 50 pickups by partners and distribution trucks the next day. By focusing on the shipping area, it prioritizes disinfecting items leaving the warehouse to reduce Covid-19 spread out into the community.</p> \n<p>Currently, the team is exploring how to use its onboard sensors to adapt to changes in the environment, such that in new territory, the robot would adjust its speed to ensure the recommended dosage is applied to new objects and surfaces.&nbsp;</p> \n<p>A unique challenge is that the shipping area is constantly changing, so each night, the robot encounters a slightly new environment. When the robot is deployed, it doesn’t necessarily know which of the staging aisles will be occupied, or how full each aisle might be. Therefore, the team notes that they need to teach the robot to differentiate between the occupied and unoccupied aisles, so it can change its planned path accordingly.</p> \n<p>As far as production went, “in-house manufacturing” took on a whole new meaning for this prototype and the team. The UV-C lamps were assembled in Pierson's basement, and CSAIL PhD student Jonathan Romanishin crafted a makeshift shop in his apartment for the electronics board assembly.&nbsp;</p> \n<p>“As we drive the robot around the food bank, we are also researching new control policies that will allow the robot to adapt to changes in the environment and ensure all areas receive the proper estimated dosage,” says Pierson. “We are focused on remote operation to minimize&nbsp; human supervision, and, therefore, the additional risk of spreading Covid-19, while running our system.”&nbsp;</p> \n<p>For immediate next steps, the team is focused on increasing the capabilities of the robot at GBFB, as well as eventually implementing design upgrades. Their broader intention focuses on how to make these systems more capable at adapting to our world: how a robot can dynamically change its plan based on estimated UV-C dosages, how it can work in new environments, and how to coordinate teams of UV-C robots to work together.</p> \n<p>“We are excited to see the UV-C disinfecting robot support our community in this time of need,” says CSAIL director and project lead Daniela Rus. “The insights we received from the work at GBFB has highlighted several algorithmic challenges. We plan to tackle these in order to extend the scope of autonomous UV disinfection in complex spaces, including dorms, schools, airplanes, and grocery stores.”&nbsp;</p> \n<p>Currently, the team’s focus is on GBFB, although the algorithms and systems they are developing could be transferred to other use cases in the future, like warehouses, grocery stores, and schools.&nbsp;</p> \n<p>\"MIT has been a great partner, and when they came to us, the team was eager to start the integration, which took just four weeks to get up and running,” says Ava Robotics CEO Youssef Saleh. “The opportunity for robots to solve workplace challenges is bigger than ever, and collaborating with MIT to make an impact at the food bank has been a great experience.\"&nbsp;</p> \n<p>Pierson and Romanishin worked alongside Hunter Hansen (software capabilities), Bryan Teague of MIT Lincoln Laboratory (who assisted with the UV-C lamp assembly), Igor Gilitschenski and Xiao Li (assisting with future autonomy research), MIT professors Daniela Rus and Saman Amarasinghe, and Ava leads Marcio Macedo and Youssef Saleh.&nbsp;</p> \n<p>This project was supported in part by Ava Robotics, who provided their platform and team support.</p>","descriptionType":"html","publishedDate":"Mon, 29 Jun 2020 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"bec6f9d74a99302b8dba390439472f93","bgimgJsdelivr":"","metaImg":"","author":"Rachel Gordon | MIT CSAIL","publishedOrCreatedDate":1598320622357},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"This flat structure morphs into shape of a human face when temperature changes","link":"https://news.mit.edu/2019/mesh-structure-shape-temperature-changes-0930","description":"<p>Researchers at MIT and elsewhere have designed 3-D printed mesh-like structures that morph from flat layers into predetermined shapes, in response to changes in ambient temperature. The new structures can transform into configurations that are more complex than what other shape-shifting materials and structures can achieve.</p>\n<p>As a demonstration, the researchers printed a flat mesh that, when exposed to a certain temperature difference, deforms into the shape of a human face. They also designed a mesh embedded with conductive liquid metal, that curves into a dome to form an active antenna, the resonance frequency of which changes as it deforms.</p>\n<p>The team’s new design method can be used to determine the specific pattern of flat mesh structures to print, given the material’s properties, in order to make the structure transform into a desired shape.</p>\n<p>The researchers say that down the road, their technique may be used to design deployable structures, such as tents or coverings that automatically unfurl and inflate in response to changes in temperature or other ambient conditions.</p>\n<p>Such complex, shape-shifting structures could also be of use as stents or scaffolds for artificial tissue, or as deformable lenses in telescopes. Wim van Rees, assistant professor of mechanical engineering at MIT, also sees applications in soft robotics.</p>\n<p>“I’d like to see this incorporated in, for example, a robotic jellyfish that changes shape to swim as we put it in water,” says van Rees. “If you could use this as an actuator, like an artificial muscle, the actuator could be any arbitrary shape that transforms into another arbitrary shape. Then you’re entering an entirely new design space in soft robotics.”</p>\n<p>Van Rees and his colleagues are publishing their results this week in the <em>Proceedings of the National Academy of Sciences</em>. His co-authors are J. William Boley of Boston University; Ryan Truby, Arda Kotikian, Jennifer Lewis, and L. Mahadevan of Harvard University; Charles Lissandrello of Draper Laboratory; and Mark Horenstein of Boston University.</p>\n<p><strong>Gift wrap’s limit</strong></p>\n<p>Two years ago, van Rees came up with a theoretical design for how to transform a thin flat sheet into a complex shape such as a human face. Until then, researchers in the field of 4-D materials — materials designed to deform over time — had developed ways for certain materials to change, or morph, but only into relatively simple structures.</p>\n<p>“My goal was to start with a complex 3-D shape that we want to achieve, like a human face, and then ask, ‘How do we program a material so it gets there?’” van Rees says. “That’s a problem of inverse design.”</p>\n<p>He came up with a formula to compute the expansion and contraction that regions of a bilayer material sheet would have to achieve in order to reach a desired shape, and developed a code to simulate this in a theoretical material. He then put the formula to work, and visualized how the method could transform a flat, continuous disc into a complex human face.</p>\n<p>But he and his collaborators quickly found that the method wouldn’t apply to most physical materials, at least if they were trying to work with continuous sheets. While van Rees used a continuous sheet for his simulations, it was of an idealized material, with no physical constraints on the amount of expansion and contraction it could achieve. Most materials, in contrast, have very limited growth capabilities. This limitation has profound consequences on a property known as double curvature, meaning a surface that can curve simultaneously in two perpendicular directions — an effect that is described in an almost 200-year-old theorem by Carl Friedrich Gauss called the Theorema Egregium, Latin for “Remarkable Theorem.”</p>\n<p>If you’ve ever tried to gift wrap a soccer ball, you’ve experienced this concept in practice: To transform paper, which has no curvature at all, to the shape of a ball, which has positive double curvature, you have to crease and crumple the paper at the sides and bottom to completely wrap the ball. In other words, for the paper sheet to adapt to a shape with double curvature, it would have to stretch or contract, or both, in the necessary places to wrap a ball uniformly.</p>\n<p>To impart double curvature to a shape-shifting sheet, the researchers switched the basis of the structure from a continuous sheet to a lattice, or mesh. The idea was twofold: first, a temperature-induced bending of the lattice’s ribs would result in much larger expansions and contractions of the mesh nodes, than could be achieved in a continuous sheet. Second, the voids in the lattice can easily accommodate large changes in surface area when the ribs are designed to grow at different rates across the sheet.</p>\n<p>The researchers also designed each individual rib of the lattice to bend by a predetermined degree in order to create the shape of, say, a nose rather than an eye-socket.</p> \n<p>For each rib, they incorporated four skinnier ribs, arranging two to line up atop the other two. All four miniribs were made from carefully selected variations of the same base material, to calibrate the required different responses to temperature.</p>\n<p>When the four miniribs were bonded together in the printing process to form one larger rib, the rib as a whole could curve due to the difference in temperature response between the materials of the smaller ribs: If one material is more responsive to temperature, it may prefer to elongate. But because it is bonded to a less responsive rib, which resists the elongation, the whole rib will curve instead.</p>\n<p>The researchers can play with the arrangement of the four ribs to “preprogram” whether the rib as a whole curves up to form part of a nose, or dips down as part of an eye socket.</p>\n<p><strong>Shapes unlocked</strong></p>\n<p>To fabricate a lattice that changes into the shape of a human face, the researchers started with a 3-D image of a face — to be specific, the face of Gauss, whose principles of geometry underly much of the team’s approach. From this image, they created a map of the distances a flat surface would require to rise up or dip down to conform to the shape of the face. Van Rees then devised an algorithm to translate these distances into a lattice with a specific pattern of ribs, and ratios of miniribs within each rib.</p>\n<p>The team printed the lattice from PDMS, a common rubbery material which naturally expands when exposed to an increase in temperature. They adjusted the material’s temperature responsiveness by infusing one solution of it with glass fibers, making it physically stiffer and more resistant to a change in temperature. After printing lattice patterns of the material, they cured the lattice in a 250-degree-Celsius oven, then took it out and placed it in a saltwater bath, where it cooled to room temperature and morphed into the shape of a human face.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-1.gif\" /></p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-2.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><em><span style=\"font-size: 10px;\">Courtesy of the researchers</span></em></p>\n<p>The team also printed a latticed disc made from ribs embedded with a liquid metal ink — an antenna of sorts, that changed its resonant frequency as the lattice transformed into a dome.</p>\n<p>Van Rees and his colleagues are currently investigating ways to apply the design of complex shape-shifting to stiffer materials, for sturdier applications, such as temperature-responsive tents and self-propelling fins and wings.</p>\n<p>This research was supported, in part, by the National Science Foundation, and Draper Laboratory.</p>","descriptionType":"html","publishedDate":"Mon, 30 Sep 2019 18:59:59 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-1.gif","linkMd5":"77ba5259c1b65209944a84af9bf4f01d","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn16@2020_5/2020/08/25/01-57-17-311_053559e65c44b78a.webp","destWidth":500,"destHeight":281,"sourceBytes":6895871,"destBytes":3180834,"author":"Jennifer Chu | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn16@2020_5/2020/08/25/01-57-17-311_053559e65c44b78a.webp","https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn56@2020_3/2020/08/25/01-57-26-429_8a0b87dd6580e87f.webp"},"publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Soft robotics breakthrough manages immune response for implanted devices","link":"https://news.mit.edu/2019/soft-robotics-breakthrough-manages-implanted-devices-immune-response-0904","description":"<p>Researchers from the Institute for Medical Engineering and Science (IMES) at MIT; the National University of Ireland Galway (NUI Galway); and AMBER, the SFI Research Centre for Advanced Materials and BioEngineering Research, recently announced a significant breakthrough in soft robotics that could help patients requiring in-situ (implanted) medical devices such as breast implants, pacemakers, neural probes, glucose biosensors, and drug and cell delivery devices.</p> \n<p>The implantable medical devices market is currently estimated at approximately $100 billion, with significant growth potential into the future as new technologies for drug delivery and health monitoring are developed. These devices are not without problems, caused in part by the body’s own protection responses. These complex and unpredictable foreign-body responses impair device function and drastically limit the long-term performance and therapeutic efficacy of these devices.</p> \n<p>One such foreign body response is fibrosis, a process whereby a dense fibrous capsule surrounds the implanted device, which can cause device failure or impede its function. Implantable medical devices have various failure rates that can be attributed to fibrosis, ranging from 30-50 percent for implantable pacemakers or 30 percent for mammoplasty prosthetics. In the case of biosensors or drug/cell delivery devices, the dense fibrous capsule which can build up around the implanted device can seriously impede its function, with consequences for the patient and costs to the health care system.</p> \n<p>A radical new vision for medical devices to address this problem was published in the internationally respected journal, <em>Science Robotics</em>. The <a href=\"https://robotics.sciencemag.org/content/4/33/eaax7043\">study</a> was led by researchers from NUI Galway, IMES, and the SFI research center AMBER, among others. The research describes the use of soft robotics to modify the body’s response to implanted devices. Soft robots are flexible devices that can be implanted into the body.</p> \n<p>The transatlantic partnership of scientists has created a tiny, mechanically actuated soft robotic device known as a dynamic soft reservoir (DSR) that has been shown to significantly reduce the build-up of the fibrous capsule by manipulating the environment at the interface between the device and the body. The device uses mechanical oscillation to modulate how cells respond around the implant. In a bio-inspired design, the DSR can change its shape at a microscope scale through an actuating membrane.</p> \n<p>IMES core faculty member, assistant professor at the Department of Mechanical Engineering, and W.M. Keck Career Development Professor in Biomedical Engineering Ellen Roche, the senior co-author of the study, is a former researcher at NUI Galway who won international acclaim in 2017 for her work in creating a soft robotic sleeve to help patients with heart failure. Of this research, Roche says “This study demonstrates how mechanical perturbations of an implant can modulate the host foreign body response. This has vast potential for a range of clinical applications and will hopefully lead to many future collaborative studies between our teams.”</p> \n<p>Garry Duffy, professor in anatomy at NUI Galway and AMBER principal investigator, and a senior co-author of the study, adds “We feel the ideas described in this paper could transform future medical devices and how they interact with the body. We are very excited to develop this technology further and to partner with people interested in the potential of soft robotics to better integrate devices for longer use and superior patient outcomes. It’s fantastic to build and continue the collaboration with the Dolan and Roche labs, and to develop a trans-Atlantic network of soft roboticists.”</p> \n<p>The first author of the study, Eimear Dolan, lecturer of biomedical engineering at NUI Galway and former researcher in the Roche and Duffy labs at MIT and NUI Galway, says “We are very excited to publish this study, as it describes an innovative approach to modulate the foreign-body response using soft robotics. I recently received a Science Foundation Ireland Royal Society University Research Fellowship to bring this technology forward with a focus on Type 1 diabetes. It is a privilege to work with such a talented multi-disciplinary team, and I look forward to continuing working together.”</p>","descriptionType":"html","publishedDate":"Wed, 04 Sep 2019 16:00:01 +0000","feedId":12364,"bgimg":"","linkMd5":"b9c34a1a48451aac6d516e90dbb70056","bgimgJsdelivr":"","metaImg":"","author":"Institute for Medical Engineering and Science","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"“Sensorized” skin helps soft robots find their bearings","link":"https://news.mit.edu/2020/sensorized-skin-soft-robots-0213","description":"<p>For the first time, MIT researchers have enabled a soft robotic arm to understand its configuration in 3D space, by leveraging only motion and position data from its own “sensorized” skin.</p>\n<p>Soft robots constructed from highly compliant materials, similar to those found in living organisms, are being championed as safer, and more adaptable, resilient, and bioinspired alternatives to traditional rigid robots. But giving autonomous control to these deformable robots is a monumental task because they can move in a virtually infinite number of directions at any given moment. That makes it difficult to train planning and control models that drive automation.</p>\n<p>Traditional methods to achieve autonomous control use large systems of multiple motion-capture cameras that provide the robots feedback about 3D movement and positions. But those are impractical for soft robots in real-world applications.</p>\n<p>In a paper being published in the journal <em>IEEE Robotics and Automation Letters</em>, the researchers describe a system of soft sensors that cover a robot’s body to provide “proprioception” — meaning awareness of motion and position of its body. That feedback runs into a novel deep-learning model that sifts through the noise and captures clear signals to estimate the robot’s 3D configuration. The researchers validated their system on a soft robotic arm resembling an elephant trunk, that can predict its own position as it autonomously swings around and extends.</p>\n<p>The sensors can be fabricated using off-the-shelf materials, meaning any lab can develop their own systems, says Ryan Truby, a postdoc in the MIT Computer Science and Artificial Laboratory (CSAIL) who is co-first author on the paper along with CSAIL postdoc Cosimo Della Santina.</p>\n<p>“We’re sensorizing soft robots to get feedback for control from sensors, not vision systems, using a very easy, rapid method for fabrication,” he says. “We want to use these soft robotic trunks, for instance, to orient and control themselves automatically, to pick things up and interact with the world. This is a first step toward that type of more sophisticated automated control.”</p>\n<p>One future aim is to help make artificial limbs that can more dexterously handle and manipulate objects in the environment. “Think of your own body: You can close your eyes and reconstruct the world based on feedback from your skin,” says co-author Daniela Rus, director of CSAIL and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “We want to design those same capabilities for soft robots.”</p>\n<p><strong>Shaping soft sensors</strong></p>\n<p>A longtime goal in soft robotics has been fully integrated body sensors. Traditional rigid sensors detract from a soft robot body’s natural compliance, complicate its design and fabrication, and can cause various mechanical failures. Soft-material-based sensors are a more suitable alternative, but require specialized materials and methods for their design, making them difficult for many robotics labs to fabricate and integrate in soft robots.</p>\n<p>While working in his CSAIL lab one day looking for inspiration for sensor materials, Truby made an interesting connection. “I found these sheets of conductive materials used for electromagnetic interference shielding, that you can buy anywhere in rolls,” he says. These materials have “piezoresistive” properties, meaning they change in electrical resistance when strained. Truby realized they could make effective soft sensors if they were placed on certain spots on the trunk. As the sensor deforms in response to the trunk’s stretching and compressing, its electrical resistance is converted to a specific output voltage. The voltage is then used as a signal correlating to that movement.</p>\n<p>But the material didn’t stretch much, which would limit its use for soft robotics. Inspired by kirigami —&nbsp;a variation of origami that includes making cuts in a material — Truby designed and laser-cut rectangular strips of conductive silicone sheets into various patterns, such as rows of tiny holes or crisscrossing slices like a chain link fence. That made them far more flexible, stretchable, “and beautiful to look at,” Truby says.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-1_0.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-2.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><em style=\"font-size: 10px;\">Credit: Ryan L. Truby, MIT&nbsp;CSAIL</em></p>\n<p>The researchers’ robotic trunk comprises three segments, each with four fluidic actuators (12 total) used to move the arm. They fused one sensor over each segment, with each sensor covering and gathering data from one embedded actuator in the soft robot. They used “plasma bonding,” a technique that energizes a surface of a material to make it bond to another material. It takes roughly a couple hours to shape dozens of sensors that can be bonded to the soft robots using a handheld plasma-bonding device.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-3.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><span style=\"font-size:10px;\"><em>Credit: Ryan L. Truby, MIT&nbsp;CSAIL</em></span></p>\n<p><strong>“Learning” configurations</strong></p>\n<p>As hypothesized, the sensors did capture the trunk’s general movement. But they were really noisy. “Essentially, they’re nonideal sensors in many ways,” Truby says. “But that’s just a common fact of making sensors from soft conductive materials. Higher-performing and more reliable sensors require specialized tools that most robotics labs do not have.”</p> \n<p>To estimate the soft robot’s configuration using only the sensors, the researchers built a deep neural network to do most of the heavy lifting, by sifting through the noise to capture meaningful feedback signals. The researchers developed a new model to kinematically describe the soft robot’s shape that vastly reduces the number of variables needed for their model to process.</p>\n<p>In experiments, the researchers had the trunk swing around and extend itself in random configurations over approximately an hour and a half. They used the traditional motion-capture system for ground truth data. In training, the model analyzed data from its sensors to predict a configuration, and compared its predictions to that ground truth data which was being collected simultaneously. In doing so, the model “learns” to map signal patterns from its sensors to real-world configurations. Results indicated, that for certain and steadier configurations, the robot’s estimated shape matched the ground truth.</p>\n<p>Next, the researchers aim to explore new sensor designs for improved sensitivity and to develop new models and deep-learning methods to reduce the required training for every new soft robot. They also hope to refine the system to better capture the robot’s full dynamic motions.</p>\n<p>Currently, the neural network and sensor skin are not sensitive to capture subtle motions or dynamic movements. But, for now, this is an important first step for learning-based approaches to soft robotic control, Truby says: “Like our soft robots, living systems don’t have to be totally precise. Humans are not precise machines, compared to our rigid robotic counterparts, and we do just fine.”</p>","descriptionType":"html","publishedDate":"Thu, 13 Feb 2020 04:59:59 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-1_0.gif","linkMd5":"33f0571d315efb232788fa019e2672ba","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn20@2020_1/2020/08/25/01-57-15-533_2bb181d23032cc59.webp","destWidth":500,"destHeight":281,"sourceBytes":4486017,"destBytes":2431682,"author":"Rob Matheson | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-1_0.gif":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn20@2020_1/2020/08/25/01-57-15-533_2bb181d23032cc59.webp","https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn47@2020_1/2020/08/25/01-57-28-167_9c526646fbe25ca5.webp","https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-3.gif":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn43@2020_6/2020/08/25/01-57-30-437_14ab6c28bb72b7f2.webp"},"publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Teaching artificial intelligence to connect senses like vision and touch","link":"https://news.mit.edu/2019/teaching-ai-to-connect-senses-vision-touch-0617","description":"<p>In Canadian author Margaret Atwood’s book \"Blind Assassins,<em>\" </em>she says that “touch comes before sight, before speech. It’s the first language and the last, and it always tells the truth.”</p>\n<p>While our sense of touch gives us a channel to feel the physical world, our eyes help us immediately understand the full picture of these tactile signals.</p>\n<p>Robots that have been programmed to see or feel can’t use these signals quite as interchangeably. To better bridge this sensory gap, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have come up with a predictive artificial intelligence (AI) that can learn to see by touching, and learn to feel by seeing.</p>\n<p>The team’s system can create realistic tactile signals from visual inputs, and predict which object and what part is being touched directly from those tactile inputs. They used a KUKA robot arm with a special tactile sensor called <a href=\"http://news.mit.edu/2011/tactile-imaging-gelsight-0809\">GelSight</a>, designed by another group at MIT.</p>\n<p>Using a simple web camera, the team recorded nearly 200 objects, such as tools, household products, fabrics, and more, being touched more than 12,000 times. Breaking those 12,000 video clips down into static frames, the team compiled “VisGel,” a dataset of more than 3 million visual/tactile-paired images.</p>\n<p>“By looking at the scene, our model can imagine the feeling of touching a flat surface or a sharp edge”, says Yunzhu Li, CSAIL PhD student and lead author on a new paper about the system. “By blindly touching around, our model can predict the interaction with the environment purely from tactile feelings. Bringing these two senses together could empower the robot and reduce the data we might need for tasks involving manipulating and grasping objects.”</p>\n<p>Recent work to equip robots with more human-like physical senses, such as MIT’s 2016 project using deep learning to <a href=\"http://andrewowens.com/vis/\" target=\"_blank\">visually indicate sounds,</a> or a model that <a href=\"http://news.mit.edu/2017/computer-systems-predict-objects-responses-physical-forces-1214\">predicts objects’ responses to physical forces</a>, both use large datasets that aren’t available for understanding interactions between vision and touch.</p>\n<p>The team’s technique gets around this by using the VisGel dataset, and something called generative adversarial networks (GANs).</p>\n<p>GANs use visual or tactile images to generate images in the other modality. They work by using a “generator” and a “discriminator” that compete with each other, where the generator aims to create real-looking images to fool the discriminator. Every time the discriminator “catches” the generator, it has to expose the internal reasoning for the decision, which allows the generator to repeatedly improve itself.</p>\n<p><strong>Vision to touch </strong></p>\n<p>Humans can infer how an object feels just by seeing it. To better give machines this power, the system first had to locate the position of the touch, and then deduce information about the shape and feel of the region.</p>\n<p>The reference images — without any robot-object interaction — helped the system encode details about the objects and the environment. Then, when the robot arm was operating, the model could simply compare the current frame with its reference image, and easily identify the location and scale of the touch.</p>\n<p>This might look something like feeding the system an image of a computer mouse, and then “seeing” the area where the model predicts the object should be touched for pickup — which could vastly help machines plan safer and more efficient actions.</p>\n<p><strong>Touch to vision</strong></p>\n<p>For touch to vision, the aim was for the model to produce a visual image based on tactile data. The model analyzed a tactile image, and then figured out the shape and material of the contact position. It then looked back to the reference image to “hallucinate” the interaction.</p>\n<p>For example, if during testing the model was fed tactile data on a shoe, it could produce an image of where that shoe was most likely to be touched.</p>\n<p>This type of ability could be helpful for accomplishing tasks in cases where there’s no visual data, like when a light is off, or if a person is blindly reaching into a box or unknown area.</p>\n<p><strong>Looking ahead </strong></p>\n<p>The current dataset only has examples of interactions in a controlled environment. The team hopes to improve this by collecting data in more unstructured areas, or by using a new MIT-designed <a href=\"http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529\">tactile glove</a>, to better increase the size and diversity of the dataset.</p>\n<p>There are still details that can be tricky to infer from switching modes, like telling the color of an object by just touching it, or telling how soft a sofa is without actually pressing on it. The researchers say this could be improved by creating more robust models for uncertainty, to expand the distribution of possible outcomes.</p>\n<p>In the future, this type of model could help with a more harmonious relationship between vision and robotics, especially for object recognition, grasping, better scene understanding, and helping with seamless human-robot integration in an assistive or manufacturing setting.</p>\n<p>“This is the first method that can convincingly translate between visual and touch signals”, says Andrew Owens, a postdoc at the University of California at Berkeley. “Methods like this have the potential to be very useful for robotics, where you need to answer questions like ‘is this object hard or soft?’, or ‘if I lift this mug by its handle, how good will my grip be?’ This is a very challenging problem, since the signals are so different, and this model has demonstrated great capability.”</p>\n<p>Li wrote the paper alongside MIT professors Russ Tedrake and Antonio Torralba, and MIT postdoc Jun-Yan Zhu. It will be presented next week at The Conference on Computer Vision and Pattern Recognition in Long Beach, California.</p>","descriptionType":"html","publishedDate":"Mon, 17 Jun 2019 04:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"e715fa76cea7d0d235c2c3fcd66dea9f","bgimgJsdelivr":"","metaImg":"","author":"Rachel Gordon | MIT CSAIL","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Tiny motor can “walk” to carry out tasks","link":"https://news.mit.edu/2019/micro-robots-walk-0702","description":"<p>Years ago, MIT Professor Neil Gershenfeld had an audacious thought. Struck by the fact that all the world’s living things are built out of combinations of just 20 amino acids, he wondered: Might it be possible to create a kit of just 20 fundamental parts that could be used to assemble all of the different technological products in the world?</p>\n<p>Gershenfeld and his students have been making steady progress in that direction ever since. Their latest achievement, presented this week at an international robotics conference, consists of a set of five tiny fundamental parts that can be assembled into a wide variety of functional devices, including a tiny “walking” motor that can move back and forth across a surface or turn the gears of a machine.<img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-1.gif\" style=\"width: 500px; height: 281px;\" /></p>\n<p>Previously, Gershenfeld and his students showed that structures assembled from many small, identical subunits can have <a href=\"http://news.mit.edu/2012/reconfigurable-robots-turn-into-anything-1130\">numerous mechanical properties</a>. Next, they demonstrated that a combination of rigid and flexible part types can be used to create <a href=\"http://news.mit.edu/2019/engineers-demonstrate-lighter-flexible-airplane-wing-0401\">morphing airplane wings</a>, a longstanding goal in aerospace engineering. Their latest work adds components for movement and logic, and will be presented at the International Conference on Manipulation, Automation and Robotics at Small Scales (MARSS) in Helsinki, Finland, in a paper by Gershenfeld and MIT graduate student Will Langford, which was selected as the conference's best student paper.</p>\n<p>Their work offers an alternative to today’s approaches to contructing robots, which largely fall into one of two types: custom machines that work well but are relatively expensive and inflexible, and reconfigurable ones that sacrifice performance for versatility. In the new approach, Langford came up with a set of five millimeter-scale components, all of which can be attached to each other by a standard connector. These parts include the previous rigid and flexible types, along with electromagnetic parts, a coil, and a magnet. In the future, the team plans to make these out of still smaller basic part types.</p>\n<p>Using this simple kit of tiny parts, Langford assembled them into a novel kind of motor that moves an appendage in discrete mechanical steps, which can be used to turn a gear wheel, and a mobile form of the motor that turns those steps into locomotion, allowing it to “walk” across a surface in a way that is reminiscent of the molecular motors that move muscles. These parts could also be assembled into hands for gripping, or legs for walking, as needed for a particular task, and then later reassembled as those needs change. Gershenfeld refers to them as “digital materials,” discrete parts that can be reversibly joined, forming a kind of functional micro-LEGO.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-2.gif\" /></p>\n<p>The new system is a significant step toward creating a standardized kit of parts that could be used to assemble robots with specific capabilities adapted to a particular task or set of tasks. Such purpose-built robots could then be disassembled and reassembled as needed in a variety of forms, without the need to design and manufacture new robots from scratch for each application.</p>\n<p>Langford's initial motor has an ant-like ability to lift seven times its own weight. But if greater forces are required, many of these parts can be added to provide more oomph. Or if the robot needs to move in more complex ways, these parts could be distributed throughout the structure. The size of the building blocks can be chosen to match their application; the team has made nanometer-sized parts to make nanorobots, and meter-sized parts to make megarobots. Previously, specialized techniques were needed at each of these length scale extremes.</p>\n<p>“One emerging application is to make tiny robots that can work in confined spaces,” Gershenfeld says. Some of the devices assembled in this project, for example, are smaller than a penny yet can carry out useful tasks.</p>\n<p>To build in the “brains,” Langford has added part types that contain millimeter-sized integrated circuits, along with a few other part types to take care of connecting electrical signals in three dimensions.</p>\n<p>The simplicity and regularity of these structures makes it relatively easy for their assembly to be automated. To do that, Langford has developed a novel machine that's like a cross between a 3-D printer and the pick-and-place machines that manufacture electronic circuits, but unlike either of those, this one can produce complete robotic systems directly from digital designs. Gershenfeld says this machine is a first step toward to the project's ultimate goal of “making an assembler that can assemble itself out of the parts that it's assembling.”</p> \n<p>“Standardization is an extremely important issue in microrobotics, to reduce the production costs and, as a result, to improve acceptance of this technology to the level of regular industrial robots,” says Sergej Fatikow, head of the Division of Microrobotics and&nbsp;Control Engineering, at the University of Oldenburg, Germany, who was not associated with this research. The new work “addresses assembling of sophisticated microrobotic systems from a small set of standard building blocks, which may revolutionize the field of microrobotics and open up numerous applications at small scales,” he says.</p> \n<p>“This work changes the way we think about the integration of structure, actuation, and control,” says George Small, chief technology officer for Moog, Inc, a leading maker of actuators for robotics, who also was not connected with this work. “It opens the door to new ways of designing, assembling, and reusing these active structures to provide innovative new solutions for our customers,” he says.</p>","descriptionType":"html","publishedDate":"Tue, 02 Jul 2019 14:58:10 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-1.gif","linkMd5":"6271779ddbd4c69fa045f9a8358a7655","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_6/2020/08/25/01-57-16-286_8440620f7df6c2f8.webp","destWidth":500,"destHeight":281,"sourceBytes":7294189,"destBytes":2829252,"author":"David L. Chandler | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_6/2020/08/25/01-57-16-286_8440620f7df6c2f8.webp","https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn59@2020_1/2020/08/25/01-57-27-990_2446456c2227d71e.webp"},"publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Autonomous system improves environmental sampling at sea","link":"https://news.mit.edu/2019/autonomous-system-sea-sampling-1104","description":"<p>An autonomous robotic system invented by researchers at MIT and the Woods Hole Oceanographic Institution (WHOI) efficiently sniffs out the most scientifically interesting — but hard-to-find —&nbsp;sampling spots in vast, unexplored waters.</p>\n<p>Environmental scientists are often interested in gathering samples at the most interesting locations, or “maxima,” in an environment. One example could be a source of leaking chemicals, where the concentration is the highest and mostly unspoiled by external factors. But a maximum can be any quantifiable value that researchers want to measure, such as water depth or parts of coral reef most exposed to air.</p>\n<p>Efforts to deploy maximum-seeking robots suffer from efficiency and accuracy issues. Commonly, robots will move back and forth like lawnmowers to cover an area, which is time-consuming and collects many uninteresting samples. Some robots sense and follow high-concentration trails to their leak source. But they can be misled. For example, chemicals can get trapped and accumulate in crevices far from a source. Robots may identify those high-concentration spots as the source yet be nowhere close.</p>\n<p>In a paper being presented at the International Conference on Intelligent Robots and Systems (IROS), the researchers describe “PLUMES,” a system that enables autonomous mobile robots to zero in on a maximum far faster and more efficiently. PLUMES leverages probabilistic techniques to predict which paths are likely to lead to the maximum, while navigating obstacles, shifting currents, and other variables. As it collects samples, it weighs what it’s learned to determine whether to continue down a promising path or search the unknown — which may harbor more valuable samples.</p>\n<p>Importantly, PLUMES reaches its destination without ever getting trapped in those tricky high-concentration spots. “That’s important, because it’s easy to think you’ve found gold, but really you’ve found fool’s gold,” says co-first author Victoria Preston, a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and in the MIT-WHOI Joint Program.</p>\n<p>The researchers built a PLUMES-powered robotic boat that successfully detected the most exposed coral head in the Bellairs Fringing Reef in Barbados —&nbsp;meaning, it was located in the shallowest spot —&nbsp;which is useful for studying how sun exposure impacts coral organisms. In 100 simulated trials in diverse underwater environments, a virtual PLUMES robot also consistently collected seven to eight times more samples of maxima than traditional coverage methods in allotted time frames.</p>\n<p>“PLUMES does the minimal amount of exploration necessary to find the maximum and then concentrates quickly on collecting valuable samples there,” says co-first author Genevieve Flaspohler, a PhD student and in CSAIL and the MIT-WHOI Joint Program.</p>\n<p>Joining Preston and Flaspohler on the paper are: Anna P.M. Michel and Yogesh Girdhar, both scientists in the Department of Applied Ocean Physics and Engineering at the WHOI; and Nicholas Roy, a professor in CSAIL and in the Department of Aeronautics and Astronautics. &nbsp;</p>\n<p><strong>Navigating an exploit-explore tradeoff</strong></p>\n<p>A key insight of PLUMES was using techniques from probability to reason about navigating the notoriously complex tradeoff between exploiting what’s learned about the environment and exploring unknown areas that may be more valuable.</p>\n<p>“The major challenge in maximum-seeking is allowing the robot to balance exploiting information from places it already knows to have high concentrations and exploring places it doesn’t know much about,” Flaspohler says. “If the robot explores too much, it won’t collect enough valuable samples at the maximum. If it doesn’t explore enough, it may miss the maximum entirely.”</p>\n<p>Dropped into a new environment, a PLUMES-powered robot uses a probabilistic statistical model called a Gaussian process to make predictions about environmental variables, such as chemical concentrations, and estimate sensing uncertainties. PLUMES then generates a distribution of possible paths the robot can take, and uses the estimated values and uncertainties to rank each path by how well it allows the robot to explore and exploit.</p>\n<p>At first, PLUMES will choose paths that randomly explore the environment. Each sample, however, provides new information about the targeted values in the surrounding environment — such as spots with highest concentrations of chemicals or shallowest depths. The Gaussian process model exploits that data to narrow down possible paths the robot can follow from its given position to sample from locations with even higher value. PLUMES uses a novel objective function —&nbsp;commonly used in machine-learning to maximize a reward — to make the call of whether the robot should exploit past knowledge or explore the new area.</p>\n<p><strong>“Hallucinating” paths</strong></p>\n<p>The decision where to collect the next sample relies on the system’s ability to “hallucinate” all possible future action from its current location. To do so, it leverages a modified version of Monte Carlo Tree Search (MCTS), a path-planning technique popularized for powering artificial-intelligence systems that master complex games, such as Go and Chess.</p>\n<p>MCTS uses a decision tree — a map of connected nodes and lines — to simulate a path, or sequence of moves, needed to reach a final winning action. But in games, the space for possible paths is finite. In unknown environments, with real-time changing dynamics, the space is effectively infinite, making planning extremely difficult. The researchers designed “continuous-observation MCTS,” which leverages the Gaussian process and the novel objective function to search over this unwieldy space of possible real paths.</p>\n<p>The root of this MCTS decision tree starts with a “belief” node, which is the next immediate step the robot can take. This node contains the entire history of the robot’s actions and observations up until that point. Then, the system expands the tree from the root into new lines and nodes, looking over several steps of future actions that lead to explored and unexplored areas.</p>\n<p>Then, the system simulates what would happen if it took a sample from each of those newly generated nodes, based on some patterns it has learned from previous observations. Depending on the value of the final simulated node, the entire path receives a reward score, with higher values equaling more promising actions. Reward scores from all paths are rolled back to the root node. The robot selects the highest-scoring path, takes a step, and collects a real sample. Then, it uses the real data to update its Gaussian process model and repeats the “hallucination” process.</p>\n<p>“As long as the system continues to hallucinate that there may be a higher value in unseen parts of the world, it must keep exploring,” Flaspohler says. “When it finally converges on a spot it estimates to be the maximum, because it can’t hallucinate a higher value along the path, it then stops exploring.”</p>\n<p>Now, the researchers are collaborating with scientists at WHOI to use PLUMES-powered robots to localize chemical plumes at volcanic sites and study methane releases in melting coastal estuaries in the Arctic. Scientists are interested in the source of chemical gases released into the atmosphere, but these test sites can span hundreds of square miles.</p>\n<p>“They can [use PLUMES to] spend less time exploring that huge area and really concentrate on collecting scientifically valuable samples,” Preston says.</p>","descriptionType":"html","publishedDate":"Mon, 04 Nov 2019 19:54:51 +0000","feedId":12364,"bgimg":"","linkMd5":"31a64989e2f0a49e07019a05100643a8","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Helping autonomous vehicles see around corners","link":"https://news.mit.edu/2019/helping-autonomous-vehicles-see-around-corners-1028","description":"<p>To improve the safety of autonomous systems, MIT engineers have developed a system that can sense tiny changes in shadows on the ground to determine if there’s a moving object coming around the corner. &nbsp;</p>\n<p>Autonomous cars could one day use the system to quickly avoid a potential collision with another car or pedestrian emerging from around a building’s corner or from in between parked cars. In the future, robots that may navigate hospital hallways to make medication or supply deliveries could use the system to avoid hitting people.</p>\n<p>In a paper being presented at next week’s International Conference on Intelligent Robots and Systems (IROS), the researchers describe successful experiments with an autonomous car driving around a parking garage and an autonomous wheelchair navigating hallways. When sensing and stopping for an approaching vehicle, the car-based system beats traditional LiDAR — which can only detect visible objects — by more than half a second.</p>\n<p>That may not seem like much, but fractions of a second matter when it comes to fast-moving autonomous vehicles, the researchers say.</p>\n<p>“For applications where robots are moving around environments with other moving objects or people, our method can give the robot an early warning that somebody is coming around the corner, so the vehicle can slow down, adapt its path, and prepare in advance to avoid a collision,” adds co-author Daniela Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “The big dream is to provide ‘X-ray vision’ of sorts to vehicles moving fast on the streets.”</p>\n<p>Currently, the system has only been tested in indoor settings. Robotic speeds are much lower indoors, and lighting conditions are more consistent, making it easier for the system to sense and analyze shadows.</p>\n<p>Joining Rus on the paper are: first author Felix Naser SM ’19, a former CSAIL researcher; Alexander Amini, a CSAIL graduate student; Igor Gilitschenski, a CSAIL postdoc; recent graduate Christina Liao ’19; Guy Rosman of the Toyota Research Institute; and Sertac Karaman, an associate professor of aeronautics and astronautics at MIT.</p>\n<p><strong>Extending ShadowCam</strong></p>\n<p>For their work, the researchers built on their system, called “ShadowCam,” that uses computer-vision techniques to detect and classify changes to shadows on the ground. MIT professors William Freeman and Antonio Torralba, who are not co-authors on the IROS paper, collaborated on the earlier versions of the system, which were presented at conferences in 2017 and 2018.</p>\n<p>For input, ShadowCam uses sequences of video frames from a camera targeting a specific area, such as the floor in front of a corner. It detects changes in light intensity over time, from image to image, that may indicate something moving away or coming closer. Some of those changes may be difficult to detect or invisible to the naked eye, and can be determined by various properties of the object and environment. ShadowCam computes that information and classifies each image as containing a stationary object or a dynamic, moving one. If it gets to a dynamic image, it reacts accordingly.</p>\n<p>Adapting ShadowCam for autonomous vehicles required a few advances. The early version, for instance, relied on lining an area with augmented reality labels called “AprilTags,” which resemble simplified QR codes. Robots scan AprilTags to detect and compute their precise 3D position and orientation relative to the tag. ShadowCam used the tags as features of the environment to zero in on specific patches of pixels that may contain shadows. But modifying real-world environments with AprilTags is not practical.</p>\n<p>The researchers developed a novel process that combines image registration and a new visual-odometry technique. Often used in computer vision, image registration essentially overlays multiple images to reveal variations in the images. Medical image registration, for instance, overlaps medical scans to compare and analyze anatomical differences.</p>\n<p>Visual odometry, used for Mars Rovers, estimates the motion of a camera in real-time by analyzing pose and geometry in sequences of images. The researchers specifically employ “Direct Sparse Odometry” (DSO), which can compute feature points in environments similar to those captured by AprilTags. Essentially, DSO plots features of an environment on a 3D point cloud, and then a computer-vision pipeline selects only the features located in a region of interest, such as the floor near a corner. (Regions of interest were annotated manually beforehand.)</p>\n<p>As ShadowCam takes input image sequences of a region of interest, it uses the DSO-image-registration method to overlay all the images from same viewpoint of the robot. Even as a robot is moving, it’s able to zero in on the exact same patch of pixels where a shadow is located to help it detect any subtle deviations between images.</p>\n<p>Next is signal amplification, a technique introduced in the first paper. Pixels that may contain shadows get a boost in color that reduces the signal-to-noise ratio. This makes extremely weak signals from shadow changes far more detectable. If the boosted signal reaches a certain threshold — based partly on how much it deviates from other nearby shadows —&nbsp;ShadowCam classifies the image as “dynamic.” Depending on the strength of that signal, the system may tell the robot to slow down or stop.</p>\n<p>“By detecting that signal, you can then be careful. It may be a shadow of some person running from behind the corner or a parked car, so the autonomous car can slow down or stop completely,” Naser says.</p>\n<p><strong>Tag-free testing</strong></p>\n<p>In one test, the researchers evaluated the system’s performance in classifying moving or stationary objects using AprilTags and the new DSO-based method. An autonomous wheelchair steered toward various hallway corners while humans turned the corner into the wheelchair’s path. Both methods achieved the same 70-percent classification accuracy, indicating AprilTags are no longer needed.</p>\n<p>In a separate test, the researchers implemented ShadowCam in an autonomous car in a parking garage, where the headlights were turned off, mimicking nighttime driving conditions. They compared car-detection times versus LiDAR. In an example scenario, ShadowCam detected the car turning around pillars about 0.72 seconds faster than LiDAR. Moreover, because the researchers had tuned ShadowCam specifically to the garage’s lighting conditions, the system achieved a classification accuracy of around 86 percent.</p>\n<p>Next, the researchers are developing the system further to work in different indoor and outdoor lighting conditions. In the future, there could also be ways to speed up the system’s shadow detection and automate the process of annotating targeted areas for shadow sensing.</p>\n<p>This work was funded by the Toyota Research Institute.</p>","descriptionType":"html","publishedDate":"Mon, 28 Oct 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"d1ee387a090ce54a2dc3ba7a031a1c22","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Technique helps robots find the front door ","link":"https://news.mit.edu/2019/technique-helps-robots-find-front-door-1104","description":"<p>In the not too distant future, robots may be dispatched as last-mile delivery vehicles to drop your takeout order, package, or meal-kit subscription at your doorstep — if they can find the door.</p>\n<p>Standard approaches for robotic navigation involve mapping an area ahead of time, then using algorithms to guide a robot toward a specific goal or GPS coordinate on the map. While this approach might make sense for exploring specific environments, such as the layout of a particular building or planned obstacle course, it can become unwieldy in the context of last-mile delivery.</p>\n<p>Imagine, for instance, having to map in advance every single neighborhood within a robot’s delivery zone, including the configuration of each house within that neighborhood along with the specific coordinates of each house’s front door. Such a task can be difficult to scale to an entire city, particularly as the exteriors of houses often change with the seasons. Mapping every single house could also run into issues of security and privacy.</p>\n<p>Now MIT engineers have developed a navigation method that doesn’t require mapping an area in advance. Instead, their approach enables a robot to use clues in its environment to plan out a route to its destination, which can be described in general semantic terms, such as “front door” or “garage,” rather than as coordinates on a map. For example, if a robot is instructed to deliver a package to someone's front door, it might start on the road and see a driveway, which it has been trained to recognize as likely to lead toward a sidewalk, which in turn is likely to lead to the front door.</p> \n<p></p> \n<p></p> \n<p>The new technique can greatly reduce the time a robot spends exploring a property before identifying its target, and it doesn’t rely on maps of specific residences.&nbsp;</p>\n<p>“We wouldn’t want to have to make a map of every building that we’d need to visit,” says Michael Everett, a graduate student in MIT’s Department of Mechanical Engineering. “With this technique, we hope to drop a robot at the end of any driveway and have it find a door.”</p>\n<p>Everett will present the group’s results this week at the International Conference on Intelligent Robots and Systems. The paper, which is co-authored by Jonathan How, professor of aeronautics and astronautics at MIT, and Justin Miller of the Ford Motor Company, is a finalist for “Best Paper for Cognitive Robots.”</p>\n<p><strong>“A sense of what things are”</strong></p>\n<p>In recent years, researchers have worked on introducing natural, semantic language to robotic systems, training robots to recognize objects by their semantic labels, so they can visually process a door as a door, for example, and not simply as a solid, rectangular obstacle.</p>\n<p>“Now we have an ability to give robots a sense of what things are, in real-time,” Everett says.</p>\n<p>Everett, How, and Miller are using similar semantic techniques as a springboard for their new navigation approach, which leverages pre-existing algorithms that extract features from visual data to generate a new map of the same scene, represented as semantic clues, or context.</p>\n<p>In their case, the researchers used an algorithm to build up a map of the environment as the robot moved around, using the semantic labels of each object and a depth image. This algorithm is called semantic SLAM (Simultaneous Localization and Mapping).</p>\n<p>While other semantic algorithms have enabled robots to recognize and map objects in their environment for what they are, they haven’t allowed a robot to make decisions in the moment while navigating a new environment, on the most efficient path to take to a semantic destination such as a “front door.”</p>\n<p>“Before, exploring was just, plop a robot down and say ‘go,’ and it will move around and eventually get there, but it will be slow,” How says.</p>\n<p><strong>The cost to go</strong></p>\n<p>The researchers looked to speed up a robot’s path-planning through a semantic, context-colored world. They developed a new “cost-to-go estimator,” an algorithm that converts a semantic map created by preexisting SLAM algorithms into a second map, representing the likelihood of any given location being close to the goal.</p>\n<p>“This was inspired by image-to-image translation, where you take a picture of a cat and make it look like a dog,” Everett says. “The same type of idea happens here where you take one image that looks like a map of the world, and turn it into this other image that looks like the map of the world but now is colored based on how close different points of the map are to the end goal.”</p>\n<p>This cost-to-go map is colorized, in gray-scale, to represent darker regions as locations far from a goal, and lighter regions as areas that are close to the goal. For instance, the sidewalk, coded in yellow in a semantic map, might be translated by the cost-to-go algorithm as a darker region in the new map, compared with a driveway, which is progressively lighter as it approaches the front door — the lightest region in the new map.</p>\n<p>The researchers trained this new algorithm on satellite images from Bing Maps containing 77 houses from one urban and three suburban neighborhoods. The system converted a semantic map into a cost-to-go map, and mapped out the most efficient path, following lighter regions in the map, to the end goal. For each satellite image, Everett assigned semantic labels and colors to context features in a typical front yard, such as grey for a front door, blue for a driveway, and green for a hedge.</p>\n<p>During this training process, the team also applied masks to each image to mimic the partial view that a robot’s camera would likely have as it traverses a yard.</p>\n<p>“Part of the trick to our approach was [giving the system] lots of partial images,” How explains. “So it really had to figure out how all this stuff was interrelated. That’s part of what makes this work robustly.”</p>\n<p>The researchers then tested their approach in a simulation of an image of an entirely new house, outside of the training dataset, first using the preexisting SLAM algorithm to generate a semantic map, then applying their new cost-to-go estimator to generate a second map, and path to a goal, in this case, the front door.</p>\n<p>The group’s new cost-to-go technique found the front door 189 percent faster than classical navigation algorithms, which do not take context or semantics into account, and instead spend excessive steps exploring areas that are unlikely to be near their goal.</p>\n<p>Everett says the results illustrate how robots can use context to efficiently locate a goal, even in unfamiliar, unmapped environments.</p>\n<p>“Even if a robot is delivering a package to an environment it’s never been to, there might be clues that will be the same as other places it’s seen,” Everett says. “So the world may be laid out a little differently, but there’s probably some things in common.”</p>\n<p>This research is supported, in part, by the Ford Motor Company.</p>","descriptionType":"html","publishedDate":"Mon, 04 Nov 2019 05:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"88c42a4b03ae6a82284a960ebab525f7","bgimgJsdelivr":"","metaImg":"","author":"Jennifer Chu | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Engineers design bionic “heart” for testing prosthetic valves, other cardiac devices","link":"https://news.mit.edu/2020/bionic-heart-prosthetic-valve-cardiac-0129","description":"<p>As the geriatric population is expected to balloon in the coming decade, so too will rates of heart disease in the United States. The demand for prosthetic heart valves and other cardiac devices — a market that is valued at more than $5 billion dollars today — is predicted to rise by almost 13 percent in the next six years.</p>\n<p>Prosthetic valves are designed to mimic a real, healthy heart valve in helping to circulate blood through the body. However, many of them have issues such as leakage around the valve, and engineers working to improve these designs must test them repeatedly, first in simple benchtop simulators, then in animal subjects, before reaching human trials — an arduous and expensive process.</p>\n<p>Now engineers at MIT and elsewhere have developed a bionic “heart” that offers a more realistic model for testing out artificial valves and other cardiac devices.</p>\n<p>The device is a real biological heart whose tough muscle tissue has been replaced with a soft robotic matrix of artificial heart muscles, resembling bubble wrap. The orientation of the artificial muscles mimics the pattern of the heart’s natural muscle fibers, in such a way that when the researchers remotely inflate the bubbles, they act together to squeeze and twist the inner heart, similar to the way a real, whole heart beats and pumps blood.</p>\n<p>With this new design, which they call a “biorobotic hybrid heart,” the researchers envision that device designers and engineers could iterate and fine-tune designs more quickly by testing on the biohybrid heart, significantly reducing the cost of cardiac device development.</p>\n<p>“Regulatory testing of cardiac devices requires many fatigue tests and animal tests,” says Ellen Roche, assistant professor of mechanical engineering at MIT. “[The new device] could realistically represent what happens in a real heart, to reduce the amount of animal testing or iterate the design more quickly.”</p>\n<p>Roche and her colleagues have published their results today in the journal <em>Science Robotics.</em> Her co-authors are lead author and MIT graduate student Clara Park, along with Yiling Fan, Gregor Hager, Hyunwoo Yuk, Manisha Singh, Allison Rojas, and Xuanhe Zhao at MIT, along with collaborators from Nanyang Technology University, the Royal College of Surgeons in Dublin, Boston’s Children’s Hospital, Harvard Medical School, and Massachusetts General Hospital.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-1.gif\" style=\"width: 500px; height: 615px;\" /></p> \n<p><em><span style=\"font-size:10px;\">The structure of the biorobotic hybrid heart under magnetic resonance imaging. Credit: Christopher T. Nguyen</span></em></p>\n<p><strong>“Mechanics of the heart”</strong></p>\n<p>Before coming to MIT, Roche worked briefly in the biomedical industry, helping to test cardiac devices on artificial heart models in the lab.</p>\n<p>“At the time I didn’t feel any of these benchtop setups were representative of both the anatomy and the physiological biomechanics of the heart,” Roche recalls. “There was an unmet need in terms of device testing.”</p>\n<p>In separate research as part of her doctoral work at Harvard University, she developed a soft, robotic, implantable sleeve, designed to wrap around a whole, live heart, to help it pump blood in patients suffering from heart failure.</p>\n<p>At MIT, she and Park wondered if they could combine the two research avenues, to develop a hybrid heart: a heart that is made partly of chemically preserved, explanted heart tissue and partly of soft artificial actuators that help the heart pump blood. Such a model, they proposed, should be a more realistic and durable environment in which to test cardiac devices, compared with models that are either entirely artificial but do not capture the heart’s complex anatomy, or are made from a real explanted heart, requiring highly controlled conditions to keep the tissue alive.</p>\n<p>The team briefly considered wrapping a whole, explanted heart in a soft robotic sleeve, similar to Roche’s previous work, but realized the heart’s outer muscle tissue, the myocardium, quickly stiffened when removed from the body. Any robotic contraction by the sleeve would fail to translate sufficiently to the heart within.</p>\n<p>Instead, the team looked for ways to design a soft robotic matrix to replace the heart’s natural muscle tissue, in both material and function. They decided to try out their idea first on the heart’s left ventricle, one of four chambers in the heart, which pumps blood to the rest of the body, while the right ventricle uses less force to pump blood to the lungs.</p>\n<p>“The left ventricle is the harder one to recreate given its higher operating pressures, and we like to start with the hard challenges,” Roche says.</p>\n<p><strong>The heart, unfurled</strong></p>\n<p>The heart normally pumps blood by squeezing and twisting, a complex combination of motions that is a result of the alignment of muscle fibers along the outer myocardium that covers each of the heart’s ventricles. The team planned to fabricate a matrix of artificial muscles resembling inflatable bubbles, aligned in the orientations of the natural cardiac muscle. But copying these patterns by studying a ventricle’s three-dimensional geometry proved extremely challenging.</p>\n<p>They eventually came across the helical ventricular myocardial band theory, the idea that cardiac muscle is essentially a large helical band that wraps around each of the heart’s ventricles. This theory is still a subject of debate by some researchers, but Roche and her colleagues took it as inspiration for their design. Instead of trying to copy the left ventricle’s muscle fiber orientation from a 3D perspective, the team decided to remove the ventricle’s outer muscle tissue and unwrap it to form a long, flat band — a geometry that should be far easier to recreate. In this case, they used the cardiac tissue from an explanted pig heart.</p>\n<p>In collaboration with co-lead author Chris Nguyen at MGH, the researchers used diffusion tensor imaging, an advanced technique that typically tracks how water flows through white matter in the brain, to map the microscopic fiber orientations of a left ventricle’s unfurled, two-dimensional muscle band. They then fabricated a matrix of artificial muscle fibers made from thin air tubes, each connected to a series of inflatable pockets, or bubbles, the orientation of which they patterned after the imaged muscle fibers.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-2.gif\" style=\"width: 500px; height: 380px;\" /></p> \n<p><em><span style=\"font-size:10px;\">Motion of the biorobotic hybrid heart mimics the pumping motion of the heart under echocardiography. Credit: Mossab Saeed</span></em></p>\n<p>The soft matrix consists of two layers of silicone, with a water-soluble layer between them to prevent the layers from sticking, as well as two layers of laser-cut paper, which ensures that the bubbles inflate in a specific orientation.</p>\n<p>The researchers also developed a new type of bioadhesive to glue the bubble wrap to the ventricle’s real, intracardiac tissue. While adhesives exist for bonding biological tissues to each other, and and for materials like silicone to each other, the team &nbsp;realized few soft adhesives do an adequate job of gluing together biological tissue with synthetic materials, silicone in particular.</p>\n<p>So Roche collaborated with Zhao, associate professor of mechanical engineering at MIT, who specializes in developing hydrogel-based adhesives. The new adhesive, named TissueSil, was made by functionalizing silicone in a chemical cross-linking process, to bond with components in heart tissue. The result was a viscous liquid that the researchers brushed onto the soft robotic matrix. They also brushed the glue onto a new explanted pig heart that had its left ventricle removed but its endocardial structures preserved. When they wrapped the artificial muscle matrix around this tissue, the two bonded tightly.</p>\n<p>Finally, the researchers placed the entire hybrid heart in a mold that they had previously cast of the original, whole heart, and filled the mold with silicone to encase the hybrid heart in a uniform covering — a step that produced a form similar to a real heart and ensured that the robotic bubble wrap fit snugly around the real ventricle.</p>\n<p>“That way, you don’t lose transmission of motion from the synthetic muscle to the biological tissue,” Roche says.</p>\n<p>When the researchers pumped air into the bubble wrap at frequencies resembling a naturally beating heart, and imaged the bionic heart’s response, it contracted in a manner similar to the way a real heart moves to pump blood through the body.</p>\n<p>Ultimately, the researchers hope to use the bionic heart as a realistic environment to help designers test cardiac devices such as prosthetic heart valves.</p>\n<p>“Imagine that a patient before cardiac device implantation could have their heart scanned, and then clinicians could tune the device to perform optimally in the patient well before the surgery,” says Nyugen. “Also, with further tissue engineering, we could potentially see the biorobotic hybrid heart be used as an artificial heart — a very needed potential solution given the global heart failure epidemic where millions of people are at the mercy of a competitive heart transplant list.”</p>\n<p>This research was supported in part by the National Science Foundation.</p>","descriptionType":"html","publishedDate":"Wed, 29 Jan 2020 19:00:00 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-1.gif","linkMd5":"a453ff748563a20e6aca74e4a66dfbc3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn7@2020_6/2020/08/25/01-57-16-045_272b9257fbba6026.webp","destWidth":500,"destHeight":615,"sourceBytes":7636396,"destBytes":1583556,"author":"Jennifer Chu | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn7@2020_6/2020/08/25/01-57-16-045_272b9257fbba6026.webp","https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn67@2020_5/2020/08/25/01-57-29-303_03dc07760ab19b8d.webp"},"publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"System prevents speedy drones from crashing in unfamiliar areas","link":"https://news.mit.edu/2019/system-prevents-speedy-drones-crashing-unfamiliar-areas-1025","description":"<p>Autonomous drones are cautious when navigating the unknown. They creep forward, frequently mapping unfamiliar areas before proceeding lest they crash into undetected objects. But this slowdown isn’t ideal for drones carrying out time-sensitive tasks, such as flying search-and-rescue missions through dense forests. &nbsp;</p>\n<p>Now MIT researchers have developed a trajectory-planning model that helps drones fly at high speeds through previously unexplored areas, while staying safe.</p>\n<p>The model — aptly named “FASTER” — estimates the quickest possible path from a starting point to a destination point across all areas the drone can and can’t see, with no regard for safety. But, as the drone flies, the model continuously logs collision-free “back-up” paths that slightly deviate from that fast flight path. When the drone is unsure about a particular area, it detours down the back-up path and replans its path. The drone can thus cruise at high speeds along the quickest trajectory while occasionally slowing down slightly to ensure safety.</p>\n<p>“We always want to execute the fastest path, but we don’t always know it’s safe. If, as we move along this fastest path, we discover there’s a problem, we need to have a backup plan,” says Jesus Tordesillas, a graduate student in the Department of Aeronautics and Astronautics (AeroAstro) and first author on a paper describing the model being presented at next month’s International Conference on Intelligent Robots and Systems. “We obtain a higher velocity trajectory that may not be safe and a slow-velocity trajectory that’s completely safe. The two paths are stitched together at first, but then one deviates for performance and the other for safety.”</p>\n<p>In forest simulations, where a virtual drone navigates around cylinders representing trees, FASTER-powered drones safely completed flight paths about two times quicker than traditional models. In real-life tests, FASTER-powered drones maneuvering around cardboard boxes in a large room achieved speeds of 7.8 meters per second. That’s pushing limits for how fast the drones can fly, based on weight and reaction times, the researchers say.</p>\n<p>“That’s about as fast as you can go,” says co-author Jonathan How, the Richard Cockburn Maclaurin Professor of Aeronautics and Astronautics. “If you were standing in a room with a drone flying 7 to 8 meters per second in it, you’d probably take a step back.\"</p>\n<p>The paper’s other co-author is Brett T. Lopez, a former PhD student in AeroAstro and now a postdoc at NASA’s Jet Propulsion Laboratory.</p>\n<p><strong>Splitting paths </strong></p>\n<p>Drones use cameras to capture environment as voxels, 3D cubes generated from depth information. As the drone flies, each detected voxel gets labeled as “free-known space,” unoccupied by objects, and “occupied-known space,” which contains objects. The rest of the environment is “unknown space.”&nbsp;</p>\n<p>FASTER utilizes all of those areas to plan three types of trajectories — “whole,” “safe,” and “committed.” The whole trajectory is the entire path from starting point A to goal location B, through known and unknown areas. To do so, “convex decomposition,” a technique that breaks down complex models into discrete components, generates overlapping polyhedrons that model those three areas in an environment. Using some geometric techniques and mathematical constraints, the model uses these polyhedrons to compute an optimal whole trajectory.</p>\n<p>Simultaneously, the model plans a safe trajectory. Somewhere along the whole trajectory, it plots a “rescue” point that indicates the last moment a drone can detour to unobstructed free-known space, based on its speed and other factors. To find a safe destination, it computes new polyhedrons that cover the free-known space. Then, it locates a spot inside these new polyhedrons. Basically, the drone stops in a spot that’s safe but as close as possible to unknown space, enabling a very quick and efficient detour.</p>\n<p><strong>Committed trajectory</strong></p>\n<p>The committed trajectory consists of the first interval of the whole trajectory, as well as the entire safe trajectory. But this first interval is independent of the safe trajectory, and therefore it is not affected by the braking needed for the safe trajectory.</p>\n<p>The drone computes one whole trajectory at a time, while always keeping track of the safe trajectory. But it’s given a time limit: When it reaches the rescue point, it must have successfully computed the next whole trajectory through known or unknown space. If it does, it will continue following the whole trajectory. Otherwise, it diverts to the safe trajectory. This approach enables the drone to maintain high velocities along the committed trajectories, which is key to achieving high overall speeds.</p>\n<p>For this to all work, the researchers designed ways for the drones to process all the planning data very quickly, which was challenging. Because the maps are so varied, for instance, the time limit given to each committed trajectory initially varied dramatically. That was computationally expensive and slowed down the drone’s planning, so the researchers developed a method to quickly compute fixed times for all the intervals along the trajectories, which simplified computations. The researchers also designed methods to reduce how many polyhedrons the drone must process to map its surroundings. Both of those methods dramatically increased planning times.</p> \n<p>\"How to increase the flight speed and maintain safety is one of the hardest problems for drone’s motion planning,” says Sikang Liu, a software engineer at Waymo, formerly Google’s self-driving car project, and an expert in trajectory-planning algorithms. “This work showed a great solution to this problem by enhancing the existing trajectory generation framework. In the trajectory optimization pipeline, the time allocation is always a tricky problem that could lead to convergence issue and undesired behavior. This paper addressed this problem through a novel approach … which could be an insightful contribution to this field.\"</p>\n<p>The researchers are currently building larger FASTER-powered drones with propellers designed to enable steady horizontal flight. Traditionally, drones will need to roll and pitch as they’re flying. But this custom drone would stay completely flat for various applications.</p>\n<p>A potential application for FASTER, which has been developed with support by U.S. Department of Defense,&nbsp;could be improving search-and-rescue missions in forest environments, which present many planning and navigational challenges for autonomous drones. “But the unknown area doesn’t have to be forest,” How says. “It could be any area where you don’t know what’s coming, and it matters how quickly you acquire that knowledge. The main motivation is building more agile drones.”</p>","descriptionType":"html","publishedDate":"Fri, 25 Oct 2019 13:21:43 +0000","feedId":12364,"bgimg":"","linkMd5":"5aaab6a954f655d14e7d6195385c0f09","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Autonomous boats can target and latch onto each other","link":"https://news.mit.edu/2019/autonomous-robot-boats-latch-0605","description":"<p>The city of Amsterdam envisions a future where fleets of autonomous boats cruise its many canals to transport goods and people, collect trash, or self-assemble into floating stages and bridges. To further that vision, MIT researchers have given new capabilities to their fleet of robotic boats — which are being developed as part of an ongoing project — that lets them target and clasp onto each other, and keep trying if they fail.</p>\n<p>About a quarter of Amsterdam’s surface area is water, with 165 canals winding alongside busy city streets. Several years ago, MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute) teamed up on the “<a href=\"http://senseable.mit.edu/roboat/\">Roboat</a>” project. The idea is to build a fleet of autonomous robotic boats — rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware — that provides intelligent mobility on water to relieve congestion in the city’s busy streets.</p>\n<p>One of project’s objectives is to create roboat units that provide on-demand transporation on waterways. Another objective is using the roboat units to automatically form “pop-up” structures, such as foot bridges, performance stages, or even food markets. The structures could then automatically disassemble at set times and reform into target structures for different activities. Additionally, the roboat units could be used as agile sensors to gather data on the city’s infrastructure, and air and water quality, among other things.</p>\n<p>In 2016, MIT researchers&nbsp;<a href=\"http://news.mit.edu/2016/autonomous-fleet-amsterdam-roboat-0919\">tested</a> a roboat prototype that cruised around Amsterdam’s canals, moving forward, backward, and laterally along a preprogrammed path. Last year, researchers <a href=\"http://news.mit.edu/2018/fleet-autonomous-boats-service-cities-reducing-road-traffic-0523\">designed</a> low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms.&nbsp;</p>\n<p>In a paper presented at the International Conference on Robotics and Automation, the researchers describe roboat units that can now identify and connect to docking stations. Control algorithms guide the roboats to the target, where they automatically connect to a customized latching mechanism with millimeter precision. Moreover, the roboat notices if it has missed the connection, backs up, and tries again.</p>\n<p>The researchers tested the latching technique in a swimming pool at MIT and in the Charles River, where waters are rougher. In both instances, the roboat units were usually able to successfully connect in about 10 seconds, starting from around 1 meter away, or they succeeded after a few failed attempts. In Amsterdam, the system could be especially useful for overnight garbage collection. Roboat units could sail around a canal, locate and latch onto platforms holding trash containers, and haul them back to collection facilities.</p>\n<p>“In Amsterdam, canals were once used for transportation and other things the roads are now used for. Roads near canals are now very congested —&nbsp;and have noise and pollution —&nbsp;so the city wants to add more functionality back to the canals,” says first author Luis Mateos, a graduate student in the Department of Urban Studies and Planning (DUSP) and a researcher in the MIT Senseable City Lab. “Self-driving technologies can save time, costs and energy, and improve the city moving forward.”</p>\n<p>“The aim is to use roboat units to bring new capabilities to life on the water,” adds co-author Daniela Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “The new latching mechanism is very important for creating pop-up structures. Roboat does not need latching for autonomous transporation on water, but you need the latching to create any structure, whether it’s mobile or fixed.”</p>\n<p>Joining Mateos on the paper are: Wei Wang, a joint postdoc in CSAIL and the Senseable City Lab; Banti Gheneti, a graduate student in the Department of Electrical Engineering and Computer Science; Fabio Duarte, a DUSP and Senseable City Lab research scientist; and Carlo Ratti, director of the Senseable City Lab and a principal investigator and professor of the practice in DUSP.</p>\n<p><strong>Making the connection</strong></p>\n<p>Each roboat is equipped with latching mechanisms, including ball and socket components, on its front, back, and sides. The ball component resembles a badminton shuttlecock —&nbsp;a cone-shaped, rubber body with a metal ball at the end. The socket component is a wide funnel that guides the ball component into a receptor. Inside the funnel, a laser beam acts like a security system that detects when the ball crosses into the receptor. That activates a mechanism with three arms that closes around and captures the ball, while also sending a feedback signal to both roboats that the connection is complete.</p>\n<p>On the software side, the roboats run on custom computer vision and control techniques. Each roboat has a LIDAR system and camera, so they can autonomously move from point to point around the canals. Each docking station — typically an unmoving roboat —&nbsp;has a sheet of paper imprinted with an augmented reality tag, called an AprilTag, which resembles a simplified QR code. Commonly used for robotic applications, AprilTags enable robots to detect and compute their precise 3-D position and orientation relative to the tag.</p>\n<p>Both the AprilTags and cameras are located in the same locations in center of the roboats. When a traveling roboat is roughly one or two meters away from the stationary AprilTag, the roboat calculates its position and orientation to the tag. Typically, this would generate a 3-D map for boat motion, including roll, pitch, and yaw (left and right). But an algorithm strips away everything except yaw. This produces an easy-to-compute 2-D plane that measures the roboat camera’s distance away and distance left and right of the tag. Using that information, the roboat steers itself toward the tag. By keeping the camera and tag perfectly aligned, the roboat is able to precisely connect.</p>\n<p>The funnel compensates for any misalignment in the roboat’s pitch (rocking up and down) and heave (vertical up and down), as canal waves are relatively small. If, however, the roboat goes beyond its calculated distance, and doesn’t receive a feedback signal from the laser beam, it knows it has missed. “In challenging waters, sometimes roboat units at the current one-quarter scale, are not strong enough to overcome wind gusts or heavy water currents,” Mateos says. “A logic component on the roboat says, ‘You missed, so back up, recalculate your position, and try again.’”</p>\n<p><strong>Future iterations</strong></p>\n<p>The researchers are now designing roboat units roughly four times the size of the current iterations, so they’ll be more stable on water. Mateos is also working on an update to the funnel that includes tentacle-like rubber grippers that tighten around the pin — like a squid grasping its prey. That could help give the roboat units more control when, say, they’re towing platforms or other roboats through narrow canals.</p>\n<p>In the works is also a system that displays the AprilTags on an LCD monitor that changes codes to signal multiple roboat units to assemble in a given order. At first, all roboat units will be given a code to stay exactly a meter apart. Then, the code changes to direct the first roboat to latch. After, the screen switches codes to order the next roboat to latch, and so on. “It’s like the telephone game. The changing code passes a message to one roboat at a time, and that message tells them what to do,” Mateos says.</p>\n<p>Darwin Caldwell, the research director of Advanced Robotics at the Italian Institute of Technology, envisions even more possible applications for the autonomous latching capability. “I can certainly see this type of autonomous docking being of use in many areas of robotic ‘refuelling’ and docking … beyond aquatic/naval systems,” he says, “including inflight refuelling, space docking, cargo container handling, [and] robot in-house recharging.”</p>\n<p>The research was funded by the AMS Institute and the City of Amsterdam.</p>","descriptionType":"html","publishedDate":"Wed, 05 Jun 2019 04:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"32bfa02a8b593a4ad36d571afa566907","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Flexible yet sturdy robot is designed to “grow” like a plant","link":"https://news.mit.edu/2019/robot-grow-like-plant-1107","description":"<p>In today’s factories and warehouses, it’s not uncommon to see robots whizzing about, shuttling items or tools from one station to another. For the most part, robots navigate pretty easily across open layouts. But they have a much harder time winding through narrow spaces to carry out tasks such as reaching for a product at the back of a cluttered shelf, or snaking around a car’s engine parts to unscrew an oil cap.</p>\n<p>Now MIT engineers have developed a robot designed to extend a chain-like appendage flexible enough to twist and turn in any necessary configuration, yet rigid enough to support heavy loads or apply torque to assemble parts in tight spaces. When the task is complete, the robot can retract the appendage and extend it again, at a different length and shape, to suit the next task.</p>\n<p>The appendage design is inspired by the way plants grow, which involves the transport of nutrients, in a fluidized form, up to the plant’s tip. There, they are converted into solid material to produce, bit by bit, a supportive stem.</p>\n<p>Likewise, the robot consists of a “growing point,” or gearbox, that pulls a loose chain of interlocking blocks into the box. Gears in the box then lock the chain units together and feed the chain out, unit by unit, as a rigid appendage.</p>\n<p>The researchers presented the plant-inspired “growing robot” this week at the IEEE International Conference on Intelligent Robots and Systems (IROS) in Macau. They envision that grippers, cameras, and other sensors could be mounted onto the robot’s gearbox, enabling it to meander through an aircraft’s propulsion system and tighten a loose screw, or to reach into a shelf and grab a product without disturbing the organization of surrounding inventory, among other tasks.</p>\n<p>“Think about changing the oil in your car,” says Harry Asada, professor of mechanical engineering at MIT. “After you open the engine roof, you have to be flexible enough to make sharp turns, left and right, to get to the oil filter, and then you have to be strong enough to twist the oil filter cap to remove it.”</p>\n<p>“Now we have a robot that can potentially accomplish such tasks,” says Tongxi Yan, a former graduate student in Asada’s lab, who led the work. “It can grow, retract, and grow again to a different shape, to adapt to its environment.”</p>\n<p>The team also includes MIT graduate student Emily Kamienski and visiting scholar Seiichi Teshigawara, who presented the results at the conference.</p> \n<p></p> \n<p></p> \n<p><strong>The last foot</strong></p>\n<p>The design of the new robot is an offshoot of Asada’s work in addressing the “last one-foot problem” — an engineering term referring to the last step, or foot, of a robot’s task or exploratory mission. While a robot may spend most of its time traversing open space, the last foot of its mission may involve more nimble navigation through tighter, more complex spaces to complete a task.</p>\n<p>Engineers have devised various concepts and prototypes to address the last one-foot problem, including robots made from soft, balloon-like materials that grow like vines to squeeze through narrow crevices. But Asada says such soft extendable robots aren’t sturdy enough to support “end effectors,” or add-ons such as grippers, cameras, and other sensors that would be necessary in carrying out a task, once the robot has wormed its way to its destination.</p>\n<p>“Our solution is not actually soft, but a clever use of rigid materials,” says Asada, who is the Ford Foundation Professor of Engineering.</p>\n<p><strong>Chain links</strong></p>\n<p>Once the team defined the general functional elements of plant growth, they looked to mimic this in a general sense, in an extendable robot.</p>\n<p>“The realization of the robot is totally different from a real plant, but it exhibits the same kind of functionality, at a certain abstract level,” Asada says.</p>\n<p>The researchers designed a gearbox to represent the robot’s “growing tip,” akin to the bud of a plant, where, as more nutrients flow up to the site, the tip feeds out more rigid stem. Within the box, they fit a system of gears and motors, which works to pull up a fluidized material — in this case, a bendy sequence of 3-D-printed plastic units interlocked with each other, similar to a bicycle chain.</p>\n<p>As the chain is fed into the box, it turns around a winch, which feeds it through a second set of motors programmed to lock certain units in the chain to their neighboring units, creating a rigid appendage as it is fed out of the box.</p>\n<p>The researchers can program the robot to lock certain units together while leaving others unlocked, to form specific shapes, or to “grow” in certain directions. In experiments, they were able to program the robot to turn around an obstacle as it extended or grew out from its base.</p>\n<p>“It can be locked in different places to be curved in different ways, and have a wide range of motions,” Yan says.</p>\n<p>When the chain is locked and rigid, it is strong enough to support a heavy, one-pound weight. If a gripper were attached to the robot’s growing tip, or gearbox, the researchers say the robot could potentially grow long enough to meander through a narrow space, then apply enough torque to loosen a bolt or unscrew a cap.</p>\n<p>Auto maintenance is a good example of tasks the robot could assist with, according to Kamienski. “The space under the hood is relatively open, but it’s that last bit where you have to navigate around an engine block or something to get to the oil filter, that a fixed arm wouldn’t be able to navigate around. This robot could do something like that.”</p>\n<p>This research was funded, in part, by NSK Ltd.</p>","descriptionType":"html","publishedDate":"Thu, 07 Nov 2019 15:28:28 +0000","feedId":12364,"bgimg":"","linkMd5":"1cb84668e3dda18023d8df8d729023d1","bgimgJsdelivr":"","metaImg":"","author":"Jennifer Chu | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Assembler robots make large structures from little pieces","link":"https://news.mit.edu/2019/robots-large-structures-little-pieces-1016","description":"<p>Today’s commercial aircraft are typically manufactured in sections, often in different locations — wings at one factory, fuselage sections at another, tail components somewhere else — and then flown to a central plant in huge cargo planes for final assembly.</p>\n<p>But what if the final assembly was the only assembly, with the whole plane built out of a large array of tiny identical pieces, all put together by an army of tiny robots?</p>\n<p>That’s the vision that graduate student Benjamin Jenett, working with Professor Neil Gershenfeld in MIT’s Center for Bits and Atoms (CBA), has been pursuing as his doctoral thesis work. It’s now reached the point that prototype versions of such robots can assemble small structures and even work together as a team to build up a larger assemblies.</p>\n<p>The new work appears in the October issue of the <em>IEEE Robotics and Automation Letters</em>, in a paper by Jenett, Gershenfeld, fellow graduate student Amira Abdel-Rahman, and CBA alumnus Kenneth Cheung SM ’07, PhD ’12, who is now at NASA’s Ames Research Center, where he leads the <a href=\"https://gameon.nasa.gov/projects/automated-reconfigurable-mission-adaptive-digital-assembly-systems-armadas/\" target=\"_blank\">ARMADAS</a> project to design a lunar base that could be built with robotic assembly.</p>\n<p>“This paper is a treat,” says Aaron Becker, an associate professor of electrical and computer engineering at the University of Houston, who was not associated with this work. “It combines top-notch mechanical design with jaw-dropping demonstrations, new robotic hardware, and a simulation suite with over 100,000 elements,” he says.</p>\n<p>“What’s at the heart of this is a new kind of robotics, that we call relative robots,” Gershenfeld says. Historically, he explains, there have been two broad categories of robotics — ones made out of expensive custom components that are carefully optimized for particular applications such as factory assembly, and ones made from inexpensive mass-produced modules with much lower performance. The new robots, however, are an alternative to both. They’re much simpler than the former, while much more capable than the latter, and they have the potential to revolutionize the production of large-scale systems, from airplanes to bridges to entire buildings.</p>\n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-1.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><em><span style=\"font-size: 10px;\"><span style=\"caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: Calibri, Arial, Helvetica, sans-serif; text-size-adjust: auto;\">Experiments demonstrating&nbsp;relative robotic assembly of 1D, 2D, and 3D discrete&nbsp;cellular structures</span></span></em></p> \n<p>According to Gershenfeld, the key difference lies in the relationship between the robotic device and the materials that it is handling and manipulating. With these new kinds of robots, “you can’t separate the robot from the structure — they work together as a system,” he says. For example, while most mobile robots require highly precise navigation systems to keep track of their position, the new assembler robots only need to keep track of where they are in relation to the small subunits, called voxels, that they are currently working on. Every time the robot takes a step onto the next voxel, it readjusts its sense of position, always in relation to the specific components that it is standing on at the moment.</p>\n<p>The underlying vision is that just as the most complex of images can be reproduced by using an array of pixels on a screen, virtually any physical object can be recreated as an array of smaller three-dimensional pieces, or voxels, which can themselves be made up of simple struts and nodes. The team has shown that these simple components can be arranged to distribute loads efficiently; they are largely made up of open space so that the overall weight of the structure is minimized. The units can be picked up and placed in position next to one another by the simple assemblers, and then fastened together using latching systems built into each voxel.</p>\n<p>The robots themselves resemble a small arm, with two long segments that are hinged in the middle, and devices for clamping onto the voxel structures on each end. The simple devices move around like inchworms, advancing along a row of voxels by repeatedly opening and closing their V-shaped bodies to move from one to the next. Jenett has dubbed the little robots BILL-E (a nod to the movie robot WALL-E), which stands for Bipedal Isotropic Lattice Locomoting Explorer.</p> \n<p>Jenett has built several versions of the assemblers as proof-of-concept designs, along with corresponding voxel designs featuring latching mechanisms to easily attach or detach each one from its neighbors. He has used these prototypes to demonstrate the assembly of the blocks into linear, two-dimensional, and three-dimensional structures. “We’re not putting the precision in the robot; the precision comes from the structure” as it gradually takes shape, Jenett says. “That’s different from all other robots. It just needs to know where its next step is.”</p>\n<p></p> \n<p>As it works on assembling the pieces, each of the tiny robots can count its steps over the structure, says Gershenfeld, who is the director of CBA. Along with navigation, this lets the robots correct errors at each step, eliminating most of the complexity of typical robotic systems, he says. “It’s missing most of the usual control systems, but as long as it doesn’t miss a step, it knows where it is.” For practical assembly applications, swarms of such units could be working together to speed up the process, thanks to control software developed by Abdel-Rahman that can allow the robots to coordinate their work and avoid getting in each other’s way.</p>\n<p>This kind of assembly of large structures from identical subunits using a simple robotic system, much like a child assembling a large castle out of LEGO blocks, has already attracted the interest of some major potential users, including NASA, MIT’s collaborator on this research, and the European aerospace company Airbus SE, which also helped to sponsor the study.</p>\n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-4.gif\" /></p> \n<p><em><span style=\"font-size: 10px;\">Computer simulation shows a group of four assembler robots at work on building a three-dimensional structure. Whole swarms of such robots could be unleashed to create large structures such as airplane wings or space habitats. Illustration courtesy of the researchers</span></em></p> \n<p>One advantage of such assembly is that repairs and maintenance can be handled easily by the same kind of robotic process as the initial assembly. Damaged sections can be disassembled from the structure and replaced with new ones, producing a structure that is just as robust as the original. “Unbuilding is as important as building,” Gershenfeld says, and this process can also be used to make modifications or improvements to the system over time.</p>\n<p>“For a space station or a lunar habitat, these robots would live on the structure, continuously maintaining and repairing it,” says Jenett.</p>\n<p>Ultimately, such systems could be used to construct entire buildings, especially in difficult environments such as in space, or on the moon or Mars, Gershenfeld says. This could eliminate the need to ship large preassembled structures all the way from Earth. Instead it could be possible to send large batches of the tiny subunits — or form them from local materials using systems that could crank out these subunits at their final destination point. “If you can make a jumbo jet, you can make a building,” Gershenfeld says.</p>\n<p>Sandor Fekete, director of the Institute of Operating Systems and Computer Networks at the Technical University of Braunschweig, in Germany, who was not involved in this work, says “Ultralight, digital materials such as [these] open amazing perspectives for constructing efficient, complex, large-scale structures, which are of vital importance in aerospace applications.”</p>\n<p>But assembling such systems is a challenge, says Fekete, who plans to join the research team for further development of the control systems. “This is where the use of small and simple robots promises to provide the next breakthrough: Robots don’t get tired or bored, and using many miniature robots seems like the only way to get this critical job done. This extremely original and clever work by Ben Jenett and collaborators makes a giant leap towards the construction of dynamically adjustable airplane wings, enormous solar sails or even reconfigurable space habitats.”</p>\n<p>In the process, Gershenfeld says, “we feel like we’re uncovering a new field of hybrid material-robot systems.”</p>","descriptionType":"html","publishedDate":"Wed, 16 Oct 2019 04:00:00 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-1.gif","linkMd5":"fe099c7b7d40176efaf0d171e59d0e8e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn23@2020_1/2020/08/25/01-57-17-085_5e7fa4f859a924ef.webp","destWidth":500,"destHeight":281,"sourceBytes":7838286,"destBytes":3801586,"author":"David L. Chandler | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn23@2020_1/2020/08/25/01-57-17-085_5e7fa4f859a924ef.webp","https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-4.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn63@2020_4/2020/08/25/01-57-28-436_bb092c7e3d55ec8f.webp"},"publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Giving robots a faster grasp","link":"https://news.mit.edu/2019/robotic-faster-grip-adjust-1017","description":"<p>If you’re at a desk with a pen or pencil handy, try this move: Grab the pen by one end with your thumb and index finger, and push the other end against the desk. Slide your fingers down the pen, then flip it upside down, without letting it drop. Not too hard, right?</p>\n<p>But for a robot — say, one that’s sorting through a bin of objects and attempting to get a good grasp on one of them — this is a computationally taxing maneuver. Before even attempting the move it must calculate a litany of properties and probabilities, such as the friction and geometry of the table, the pen, and its two fingers, and how various combinations of these properties interact mechanically, based on fundamental laws of physics.</p>\n<p>Now MIT engineers have found a way to significantly speed up the planning process required for a robot to adjust its grasp on an object by pushing that object against a stationary surface. Whereas traditional algorithms would require tens of minutes for planning out a sequence of motions, the new team’s approach shaves this preplanning process down to less than a second.</p>\n<p>Alberto Rodriguez, associate professor of mechanical engineering at MIT, says the speedier planning process will enable robots, particularly in industrial settings, to quickly figure out how to push against, slide along, or otherwise use features in their environments to reposition objects in their grasp. Such nimble manipulation is useful for any tasks that involve picking and sorting, and even intricate tool use.</p>\n<p>“This is a way to extend the dexterity of even simple robotic grippers, because at the end of the day, the environment is something every robot has around it,” Rodriguez says.</p>\n<p>The team’s results are published today in <em>The International Journal of Robotics Research</em>. Rodriguez’ co-authors are lead author Nikhil Chavan-Dafle, a graduate student in mechanical engineering, and Rachel Holladay, a graduate student in electrical engineering and computer science.</p>\n<p><strong>Physics in a cone</strong></p>\n<p>Rodriguez’ group works on enabling robots to leverage their environment to help them accomplish physical tasks, such as picking and sorting objects in a bin. &nbsp;</p>\n<p>Existing algorithms typically take hours to preplan a sequence of motions for a robotic gripper, mainly because, for every motion that it considers, the algorithm must first calculate whether that motion would satisfy a number of physical laws, such as Newton’s laws of motion and Coulomb’s law describing frictional forces between objects.</p>\n<p>“It’s a tedious computational process to integrate all those laws, to consider all possible motions the robot can do, and to choose a useful one among those,” Rodriguez says.</p>\n<p>He and his colleagues found a compact way to solve the physics of these manipulations, in advance of deciding how the robot’s hand should move. They did so by using “motion cones,” which are essentially visual, cone-shaped maps of friction.</p>\n<p>The inside of the cone depicts all the pushing motions that could be applied to an object in a specific location, while satisfying the fundamental laws of physics and enabling the robot to keep hold of the object. The space outside of the cone represents all the pushes that would in some way cause an object to slip out of the robot’s grasp.</p>\n<p>“Seemingly simple variations, such as how hard robot grasps the object, can significantly change how the object moves in the grasp when pushed,” Holladay explains. “Based on how hard you’re grasping, there will be a different motion. And that’s part of the physical reasoning that the algorithm handles.”</p>\n<p>The team’s algorithm calculates a motion cone for different possible configurations between a robotic gripper, an object that it is holding, and the environment against which it is pushing, in order to select and sequence different feasible pushes to reposition the object.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Robot-Gripper.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><em><span style=\"font-size:10px;\">A new algorithm speeds up the planning process for robotic grippers.&nbsp;A robot in the lab is shown picking up a block letter, T, and pushing it against a nearby wall to re-angle it, before setting it back down in an upright position.</span></em></p>\n<p>“It’s a complicated process but still much faster than the traditional method — fast enough that planning an entire series of pushes takes half a second,” Holladay says.</p>\n<p><strong>Big plans</strong></p>\n<p>The researchers tested the new algorithm on a physical setup with a three-way interaction, in which a simple robotic gripper was holding a T-shaped block and pushing against a vertical bar. They used multiple starting configurations, with the robot gripping the block at a particular position and pushing it against the bar from a certain angle. For each starting configuration, the algorithm instantly generated the map of all the possible forces that the robot could apply and the position of the block that would result.</p>\n<p>“We did several thousand pushes to verify our model correctly predicts what happens in the real world,” Holladay says. “If we apply a push that’s inside the cone, the grasped object should remain under control. If it’s outside, the object should slip from the grasp.”</p>\n<p>The researchers found that the algorithm’s predictions reliably matched the physical outcome in the lab, planning out sequences of motions — such as reorienting the block against the bar before setting it down on a table in an upright position — in less than a second, compared with traditional algorithms that take over 500 seconds to plan out.</p>\n<p>“Because we have this compact representation of the mechanics of this three-way-interaction between robot, object, and their environment, we can now attack bigger planning problems,” Rodriguez says.</p>\n<p>The group is hoping to apply and extend its approach to enable a robotic gripper to handle different types of tools, for instance in a manufacturing setting.</p>\n<p>“Most factory robots that use tools have a specially designed hand, so instead of having the abiity to grasp a screwdriver and use it in a lot of different ways, they just make the hand a screwdriver,” Holladay says. “You can imagine that requires less dexterous planning, but it’s much more limiting. We’d like a robot to be able to use and pick lots of different things up.”</p>\n<p>This research was supported, in part, by Mathworks, the MIT-HKUST Alliance, and the National Science Foundation.</p>","descriptionType":"html","publishedDate":"Thu, 17 Oct 2019 13:39:41 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Robot-Gripper.gif","linkMd5":"df67e69453094d5de1d32fd338012bff","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn32@2020_3/2020/08/25/01-57-23-040_79cf4a8507cc1726.webp","destWidth":500,"destHeight":281,"sourceBytes":7742871,"destBytes":7167986,"author":"Jennifer Chu | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Robot-Gripper.gif":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn32@2020_3/2020/08/25/01-57-23-040_79cf4a8507cc1726.webp"},"publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Bringing human-like reasoning to driverless car navigation","link":"https://news.mit.edu/2019/human-reasoning-ai-driverless-car-navigation-0523","description":"<p>With aims of bringing more human-like reasoning to autonomous vehicles, MIT researchers have created a system that uses only simple maps and visual data to enable driverless cars to navigate routes in new, complex environments.</p>\n<p>Human drivers are exceptionally good at navigating roads they haven’t driven on before, using observation and simple tools. We simply match what we see around us to what we see on our GPS devices to determine where we are and where we need to go. Driverless cars, however, struggle with this basic reasoning. In every new area, the cars must first map and analyze all the new roads, which is very time consuming. The systems also rely on complex maps —&nbsp;usually generated by 3-D scans —&nbsp;which are computationally intensive to generate and process on the fly.</p>\n<p>In a paper being presented at this week’s International Conference on Robotics and Automation, MIT researchers describe an autonomous control system that “learns” the steering patterns of human drivers as they navigate roads in a small area, using only data from video camera feeds and a simple GPS-like map. Then, the trained system can control a driverless car along a planned route in a brand-new area, by imitating the human driver.</p>\n<p>Similarly to human drivers, the system also detects any mismatches between its map and features of the road. This helps the system determine if its position, sensors, or mapping are incorrect, in order to correct the car’s course.</p>\n<p>To train the system initially, a human operator controlled an automated&nbsp;Toyota Prius — equipped with several cameras and a basic GPS navigation system —&nbsp;to collect data from local suburban streets including various road structures and obstacles. When deployed autonomously, the system successfully navigated the car along a preplanned path in a different forested area, designated for autonomous vehicle tests.</p>\n<p>“With our system, you don’t need to train on every road beforehand,” says first author Alexander Amini, an MIT graduate student. “You can download a new map for the car to navigate through roads it has never seen before.”</p>\n<p>“Our objective is to achieve autonomous navigation that is robust for driving in new environments,” adds co-author Daniela Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “For example, if we train an autonomous vehicle to drive in an urban setting such as the streets of Cambridge, the system should also be able to drive smoothly in the woods, even if that is an environment it has never seen before.”</p>\n<p>Joining Rus and Amini on the paper are Guy Rosman, a researcher at the Toyota Research Institute, and Sertac Karaman, an associate professor of aeronautics and astronautics at MIT.</p> \n<p></p> \n<p><strong>Point-to-point navigation</strong></p>\n<p>Traditional navigation systems process data from sensors through multiple modules customized for tasks such as localization, mapping, object detection, motion planning, and steering control. For years, Rus’s group has been developing “end-to-end” navigation systems, which process inputted sensory data and output steering commands, without a need for any specialized modules.</p>\n<p>Until now, however, these models were strictly designed to safely follow the road, without any real destination in mind. In the new paper, the researchers advanced their end-to-end system to drive from goal to destination, in a previously unseen environment. To do so, the researchers trained their system to predict a full probability distribution over all possible steering commands at any given instant while driving.</p>\n<p>The system uses a machine learning model called a convolutional neural network (CNN), commonly used for image recognition. During training, the system watches and learns how to steer from a human driver. The CNN correlates steering wheel rotations to road curvatures it observes through cameras and an inputted map. Eventually, it learns the most likely steering command for various driving situations, such as straight roads, four-way or T-shaped intersections, forks, and rotaries.</p>\n<p>“Initially, at a T-shaped intersection, there are many different directions the car could turn,” Rus says. “The model starts by thinking about all those directions, but as it sees more and more data about what people do, it will see that some people turn left and some turn right, but nobody goes straight. Straight ahead is ruled out as a possible direction, and the model learns that, at T-shaped intersections, it can only move left or right.”</p>\n<p></p> \n<p><strong>What does the map say?</strong></p>\n<p>In testing, the researchers input the system with a map with a randomly chosen route. When driving, the system extracts visual features from the camera, which enables it to predict road structures. For instance, it identifies a distant stop sign or line breaks on the side of the road as signs of an upcoming intersection. At each moment, it uses its predicted probability distribution of steering commands to choose the most likely one to follow its route.</p>\n<p>Importantly, the researchers say, the system uses maps that are easy to store and process. Autonomous control systems typically use LIDAR scans to create massive, complex maps that take roughly 4,000 gigabytes (4 terabytes) of data to store just the city of San Francisco. For every new destination, the car must create new maps, which amounts to tons of data processing. Maps used by the researchers’ system, however, captures the entire world using just 40 gigabytes of data. &nbsp;</p>\n<p>During autonomous driving, the system also continuously matches its visual data to the map data and notes any mismatches. Doing so helps the autonomous vehicle better determine where it is located on the road. And it ensures the car stays on the safest path if it’s being fed contradictory input information: If, say, the car is cruising on a straight road with no turns, and the GPS indicates the car must turn right, the car will know to keep driving straight or to stop.</p>\n<p>“In the real world, sensors do fail,” Amini says. “We want to make sure that the system is robust to different failures of different sensors by building a system that can accept these noisy inputs and still navigate and localize itself correctly on the road.”</p>","descriptionType":"html","publishedDate":"Thu, 23 May 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"d351bfb5ba3f1fafbb9d4bed9e1dcf77","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"MIT’s fleet of autonomous boats can now shapeshift","link":"https://news.mit.edu/2019/roboats-autonomous-connect-assemble-0829","description":"<p>MIT’s fleet of robotic boats&nbsp;has been updated with new capabilities to “shapeshift,” by autonomously disconnecting and reassembling into a variety of configurations, to form floating structures in Amsterdam’s many canals.</p>\n<p>The autonomous boats — rectangular hulls equipped with sensors, thrusters, microcontrollers, GPS modules, cameras, and other hardware — are being developed as part of the ongoing “<a href=\"http://senseable.mit.edu/roboat/\">Roboat</a>” project between MIT and the Amsterdam Institute for Advanced Metropolitan Solutions (AMS Institute). The&nbsp;project is led by MIT professors Carlo Ratti, Daniela Rus, Dennis Frenchman, and Andrew&nbsp;Whittle. In the future, Amsterdam wants the roboats to cruise its 165 winding canals, transporting goods and people, collecting trash, or self-assembling into “pop-up” platforms — such as bridges and stages — to help relieve congestion on the city’s busy streets.</p>\n<p>In 2016, MIT researchers&nbsp;<a href=\"http://news.mit.edu/2016/autonomous-fleet-amsterdam-roboat-0919\">tested</a> a roboat prototype that could move forward, backward, and laterally along a preprogrammed path in the canals. Last year, researchers <a href=\"http://news.mit.edu/2018/fleet-autonomous-boats-service-cities-reducing-road-traffic-0523\">designed</a> low-cost, 3-D-printed, one-quarter scale versions of the boats, which were more efficient and agile, and came equipped with advanced trajectory-tracking algorithms.&nbsp;In June, they created an autonomous <a href=\"http://news.mit.edu/2019/autonomous-robot-boats-latch-0605\">latching</a> mechanism that let the boats target and clasp onto each other, and keep trying if they fail.</p>\n<p>In a new paper presented at the last week’s IEEE International Symposium on Multi-Robot and Multi-Agent Systems, the researchers describe an algorithm that enables the roboats to smoothly reshape themselves as efficiently as possible. The algorithm handles all the planning and tracking that enables groups of roboat units to unlatch from one another in one set configuration, travel a collision-free path, and reattach to their appropriate spot on the new set configuration.<br /> <p>In demonstrations in an MIT pool and in computer simulations, groups of linked roboat units rearranged themselves from straight lines or squares into other configurations, such as rectangles and “L” shapes. The experimental transformations only took a few minutes. More complex shapeshifts may take longer, depending on the number of moving units — which could be dozens —&nbsp;and differences between the two shapes.</p><p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/roboats-1.gif\" /></p> <p>“We’ve enabled the roboats to now make and break connections with other roboats, with hopes of moving activities on the streets of Amsterdam to the water,” says Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “A set of boats can come together to form linear shapes as pop-up bridges, if we need to send materials or people from one side of a canal to the other. Or, we can create pop-up wider platforms for flower or food markets.”</p><p>Joining Rus on the paper are: Ratti, director of MIT’s Senseable City Lab, and, also from the lab, first author Banti Gheneti, Ryan Kelly, and Drew Meyers, all researchers; postdoc Shinkyu Park; and research fellow Pietro Leoni.</p><p><strong>Collision-free trajectories</strong></p><p>For their work, the researchers had to tackle challenges with autonomous planning, tracking, and connecting groups of roboat units. Giving each unit unique capabilities to, for instance, locate each other, agree on how to break apart and reform, and then move around freely, would require complex communication and control techniques that could make movement inefficient and slow.</p><p>To enable smoother operations, the researchers developed two types of units: coordinators and workers. One or more workers connect to one coordinator to form a single entity, called a “connected-vessel platform” (CVP). All coordinator and worker units have four propellers, a wireless-enabled microcontroller, and several automated latching mechanisms and sensing systems that enable them to link together.</p><p>Coordinators, however, also come equipped with GPS for navigation, and an inertial measurement unit (IMU), which computes localization, pose, and velocity. Workers only have actuators that help the CVP steer along a path. Each coordinator is aware of and can wirelessly communicate with all connected workers. Structures comprise multiple CVPs, and individual CVPs can latch onto one another to form a larger entity.</p><p>During shapeshifting, all connected CVPs in a structure compare the geometric differences between its initial shape and new shape. Then, each CVP determines if it stays in the same spot and if it needs to move. Each moving CVP is then assigned a time to disassemble and a new position in the new shape.</p><p>Each CVP uses a custom trajectory-planning technique to compute a way to reach its target position without interruption, while optimizing the route for speed. To do so, each CVP precomputes all collision-free regions around the moving CVP as it rotates and moves away from a stationary one.</p><p>After precomputing those collision-free regions, the CVP then finds the shortest trajectory to its final destination, which still keeps it from hitting the stationary unit. Notably, optimization techniques are used to make the whole trajectory-planning process very efficient, with the precomputation taking little more than 100 milliseconds to find and refine safe paths. Using data from the GPS and IMU, the coordinator then estimates its pose and velocity at its center of mass, and wirelessly controls all the propellers of each unit and moves into the target location.</p><p>In their experiments, the researchers tested three-unit CVPs, consisting of one coordinator and two workers, in several different shapeshifting scenarios. Each scenario involved one CVP unlatching from the initial shape and moving and relatching to a target spot around a second CVP.</p><p>Three CVPs, for instance, rearranged themselves from a connected straight line — where they were latched together at their sides —&nbsp;into a straight line connected at front and back, as well as an “L.” In computer simulations, up to 12 roboat units rearranged themselves from, say, a rectangle into a square or from a solid square into a Z-like shape.</p> <p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/roboats-2.gif\" style=\"width: 500px; height: 281px;\" /></p><p><strong>Scaling up</strong></p><p>Experiments were conducted on quarter-sized roboat units, which measure about 1 meter long and half a meter wide. But the researchers believe their trajectory-planning algorithm will scale well in controlling full-sized units, which will measure about 4 meters long and 2 meters wide.</p><p>The researchers hope to use the roboats to form into a dynamic “bridge” across a 60-meter canal between the NEMO Science Museum in Amsterdam’s city center and an area that’s under development. Called <a href=\"http://senseable.mit.edu/roundaround/\">RoundAround</a>, the idea is to employ roboats to sail in a continuous circle across the canal, picking up and dropping off passengers at docks and stopping or rerouting when they detect anything in the way. Currently, walking around that waterway takes about 10 minutes, but the bridge can cut that time to around two minutes. This is still an explorative concept.</p><p>“This will be the world’s first bridge comprised of a fleet of autonomous boats,” Ratti says. “A regular bridge would be super expensive, because you have boats going through, so you’d need to have a mechanical bridge that opens up or a very high bridge. But we can connect two sides of canal [by using] autonomous boats that become dynamic, responsive architecture that float on the water.”</p><p>To reach that goal, the researchers are further developing the roboats to ensure they can safely hold people, and are robust to all weather conditions, such as heavy rain. They’re also making sure the roboats can effectively connect to the sides of the canals, which can vary greatly in structure and design.</p> </p>","descriptionType":"html","publishedDate":"Thu, 29 Aug 2019 04:00:00 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/roboats-1.gif","linkMd5":"b26d9d53fbfe21bde00a69f1747c70cf","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn36@2020_1/2020/08/25/01-57-16-772_80656559f198c3b4.webp","destWidth":500,"destHeight":281,"sourceBytes":8006981,"destBytes":4439350,"author":"Rob Matheson | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/roboats-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn36@2020_1/2020/08/25/01-57-16-772_80656559f198c3b4.webp","https://news.mit.edu/sites/default/files/images/inline/images/roboats-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn51@2020_6/2020/08/25/01-57-28-283_0f7a43e9d5a00deb.webp"},"publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Boston-area girls discover a passion for coding","link":"https://news.mit.edu/2019/boston-area-girls-discover-passion-coding-mit-lincoln-laboratory-1213","description":"<p>“My goal is to make computing 'normal' for girls,” says Sabina Chen from Lincoln Laboratory's Advanced Capabilities and Systems Group, who led a workshop that taught middle school girls how to program a robotic car to autonomously follow colored cones. The girls attended this enrichment class for eight consecutive Saturdays from September to November. “The class is about exposure [to computing] and interest-building,” she explains.</p> \n<p>While Chen was introducing the 20 middle school girls to coding and computer visualization software at the Beaver Works facility in MIT's Building 31, Eyassu Shimelis of the laboratory's Advanced Concepts and Technologies Group was conducting a similar series of classes for 21 high school girls in MIT's Building 33.</p> \n<p>The motivation behind holding the girls-only workshops is to foster a curiosity and familiarity in computing that may lead to a future increase in the number of women engaged in computer science. According to ComputerScience.org, in 2018 only 18 percent of bachelor's degrees in computer science were awarded to women; in electrical engineering, a major that often leads to professions involving computing, the percentage is even lower, at 13.7. The Bureau of Labor Statistics reports that women make up only about 21 percent of computer programmers, 19 percent of software developers, and 32 percent of website developers.</p> \n<p>While multiple theories exist as to why women are underrepresented in computer-related majors and jobs, one consensus is that young women do not have confidence in their ability to master computers. “The girls came in thinking they can't do it,” Chen says, adding that she finds the course worthwhile when “their eyes sort of sparkle when they realize they can do it.”</p> \n<p>Both workshops are based on a rigorous four-week course offered to high school seniors through the <a href=\"https://beaverworks.ll.mit.edu/CMS/bw/BWSI\">Beaver Works Summer Institute</a> (BWSI). The summer course lets students explore the various technologies that can be used to enable an MIT-designed RACECAR (Rapid Autonomous Complex Environment Competing Ackermann steeRing) robotic vehicle to compete in fast, autonomous navigation around a mini “Grand Prix” racetrack. Although the Saturday sessions could not delve as deeply as the summer course into the software and various systems needed for the cars to tackle the obstacle-ridden Grand Prix, these weekend “crash courses” did cover the coding and computer-vison technology that allowed the girls to race their cars around a circular course set up in Building 31's high-bay space.</p> \n<p>Chen developed the curriculum for the middle school program that was offered to both boys and girls this past summer in conjunction with the BWSI. “It is designed so students learn a specific concept, apply it, and see an immediate result,” she says. The gratification of witnessing a hands-on application of a lesson is what keeps the students interested. Her curriculum will soon be online so that schools, robotics clubs, or even interested individuals can adapt it for themselves.&nbsp;</p> \n<p>Shimelis taught a similar version of a <a href=\"https://www.ll.mit.edu/news/high-school-students-get-ready-beaver-works-summer-institute\">RACECAR preliminary course</a> for Boston-area high schoolers. That course was developed in 2018 by Andrew Fishberg, who passed along his program when he moved on to tackle graduate studies at MIT. Shimelis is tweaking the course to address feedback from BWSI RACECAR students and teaching assistants, and to adapt it to his teaching style.</p> \n<p>Both Chen and Shimelis say they did not modify their courses for the girls-only sessions that were new this fall. They agree that the girls were eager to learn and capable of handling the classwork. “Many of the girls were faster at grasping the concepts than students in my summer course,” Shimelis notes. This is high praise, because to be accepted for the BWSI program, students must complete a prerequisite RACECAR online tutorial and submit teacher recommendations and stellar school transcripts.</p> \n<p>Chen says she was pleased by the change she saw in the girls from the beginning to the end of her workshop. “At the end, they were a lot more sure of themselves and more willing to explore their own ideas without fear.”</p> \n<p>According to Chen and Shimelis, the success of the two workshops can, in large part, be attributed to the dedicated help of a number of people. Sertac Karaman, an MIT engineering professor who developed the original RACECAR course for undergraduate students, provided guidance to both the instructors and students. A cadre of volunteers served as teaching assistants: Kourosh Arasteh, Olivia Brown, Juliana Furgala, Saivamsi Hanumanthu, Elisa Kircheim, Tzofi Klinghoffer, Marko Kocic, Aryk Ledet, Harrison Packer, Joyce Tam, and Jing Wang from Lincoln Laboratory's staff; and Andrew Schoer, a Boston University grad student who is a participant in the Laboratory's Lincoln Scholars tuition assistance program.</p> \n<p>The success of the workshops is captured in one student's answer to a course-evaluation question about what she gained: “I see myself coding in the future!”</p>","descriptionType":"html","publishedDate":"Fri, 13 Dec 2019 17:20:01 +0000","feedId":12364,"bgimg":"","linkMd5":"9cd927a88f4a980325c9fd26426bd3f1","bgimgJsdelivr":"","metaImg":"","author":"Dorothy Ryan | Lincoln Laboratory","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Bringing artificial intelligence and MIT to middle school classrooms","link":"https://news.mit.edu/2019/bringing-artificial-intelligence-and-mit-middle-school-classrooms-1230","description":"<p>In the age of Alexa, YouTube recommendations, and Spotify playlists, artificial intelligence has become a way of life, improving marketing and advertising, e-commerce, and more. But what are the ethical implications of technology that collects and learns personal information? How should society navigate these issues and shape the future?</p> \n<p>A new curriculum designed for middle school students aims to help them understand just that at an early age, as they grow up surrounded by the technology. The open-source educational material, designed by an MIT team and piloted at this year’s Massachusetts STEM Week this past fall, teaches students how AI systems are designed, how they can be used to influence the public — and also how to use them to be successful in jobs of the future.</p> \n<p>During Mass STEM Week in October, middle schools across the commonwealth replaced their regular curriculum with an immersive week of hands-on learning led by a team including Cynthia Breazeal, associate professor of media arts and sciences at MIT; Randi Williams ’18, graduate research assistant in the <a href=\"https://robotic.media.mit.edu/\">Personal Robots Group</a> at the MIT Media Lab; and the nonprofit organization <a href=\"https://i2learning.org/\">i2 Learning</a>.</p> \n<p>“Preparing students for the future means having them engage in technology through hands-on activities. We provide students with tools and conceptual frameworks where we want them to engage with our materials as conscientious designers of AI-enabled technologies,” Breazeal says. “As they think through designing a solution to address a problem in their community, we get them to think critically about the ethical implications of the technology.”</p> \n<p>Three years ago, the Personal Robots Group began a program around teaching AI concepts to preschoolers. This effort then broadened into planning learning experiences for more children, and the group developed a curriculum geared toward middle school students. Last spring, an AI curriculum was shared with teachers and piloted in Somerville, Massachusetts, to determine which activities resonated the most in the classrooms.</p> \n<p>“We want to make a curriculum in which middle-schoolers can build and use AI — and, more importantly, we want them to take into account the societal impact of any technology,” says Williams.</p> \n<p>This curriculum, <a href=\"https://www.media.mit.edu/projects/ai-5-8/overview/\">How to Train Your Robot</a>, was first piloted at an i2 summer camp in Boston before being presented to teachers from local schools during Mass STEM Week. The teachers, many of whom had little familiarity with STEM subjects, also participated in two days of professional development training to prepare them to deliver more than 20 class hours of AI content to their students. The curriculum ran in three schools across six classrooms.</p> \n<p>The AI curriculum incorporates the work of Blakeley Hoffman Payne, a graduate research assistant in the Personal Robots Group, whose research focuses on the ethics of artificial intelligence and how to teach children to design, use, and think about AI. Students participated in discussions and creative activities, designing robot companions and using machine learning to solve real-world problems they have observed. At the end of the week, students share their inventions with their communities.</p> \n<p>“AI is an area that is becoming increasingly important in people’s lives,” says Ethan Berman, founder of i2 Learning and MIT parent. “This curriculum is very relevant to both students and teachers. Beyond just being a class on technology, it focuses on what it means to be a global citizen.”</p> \n<p>The creative projects provided opportunities for students to consider problems from a variety of angles, including thinking about issues of bias ahead of time, before a system is designed. For example, for one project that focused on sign language, the student trained her algorithm for understanding sign language around students of a wide range of skin tones, and incorporated adults, too — considering potential algorithmic bias to inform the design of the system.</p> \n<p>Another group of students built a “library robot,” designed to help find and retrieve a book for people with mobility challenges. Students had to think critically about why and how this might be helpful, and also to consider the job of a librarian and how this would impact a librarian’s work. They considered how a robot that finds and retrieves books might be able to free up more of a librarian’s time to actually help people and find information for them.</p> \n<p>Some of the current opportunities include scaling for more classrooms and schools, and also incorporating some other disciplines. There is interest in incorporating social studies, math, science, art, and music by finding ways to weave these other subjects into the AI projects. The main focus is on experiential learning that impacts how students think about AI.</p> \n<p>“We hope students walk away with a different understanding of AI and how it works in the world,” says Williams, “and that they feel empowered to play an important role in shaping the technology.”</p>","descriptionType":"html","publishedDate":"Mon, 30 Dec 2019 15:45:01 +0000","feedId":12364,"bgimg":"","linkMd5":"45b1c87a69777fc68ab382a594da7663","bgimgJsdelivr":"","metaImg":"","author":"Stefanie Koperniak | MIT Open Learning","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Predicting people's driving personalities","link":"https://news.mit.edu/2019/predicting-driving-personalities-1118","description":"<p>Self-driving cars are coming. But for all their fancy sensors and intricate data-crunching abilities, even the most cutting-edge cars lack something that (almost) every 16-year-old with a learner’s permit has: social awareness.<br /> <br /> While autonomous technologies have improved substantially, they still ultimately view the drivers around them as obstacles made up of ones and zeros, rather than human beings with specific intentions, motivations, and personalities.<br /> <br /> But recently a team led by researchers at MIT’s <a href=\"http://csail.mit.edu\">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) has been exploring whether self-driving cars can be programmed to classify the social personalities of other drivers, so that they can better predict what different cars will do — and, therefore, be able to drive more safely among them.<br /> <br /> In a new paper, the scientists integrated tools from social psychology to classify driving behavior with respect to how selfish or selfless a particular driver is.<br /> <br /> Specifically, they used something called social value orientation (SVO), which represents the degree to which someone is selfish (“egoistic”) versus altruistic or cooperative (“prosocial”). The system then estimates drivers’ SVOs to create real-time driving trajectories for self-driving cars.</p> \n<p>Testing their algorithm on the tasks of merging lanes and making unprotected left turns, the team showed that they could better predict the behavior of other cars by a factor of 25 percent. For example, in the left-turn simulations their car knew to wait when the approaching car had a more egoistic driver, and to then make the turn when the other car was more prosocial.</p> \n<p>While not yet robust enough to be implemented on real roads, the system could have some intriguing use cases, and not just for the cars that drive themselves. Say you’re a human driving along and a car suddenly enters your blind spot — the system could give you a warning in the rear-view mirror that the car has an aggressive driver, allowing you to adjust accordingly. It could also allow self-driving cars to actually learn to exhibit more human-like behavior that will be easier for human drivers to understand.<br /> <br /> “Working with and around humans means figuring out their intentions to better understand their behavior,” says graduate student Wilko Schwarting, who was lead author on the new paper that will be published this week in the latest issue of the <em>Proceedings of the National Academy of Sciences</em>. “People’s tendencies to be collaborative or competitive often spills over into how they behave as drivers. In this paper, we sought to understand if this was something we could actually quantify.”<br /> <br /> Schwarting’s co-authors include MIT professors Sertac Karaman and Daniela Rus, as well as research scientist Alyssa Pierson and former CSAIL postdoc Javier Alonso-Mora.<br /> <br /> A central issue with today’s self-driving cars is that they’re programmed to assume that all humans act the same way. This means that, among other things, they’re quite conservative in their decision-making at four-way stops and other intersections.<br /> <br /> While this caution reduces the chance of fatal accidents, it also <a href=\"https://www.popsci.com/self-driving-cars-unprotected-left-turns/\">creates bottlenecks</a> that can be frustrating for other drivers, not to mention hard for them to understand. (This may be why the majority of traffic incidents have involved <a href=\"https://twitter.com/wired/status/1053031276251234305\">getting rear-ended by impatient drivers</a>.)<br /> <br /> “Creating more human-like behavior in autonomous vehicles (AVs) is fundamental for the safety of passengers and surrounding vehicles, since behaving in a predictable manner enables humans to understand and appropriately respond to the AV’s actions,” says Schwarting.<br /> <br /> To try to expand the car’s social awareness, the CSAIL team combined methods from social psychology with game theory, a theoretical framework for conceiving social situations among competing players.<br /> <br /> The team modeled road scenarios where each driver tried to maximize their own utility and analyzed their “best responses” given the decisions of all other agents. Based on that small snippet of motion from other cars, the team’s algorithm could then predict the surrounding cars’ behavior as cooperative, altruistic, or egoistic — grouping the first two as “prosocial.” People’s scores for these qualities rest on a continuum with respect to how much a person demonstrates care for themselves versus care for others.<br /> <br /> In the merging and left-turn scenarios, the two outcome options were to either let somebody merge into your lane (“prosocial”) or not (“egoistic”). The team’s results showed that, not surprisingly, merging cars are deemed more competitive than non-merging cars.<br /> <br /> The system was trained to try to better understand when it’s appropriate to exhibit different behaviors. For example, even the most deferential of human drivers knows that certain types of actions — like making a lane change in heavy traffic — require a moment of being more assertive and decisive.<br /> <br /> For the next phase of the research, the team plans to work to apply their model to pedestrians, bicycles, and other agents in driving environments. In addition, they will be investigating other robotic systems acting among humans, such as household robots, and integrating SVO into their prediction and decision-making algorithms. Pierson says that the ability to estimate SVO distributions directly from observed motion, instead of in laboratory conditions, will be important for fields far beyond autonomous driving.<br /> <br /> “By modeling driving personalities and incorporating the models mathematically using the SVO in the decision-making module of a robot car, this work opens the door to safer and more seamless road-sharing between human-driven and robot-driven cars,” says Rus.<br /> <br /> The research was supported by the Toyota Research Institute for the MIT team. The Netherlands Organization for Scientific Research provided support for the specific participation of Mora.</p> \n<p></p>","descriptionType":"html","publishedDate":"Mon, 18 Nov 2019 21:10:01 +0000","feedId":12364,"bgimg":"","linkMd5":"f48787eb6e41626e19b6a05904e3a8a0","bgimgJsdelivr":"","metaImg":"","author":"Adam Conner-Simons | Rachel Gordon | MIT CSAIL","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Study: Social robots can benefit hospitalized children","link":"https://news.mit.edu/2019/social-robots-benefit-sick-children-0626","description":"<p>A new study demonstrates, for the first time, that “social robots” used in support sessions held in pediatric units at hospitals can lead to more positive emotions in sick children.</p>\n<p>Many hospitals host interventions in pediatric units, where child life specialists will provide clinical interventions to hospitalized children for developmental and coping support. This involves play, preparation, education, and behavioral distraction for both routine medical care, as well as before, during, and after difficult procedures. Traditional interventions include therapeutic medical play and normalizing the environment through activities such as arts and crafts, games, and celebrations.</p>\n<p>For the study, published today in the journal <em>Pediatrics</em>, researchers from the MIT Media Lab, Boston Children’s Hospital, and Northeastern University&nbsp;deployed a robotic teddy bear, “<a href=\"http://www.media.mit.edu/projects/huggable-a-social-robot-for-pediatric-care/overview/\">Huggable</a>,” across several pediatric units at Boston Children’s Hospital. More than 50 hospitalized children were randomly split into three groups of interventions that involved Huggable, a tablet-based virtual Huggable, or a traditional plush teddy bear. In general, Huggable improved various patient outcomes over those other two options. &nbsp;</p>\n<p>The study primarily demonstrated the feasibility of integrating Huggable into the interventions. But results also indicated that children playing with Huggable experienced more positive emotions overall. They also got out of bed and moved around more, and emotionally connected with the robot, asking it personal questions and inviting it to come back later to meet their families. “Such improved emotional, physical, and verbal outcomes are all positive factors that could contribute to better and faster recovery in hospitalized children,” the researchers write in their study.</p>\n<p>Although it is a small study, it is the first to explore social robotics in a real-world inpatient pediatric setting with ill children, the researchers say. Other studies have been conducted in labs, have studied very few children, or were conducted in public settings without any patient identification.</p>\n<p>But Huggable is designed only to assist health care specialists — not replace them, the researchers stress. “It’s a companion,” says co-author Cynthia Breazeal, an associate professor of media arts and sciences and founding director of the Personal Robots group. “Our group designs technologies with the mindset that they’re teammates. We don’t just look at the child-robot interaction. It’s about [helping] specialists and parents, because we want technology to support everyone who’s invested in the quality care of a child.”</p>\n<p>“Child life staff provide a lot of human interaction to help normalize the hospital experience, but they can’t be with every kid, all the time. Social robots create a more consistent presence throughout the day,” adds first author Deirdre Logan, a pediatric psychologist at Boston Children’s Hospital. “There may also be kids who don’t always want to talk to people, and respond better to having a robotic stuffed animal with them. It’s exciting knowing what types of support we can provide kids who may feel isolated or scared about what they’re going through.”</p>\n<p>Joining Breazeal and Logan on the paper are: Sooyeon Jeong, a PhD student in the Personal Robots group; Brianna O’Connell, Duncan Smith-Freedman, and Peter Weinstock, all of Boston Children’s Hospital; and Matthew Goodwin and James Heathers, both of Northeastern University.</p>\n<p><strong>Boosting mood</strong></p>\n<p>First prototyped in 2006, Huggable is a plush teddy bear with a screen depicting animated eyes. While the eventual goal is to make the robot fully autonomous, it is currently operated remotely by a specialist in the hall outside a child’s room. Through custom software, a specialist can control the robot’s facial expressions and body actions, and direct its gaze. The specialists could also talk through a speaker — with their voice automatically shifted to a higher pitch to sound more childlike —&nbsp;and monitor the participants via camera feed. The tablet-based avatar of the bear had identical gestures and was also remotely operated.</p>\n<p>During the interventions involving Huggable —&nbsp;involving kids ages 3 to 10 years —&nbsp;a specialist would sing nursery rhymes to younger children through robot and move the arms during the song. Older kids would play the I Spy game, where they have to guess an object in the room described by the specialist through Huggable. &nbsp;</p>\n<p>Through self-reports and questionnaires, the researchers recorded how much the patients and families liked interacting with Huggable. Additional questionnaires assessed patient’s positive moods, as well as anxiety and perceived pain levels. The researchers also used cameras mounted in the child’s room to capture and analyze speech patterns, characterizing them as joyful or sad, using software.</p>\n<p>A greater percentage of children and their parents reported that the children enjoyed playing with Huggable more than with the avatar or traditional teddy bear. Speech analysis backed up that result, detecting significantly more joyful expressions among the children during robotic interventions. Additionally, parents noted lower levels of perceived pain among their children.</p>\n<p>The researchers noted that 93 percent of patients completed the Huggable-based interventions, and found few barriers to practical implementation, as determined by comments from the specialists.</p>\n<p>A previous paper based on the same study found that the robot also seemed to facilitate greater family involvement in the interventions, compared to the other two methods, which improved the intervention overall. “Those are findings we didn’t necessarily expect in the beginning,” says Jeong, also a co-author on the previous paper. “We didn’t tell family to join any of the play sessions — it just happened naturally. When the robot came in, the child and robot and parents all interacted more, playing games or in introducing the robot.”</p>\n<p><strong>An automated, take-home bot</strong></p>\n<p>The study also generated valuable insights for developing a fully autonomous Huggable robot, which is the researchers’ ultimate goal. They were able to determine which physical gestures are used most and least often, and which features specialists may want for future iterations. Huggable, for instance, could introduce doctors before they enter a child’s room or learn a child’s interests and share that information with specialists. The researchers may also equip the robot with computer vision, so it can detect certain objects in a room to talk about those with children.</p>\n<p>“In these early studies, we capture data … to wrap our heads around an authentic use-case scenario where, if the bear was automated, what does it need to do to provide high-quality standard of care,” Breazeal says.</p>\n<p>In the future, that automated robot could be used to improve continuity of care. A child would take home a robot after a hospital visit to further support engagement, adherence to care regimens, and monitoring well-being.</p>\n<p>“We want to continue thinking about how robots can become part of the whole clinical team and help everyone,” Jeong says. “When the robot goes home, we want to see the robot monitor a child’s progress. … If there’s something clinicians need to know earlier, the robot can let the clinicians know, so [they’re not] surprised at the next appointment that the child hasn’t been doing well.”</p> \n<p>“This is a highly innovative and unique adjunctive intervention tool that appears to be highly engaging with children,” says Kevin Hommel, a faculty member of behavioral medicine and clinical psychology at Cincinnati Children's Hospital Medical Center. “I do think there is promise for social robots to alleviate some of the emotional distress that hospitalized children can feel. One thing I think will be important to examine in a future trial will be long term use. Do children continue to engage with social robots or do they lose their luster after some period of time? That will ultimately determine the viability of such a product in health care settings.”</p>\n<p>Next, the researchers are hoping to zero in on which specific patient populations may benefit the most from the Huggable interventions. “We want to find the sweet spot for the children who need this type of of extra support,” Logan says.</p>","descriptionType":"html","publishedDate":"Wed, 26 Jun 2019 04:01:00 +0000","feedId":12364,"bgimg":"","linkMd5":"cb49d52024e50f94bfd2f77e4f3d1bbb","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Better autonomous “reasoning” at tricky intersections","link":"https://news.mit.edu/2019/risk-model-autonomous-vehicles-1104","description":"<p>MIT and Toyota researchers have designed a new model to help autonomous vehicles determine when it’s safe to merge into traffic at intersections with obstructed views.</p>\n<p>Navigating intersections can be dangerous for driverless cars and humans alike. In 2016, roughly 23 percent of fatal and 32 percent of nonfatal U.S. traffic accidents occurred at intersections, according to a 2018 Department of Transportation study. Automated systems that help driverless cars and human drivers steer through intersections can require direct visibility of the objects they must avoid. When their line of sight is blocked by nearby buildings or other obstructions, these systems can fail.</p>\n<p>The researchers developed a model that instead uses its own uncertainty to estimate the risk of potential collisions or other traffic disruptions at such intersections. It weighs several critical factors, including all nearby visual obstructions, sensor noise and errors, the speed of other cars, and even the attentiveness of other drivers. Based on the measured risk, the system may advise the car to stop, pull into traffic, or nudge forward to gather more data.</p>\n<p>“When you approach an intersection there is potential danger for collision. Cameras and other sensors require line of sight. If there are occlusions, they don’t have enough visibility to assess whether it’s likely that something is coming,” says Daniela Rus, director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science. “In this work, we use a predictive-control model that’s more robust to uncertainty, to help vehicles safely navigate these challenging road situations.”</p>\n<p>The researchers tested the system in more than 100 trials of remote-controlled cars turning left at a busy, obstructed intersection in a mock city, with other cars constantly driving through the cross street. Experiments involved fully autonomous cars and cars driven by humans but assisted by the system. In all cases, the system successfully helped the cars avoid collision from 70 to 100 percent of the time, depending on various factors. Other similar models implemented in the same remote-control cars sometimes couldn’t complete a single trial run without a collision.</p>\n<p>Joining Rus on the paper are: first author Stephen G. McGill, Guy Rosman, and Luke Fletcher of the Toyota Research Institute (TRI); graduate students Teddy Ort and Brandon Araki, researcher Alyssa Pierson, and postdoc Igor Gilitschenski, all of CSAIL; Sertac Karaman, an MIT associate professor of aeronautics and astronautics; and John J. Leonard, the Samuel C. Collins Professor of Mechanical and Ocean Engineering of MIT and a TRI technical advisor.</p>\n<p><strong>Modeling road segments</strong></p>\n<p>The model is specifically designed for road junctions in which there is no stoplight and a car must yield before maneuvering into traffic at the cross street, such as taking a left turn through multiple lanes or roundabouts. In their work, the researchers split a road into small segments. This helps the model determine if any given segment is occupied to estimate a conditional risk of collision.</p>\n<p>Autonomous cars are equipped with sensors that measure the speed of other cars on the road. When a sensor clocks a passing car traveling into a visible segment, the model uses that speed to predict the car’s progression through all other segments. A probabilistic “Bayesian network” also considers uncertainties — such as noisy sensors or unpredictable speed changes — to determine the likelihood that each segment is occupied by a passing car.</p>\n<p>Because of nearby occlusions, however, this single measurement may not suffice. Basically, if a sensor can’t ever see a designated road segment, then the model assigns it a high likelihood of being occluded. From where the car is positioned, there’s increased risk of collision if the car just pulls out fast into traffic. This encourages the car to nudge forward to get a better view of all occluded segments. As the car does so, the model lowers its uncertainty and, in turn, risk.</p>\n<p>But even if the model does everything correctly, there’s still human error, so the model also estimates the awareness of other drivers. “These days, drivers may be texting or otherwise distracted, so the amount of time it takes to react may be a lot longer,” McGill says. “We model that conditional risk, as well.”</p>\n<p>That depends on computing the probability that a driver saw or didn’t see the autonomous car pulling into the intersection. To do so, the model looks at the number of segments a traveling car has passed through before the intersection. The more segments it had occupied before reaching the intersection, the higher the likelihood it has spotted the autonomous car and the lower the risk of collision.</p>\n<p>The model sums all risk estimates from traffic speed, occlusions, noisy sensors, and driver awareness. It also considers how long it will take the autonomous car to steer a preplanned path through the intersection, as well as all safe stopping spots for crossing traffic. This produces a total risk estimate.</p>\n<p>That risk estimate gets updated continuously for wherever the car is located at the intersection. In the presence of multiple occlusions, for instance, it’ll nudge forward, little by little, to reduce uncertainty. When the risk estimate is low enough, the model tells the car to drive through the intersection without stopping. Lingering in the middle of the intersection for too long, the researchers found, also increases risk of a collision.</p>\n<p><strong>Assistance and intervention</strong></p>\n<p>Running the model on remote-control cars in real-time indicates that it’s efficient and fast enough to deploy into full-scale autonomous test cars in the near future, the researchers say. (Many other models are too computationally heavy to run on those cars.) The model still needs far more rigorous testing before being used for real-world implementation in production vehicles.</p>\n<p>The model would serve as a supplemental risk metric that an autonomous vehicle system can use to better reason about driving through intersections safely. The model could also potentially be implemented in certain “advanced driver-assistive systems” (ADAS), where humans maintain shared control of the vehicle.</p>\n<p>Next, the researchers aim to include other challenging risk factors in the model, such as the presence of pedestrians in and around the road junction.</p>","descriptionType":"html","publishedDate":"Mon, 04 Nov 2019 17:44:49 +0000","feedId":12364,"bgimg":"","linkMd5":"baffdaa70366101c4d6908c815a212a7","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Robots shoot for the moon in MIT’s annual 2.007 competition","link":"https://news.mit.edu/2019/robots-shoot-moon-mit-annual-2007-competition-0510","description":"<p>In their historic lunar mission 50 years ago, Apollo 11 astronauts Buzz Aldrin and Neil Armstrong collected and returned to Earth more than 48 pounds of lunar material, including 50 moon rocks that researchers have been analyzing intensely ever since. If they’d only had help from some MIT robots, the astronauts might have been able to bring back even more lunar loot.</p>\n<p>On Thursday evening, students of MIT’s popular class 2.007 (Design and Manufacturing I) proved that robots can be efficient, ingenious, and even highly entertaining moon-rock scavengers.</p>\n<p>Over four often nail-biting hours, 32 student finalists, winnowed from a roster of 165, competed head to machined head, in the course’s annual robot competition, held in the ice rink at MIT’s Johnson Athletic Center. This year’s theme, Moonshot, was an homage to the Apollo 11 moon landing, celebrating its 50th anniversary this year.</p>\n<p>The course designers and machinists took the theme to heart, constructing two huge, identical game boards over which pairs of student-designed robots faced off. At the center of each board stood a replica of the Apollo 11 lunar module, or LEM, which served as the competition’s starting point. Ramps on either side of the LEM sloped down to a lunar-like surface, littered with “moon rocks” — stones of various sizes and shapes, which, for practical purposes, were of Earthly origin.</p>\n<p>The challenge called for students to maneuver their robots, which either moved autonomously or were remotely controlled, from the LEM’s starting point down to the “lunar” surface to collect as many moon rocks as possible, and return them up to the LEM, within two minutes. Robots gained more points by planting a small flag on a hillside, spinning a wheel to “charge” the LEM’s battery, and pulling a cord to jettison two weights — a particularly tricky task that, if accomplished, would trigger the LEM to “lift off,” to dramatic smoke and sound effects.</p>\n<p>“The competition name is very apropos of the challenge that the students face, because for many of them, making a robot by themselves for the first time is a moonshot,” says Amos Winter, course co-instructor and associate professor of mechanical engineering at MIT. Winter and Associate Professor Sangbae Kim served as the competition’s emcees, both suited up for the occasion in astronaut gear.</p>\n<p>The 2.007 competition is a yearly tradition that dates back to the 1970s, with the course’s first instructor, Woodie Flowers, the Pappalardo Professor Emeritus of Mechanical Engineering, who developed 2.007 as one of the first hands-on, project-based undergraduate courses.</p>\n<p>Each year, at the start of the semester, students are given the same toolbox of parts, including gears, wheels, circuit boards, and microcontrollers. Through lectures and time — lots of time — in the lab, students learn to design and machine their own robot, to carry out that year’s competition challenges.</p>\n<p>This year’s challenge inspired a range of robotic strategies and designs, including a bot, aptly named Scissorlift, that stretched itself up via a scissoring mechanism to plant a flag, and a two-bot system named Lifties, comprising one robot that hoisted rocks up to a second robot via a telescoping arm.</p>\n<p>While most students hoped for a win, sophomore Jaeyoung Jung simply wanted to entertain. After requesting that the event’s overhead music be turned down, Jung, sporting a tuxedo, made some music of his own, playing a recorder that he had rigged to maneuver his robot. Each note he played was converted to electrical signals that were picked up by a computer, which in turn sent a corresponding command to the robot’s controller to spin a wheel and charge up the LEM.</p>\n<p>Though the competition’s first music-controlled robot didn’t make it through the first round, it was met with cheers from an often raucous crowd of family and friends, who were treated sporadically with a confetti of custom-designed foam astronauts fired from an air cannon.</p>\n<p>Among the enthusiastic crowd was Evelyn Wang, head of the Department of Mechanical Engineering, and her two young children, who were seeing the engineering spectacle for the first time. The competition brought back memories for Wang, who participated in 2.007 when she herself was an MIT undergraduate. That year, she recalls having to compete on a game board dubbed “Ballcano,” for a volcano-like structure that spit out balls, which robots had to catch and distribute at various locations across the game board.</p>\n<p>“It was the first time I learned how to design, build, machine, and work with different actuation mechanisms and motors and pneumatics,” says Wang, who proudly remembers taking home fourth place.</p>\n<p>As the night wore on, robots battled over who could scrabble up the most moon rocks, using a variety of designs, from grippers and grabbers to snowplow- and comb-like sweepers, and rotating flippers and flaps. Between each bout, course assistants quickly repositioned the moon rocks and swept the game board of any residual moon rock dust that could make a robot slip. To contend with this potential hazard, some students designed their bots with extra traction, lining their wheels with Velcro or, in the case of one bot named Sloth, rubber bands.</p>\n<p>Sophomore Jessica Xu, whose spiky, rock-snatching “Cactus-bot” made it all the way to the semifinals, says that 2.007’s hands-on experience has helped to steer her toward a mechanically-oriented career.</p>\n<p>“This is my first experience ever even thinking of building a robot,” Xu says. “I started the class googling, ‘What are mechanisms that robots even do?’ Because I wasn’t even sure what the possibilities were. I came into college wanting to do something that applies to health care. Now I’m hoping to concentrate in medical devices, applying the mechanical side. I’m excited to see what it could be.”</p>\n<p>In the end, it was a powerful, motor-heavy bot named Rocky that gobbled up rocks “like Cookie Monster,” as Winter reported to the crowd, that took home the prize. Rocky’s designer, sophomore Sam Ubellacker, says it could have been the bot’s drive train that made the difference. While most students included two motors in their drive trains, Ubellacker opted for four, in order to move twice as fast as his competitors — an 11th-hour decision that ultimately paid off.</p>\n<p>“I pretty much redesigned my entire robot the week before this competition, because I realized my other one wasn’t going to score any points,” says Ubellacker, who, as it happens, has kept up the family tradition — his brother Wyatt won first prize in 2011. “I’ve probably worked about 100 hours this week on this robot. I’m just glad that it worked out.”</p>\n<p>He credits his success, and all the know-how he’s gained throughout the semester, to all the experts behind 2.007.</p>\n<p>“I didn’t know much about machining going in,” Ubellacker says. “Interacting with the machinists and the staff will be my most memorable experiences. They’re all really cool people, and they shared all this knowledge with me. This was all really great.”</p>","descriptionType":"html","publishedDate":"Fri, 10 May 2019 20:35:30 +0000","feedId":12364,"bgimg":"","linkMd5":"571aa36bc43db300a16d06224bfbcfcd","bgimgJsdelivr":"","metaImg":"","author":"Jennifer Chu | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"To self-drive in the snow, look under the road","link":"https://news.mit.edu/2020/to-self-drive-in-snow-look-under-road-0226","description":"<p>Car companies have been feverishly working to improve the technologies behind self-driving cars. But so far even the most high-tech vehicles still fail when it comes to safely navigating in rain and snow.&nbsp;</p> \n<p>This is because these weather conditions wreak havoc on the most common approaches for sensing, which usually involve either lidar sensors or cameras. In the snow, for example, cameras can no longer recognize lane markings and traffic signs, while the lasers of lidar sensors malfunction when there’s, say, stuff flying down from the sky.</p> \n<p>MIT researchers have recently been wondering whether an entirely different approach might work. Specifically, what if we instead looked under the road?&nbsp;</p> \n<p>A team from MIT’s <a href=\"http://csail.mit.edu\">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) has developed a new system that uses an existing technology called ground-penetrating radar (GPR) to send electromagnetic pulses underground that measure the area’s specific combination of soil, rocks, and roots. Specifically, the CSAIL team used a particular form of GPR instrumentation developed at <a href=\"https://www.ll.mit.edu/\" target=\"_blank\">MIT Lincoln Laboratory</a> called <a href=\"http://news.mit.edu/2016/pinpointing-vehicles-with-high-precision-under-adverse-weather-conditions-0623\" target=\"_self\">localizing ground-penetrating radar</a>, or LGPR. The mapping process creates a unique fingerprint of sorts that the car can later use to localize itself when it returns to that particular plot of land.</p> \n<p>“If you or I grabbed a shovel and dug it into the ground, all we’re going to see is a bunch of dirt,” says CSAIL PhD student Teddy Ort, lead author on a new paper about the project that will be published in the <em>IEEE Robotics and Automation Letters</em> journal later this month. “But LGPR can quantify the specific elements there and compare that to the map it’s already created, so that it knows exactly where it is, without needing cameras or lasers.”</p> \n<p>In tests, the team found that in snowy conditions the navigation system’s average margin of error was on the order of only about an inch compared to clear weather. The researchers were surprised to find that it had a bit more trouble with rainy conditions, but was still only off by an average of 5.5 inches. (This is because rain leads to more water soaking into the ground, leading to a larger disparity between the original mapped LGPR reading and the current condition of the soil.)</p> \n<p>The researchers said the system’s robustness was further validated by the fact that, over a period of six months of testing, they never had to unexpectedly step in to take the wheel.&nbsp;</p> \n<p>“Our work demonstrates that this approach is actually a practical way to help self-driving cars navigate poor weather without actually having to be able to ‘see’ in the traditional sense using laser scanners or cameras,” says MIT Professor Daniela Rus, director of CSAIL and senior author on the new paper, which will also be presented in May at the International Conference on Robotics and Automation in Paris.</p> \n<p>While the team has only tested the system at low speeds on a closed country road, Ort said that existing work from Lincoln Laboratory suggests that the system could easily be extended to highways and other high-speed areas.&nbsp;</p> \n<p>This is the first time that developers of self-driving systems have employed ground-penetrating radar, which has previously been used in fields like construction planning, landmine detection, and even <a href=\"http://ieeexplore.ieee.org/document/6970584\" target=\"_blank\">lunar exploration</a>. The approach wouldn’t be able to work completely on its own, since it can’t detect things above ground. But its ability to localize in bad weather means that it would couple nicely with lidar and vision approaches.</p> \n<p>“Before releasing autonomous vehicles on public streets, localization and navigation have to be totally reliable at all times,” says Roland Siegwart, a professor of autonomous systems at ETH Zurich who was not involved in the project. “The CSAIL team’s innovative and novel concept has the potential to push autonomous vehicles much closer to real-world deployment.”&nbsp;</p> \n<p>One major benefit of mapping out an area with LGPR is that underground maps tend to hold up better over time than maps created using vision or lidar, since features of an above-ground map are much more likely to change. LGPR maps also take up only about 80 percent of the space used by traditional 2D sensor maps that many companies use for their cars.&nbsp;</p> \n<p>While the system represents an important advance, Ort notes that it’s far from road-ready. Future work will need to focus on designing mapping techniques that allow LGPR datasets to be stitched together to be able to deal with multi-lane roads and intersections. In addition, the current hardware is bulky and 6 feet wide, so major design advances need to be made before it’s small and light enough to fit into commercial vehicles.</p> \n<p>Ort and Rus co-wrote the paper with CSAIL postdoc Igor Gilitschenski. The project was supported, in part, by MIT Lincoln Laboratory.</p>","descriptionType":"html","publishedDate":"Wed, 26 Feb 2020 19:50:00 +0000","feedId":12364,"bgimg":"","linkMd5":"3d13a695f9e75b903a0f1f1f05dae55a","bgimgJsdelivr":"","metaImg":"","author":"Adam Conner-Simons | MIT CSAIL","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Intelligent Towing Tank propels human-robot-computer research","link":"https://news.mit.edu/2019/intelligent-towing-tank-propels-research-1209","description":"<p>In its first year of operation, the Intelligent Towing Tank (ITT) conducted about 100,000 total experiments, essentially completing the equivalent of a PhD student’s five years’ worth of experiments in a matter of weeks.</p> \n<p>The automated experimental facility, developed in the MIT Sea Grant Hydrodynamics Laboratory, automatically and adaptively performs, analyzes, and designs experiments exploring vortex-induced vibrations (VIVs). Important for engineering offshore ocean structures like marine drilling risers that connect underwater oil wells to the surface, VIVs remain somewhat of a phenomenon to researchers due to the high number of parameters involved.</p> \n<p>Guided by active learning, the ITT conducts series of experiments wherein the parameters of each next experiment are selected by a computer. Using an “explore-and-exploit” methodology, the system dramatically reduces the number of experiments required to explore and map the complex forces governing VIVs.</p> \n<p>What began as then-PhD candidate Dixia Fan’s quest to cut back on conducting a thousand or so laborious experiments — by hand — led to the design of the innovative system and a <a href=\"https://robotics.sciencemag.org/content/4/36/eaay5063\">paper</a> recently published in the journal <em>Science Robotics</em>.</p> \n<p>Fan, now a postdoc, and a team of researchers from the MIT Sea Grant College Program and MIT’s Department of Mechanical Engineering, École Normale Supérieure de Rennes, and Brown University, reveal a potential paradigm shift in experimental research, where humans, computers, and robots can collaborate more effectively to accelerate scientific discovery.</p> \n<p>The 33-foot whale of a tank comes alive, working without interruption or supervision on the venture at hand — in this case, exploring a canonical problem in the field of fluid-structure interactions. But the researchers envision applications of the active learning and automation approach to experimental research across disciplines, potentially leading to new insights and models in multi-input/multi-output nonlinear systems.</p> \n<p>VIVs are inherently-nonlinear motions induced on a structure&nbsp;in an oncoming irregular cross-stream, which prove vexing to study. The researchers report that the number of experiments completed by the ITT is already comparable to the total number of experiments done to date worldwide on the subject of VIVs.</p> \n<p>The reason for this is the large number of independent parameters, from flow velocity to pressure, involved in studying the complex forces at play. According to Fan, a systematic brute-force approach — blindly conducting 10 measurements per parameter in an eight-dimensional parametric space — would require 100 million experiments.</p> \n<p>With the ITT, Fan and his collaborators have taken the problem into a wider parametric space than previously practicable to explore. “If we performed traditional techniques on the problem we studied,” he explains, “it would take 950 years to finish the experiment.” Clearly infeasible, so Fan and the team integrated a Gaussian process regression learning algorithm into the ITT. In doing so, the researchers reduced the experimental burden by several orders of magnitude, requiring only a few thousand experiments.</p> \n<p>The robotic system automatically conducts an initial sequence of experiments, periodically towing a submerged structure along the length of the tank at a constant velocity. Then, the ITT takes partial control over the parameters of each next experiment by minimizing suitable acquisition functions of quantified uncertainties and adapting to achieve a range of objectives, like reduced drag.</p> \n<p>Earlier this year, Fan was awarded an&nbsp;MIT Mechanical Engineering de Florez Award for \"Outstanding Ingenuity and Creative Judgment\"&nbsp;in the development of the ITT. “Dixia’s design of the Intelligent Towing Tank is an outstanding example of using novel methods to reinvigorate mature fields,” says Michael Triantafyllou, Henry L. and Grace Doherty Professor in Ocean Science and Engineering, who acted as Fan’s doctoral advisor.</p> \n<p>Triantafyllou, a co-author on this paper and the director of the MIT Sea Grant College Program, says, “MIT Sea Grant has committed resources and funded projects using deep-learning methods in ocean-related problems for several years that are already paying off.” Funded by the National Oceanic and Atmospheric Administration and administered by the National Sea Grant Program, MIT Sea Grant is a federal-Institute partnership that brings the research and engineering core of MIT&nbsp;to bear on ocean-related challenges.</p> \n<p>Fan’s research points to a number of others utilizing automation and artificial intelligence in science: At Caltech, a robot scientist named “Adam” generates and tests hypotheses; at the Defense Advanced Research Projects Agency, the Big Mechanism program reads tens of thousands of research papers to generate new models.</p> \n<p>Similarly, the ITT applies human-computer-robot collaboration to accelerate experimental efforts. The system demonstrates a potential paradigm shift in conducting research, where automation and uncertainty quantification can considerably accelerate scientific discovery. The researchers assert that the machine learning methodology described in this paper can be adapted and applied in and beyond fluid mechanics, to other experimental fields.</p> \n<p>Other contributors to the paper include George Karniadakis from Brown University, who is also affiliated with MIT Sea Grant; Gurvan Jodin from ENS Rennes; MIT PhD candidate in mechanical engineering Yu Ma; and Thomas Consi, Luca Bonfiglio, and Lily Keyes from MIT Sea Grant.</p> \n<p>This work was supported by DARPA, Fariba Fahroo, and Jan Vandenbrande through an EQUiPS (Enabling Quantification of Uncertainty in Physical Systems) grant, as well as Shell, Subsea 7, and the MIT Sea Grant College Program.</p>","descriptionType":"html","publishedDate":"Mon, 09 Dec 2019 21:05:01 +0000","feedId":12364,"bgimg":"","linkMd5":"12a9b684d9e5140a5cd308d29225d62f","bgimgJsdelivr":"","metaImg":"","author":"MIT Sea Grant","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Muscle signals can pilot a robot","link":"https://news.mit.edu/2020/conduct-a-bot-muscle-signals-can-pilot-robot-mit-csail-0427","description":"<p>Albert Einstein famously postulated that “the only real valuable thing is intuition,” arguably one of the most important keys to understanding intention and communication.&nbsp;</p> \n<p>But intuitiveness is hard to teach — especially to a machine. Looking to improve this, a team from MIT’s <a href=\"http://csail.mit.edu\">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) came up with a method that dials us closer to more seamless human-robot collaboration. The system, called “Conduct-A-Bot,” uses human muscle signals from wearable sensors to pilot a robot’s movement.&nbsp;</p> \n<p>“We envision a world in which machines help people with cognitive and physical work, and to do so, they adapt to people rather than the other way around,” says Professor Daniela Rus, director of CSAIL, deputy dean of research for the MIT Stephen A. Schwarzman College of Computing, and co-author on a paper about the system.&nbsp;</p> \n<p>To enable seamless teamwork between people and machines, electromyography and motion sensors are worn on the biceps, triceps, and forearms to measure muscle signals and movement. Algorithms then process the signals to detect gestures in real time, without any offline calibration or per-user training data. The system uses just two or three wearable sensors, and nothing in the environment — largely reducing the barrier to casual users interacting with robots.</p> \n<p>While Conduct-A-Bot could potentially be used for various scenarios, including navigating menus on electronic devices or supervising autonomous robots, for this research the team used a Parrot Bebop 2 drone, although any commercial drone could be used.</p> \n<p>By detecting actions like rotational gestures, clenched fists, tensed arms, and activated forearms, Conduct-A-Bot can move the drone left, right, up, down, and forward, as well as allow it to rotate and stop.&nbsp;</p> \n<p>If you gestured toward the right to your friend, they could likely interpret that they should move in that direction. Similarly, if you waved your hand to the left, for example, the drone would follow suit and make a left turn.&nbsp;</p> \n<p>In tests, the drone correctly responded to 82 percent of over 1,500 human gestures when it was remotely controlled to fly through hoops. The system also correctly identified approximately 94 percent of cued gestures when the drone was not being controlled.</p> \n<p>“Understanding our gestures could help robots interpret more of the nonverbal cues that we naturally use in everyday life,” says Joseph DelPreto, lead author on the new paper. “This type of system could help make interacting with a robot more similar to interacting with another person, and make it easier for someone to start using robots without prior experience or external sensors.”&nbsp;</p> \n<p>This type of system could eventually target a range of applications for human-robot collaboration, including remote exploration, assistive personal robots, or manufacturing tasks like delivering objects or lifting materials.&nbsp;</p> \n<p>These intelligent tools are also consistent with social distancing — and could potentially open up a realm of future contactless work. For example, you can imagine machines being controlled by humans to safely clean a hospital room, or drop off medications, while letting us humans stay a safe distance.</p>\n<p>Muscle signals can often provide information about states that are hard to observe from vision, such as joint stiffness or fatigue.&nbsp;&nbsp;&nbsp;&nbsp;</p> \n<p>For example, if you watch a video of someone holding a large box, you might have difficulty guessing how much effort or force was needed — and a machine would also have difficulty gauging that from vision alone. Using muscle sensors opens up possibilities to estimate not only motion, but also the force and torque required to execute that physical trajectory.</p> \n<p>For the gesture vocabulary currently used to control the robot, the movements were detected as follows:&nbsp;</p> \n<ul> \n <li> <p>stiffening the upper arm to stop the robot (similar to briefly cringing when seeing something going wrong): biceps and triceps muscle signals;</p> </li> \n <li> <p>waving the hand left/right and up/down to move the robot sideways or vertically: forearm muscle signals (with the forearm accelerometer indicating hand orientation);</p> </li> \n <li> <p>fist clenching to move the robot forward: forearm muscle signals; and</p> </li> \n <li> <p>rotating clockwise/counterclockwise to turn the robot: forearm gyroscope.</p> </li> \n</ul> \n<p>Machine learning classifiers detected the gestures using the wearable sensors. Unsupervised classifiers processed the muscle and motion data and clustered it in real time to learn how to separate gestures from other motions. A neural network also predicted wrist flexion or extension from forearm muscle signals.&nbsp;&nbsp;</p> \n<p>The system essentially calibrates itself to each person's signals while they're making gestures that control the robot, making it faster and easier for casual users to start interacting with robots.</p> \n<p>In the future, the team hopes to expand the tests to include more subjects. And while the movements for Conduct-A-Bot cover common gestures for robot motion, the researchers want to extend the vocabulary to include more continuous or user-defined gestures. Eventually, the hope is to have the robots learn from these interactions to better understand the tasks and provide more predictive assistance or increase their autonomy.&nbsp;</p> \n<p>“This system moves one step closer to letting us work seamlessly with robots so they can become more effective and intelligent tools for everyday tasks,” says DelPreto. “As such collaborations continue to become more accessible and pervasive, the possibilities for synergistic benefit continue to deepen.”&nbsp;</p> \n<p>DelPreto and Rus presented the paper virtually earlier this month at the ACM/IEEE International Conference on Human Robot Interaction.</p>","descriptionType":"html","publishedDate":"Mon, 27 Apr 2020 17:30:01 +0000","feedId":12364,"bgimg":"","linkMd5":"a41bc9033f0d4ac382988649eb74538b","bgimgJsdelivr":"","metaImg":"","author":"Rachel Gordon | CSAIL","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"This robot helps you lift objects — by looking at your biceps","link":"https://news.mit.edu/2019/robots-help-you-lift-objects-looking-your-biceps-0522","description":"<p>We humans are very good at collaboration. For instance, when two people work together to carry a heavy object like a table or a sofa, they tend to instinctively coordinate their motions, constantly recalibrating to make sure their hands are at the same height as the other person’s. Our natural ability to make these types of adjustments allows us to collaborate on tasks big and small.</p> \n<p>But a computer or a robot still can’t follow a human’s lead with ease. We usually either explicitly program them using machine-speak, or train them to understand our words, à la virtual assistants like Siri or Alexa.</p> \n<p>In contrast, researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) recently showed that a smoother robot-human collaboration is possible through a new system they developed, where machines help people lift objects by monitoring their muscle movements.</p> \n<p>Dubbed <a href=\"http://people.csail.mit.edu/delpreto/icra2019/delpreto_emg_team_lifting_ICRA19.pdf\" target=\"_blank\">RoboRaise</a>, the system involves putting electromyography (EMG) sensors on a user’s biceps and triceps to monitor muscle activity. Its algorithms then continuously detect changes to the person’s arm level, as well as discrete up-and-down hand gestures the user might make for finer motor control.</p>\n<p>The team used the system for a series of tasks involving picking up and assembling mock airplane components. In experiments, users worked on these tasks with the robot and were able to control it to within a few inches of the desired heights by lifting and then tensing their arm. It was more accurate when gestures were used, and the robot responded correctly to roughly 70 percent of all gestures.</p> \n<p>Graduate student Joseph DelPreto says he could imagine people using RoboRaise to help in manufacturing and construction settings, or even as an assistant around the house.</p> \n<p>“Our approach to lifting objects with a robot aims to be intuitive and similar to how you might lift something with another person — roughly copying each other's motions while inferring helpful adjustments,” says DelPreto, lead author on a new paper about the project with MIT Professor and CSAIL Director Daniela Rus. “The key insight is to use nonverbal cues that encode instructions for how to coordinate, for example to lift a little higher or lower. Using muscle signals to communicate almost makes the robot an extension of yourself that you can fluidly control.”</p> \n<p>The project builds off the team’s <a href=\"http://news.mit.edu/2018/how-to-control-robots-with-brainwaves-hand-gestures-mit-csail-0620\">existing system</a> that allows users to instantly correct robot mistakes with brainwaves and hand gestures, now enabling continuous motion in a more collaborative way. “We aim to develop human-robot interaction where the robot adapts to the human, rather than the other way around. This way the robot becomes an intelligent tool for physical work,” says Rus.</p>\n<p>EMG signals can be tricky to work with: They’re often very noisy, and it can be difficult to predict exactly how a limb is moving based on muscle activity. Even if you can estimate how a person is moving, how you want the robot itself to respond may be unclear.</p> \n<p>RoboRaise gets around this by putting the human in control. The team’s system uses noninvasive, on-body sensors that detect the firing of neurons as you tense or relax muscles. Using wearables also gets around problems of occlusions or ambient noise, which can complicate tasks involving vision or speech.</p> \n<p>RoboRaise’s algorithm then processes biceps activity to estimate how the person’s arm is moving so the robot can roughly mimic it, and the person can slightly tense or relax their arm to move the robot up or down. If a user needs the robot to move farther away from their own position or hold a pose for a while, they can just gesture up or down for finer control; a neural network detects these gestures at any time based on biceps and triceps activity.</p> \n<p>A new user can start using the system very quickly, with minimal calibration. After putting on the sensors, they just need to tense and relax their arm a few times then lift a light weight to a few heights. The neural network that detects gestures is only trained on data from previous users.</p> \n<p>The team tested the system with 10 users through a series of three lifting experiments: one where the robot didn’t move at all, another where the robot moved in response to their muscles but didn’t help lift the object, and a third where the robot and person lifted an object together.</p> \n<p>When the person had feedback from the robot — when they could see it moving or when they were lifting something together — the achieved height was significantly more accurate compared to having no feedback.</p> \n<p>The team also tested RoboRaise on assembly tasks, such as lifting a rubber sheet onto a base structure. It was able to successfully lift both rigid and flexible objects onto the bases. RoboRaise was implemented on the team’s Baxter humanoid robot, but the team says it could be adapted for any robotic platform.</p> \n<p>In the future, the team hopes that adding more muscles or different types of sensors to the system will increase the degrees of freedom, with the ultimate goal of doing even more complex tasks. Cues like exertion or fatigue from muscle activity could also help robots provide more intuitive assistance. The team tested one version of the system that uses biceps and triceps levels to tell the robot how stiffly the person is holding their end of the object; together, the human and machine could fluidly drag an object around or rigidly pull it taut.</p> \n<p>The team will present their work at the International Conference on Robotics and Automation this week in Montreal, Canada. The project was funded in part by The Boeing Company.</p>","descriptionType":"html","publishedDate":"Wed, 22 May 2019 14:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"218fc5daf1aba8c0ff41382779e0a1ae","bgimgJsdelivr":"","metaImg":"","author":"Adam Conner-Simons | Rachel Gordon | CSAIL","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Photorealistic simulator made MIT robot racing competition a live online experience","link":"https://news.mit.edu/2020/photorealistic-simulator-made-mit-robot-racing-competition-live-online-experience-0609","description":"<p>Every spring, the basement of the Ray and Maria Stata Center becomes a racetrack for tiny self-driving cars that tear through the halls one by one. Sprinting behind each car on foot is a team of three to six students, sometimes carrying wireless routers or open laptops extended out like Olympic torches. Lining the basement walls, their classmates cheer them on, knowing the effort it took to program the algorithms steering the cars around the course during this annual MIT autonomous racing competition.</p> \n<p>The competition is the final project for Course 6.141/16.405 (Robotics: Science and Systems). It’s an end-of-semester event that gets pulses speeding, and prizes are awarded for finishing different race courses with the fastest times out of 20 teams.</p> \n<p>With campus evacuated this spring due to the Covid-19 pandemic, however, not a single robotic car burned rubber in the Stata Center basement. Instead, a new race was on as Luca Carlone, the Charles Stark Draper Assistant Professor of Aeronautics and Astronautics and member of the Institute for Data, Systems, and Society; Nicholas Roy, professor of aeronautics and astronautics; and teaching assistants (TAs) including Marcus Abate, Lukas Lao Beyer, and Caris Mariah Moses had only four weeks to figure out how to bring the excitement of this highly-anticipated race online.</p> \n<p>Because the lab sometimes uses a simple simulator for other research, Carlone says they considered taking the race in that direction. With this simple simulator, students could watch as their self-driving cars snaked around a flat map, like a car depicted by a dot moving along a GPS navigation system. Ultimately, they decided that wasn’t the right route. The racing competition needed to be noisy. Realistic. Exciting. The dynamics of the car needed to be nearly as complex as the robotic cars the students had planned to use. Building on his prior research in collaboration with MIT Lincoln Laboratory, Abate worked with Lao Beyer and engineering graduate student Sabina Chen to develop a new photorealistic simulator at the last minute.</p> \n<p>The race was back on, and Carlone was impressed by how everything from the cityscape to the sleek car designs looked “as realistic as possible.”</p> \n<p>“The modifications involved introducing an outdoor environment based on open-source assets, building in realistic car dynamics for the agent, and adding lidar sensors,” Abate says. “I also had to revamp the interfacing with Python and Robot Operating System (ROS) to make it all plug-and-play for the students.”</p> \n<p>What that means is that the race ran a lot like a racing game, such as Gran Turismo or Forza. Only instead of sitting on your couch thumbing the joystick to direct the car, students developed algorithms to anticipate every roadblock and bend ahead. For students, programming for this new environment was perhaps the biggest adjustment. “The simulator used an outdoor scene and a full-sized car with a very different dynamics model than the real-life race car in the Stata basement,” Abate says.</p> \n<p>The TAs also had to adjust to complications behind the scenes of the race’s new setting. “A huge amount of effort was put into the new simulator, as well as into the logistics of obtaining and evaluating students' software,” Lao Beyer says. “Usually, teams are able to configure the software on their race car however they want, but it is very difficult to accommodate for such a diversity of software setups in the virtual race.”</p>\n<p>Once the simulator was ready, there was no time to troubleshoot, so TAs made themselves available to debug on the fly any issues that arose. “I think that saved the day for the final project and the final race,” Carlone says.</p> \n<p>Programming their autonomous racing code wasn’t the only way that students customized their race experience, though. Co-instructor Jane Abbott brought Writing, Rhetoric, and Professional Communication (WRAP) into the course. As coordinator of the communication-intensive team that focused on helping teams work effectively, she says she was worried the silence that often looms on Zoom would suck out all the energy of the race. She suggested the TAs add a soundtrack.</p> \n<p>In the end, the remote race ran for nearly four hours, bringing together more than 100 people in one Zoom call with commentators and Mario Kart music playing. “We got to watch every student’s solution with some cool visualization code running that showed the trajectory and any obstacles hit,” says Samuel Ubellacker, an electrical engineering and computer science student who raced this year. “We got to see how each team's solution ran much clearer in the simulator because the camera was always following the race car.”</p> \n<p>For Yorai Shaoul, another electrical engineering and computer science student in the race, getting out of the basement helped him become more engaged with other teams’ projects. “Before leaving campus, we found ourselves working long hours in the Stata basement,” Shaoul says. “So focused on our robot, we failed to notice that other teams were right there next to us the whole time.”</p> \n<p>During the race, other programming solutions his team had overlooked became clear. “The TAs showcased and narrated each team’s run, finally allowing us to see the diverse approaches other teams were developing,” Shaoul says.</p> \n<p>“One thing that was nice: When we've done it live in the tunnels, you can only see a part of it,” Abbott says. “You sort of stand at a fixed point and you see the car go by. It’s like watching the marathon: you see the runners for 100 yards and then they're gone.”</p> \n<p>Over Zoom, participants could watch every impressive cruise and spectacular crash as it happened, plus replays. Many stayed to watch, and Lao Beyer says, “We managed to retain as much excitement and suspense about the final challenge as possible.” Ubellacker agrees: “It was certainly an unforgettable experience!”</p> \n<p>For those students who don’t bro down with Mario, they could also choose the music they wanted to accompany their races. “Near, far, wherever you are,” these lyrics from one team’s choice to use the “Titanic” movie theme “My Heart Will Go On” are a wink to the extra challenge of collaborating as teams at a distance.</p> \n<p>One of the masters of ceremonies for the 2020 race, Marwa Abdulhai '20, was a TA last year and says one obvious benefit of the online race is that it’s a lot easier to figure out why your car crashed. “Pros of this virtual approach have been allowing students to race through the track multiple times and knowing that the car’s performance was primarily due to the algorithm and not any physical constraints,” Abdulhai says.</p> \n<p>For Ubellacker that was actually a con, though: “The biggest element that I missed without having a physical car was not being able to experience the differences between simulation and real life.” He says, “Part of the fun to me is designing a system that works perfectly in the simulator, and then getting to figure out all the crazy ways it will fail in the real world!”</p> \n<p>Shaoul says instead of working on one car, sometimes it felt like they were working on five individual cars that lived on each team member’s computer. “With one car, it was easy to see how well it did and what required fixing, whereas virtually it was more ambiguous,” Shaoul says. “We faced challenges with keeping track of up-to-date code versions and also simple communication.”</p> \n<p>Carlone was concerned students wouldn’t be as invested in their algorithms without the experience of seeing the car’s performance play out in real life to motivate them to push harder. “Every year, the record time on that Stata Center track was getting better and better,” he says. “This year, we were a bit concerned about the performance.”</p> \n<p>Fortunately, many students were very much still in the race, with some teams beating the most optimistic predictions, despite having to adjust to new racing conditions and greater challenges collaborating as a team fully online. The winning students completed the race courses sometimes three times faster than other teams, without any collisions. “It was just beyond expectation,” Carlone says.</p>\n<p>Although this shift in the final project somewhat changed the takeaways from the course, Carlone says the experience will still advance algorithmic skills for students working on robotics, as well as introducing them to the intensity of communication required to work effectively as remote teams. “Many robotics groups are doing research using photorealistic simulation, because you can test conditions that you cannot test on the real robot,” he says. Co-instructor Roy says it worked so well, the new simulator might become a permanent feature of the course — not to replace the physical race, but as an extra element. “The robotics experience was good,” Carlone says of the 2020 race, but still: “The human experience is, of course, different.”</p>","descriptionType":"html","publishedDate":"Tue, 09 Jun 2020 19:20:01 +0000","feedId":12364,"bgimg":"","linkMd5":"07f0ea2e74b862826bf8be28a0680f92","bgimgJsdelivr":"","metaImg":"","author":"Ashley Belanger | School of Engineering","publishedOrCreatedDate":1598320622357},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Algorithm tells robots where nearby humans are headed","link":"https://news.mit.edu/2019/robots-predict-human-movement-0611","description":"<p>In 2018, researchers at MIT and the auto manufacturer BMW were testing ways in which humans and robots might work in close proximity to assemble car parts. In a replica of a factory floor setting, the team rigged up a robot on rails, designed to deliver parts between work stations. Meanwhile, human workers crossed its path every so often to work at nearby stations.&nbsp;</p>\n<p>The robot was programmed to stop momentarily if a person passed by. But the researchers noticed that the robot would often freeze in place, overly cautious, long before a person had crossed its path. If this took place in a real manufacturing setting, such unnecessary pauses could accumulate into significant inefficiencies.</p>\n<p>The team traced the problem to a limitation in the robot’s trajectory alignment algorithms used by the robot’s motion predicting software. While they could reasonably predict where a person was headed, due to the poor time alignment the algorithms couldn’t anticipate how long that person spent at any point along their predicted path — and in this case, how long it would take for a person to stop, then double back and cross the robot’s path again.</p>\n<p>Now, members of that same MIT team have come up with a solution: an algorithm that accurately aligns partial trajectories in real-time, allowing motion predictors to accurately anticipate the timing of a person’s motion. When they applied the new algorithm to the BMW factory floor experiments, they found that, instead of freezing in place, the robot simply rolled on and was safely out of the way by the time the person walked by again.</p>\n<p>“This algorithm builds in components that help a robot understand and monitor stops and overlaps in movement, which are a core part of human motion,” says Julie Shah, associate professor of aeronautics and astronautics at MIT. “This technique is one of the many way we’re working on robots better understanding people.”</p>\n<p>Shah and her colleagues, including project lead and graduate student Przemyslaw “Pem” Lasota, will present their results this month at the Robotics: Science and Systems conference in Germany.</p>\n<p><strong>Clustered up</strong></p>\n<p>To enable robots to predict human movements, researchers typically borrow algorithms from music and speech processing. These algorithms are designed to align two complete time series, or sets of related data, such as an audio track of a musical performance and a scrolling video of that piece’s musical notation.</p>\n<p>Researchers have used similar alignment algorithms to sync up real-time and previously recorded measurements of human motion, to predict where a person will be, say, five seconds from now. But unlike music or speech, human motion can be messy and highly variable. Even for repetitive movements, such as reaching across a table to screw in a bolt, one person may move slightly differently each time.</p>\n<p>Existing algorithms typically take in streaming motion data, in the form of dots representing the position of a person over time, and compare the trajectory of those dots to a library of common trajectories for the given scenario. An algorithm maps a trajectory in terms of the relative distance between dots.</p>\n<p>But Lasota says algorithms that predict trajectories based on distance alone can get easily confused in certain common situations, such as temporary stops, in which a person pauses before continuing on their path. While paused, dots representing the person’s position can bunch up in the same spot.</p>\n<p>“When you look at &nbsp;the data, you have a whole bunch of points clustered together when a person is stopped,” Lasota says. “If you’re only looking at the distance between points as your alignment metric, that can be confusing, because they’re all close together, and you don’t have a good idea of which point you have to align to.”</p>\n<p>The same goes with overlapping trajectories — instances when a person moves back and forth along a similar path. Lasota says that while a person’s current position may line up with a dot on a reference trajectory, existing algorithms can’t differentiate between whether that position is part of a trajectory heading away, or coming back along the same path.</p>\n<p>“You may have points close together in terms of distance, but in terms of time, a person’s position may actually be far from a reference point,” Lasota says.</p>\n<p><strong>It’s all in the timing</strong></p>\n<p>As a solution, Lasota and Shah devised a “partial trajectory” algorithm that aligns segments of a person’s trajectory in real-time with a library of previously collected reference trajectories. Importantly, the new algorithm aligns trajectories in both distance and timing, and in so doing, is able to accurately anticipate stops and overlaps in a person’s path.</p>\n<p>“Say you’ve executed this much of a motion,” Lasota explains. “Old techniques will say, ‘this is the closest point on this representative trajectory for that motion.’ But since you only completed this much of it in a short amount of time, the timing part of the algorithm will say, ‘based on the timing, it’s unlikely that you’re already on your way back, because you just started your motion.’”</p>\n<p>The team tested the algorithm on two human motion datasets: one in which a person intermittently crossed a robot’s path in a factory setting (these data were obtained from the team’s experiments with BMW), and another in which the group previously recorded hand movements of participants reaching across a table to install a bolt that a robot would then secure by brushing sealant on the bolt.</p>\n<p>For both datasets, the team’s algorithm was able to make better estimates of a person’s progress through a trajectory, compared with two commonly used partial trajectory alignment algorithms. Furthermore, the team found that when they integrated the alignment algorithm with their motion predictors, the robot could more accurately anticipate the timing of a person’s motion. In the factory floor scenario, for example, they found the robot was less prone to freezing in place, and instead smoothly resumed its task shortly after a person crossed its path.</p>\n<p>While the algorithm was evaluated in the context of motion prediction, it can also be used as a preprocessing step for other techniques in the field of human-robot interaction, such as action recognition and gesture detection. Shah says the algorithm will be a key tool in enabling robots to recognize and respond to patterns of human movements and behaviors. Ultimately, this can help humans and robots work together in structured environments, such as factory settings and even, in some cases, the home.</p>\n<p>“This technique could apply to any environment where humans exhibit typical patterns of behavior,” Shah says. “The key is that the [robotic] system can observe patterns that occur over and over, so that it can learn something about human behavior. This is all in the vein of work of the robot better understand aspects of human motion, to be able to collaborate with us better.”</p>\n<p>This research was funded, in part, by a NASA Space Technology Research Fellowship and the National Science Foundation.</p>","descriptionType":"html","publishedDate":"Tue, 11 Jun 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"68076e5de9630fddc90978bb85a1f385","bgimgJsdelivr":"","metaImg":"","author":"Jennifer Chu | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Professor Emeritus Woodie Flowers, innovator in design and engineering education, dies at 75","link":"https://news.mit.edu/2019/Professor-Emeritus-Woodie-Flowers-dies-75-1014","description":"<p>Woodie Flowers SM ’68, MEng ’71, PhD ’73, the Pappalardo Professor Emeritus of Mechanical Engineering, passed away on Oct. 11 at the age of 75. Flowers’ passion for design and his infectious kindness have impacted countless engineering students across the world.</p>\n<p>Flowers was instrumental in shaping MIT’s hands-on approach to engineering design education, first developing teaching methods and learning opportunities that culminated in a design competition for class 2.70, now called <a href=\"https://me-2007.mit.edu/\">2.007 (Design and Manufacturing I)</a>. This annual MIT event, which has now been held for nearly five decades, has impacted generations of students and has been emulated at universities around the world. Flowers expanded this concept to high school and elementary school students, working to help found the world-wide <a href=\"https://www.firstinspires.org/\">FIRST Robotics Competition</a>, which has introduced millions of children to science and engineering.</p>\n<p>Born in 1943, Flowers was reared in Jena, Louisiana. He became interested in mechanical engineering and design at a young age thanks in large part to his mother and his father, who was a welder with a penchant for tinkering and building. Growing up, Flowers also expressed a love of nature and traveling. When he wasn’t working on cars or building rockets as a teenager, he was camping with his family in Louisiana or collecting butterflies. This interest in nature led to an award-winning science fair project on the impact the environment has on <em>Lepidoptera</em>. Flowers’ passion for both building and nature also helped him earn the rank of Eagle Scout.</p>\n<p>Flowers received his bachelor’s degree in engineering from Louisiana Tech University in 1966. After graduating, he spent a summer as an engineering trainee for the Humble Oil Company before enrolling in MIT for graduate school. He received his master’s of science in mechanical engineering in 1968 and an engineer’s degree in 1971. Two years later he earned his doctoral degree under the supervision of the late Professor Robert Mann. For his thesis, Flowers designed a “man-interactive simulator system” for the development of prosthetics for above-knee amputees. He would continue to design above-knee prosthetics throughout his career.</p>\n<p>As Flower’s academic career progressed, his wife Margaret acted as a partner in everything he did. Early in their marriage, when Flowers was just starting out at MIT, she worked to support their family financially. Later in life, she left her own career to partner with Flowers on his work in FIRST.</p>\n<p>After earning his PhD, Flowers joined MIT’s faculty as assistant professor of mechanical engineering. Within his first year he was teaching what was then known as 2.70, now called 2.007. Under Flowers’ leadership, the class evolved into a hands-on experience for undergraduate students, culminating in a final robot competition.</p>\n<p>“From the beginning 2.007/2.70 was about building a device to accomplish a task,” Flowers explained in a <a href=\"https://youtu.be/dDbPwLODAx0\">2015 video</a>. Students were given an assortment of materials to design and build their devices. In the 1970s these materials included tongue depressors and rubber bands, but over the years the competition has gone on to include 3D-printed parts and computer chips.</p>\n<p>Despite the increased sophistication, according to Flowers the core of the course remained unchanged. “Some of the stuff that stayed the same is the wonderful way you compete like crazy but help each other out,” he said. Flowers would coin the phrase “gracious professionalism” to describe this idea of being kind and respecting and valuing others, even in the heat of competition.</p>\n<p>PBS highlighted Flowers’ innovative educational approach to class 2.007 in a 1981 documentary “Discover: The World of Science.” The network continued to cover the 2.007 robotics competition throughout the 1980s, and nearly a decade later, Flowers hosted the popular PBS series “Scientific American Frontiers,” from 1990-1993. One of the program’s objectives was to get people interested in science and engineering. He was awarded a regional Emmy Award for his work on the series.</p>\n<p>At the same time, Flowers helped develop a new program to inspire young people that built upon the competition he developed for 2.007. He collaborated with Dean Kamen, founder of FIRST (For Inspiration and Recognition of Science and Technology), to develop a robotics competition for high school students. In 1992, the inaugural FIRST Robotics Competition was held, giving high school students from around the world an opportunity to design and build their own robots.</p>\n<p>Over the past three decades,&nbsp;FIRST robotics has grown into a global movement serving 660,000 students from over 100 countries each year. It provides scholarship&nbsp;opportunities totaling over $80 million available to&nbsp;FIRST&nbsp;high school students.&nbsp;Flowers’ mantra of “gracious professionalism” remains at FIRST’s core. In 1996, William P. Murphy, Jr. founded the annual <a href=\"https://www.firstinspires.org/resource-library/frc/submitted-awards#woodieflowers\">Woodie Flowers Award</a> within FIRST to celebrate communication in engineering and design. The award “recognizes an individual who has done an outstanding job of motivation through communication while also challenging the students to be clear and succinct in recognizing the value of communication.”</p>\n<p>While working on FIRST, Flowers continued to have impact on mechanical engineering education, its future directions, and engineers’ professional role in society, in addition to envisioning how the use of digital resources could enhance residential learning.</p>\n<p>At MIT, he served as head for the systems and design division in the Department of Mechanical Engineering in the early 1990s and was named Pappalardo Professor of Mechanical Engineering in 1994.&nbsp; While Flowers retired in 2007, he remained an active member of the MIT community as professor emeritus up until his death.</p>\n<p>Flowers mentored countless engineering students during his 35 years on the MIT faculty. He served as undergraduate and master’s thesis advisor for Megan Smith ’86, SM ’88, former chief technology officer of the United States, as well as doctoral advisor to David Wallace SM ’91, PhD ’95, Ely Sachs ’76, SM ’76, PhD ’83, and Alexander Slocum ’82, SM ’83, PhD ’85, all of whom are professors of mechanical engineering at MIT.</p>\n<p>Flowers has had a lasting impact on the generations of mechanical engineering students he taught. From encouraging students to embrace ambiguity to pulling out a massive dictionary in the middle of class to help students find a precise word to articulate their point, his role in shaping students’ lives went far beyond the tenants of design and engineering. Many of his students who have gone on to be educators themselves have implemented his educational ethos in their own classrooms and labs.&nbsp;&nbsp;</p> \n<p>Throughout his career, Flowers received numerous awards and accolades for his vast contributions to engineering education. The American Society of Mechanical Engineers honored him with both the Ruth and Joel Spira Outstanding Design Educator Award and the Edwin F. Church Medal. Flowers received the J.P. Den Hartog Distinguished Educator Award, was a MacVicar Fellow, and was elected to the National Academy of Engineering. He also served as a distinguished partner and a member of the President's Council at Olin College of Engineering.</p>\n<p>Flowers is survived by his beloved wife Margaret Flowers of Weston, Massachusetts, his sister, Kay Wells of St. Augustine, Florida, his niece Catherine Calabria, also of St. Augustine, his nephew, David Morrison of Arlington, Virginia, as well as generations of grateful and adoring students.</p>\n<p>Memorial donations to FIRST and memories of Flowers may be delivered via <a href=\"https://give.firstinspires.org/campaign/woodie-flowers-memorial/c234741\" target=\"_blank\">this website</a> or mailed to FIRST c/o Director Dia Stolnitz, 200 Bedford Street, Manchester, New Hampshire, 03101.</p> \n<p>MIT’s Department of Mechanical Engineering will be organizing a digital memorial in Woodie’s honor where alumni, former colleagues, and students are welcome to share their remembrances. Remembrances may be submitted via the <a href=\"https://meche.mit.edu/remembering-woodie-flowers\" target=\"_blank\">MechE website</a>.&nbsp;</p>\n<p>This article will be updated with information about memorial services as it becomes available.</p>","descriptionType":"html","publishedDate":"Mon, 14 Oct 2019 22:00:08 +0000","feedId":12364,"bgimg":"","linkMd5":"51e6d652f5a1c0223cbb9473435e4fcc","bgimgJsdelivr":"","metaImg":"","author":"Mary Beth Gallagher | Department of Mechanical Engineering","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Sensor-packed glove learns signatures of the human grasp","link":"https://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529","description":"<p>Wearing a sensor-packed glove while handling a variety of objects, MIT researchers have compiled a massive dataset that enables an AI system to recognize objects through touch alone. The information could be leveraged to help robots identify and manipulate objects, and may aid in prosthetics design.</p>\n<p>The researchers developed a low-cost knitted glove, called “scalable tactile glove” (STAG), equipped with about 550 tiny sensors across nearly the entire hand. Each sensor captures pressure signals as humans interact with objects in various ways. A neural network processes the signals to “learn” a dataset of pressure-signal patterns related to specific objects. Then, the system uses that dataset to classify the objects and predict their weights by feel alone, with no visual input needed.</p>\n<p>In a paper published today in <em>Nature</em>, the researchers describe a dataset they compiled using STAG for 26 common objects — including a soda can, scissors, tennis ball, spoon, pen, and mug. Using the dataset, the system predicted the objects’ identities with up to 76 percent accuracy. The system can also predict the correct weights of most objects within about 60 grams.</p>\n<p>Similar sensor-based gloves used today run thousands of dollars and often contain only around 50 sensors that capture less information. Even though STAG produces very high-resolution data, it’s made from commercially available materials totaling around $10.</p>\n<p>The tactile sensing system could be used in combination with traditional computer vision and image-based datasets to give robots a more human-like understanding of interacting with objects.</p>\n<p>“Humans can identify and handle objects well because we have tactile feedback. As we touch objects, we feel around and realize what they are. Robots don’t have that rich feedback,” says Subramanian Sundaram PhD ’18, a former graduate student in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “We’ve always wanted robots to do what humans can do, like doing the dishes or other chores. If you want robots to do these things, they must be able to manipulate objects really well.”</p>\n<p>The researchers also used the dataset to measure the cooperation between regions of the hand during object interactions. For example, when someone uses the middle joint of their index finger, they rarely use their thumb. But the tips of the index and middle fingers always correspond to thumb usage. “We quantifiably show, for the first time, that, if I’m using one part of my hand, how likely I am to use another part of my hand,” he says.</p>\n<p>Prosthetics manufacturers can potentially use information to, say, choose optimal spots for placing pressure sensors and help customize prosthetics to the tasks and objects people regularly interact with.</p>\n<p>Joining Sundaram on the paper are: CSAIL postdocs Petr Kellnhofer and Jun-Yan Zhu; CSAIL graduate student Yunzhu Li; Antonio Torralba, a professor in EECS and director of the MIT-IBM Watson AI Lab; and Wojciech Matusik, an associate professor in electrical engineering and computer science and head of the Computational Fabrication group. &nbsp;</p>\n<p>STAG is laminated with an electrically conductive polymer that changes resistance to applied pressure. The researchers sewed conductive threads through holes in the conductive polymer film, from fingertips to the base of the palm. The threads overlap in a way that turns them into pressure sensors. When someone wearing the glove feels, lifts, holds, and drops an object, the sensors record the pressure at each point.</p>\n<p>The threads connect from the glove to an external circuit that translates the pressure data into “tactile maps,” which are essentially brief videos of dots growing and shrinking across a graphic of a hand. The dots represent the location of pressure points, and their size represents the force — the bigger the dot, the greater the pressure.</p>\n<p>From those maps, the researchers compiled a dataset of about 135,000 video frames from interactions with 26 objects. Those frames can be used by a neural network to predict the identity and weight of objects, and provide insights about the human grasp.</p>\n<p>To identify objects, the researchers designed a convolutional neural network (CNN), which is usually used to classify images, to associate specific pressure patterns with specific objects. But the trick was choosing frames from different types of grasps to get a full picture of the object.</p>\n<p>The idea was to mimic the way humans can hold an object in a few different ways in order to recognize it, without using their eyesight. Similarly, the researchers’ CNN chooses up to eight semirandom frames from the video that represent the most dissimilar grasps — say, holding a mug from the bottom, top, and handle.</p>\n<p>But the CNN can’t just choose random frames from the thousands in each video, or it probably won’t choose distinct grips. Instead, it groups similar frames together, resulting in distinct clusters corresponding to unique grasps. Then, it pulls one frame from each of those clusters, ensuring it has a representative sample. Then the CNN uses the contact patterns it learned in training to predict an object classification from the chosen frames.</p>\n<p>“We want to maximize the variation between the frames to give the best possible input to our network,” Kellnhofer says. “All frames inside a single cluster should have a similar signature that represent the similar ways of grasping the object. Sampling from multiple clusters simulates a human interactively trying to find different grasps while exploring an object.”</p>\n<p>For weight estimation, the researchers built a separate dataset of around 11,600 frames from tactile maps of objects being picked up by finger and thumb, held, and dropped. Notably, the CNN wasn’t trained on any frames it was tested on, meaning it couldn’t learn to just associate weight with an object. In testing, a single frame was inputted into the CNN. Essentially, the CNN picks out the pressure around the hand caused by the object’s weight, and ignores pressure caused by other factors, such as hand positioning to prevent the object from slipping. Then it calculates the weight based on the appropriate pressures.</p>\n<p>The system could be combined with the sensors already on robot joints that measure torque and force to help them better predict object weight. “Joints are important for predicting weight, but there are also important components of weight from fingertips and the palm that we capture,” Sundaram says.</p>","descriptionType":"html","publishedDate":"Wed, 29 May 2019 16:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"18b5e99378c898acd460d528110aff11","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"An immersive experience in industry ","link":"https://news.mit.edu/2019/immersive-engineering-experience-industry-0919","description":"<p>This summer, four mechanical engineering graduate students had the opportunity to gain hands-on experience working in industry. Through the recently launched <a href=\"http://meche.mit.edu/alliance-i2p-program\">Industry Immersion Project Program (I2P)</a>, students were paired with a company and tasked with tackling a short-term project. Projects in this inaugural year for the program came from a diverse range of industries, including manufacturing, robotics, and aerospace engineering.</p> \n<p>A flagship program of the <a href=\"http://meche.mit.edu/alliance\">MechE Alliance</a>, the I2P Program matches students with a company and project that best fits within their own academic experience at MIT. Projects are designed to be short term, lasting three to six months. Building upon programs such as the <a href=\"https://manufacturing.mit.edu/\">Master of Engineering in Advanced Manufacturing</a> and Design and <a href=\"https://lgo.mit.edu/\">Leaders for Global Operations</a>, which foster collaborations between students and the manufacturing industry, the I2P Program offers graduate students real-world experiences across industries.</p> \n<p>“For some students, this could be their first experience working in industry before graduating,” says Brian W. Anthony, program faculty director of the I2P Program. “Having that industry experience arms them with knowledge to help make career choices, may inform their further research, and provides skills they will utilize throughout their careers — whether they end up working in academia or industry.”</p> \n<p>Throughout the course of the projects, students are supported by both a supervisor at the company they’re working for and an academic supervisor from MIT’s mechanical engineering faculty. They also produce a report of their experience and receive academic credit for their industry projects and are enrolled in the class 2.992 (Professional Industry Immersion Project).</p> \n<p>“It’s been great hearing just how rich the experience has been from the students who participated this summer,” adds Theresa Werth, program manager for the MechE Alliance. “Not only have they spent the summer working on a project that’s relevant to their own research or thesis, they have honed some of the softer skills of professional development.”</p> \n<p>The four students participating in this year’s I2P Program have shared highlights and takeaways from their experiences:</p> \n<p><strong>Sara Nagelberg&nbsp;— 3M</strong></p> \n<p>A PhD candidate working with Associate Professor Mathias Kolle in the Bio-Inspired Photonic Engineering research group, Sara Nagelberg studies optical engineering. Through the I2P Program, this summer she worked at 3M on a project that seeks to automate surface finish analysis in manufacturing by understanding visual perception.</p> \n<p>While much of manufacturing involves automation, automating quality inspection for the surface finish on appliances or cars offers some technical challenges. The project Nagelberg worked on at 3M hopes to define what makes a surface \"good,\" then develop algorithms so that a computer can determine whether a surface finish is good quality or flawed.</p> \n<p>“The long-term goal of the project is to automate surface-quality inspection,” Nagelberg explains. She and her team identified parameters that could be used to judge the visual appearance of surfaces — things like color, glossiness, shape, and texture.</p> \n<p>“By working on this project, I learned about a variety of instruments and metrics that can be used to quantify visual surface finish parameters,” she adds.</p> \n<p>In addition to gaining experience on an interdisciplinary team at 3M, Nagelberg learned about computer vision, machine learning, and how to relate human perception to measurable parameters.</p> \n<p><strong>Katie Hahm — Amazon Robotics</strong></p> \n<p>This summer was one of transition for Katie Hahm. Having graduated with her master’s degree in June, Hahm is now a PhD candidate working in the Device Realization Lab with program director Anthony. As a master’s student, Hahm previously worked with Professor Harry Asada on designing robotic limbs to help manufacturing workers maintain positions for extended periods of time.</p> \n<p>Through the I2P Program, Hahm worked on a project at Amazon Robotics to improve efficiencies in the robotic process. “Working on this project was a great academic experience,” says Hahm. “I gained insights into the many facets and complexities of robotics.”</p> \n<p>Hahm also received a ground truth in what it’s like to work at a company like Amazon. She visited a local fulfillment center to gain a deeper understanding of their operations and visited Seattle to attend a company conference. At the conference, she and her fellow interns met with company leadership and teams from other Amazon sectors.</p> \n<p>One of the biggest takeaways from her experience at Amazon, according to Hahm, was how to approach research projects moving forward. “I learned not only valuable information from working with other professionals, but also the skills and approaches to asking more effective questions for research-oriented work,” she adds.</p> \n<p><strong>Sai Nithin Reddy Kantareddy — Amazon Robotics</strong></p> \n<p>A junior PhD candidate, much of Sai Nithin Reddy Kantareddy’s work involves using radio frequency identification (RFID) tags to sense activity and gather data about the surrounding environment. These RFID tags can then be used to connect objects to the internet of things.</p> \n<p>“Going into this summer, I knew I wanted to work on something related to sensors because of my research interest in environmental sensing,” explains Kantareddy. Through the I2P Program, Kantareddy was assigned to a project about material identification and sensing in robotics at Amazon Robotics.</p> \n<p>“Material identification for robotic applications really aligns with my own research interests,” he adds. While at Amazon Robotics, he gained hands-on experience working with sensors, cameras, and robots. He also built machine learning models on experimental data.</p> \n<p>While his background isn’t in robotics research, Kantareddy quickly learned about how robots are designed and what some of the challenges are in field implementation and warehouse automation. In addition to this in-depth technical knowledge, he also gained firsthand experience working in a team setting.</p> \n<p>“I enjoyed being part of a very resourceful and talented R&amp;D team,” he recalls. &nbsp;“I hope to take back these real-world insights and technical learnings and put them to practice in my PhD work.”</p> \n<p><strong>Abhishek Patkar — Systems Technology Inc.</strong></p> \n<p>A sophomore master’s student, Abhishek Patkar works in the flight controls group the Active Adaptive Control Laboratory, led by in Senior Research Scientist Anuradha Annaswamy. Working at Systems Technology Inc. (STI) was a natural fit. Much of STI’s work focuses on aerospace engineering.</p> \n<p>For his internship, Patkar was matched with Aditya Kotikalpudi, a senior research engineer at STI and the principal investigator for NASA’s project entitled Performance Adaptive Aeroelastic Wing. “I primarily worked on system identification and model parameter update for an aeroelastic vehicle,” says Patkar.</p> \n<p>While his internship was based in Los Angeles, California, Patkar had the opportunity to visit the University of Minnesota and witness the actual process of flight testing. He worked with the real data taken from these flight tests. Patkar also used STI software to identify aeroelastic mode shapes and obtain transfer function estimates from control surfaces to measured quantities like center body pitch rate.&nbsp;</p> \n<p>“Through this internship, I was able to learn a lot about aircraft dynamics, aeroelasticity, and the process of performing system identification on an aircraft,” Patkar adds. He expects to use this knowledge back in the flight controls group in the Active Adaptive Control Laboratory.</p>","descriptionType":"html","publishedDate":"Thu, 19 Sep 2019 17:00:01 +0000","feedId":12364,"bgimg":"","linkMd5":"339ad54ca465a2cddedf7b9ff06e9704","bgimgJsdelivr":"","metaImg":"","author":"Mary Beth Gallagher | Department of Mechanical Engineering","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Robots help patients manage chronic illness at home","link":"https://news.mit.edu/2019/catalia-health-mabu-robot-1011","description":"<p>The Mabu robot, with its small yellow body and friendly expression, serves, literally, as the face of the care management startup Catalia Health. The most innovative part of the company’s solution, however, lies behind Mabu’s large blue eyes.</p>\n<p>Catalia Health’s software incorporates expertise in psychology, artificial intelligence, and medical treatment plans to help patients manage their chronic conditions. The result is a sophisticated robot companion that uses daily conversations to give patients tips, medication reminders, and information on their condition while relaying relevant data to care providers. The information exchange can also take place on patients’ mobile phones.</p>\n<p>“Ultimately, what we’re building are care management programs to help patients in particular disease states,” says Catalia Health founder and CEO Cory Kidd SM ’03, PhD ’08. “A lot of that is getting information back to the people providing care. We’re helping them scale up their efforts to interact with every patient more frequently.”</p>\n<p>Heart failure patients first brought Mabu into their homes about a year and a half ago as part of a partnership with the health care provider Kaiser Permanente, who pays for the service. Since then, Catalia Health has also partnered with health care systems and pharmaceutical companies to help patients dealing with conditions including rheumatoid arthritis and kidney cancer.</p>\n<p>Treatment plans for chronic diseases can be challenging for patients to manage consistently, and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5944464/\">many people don’t follow them</a> as prescribed. Kidd says Mabu’s daily conversations help not only patients, but also human care givers as they make treatment decisions using data collected by their robot counterpart.</p>\n<p><strong>Robotics for change</strong></p>\n<p>Kidd was a student and faculty member at Georgia Tech before coming to MIT for his master’s degree in 2001. His work focused on addressing problems in health care caused by an aging population and an increase in the number of people managing chronic diseases.</p>\n<p>“The way we deliver health care doesn’t scale to the needs we have, so I was looking for technologies that might help with that,” Kidd says.</p>\n<p>Many studies have found that communicating with someone in person, as opposed to over the phone or online, makes that person appear more trustworthy, engaging, and likeable. At MIT, Kidd conducted studies aimed at understanding if those findings translated to robots.</p>\n<p>“What I found was when we used an interactive robot that you could look in the eye and share the same physical space with, you got the same psychological effects as face-to-face interaction,” Kidd says.</p>\n<p>As part of his PhD in the Media Lab’s Media Arts and Sciences program, Kidd tested that finding in a randomized, controlled trial with patients in a diabetes and weight management program at the Boston University Medical Center. A portion of the patients were given a robotic weight-loss coach to take home, while another group used a computer running the same software. The tabletop robot conducted regular check ups and offered tips on maintaining a healthy diet and lifestyle. Patients who received the robot were much more likely to stick with the weight loss program.</p>\n<p>Upon finishing his PhD in 2007, Kidd immediately sought to apply his research by starting the company Intuitive Automata to help people manage their diabetes using robot coaches. Even as he pursued the idea, though, Kidd says he knew it was too early to be introducing such sophisticated technology to a health care industry that, at the time, was still adjusting to electronic health records.</p>\n<p>Intuitive Automata ultimately wasn’t a major commercial success, but it did help Kidd understand the health care sector at a much deeper level as he worked to sell the diabetes and weight management programs to providers, pharmaceutical companies, insurers, and patients.</p>\n<p>“I was able to build a big network across the industry and understand how these people think about challenges in health care,” Kidd says. “It let me see how different entities think about how they fit in the health care ecosystem.”</p>\n<p>Since then, Kidd has watched the costs associated with robotics and computing plummet. Many people have also enthusiastically adopted computer assistance like Amazon’s Alexa and Apple’s Siri. Finally, Kidd says members of the health care industry have developed an appreciation for technology’s potential to complement traditional methods of care.</p>\n<p>“The common ways [care is delivered] on the provider side is by bringing patients to the doctor’s office or hospital,” Kidd explains. “Then on the pharma side, it’s call center-based. In the middle of these is the home visitation model. They’re all very human powered. If you want to help twice as many patients, you hire twice as many people. There’s no way around that.”</p>\n<p>In the summer of 2014, he founded Catalia Health to help patients with chronic conditions at scale.</p>\n<p>“It’s very exciting because I’ve seen how well this can work with patients,” Kidd says of the company’s potential. “The biggest challenge with the early studies was that, in the end, the patients didn’t want to give the robots back. From my perspective, that’s one of the things that shows this really does work.”</p>\n<p><strong>Mabu makes friends</strong></p>\n<p>Catalia Health uses artificial intelligence to help Mabu learn about each patient through daily conversations, which vary in length depending on the patient’s answers.</p>\n<p>“A lot of conversations start off with ‘How are you feeling?’ similar to what a doctor or nurse might ask,” Kidd explains. “From there, it might go off in many directions. There are a few things doctors or nurses would ask if they could talk to these patients every day.”</p>\n<p>For example, Mabu would ask heart failure patients how they are feeling, if they have shortness of breath, and about their weight.</p>\n<p>“Based on patients’ answers, Mabu might say ‘You might want to call your doctor,’ or ‘I’ll send them this information,’ or ‘Let’s check in tomorrow,’” Kidd says.</p>\n<p>Last year, Catalia Health announced a collaboration with the American Heart Association that has allowed Mabu to deliver the association’s guidelines for patients living with heart failure.</p>\n<p>“A patient might say ‘I’m feeling terrible today’ and Mabu might ask ‘Is it one of these symptoms a lot of people with your condition deal with?’ We’re trying to get down to whether it’s the disease or the drug. When that happens, we do two things: Mabu has a lot of information about problems a patient might be dealing with, so she’s able to give quick feedback. Simultaneously, she’s sending that information to a clinician — a doctor, nurse, or pharmacists — whoever’s providing care.”</p>\n<p>In addition to health care providers, Catalia also partners with pharmaceutical companies. In each case, patients pay nothing out of pocket for their robot companions. Although the data Catalia Health sends pharmaceutical companies is completely anonymized, it can help them follow their treatment’s effects on patients in real time and better understand the patient experience.</p>\n<p>Details about many of Catalia Health’s partnerships have not been disclosed, but the company did announce a collaboration with Pfizer last month to test the impact of Mabu on patient treatment plans.</p>\n<p>Over the next year, Kidd hopes to add to the company’s list of partnerships and help patients dealing a wider swath of diseases. Regardless of how fast Catalia Health scales, he says the service it provides will not diminish as Mabu brings its trademark attentiveness and growing knowledge base to every conversation.</p>\n<p>“In a clinical setting, if we talk about a doctor with good bedside manner, we don’t mean that he or she has more clinical knowledge than the next person, we simply mean they’re better at connecting with patients,” Kidd says. “I’ve looked at the psychology behind that — what does it mean to be able to do that? — and turned that into the algorithms we use to help create conversations with patients.”</p>","descriptionType":"html","publishedDate":"Fri, 11 Oct 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"f1ba0ed7ea7854ada269896578bbb74a","bgimgJsdelivr":"","metaImg":"","author":"Zach Winn | MIT News Office","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Lincoln Laboratory staff use race cars as a vehicle to teach coding","link":"https://news.mit.edu/2019/lincoln-laboratory-staff-use-racecars-to-teach-coding-0701","description":"<p>One Saturday this spring, toy-sized cars were zipping along the classroom floor of the Roxbury Innovation Center. However, no one was following them around with remote controls. Instead, each car used code to react autonomously to obstacles — code written by a classroom full of middle school students.</p> \n<p>Andrew Fishberg, a staff member in the Advanced Capabilities and Systems Group at Lincoln Laboratory, had seen how students engaged with the Rapid Autonomous Complex-Environment Competing Ackermann-steering Robot (RACECAR) during the workshop at the Beaver Works Summer Institute (BWSI). However, BWSI is aimed at high school seniors who already excel in science, technology, engineering, and math, and Fishberg was worried that the program was reaching students too late in their educations to have maximum impact. \"It only gets harder [to learn coding] the later you get to the students,\" he says. \"I think the future of these things is at the middle school age.\"</p> \n<p>So, with the help of a handful of volunteers including Eyassu Shimelis from the Advanced Concepts Technologies Group and several high school volunteers — almost all of whom were coincidentally named Dan — and the Timothy Smith Network, Fishberg designed a four-week program to introduce middle schoolers to coding by programming race cars.</p> \n<p>The Timothy Smith Network is named for a wealthy merchant who spent most of his life in Roxbury, Massachusetts, and upon his death in 1918 bequeathed his estate to improve the welfare of Roxbury residents. Since 1996, that trust has been used to bring the benefits of computer technology to residents via dozens of public technology centers and educational programs. Collaboration between the Timothy Smith Network and Lincoln Laboratory could help the RACECAR program reach middle schoolers who might not otherwise have the opportunity to learn to code. \"Our motto is inclusiveness,\" says Khalid Mustafa, the IT director of the Timothy Smith Network. \"Too often we have all these rules that filter people out. How do we invite people in?\"</p> \n<p>Both groups wanted to stress accessibility. Although coding is becoming a fundamental part of a modern education, schools in communities with limited educational budgets are significantly less likely to offer computer science classes. Holding the workshop at the Roxbury Innovation Center, instead of at the Beaver Works Center in Cambridge, made it more accessible for lower-income students and students of color.</p> \n<p>The workshop was stretched out over four hours each Saturday for a month. Between one and two dozen students attended each time. The first three workshops focused on coding basics, such as Boolean data (data that has one of two possible values, often true or false) and the difference between \"or\" and \"exclusive or\" (\"exclusive or\" is true if only one value is true, whereas \"or\" is true if at least one value is true).</p> \n<p>\"We don't want to lock anybody out because they haven't had a chance to program before, so we had to start from square one,\" Fishberg explains. He would teach a principle and demonstrate the code on screen at the front of the classroom, then have students call out answers to build a program together. More often than not, the code the students created wouldn't run the way they wanted it to. At that point, Fishberg would walk the students through the code, explaining the logic with which computers approach problems and allowing the students to find the bugs themselves. Usually, the code spat out numbers as expected within minutes.</p> \n<p>On the fourth Saturday, Fishberg brought out the race cars. Slightly larger than a phone book, the cars have the wheels and body of an off-the-shelf remote-controlled car, upon which is mounted a piece of cardboard that holds a spinning lidar, a small processor, and a battery pack. The cars cost around $500 each to build and made their debut at this RACECAR middle school event as a cost-effective alternative to the race cars used at BWSI. The excitement — from the volunteers, students, and observers — was palpable. \"Basically everything they've been learning … turns into the logic to drive the car,\" Fishberg said. \"That application really drives home the learning objectives.\"</p> \n<p>The race cars \"see\" by using lidar – each car's lidar system shoots out a pulse of laser light and measures how long it takes to bounce back. By aiming laser beams in 720 directions all around the car, the lidar system can map the distance between it and the nearest obstacles. The students began by calibrating their race cars, finding which direction marked zero degrees in the lidar's measurements by circling the car with pieces of cardstock and looking for changes in the lidar's readouts. They also calibrated the car to drive straight forward when it is prompted, and then moved on to harder coding challenges, such as having the car stop itself when it sensed an obstacle. The students were exhilarated by their successes and suggested designs for complex obstacle courses the cars could navigate. Fishberg used this as an opportunity to impart another fundamental principle of coding: KISS, or Keep It Simple, Silly.</p> \n<p>Both Fishberg and Mustafa hope to make this year's workshop the first of many. Coding is becoming a vital skill, and introducing students to it at a young age opens doors in their education — such as to the BWSI, which is aimed at high school seniors who perform well in the STEM fields. \"We're looking to expand,\" Mustafa says. \"Our goal is to really establish this [class] as a model.\"</p> \n<p>Mustafa thought that the most powerful part of the workshop and the collaboration between the Timothy Smith Network and Lincoln Laboratory was the way it brought people together who wouldn't otherwise be able to share their skills. \"How do we give each other access to each other?\" Mustafa asks. \"That's really the key.\"</p>","descriptionType":"html","publishedDate":"Mon, 01 Jul 2019 16:40:01 +0000","feedId":12364,"bgimg":"","linkMd5":"28db8257d25d240f9fca4ebd67f26d51","bgimgJsdelivr":"","metaImg":"","author":"Madelaine Millar | Lincoln Laboratory","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Giving soft robots feeling","link":"https://news.mit.edu/2020/giving-soft-robots-senses-0601","description":"<p>One of the hottest topics in robotics is the field of soft robots, which utilizes squishy and flexible materials rather than traditional rigid materials. But soft robots have been limited due to their lack of good sensing. A good robotic gripper needs to feel what it is touching (tactile sensing), and it needs to sense the positions of its fingers (proprioception). Such sensing has been missing from most soft robots.</p> \n<p>In a new pair of papers, researchers from MIT’s <a href=\"http://csail.mit.edu\">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL) came up with new tools to let robots better perceive what they’re interacting with: the ability to see and classify items, and a softer, delicate touch.&nbsp;</p> \n<p>“We wish to enable seeing the world by feeling the world. Soft robot hands have sensorized skins that allow them to pick up a range of objects, from delicate, such as potato chips, to heavy, such as milk bottles,” says CSAIL Director Daniela Rus, the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science and the deputy dean of research for the MIT Stephen A. Schwarzman College of Computing.&nbsp;</p> \n<p><a href=\"https://arxiv.org/abs/1910.01287\">One paper</a> builds off last year’s <a href=\"http://news.mit.edu/2019/new-robot-hand-gripper-soft-and-strong-0315\">research</a> from MIT and Harvard University, where a team developed a soft and strong robotic gripper in the form of a cone-shaped origami structure. It collapses in on objects much like a Venus' flytrap, to pick up items that are as much as 100 times its weight.&nbsp;</p> \n<p>To get that newfound versatility and adaptability even closer to that of a human hand, a new team came up with a sensible addition: tactile sensors, made from latex “bladders” (balloons) connected to pressure transducers. The new sensors let the gripper not only pick up objects as delicate as potato chips, but it also classifies them — letting the robot better understand what it’s picking up, while also exhibiting that light touch.&nbsp;</p> \n<p>When classifying objects, the sensors correctly identified 10 objects with over 90 percent accuracy, even when an object slipped out of grip.</p> \n<p>“Unlike many other soft tactile sensors, ours can be rapidly fabricated, retrofitted into grippers, and show sensitivity and reliability,” says MIT postdoc Josie Hughes, the lead author on a new paper about the sensors. “We hope they provide a new method of soft sensing that can be applied to a wide range of different applications in manufacturing settings, like packing and lifting.”&nbsp;</p> \n<p>In <a href=\"https://arxiv.org/pdf/1910.01287.pdf\">a second paper</a>, a group of researchers created a soft robotic finger called “GelFlex” that uses embedded cameras and deep learning to enable high-resolution tactile sensing and “proprioception” (awareness of positions and movements of the body).&nbsp;</p> \n<p>The gripper, which looks much like a two-finger cup gripper you might see at a soda station, uses a tendon-driven mechanism to actuate the fingers. When tested on metal objects of various shapes, the system had over 96 percent recognition accuracy.&nbsp;</p> \n<p>“Our soft finger can provide high accuracy on proprioception and accurately predict grasped objects, and also withstand considerable impact without harming the interacted environment and itself,” says Yu She, lead author on a new paper on GelFlex. “By constraining soft fingers with a flexible exoskeleton, and performing high-resolution sensing with embedded cameras, we open up a large range of capabilities for soft manipulators.”&nbsp;</p> \n<p><strong>Magic ball senses&nbsp;</strong></p> \n<p>The magic ball gripper is made from a soft origami structure, encased by a soft balloon. When a vacuum is applied to the balloon, the origami structure closes around the object, and the gripper deforms to its structure.&nbsp;</p> \n<p>While this motion lets the gripper grasp a much wider range of objects than ever before, such as soup cans, hammers, wine glasses, drones, and even a single broccoli floret, the greater intricacies of delicacy and understanding were still out of reach — until they added the sensors.&nbsp;&nbsp;</p> \n<p>When the sensors experience force or strain, the internal pressure changes, and the team can measure this change in pressure to identify when it will feel that again.&nbsp;</p> \n<p>In addition to the latex sensor, the team also developed an algorithm which uses feedback to let the gripper possess a human-like duality of being both strong and precise — and 80 percent of the tested objects were successfully grasped without damage.&nbsp;</p> \n<p>The team tested the gripper-sensors on a variety of household items, ranging from heavy bottles to small, delicate objects, including cans, apples, a toothbrush, a water bottle, and a bag of cookies.&nbsp;</p> \n<p>Going forward, the team hopes to make the methodology scalable, using computational design and reconstruction methods to improve the resolution and coverage using this new sensor technology. Eventually, they imagine using the new sensors to create a fluidic sensing skin that shows scalability and sensitivity.&nbsp;</p> \n<p>Hughes co-wrote the new paper with Rus, which they will present virtually at the 2020 International Conference on Robotics and Automation.&nbsp;</p> \n<p><strong>GelFlex</strong></p> \n<p>In the second paper, a CSAIL team looked at giving a soft robotic gripper more nuanced, human-like senses. Soft fingers allow a wide range of deformations, but to be used in a controlled way there must be rich tactile and proprioceptive sensing. The team used embedded cameras with wide-angle “fisheye” lenses that capture the finger’s deformations in great detail.</p> \n<p>To create GelFlex, the team used silicone material to fabricate the soft and transparent finger, and put one camera near the fingertip and the other in the middle of the finger. Then, they painted reflective ink on the front and side surface of the finger, and added LED lights on the back. This allows the internal fish-eye camera to observe the status of the front and side surface of the finger.&nbsp;</p> \n<p>The team trained neural networks to extract key information from the internal cameras for feedback. One neural net was trained to predict the bending angle of GelFlex, and the other was trained to estimate the shape and size of the objects being grabbed. The gripper could then pick up a variety of items such as a Rubik’s cube, a DVD case, or a block of aluminum.&nbsp;</p> \n<p>During testing, the average positional error while gripping was less than 0.77 millimeter, which is better than that of a human finger. In a second set of tests, the gripper was challenged with grasping and recognizing cylinders and boxes of various sizes. Out of 80 trials, only three were classified incorrectly.&nbsp;</p> \n<p>In the future, the team hopes to improve the proprioception and tactile sensing algorithms, and utilize vision-based sensors to estimate more complex finger configurations, such as twisting or lateral bending, which are challenging for common sensors, but should be attainable with embedded cameras.</p> \n<p>Yu She co-wrote the GelFlex paper with MIT graduate student Sandra Q. Liu, Peiyu Yu of Tsinghua University, and MIT Professor Edward Adelson. They will present the paper virtually at the 2020 International Conference on Robotics and Automation.</p> \n<p></p>","descriptionType":"html","publishedDate":"Mon, 01 Jun 2020 13:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"d8e6ea245bf85fbff0569794fbaa4347","bgimgJsdelivr":"","metaImg":"","author":"Rachel Gordon | MIT CSAIL","publishedOrCreatedDate":1598320622357},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"IDSS hosts inaugural Learning for Dynamics and Control conference","link":"https://news.mit.edu/2019/idss-hosts-inaugural-learning-dynamics-and-control-l4dc-conference-0710","description":"<p>Over the next decade, the biggest generator of data is expected to be devices which sense and control the physical world. From autonomy to robotics to smart cities, this data explosion — paired with advances in machine learning — creates new possibilities for designing and optimizing technological systems that use their own real-time generated data to make decisions.</p> \n<p>To address the many scientific questions and application challenges posed by the real-time physical processes of these \"dynamical\" systems, researchers from MIT and elsewhere organized a new annual conference called <a href=\"https://l4dc.mit.edu/\">Learning for Dynamics and Control</a>. Dubbed L4DC, the inaugural conference was hosted at MIT by the <a href=\"https://idss.mit.edu/\">Institute for Data, Systems, and Society</a> (IDSS).</p> \n<p>As excitement has built around machine learning and autonomy, there is an increasing need to consider both the data that physical systems produce and feedback these systems receive, especially from their interactions with humans. That extends into the domains of data science, control theory, decision theory, and optimization.</p> \n<p>“We decided to launch L4DC because we felt the need to bring together the communities of machine learning, robotics, and systems and control theory,” said IDSS Associate Director Ali Jadbabaie, a conference co-organizer and professor in IDSS, the Department of Civil and Environmental Engineering (CEE), and the Laboratory for Information and Decision Systems (LIDS).</p> \n<p>“The goal was to bring together these researchers because they all converged on a very similar set of research problems and challenges,” added co-organizer Ben Recht, of the University of California at Berkeley, in opening remarks.</p> \n<p>Over the two days of the conference, talks covered core topics from the foundations of learning of dynamics models, data-driven optimization for dynamical models and optimization for machine learning, reinforcement learning for physical systems, and reinforcement learning for both dynamical and control systems. Talks also featured examples of applications in fields like robotics, autonomy, and transportation systems.</p> \n<p>“How could self-driving cars change urban systems?” asked Cathy Wu, an assistant professor in CEE, IDSS, and LIDS, in a talk that investigated how transportation and urban systems may change over the next few decades. Only a small percentage of autonomous vehicles are needed to significantly affect traffic systems, Wu argued, which will in turn affect other urban systems. “Distribution learning provides us with an understanding for integrating autonomy into urban systems,” said Wu.</p> \n<p>Claire Tomlin of UC Berkeley presented on integrating learning into control in the context of safety in robotics. Tomlin’s team integrates learning mechanisms that help robots adapt to sudden changes, such as a gust of wind, an unexpected human behavior, or an unknown environment. “We’ve been working on a number of mechanisms for doing this computation in real time,” Tomlin said.</p> \n<p>Pablo Parillo, a professor in the Department of Electrical Engineering and Computer Science and faculty member of both IDSS and LIDS, was also a conference organizer, along with George Pappas of the University of Pennsylvania and Melanie Zellinger of ETH Zurich.</p> \n<p>L4DC was sponsored by the National Science Foundation, the U.S. Air Force Office of Scientific Research, the Office of Naval Research, and the Army Research Office, a part of the Combat Capabilities Development Command Army Research Laboratory (CCDC ARL).</p> \n<p>\"The cutting-edge combination of classical control with recent advances in artificial intelligence and machine learning will have significant and broad potential impact on Army multi-domain operations, and include a variety of systems that will incorporate autonomy, decision-making and reasoning, networking, and human-machine collaboration,\" said Brian Sadler, senior scientist for intelligent systems, U.S. Army CCDC ARL.</p> \n<p>Organizers plan to make L4DC a recurring conference, hosted at different institutions. “Everyone we invited to speak accepted,” Jadbabaie said. “The largest room in Stata was packed until the end of the conference. We take this as a testament to the growing interest in this area, and hope to grow and expand the conference further in the coming years.”</p>","descriptionType":"html","publishedDate":"Wed, 10 Jul 2019 15:35:01 +0000","feedId":12364,"bgimg":"","linkMd5":"0331a891418a714492b54ac029212bd6","bgimgJsdelivr":"","metaImg":"","author":"Scott Murray | Institute for Data, Systems, and Society","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"New US postage stamp highlights MIT research","link":"https://news.mit.edu/2020/new-us-postal-stamp-highlights-mit-research-0802","description":"<p>Letter writers across the country will soon have a fun and beautiful new Forever stamp to choose from, featuring novel research from the Media Lab's Biomechatronics research group.&nbsp;</p> \n<p>The stamp is part of a new U.S. Postal Service (USPS) series on innovation, representing computing, biomedicine, genome sequencing, robotics, and solar technology.&nbsp;For the robotics category, the USPS chose the bionic prosthesis designed and built by Matt Carney PhD ’20 and members of the Biomechatronics group, led by Professor Hugh Herr.</p> \n<p>The image used in the stamp was taken by photographer Andy Ryan, whose portfolio spans&nbsp;images from around the world, and who for many years has been capturing the MIT experience — from stunning architectural shots to the research work of labs across campus. Ryan suggested the bionic work of the biomechatronics group to USPS to represent the future of robotics. Ryan also created the images that became the computing and solar technology stamps in the series.&nbsp;</p> \n<p>“I was aware that Hugh Herr and his research team were incorporating robotic elements into the prosthetic legs they were developing and testing,” Ryan notes.&nbsp;“This vision of robotics was, in my mind, a true depiction of how robots and robotics would manifest and impact society in the future.\"&nbsp;</p> \n<p>With encouragement from Herr, Ryan submitted high-definition, stylized, and close-up images of Matt Carney working on the group's latest designs.&nbsp;</p> \n<p>Carney, who recently completed his PhD in media arts and sciences at the Media Lab, views bionic limbs as the ultimate humanoid robot, and an ideal innovation to represent and portray robotics in 2020. He was all-in for sharing that work with the world.</p> \n<p>\"Robotic prostheses integrate biomechanics, mechanical, electrical, and software engineering, and no piece is off-the-shelf,” Carney says. “To attempt to fit within the confines of the human form, and to match the bandwidth and power density of the human body, we must push the bounds of every discipline: computation, strength of materials, magnetic energy densities, sensors, biological interfaces, and so much more.\"</p> \n<p>In his childhood, Carney himself collected stamps from different corners of the globe, and so the selection of his research for a U.S. postal stamp has been especially meaningful.&nbsp;</p> \n<p>\"It's a freakin' honor to have my PhD work featured as a USPS stamp,\" Carney says, breaking into a big smile. \"I hope this feat is an inspiration to young students everywhere to crush their homework, and to build the skills to make a positive impact on the world. And while I worked insane hours to build this thing — and really tried to inspire with its design as much as its engineering — it's truly the culmination of powered prosthesis work pioneered by Dr. Hugh Herr and our entire team at the Media Lab's Biomechatronics group, and it expands on work from a global community over more than a decade of development.\"</p> \n<p>The new MIT stamp joins a venerable list of other stamps associated with the Institute.&nbsp;Formerly issued stamps have featured Apollo 11 astronaut and moonwalker Buzz Aldrin ScD ’63, Nobel Prize winner Richard Feynman ’39, and architect Robert Robinson Taylor, who graduated from MIT in 1892 and is considered the nation’s first academically trained African American architect, followed by Pritzker Prize-winning architect I.M. Pei ’40, whose work includes the Louvre Glass Pyramid and the East Building on the National Gallery in Washington, as well as numerous buildings on the MIT campus.&nbsp;</p> \n<p>The new robotics stamp, however, is the first to feature MIT research, as well as members of the MIT community.</p> \n<p>\"I'm deeply honored that a USPS Forever stamp has been created to celebrate technologically-advanced robotic prostheses, and along with that, the determination to alleviate human impairment,\" Herr says. \"Through the marriage of human physiology and robotics, persons with leg amputation can now walk with powered prostheses that closely emulate the biological leg. By integrating synthetic sensors, artificial computation, and muscle-like actuation, these technologies are already improving people's lives in profound ways, and may one day soon bring about the end of disability.\"</p> \n<p>The Innovation Stamp series will be <a href=\"http://about.usps.com/newsroom/national-releases/2020/0706-usps-announces-new-stamps-celebrating-innovation.pdf\">available for purchase</a> through the U.S. Postal Service later this month.</p> \n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>","descriptionType":"html","publishedDate":"Sun, 02 Aug 2020 04:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"10a7797ea1726c92175d429b2e06606b","bgimgJsdelivr":"","metaImg":"","author":"Alexandra Kahn | MIT Media Lab","publishedOrCreatedDate":1598320622357},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Showing robots how to do your chores","link":"https://news.mit.edu/2020/showing-robots-learn-chores-0306","description":"<p>Training interactive robots may one day be an easy job for everyone, even those without programming expertise. Roboticists are developing automated robots that can learn new tasks solely by observing humans. At home, you might someday show a domestic robot how to do routine chores. In the workplace, you could train robots like new employees, showing them how to perform many duties.</p>\n<p>Making progress on that vision, MIT researchers have designed a system that lets these types of robots learn complicated tasks that would otherwise stymie them with too many confusing rules. One such task is setting a dinner table under certain conditions. &nbsp;</p>\n<p>At its core, the researchers’ “Planning with Uncertain Specifications” (PUnS) system gives robots the humanlike planning ability to simultaneously weigh many ambiguous —&nbsp;and potentially contradictory —&nbsp;requirements to reach an end goal. In doing so, the system always chooses the most likely action to take, based on a “belief” about some probable specifications for the task it is supposed to perform.</p>\n<p>In their work, the researchers compiled a dataset with information about how eight objects — a mug, glass, spoon, fork, knife, dinner plate, small plate, and bowl — could be placed on a table in various configurations. A robotic arm first observed randomly selected human demonstrations of setting the table with the objects. Then, the researchers tasked the arm with automatically setting a table in a specific configuration, in real-world experiments and in simulation, based on what it had seen.</p>\n<p>To succeed, the robot had to weigh many possible placement orderings, even when items were purposely removed, stacked, or hidden. Normally, all of that would confuse robots too much. But the researchers’ robot made no mistakes over several real-world experiments, and only a handful of mistakes over tens of thousands of simulated test runs. &nbsp;</p>\n<p>“The vision is to put programming in the hands of domain experts, who can program robots through intuitive ways, rather than describing orders to an engineer to add to their code,” says first author Ankit Shah, a graduate student in the Department of Aeronautics and Astronautics (AeroAstro) and the Interactive Robotics Group, who emphasizes that their work is just one step in fulfilling that vision. “That way, robots won’t have to perform preprogrammed tasks anymore. Factory workers can teach a robot to do multiple complex assembly tasks. Domestic robots can learn how to stack cabinets, load the dishwasher, or set the table from people at home.”</p>\n<p>Joining Shah on the paper are AeroAstro and Interactive Robotics Group graduate student Shen Li and Interactive Robotics Group leader Julie Shah, an associate professor in AeroAstro and the Computer Science and Artificial Intelligence Laboratory.</p>\n<p></p> \n<p><strong>Bots hedging bets</strong></p>\n<p>Robots are fine planners in tasks with clear “specifications,” which help describe the task the robot needs to fulfill, considering its actions, environment, and end goal. Learning to set a table by observing demonstrations, is full of uncertain specifications. Items must be placed in certain spots, depending on the menu and where guests are seated, and in certain orders, depending on an item’s immediate availability or social conventions. Present approaches to planning are not capable of dealing with such uncertain specifications.</p>\n<p>A popular approach to planning is “reinforcement learning,” a trial-and-error machine-learning technique that rewards and penalizes them for actions as they work to complete a task. But for tasks with uncertain specifications, it’s difficult to define clear rewards and penalties. In short, robots never fully learn right from wrong.</p>\n<p>The researchers’ system, called PUnS (for Planning with Uncertain Specifications), enables a robot to hold a “belief” over a range of possible specifications. The belief itself can then be used to dish out rewards and penalties. “The robot is essentially hedging its bets in terms of what’s intended in a task, and takes actions that satisfy its belief, instead of us giving it a clear specification,” Ankit Shah says.</p>\n<p>The system is built on “linear temporal logic” (LTL), an expressive language that enables robotic reasoning about current and future outcomes. The researchers defined templates in LTL that model various time-based conditions, such as what must happen now, must eventually happen, and must happen until something else occurs. The robot’s observations of 30 human demonstrations for setting the table yielded a probability distribution over 25 different LTL formulas. Each formula encoded a slightly different preference — or specification — for setting the table. That probability distribution becomes its belief.</p>\n<p>“Each formula encodes something different, but when the robot considers various combinations of all the templates, and tries to satisfy everything together, it ends up doing the right thing eventually,” Ankit Shah says.</p>\n<p><strong>Following criteria</strong></p>\n<p>The researchers also developed several criteria that guide the robot toward satisfying the entire belief over those candidate formulas. One, for instance, satisfies the most likely formula, which discards everything else apart from the template with the highest probability. Others satisfy the largest number of unique formulas, without considering their overall probability, or they satisfy several formulas that represent highest total probability. Another simply minimizes error, so the system ignores formulas with high probability of failure.</p>\n<p>Designers can choose any one of the four criteria to preset before training and testing. Each has its own tradeoff between flexibility and risk aversion. The choice of criteria depends entirely on the task. In safety critical situations, for instance, a designer may choose to limit possibility of failure. But where consequences of failure are not as severe, designers can choose to give robots greater flexibility to try different approaches.</p>\n<p>With the criteria in place, the researchers developed an algorithm to convert the robot’s belief — the probability distribution pointing to the desired formula — into an equivalent reinforcement learning problem. This model will ping the robot with a reward or penalty for an action it takes, based on the specification it’s decided to follow.</p>\n<p>In simulations asking the robot to set the table in different configurations, it only made six mistakes out of 20,000 tries. In real-world demonstrations, it showed behavior similar to how a human would perform the task. If an item wasn’t initially visible, for instance, the robot would finish setting the rest of the table without the item. Then, when the fork was revealed, it would set the fork in the proper place. “That’s where flexibility is very important,” Ankit Shah says. “Otherwise it would get stuck when it expects to place a fork and not finish the rest of table setup.”</p>\n<p>Next, the researchers hope to modify the system to help robots change their behavior based on verbal instructions, corrections, or a user’s assessment of the robot’s performance. “Say a person demonstrates to a robot how to set a table at only one spot. The person may say, ‘do the same thing for all other spots,’ or, ‘place the knife before the fork here instead,’” Ankit Shah says. “We want to develop methods for the system to naturally adapt to handle those verbal commands, without needing additional demonstrations.”&nbsp;&nbsp;</p>","descriptionType":"html","publishedDate":"Fri, 06 Mar 2020 04:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"cf65051bfe791d65872b11d779a9063f","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Prime Minister Mark Rutte of the Netherlands tours MIT ","link":"https://news.mit.edu/2019/prime-minister-mark-rutte-netherlands-0722","description":"<p>Prime Minister Mark Rutte of the Netherlands visited MIT on Friday, taking an innovation-oriented campus tour with a focus on computing and robotics.</p>\n<p>Rutte’s visit was centered in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), where he watched robotics demonstrations and spoke with faculty and students about a variety of topics concerning innovation.</p>\n<p>Rutte was also accompanied by a larger delegation of Dutch government and business leaders, who are on a four-day visit to the Boston area, examining research in AI, robots, biotechnology, and health care. The group included Bruno Bruins, the Netherlands’ minister of medical care, as well as about 40 Dutch innovators in the areas of AI and robotics.&nbsp;</p>\n<p>On the MIT tour, Rutte was principally hosted by Daniela Rus, director of CSAIL and the Andrew and Erna Viterba Professor of Electrical Engineering and Computer Science. Rutte was also greeted by Frans Kashooek, the Charles Piper Professor in MIT’s Department of Electrical Engineering and Computer Science, who is also a CSAIL member; Kashooek is a native of the Netherlands.</p>\n<p>Rus told Rutte she was “delighted to welcome you to CSAIL and to MIT,” and, along with several CSAIL graduate students and researchers, guided him through a series of demonstrations highlighting different aspects of robotics research and development.</p>\n<p>The projects Rutte observed included a muscle-controlled robotic system CSAIL researchers call “<a href=\"http://news.mit.edu/2019/robots-help-you-lift-objects-looking-your-biceps-0522\">RoboRaise</a>,” in which sensors on human muscles relay signals to a robot, showing it how much to, for instance, help lift objects. The system could have applications in construction or manufacturing.</p>\n<p>“In the future, the machines will be always adapting to us,” Rus noted.</p>\n<p>Rutte was also given demonstrations about inexpensive 3-D printed robots; the incorporation of new, soft materials in robots; a <a href=\"http://news.mit.edu/2018/soft-robotic-fish-swims-alongside-real-ones-coral-reefs-0321\">robotic fish</a>; and “<a href=\"http://news.mit.edu/2013/simple-scheme-for-self-assembling-robots-1004\">M-Blocks</a>,” a set of square blocks that reconfigure themselves and could be the basis for self-assembling forms of robots.</p>\n<p>Rutte was highly engaged in the demonstations and asked a series of questions about them — querying about the exact mechanisms that, for instance, allow the M-Blocks to both move and stay attached to each other.</p>\n<p>“You make it look so easy,” Rutte marveled to the robotics researchers, at one point during his CSAIL tour.</p>\n<p>Rutte also had a sit-down conversation with CSAIL professors Peter Szolovits and David Sontag, whose work is at the junction of computing and health care research. Szolovits is, among other things, the principal investigator in the <a href=\"http://news.mit.edu/2015/philips-alliance-mit-0519\">MIT-Philips alliance</a>, a five-year research agreement formalized in 2015 between MIT and Royal Philips N.V., the giant Dutch technology firm which has a major division in health care innovation. Philips North America moved its headquarters to Cambridge, Massachusetts, last year.</p>\n<p>“Everything is here,” Rutte noted when talking to Sontag about the advantages of doing research in the Boston area — a reference to the ecosystem of universities, technology firms, hospitals, and capital available in the region. &nbsp;</p>\n<p>Rutte also remarked on the informal layout of the Stata Center, where CSAIL is housed, and asked Szolovits and Sontag about the “overall atmosphere” at the Institute.</p>\n<p>“It is a wonderful atmosphere,” Szolovits replied. “But for me, the best thing is the students. If I don’t know something, I ask my students.”</p>\n<p>Rutte has been prime minister of the Netherlands since 2010 and is currently serving his third term. He studied history at Leiden University, the oldest university in the Netherlands, and worked in a managerial role at Unilever before first being elected as a member of parliament in 2003.</p>\n<p>Rus also presented Rutte with gifts from MIT, including a hand-crafted glass sculpture made at the MIT Glass Lab, and an MIT cap which, she noted, could be worn by Rutte when he is cycling to work. Rutte is known, in part, for bicycling to the office, and the Netherlands has the densest set of bike paths in the world.</p>","descriptionType":"html","publishedDate":"Mon, 22 Jul 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"84c6bea57836d906fd8b4685ee121f60","bgimgJsdelivr":"","metaImg":"","author":"Peter Dizikes | MIT News Office","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Spotting objects amid clutter","link":"https://news.mit.edu/2019/spotting-objects-cars-robots-0620","description":"<p>A new MIT-developed technique enables robots to quickly identify objects hidden in a three-dimensional cloud of data, reminiscent of how some people can make sense of a densely patterned “Magic Eye” image if they observe it in just the right way.</p>\n<p>Robots typically “see” their environment through sensors that collect and translate a visual scene into a matrix of dots. Think of the world of, well, “The Matrix,” except that the 1s and 0s seen by the fictional character Neo are replaced by dots — lots of dots — whose patterns and densities outline the objects in a particular scene.</p>\n<p>Conventional techniques that try to pick out objects from such clouds of dots, or point clouds, can do so with either speed or accuracy, but not both.</p>\n<p>With their new technique, the researchers say a robot can accurately pick out an object, such as a small animal, that is otherwise obscured within a dense cloud of dots, within seconds of receiving the visual data. The team says the technique can be used to improve a host of situations in which machine perception must be both speedy and accurate, including driverless cars and robotic assistants in the factory and the home.</p>\n<p>“The surprising thing about this work is, if I ask you to find a bunny in this cloud of thousands of points, there’s no way you could do that,” says Luca Carlone, assistant professor of aeronautics and astronautics and a member of MIT’s Laboratory for Information and Decision Systems (LIDS). “But our algorithm is able to see the object through all this clutter. So we’re getting to a level of superhuman performance in localizing objects.”</p>\n<p>Carlone and graduate student Heng Yang will present details of the technique later this month at the Robotics: Science and Systems conference in Germany.</p>\n<p><strong>“Failing without knowing”</strong></p>\n<p>Robots currently attempt to identify objects in a point cloud by comparing a template object — a 3-D dot representation of an object, such as a rabbit — with a point cloud representation of the real world that may contain that object. The template image includes “features,” or collections of dots that indicate characteristic curvatures or angles of that object, such the bunny’s ear or tail. Existing algorithms first extract similar features from the real-life point cloud, then attempt to match those features and the template’s features, and ultimately rotate and align the features to the template to determine if the point cloud contains the object in question.</p>\n<p>But the point cloud data that streams into a robot’s sensor invariably includes errors, in the form of dots that are in the wrong position or incorrectly spaced, which can significantly confuse the process of feature extraction and matching. As a consequence, robots can make a huge number of wrong associations, or what researchers call “outliers” between point clouds, and ultimately misidentify objects or miss them entirely.</p>\n<p>Carlone says state-of-the-art algorithms are able to sift the bad associations from the good once features have been matched, but they do so in “exponential time,” meaning that even a cluster of processing-heavy computers, sifting through dense point cloud data with existing algorithms, would not be able to solve the problem in a reasonable time. Such techniques, while accurate, are impractical for analyzing larger, real-life datasets containing dense point clouds.</p>\n<p>Other algorithms that can quickly identify features and associations do so hastily, creating a huge number of outliers or misdetections in the process, without being aware of these errors.</p>\n<p>“That’s terrible if this is running on a self-driving car, or any safety-critical application,” Carlone says. “Failing without knowing you’re failing is the worst thing an algorithm can do.”</p>\n<p><strong>A relaxed view</strong></p>\n<p>Yang and Carlone instead devised a technique that prunes away outliers in “polynomial time,” meaning that it can do so quickly, even for increasingly dense clouds of dots. The technique can thus quickly and accurately identify objects hidden in cluttered scenes.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/objectsclutter-1.gif\" style=\"width: 500px; height: 281px;\" /></p> \n<p><em><span style=\"font-size:10px;\"><span style=\"caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: Calibri, sans-serif; text-size-adjust: auto;\">The MIT-developed technique quickly and smoothly matches objects to those hidden in dense point clouds (left), versus existing techniques (right) that produce incorrect, disjointed matches.&nbsp;</span>Gif: Courtesy of the researchers</span></em></p>\n<p>The researchers first used conventional techniques to extract features of a template object from a point cloud. They then developed a three-step process to match the size, position, and orientation of the object in a point cloud with the template object, while simultaneously identifying good from bad feature associations.</p>\n<p>The team developed an “adaptive voting scheme” algorithm to prune outliers and match an object’s size and position. For size, the algorithm makes associations between template and point cloud features, then compares the relative distance between features in a template and corresponding features in the point cloud. If, say, the distance between two features in the point cloud is five times that of the corresponding points in the template, the algorithm assigns a “vote” to the hypothesis that the object is five times larger than the template object.</p>\n<p>The algorithm does this for every feature association. Then, the algorithm selects those associations that fall under the size hypothesis with the most votes, and identifies those as the correct associations, while pruning away the others. &nbsp;In this way, the technique simultaneously reveals the correct associations and the relative size of the object represented by those associations. The same process is used to determine the object’s position. &nbsp;</p>\n<p>The researchers developed a separate algorithm for rotation, which finds the orientation of the template object in three-dimensional space.</p>\n<p>To do this is an incredibly tricky computational task. Imagine holding a mug and trying to tilt it just so, to match a blurry image of something that might be that same mug. There are any number of angles you could tilt that mug, and each of those angles has a certain likelihood of matching the blurry image.</p>\n<p>Existing techniques handle this problem by considering each possible tilt or rotation of the object as a “cost” — the lower the cost, the more likely that that rotation creates an accurate match between features. Each rotation and associated cost is represented in a topographic map of sorts, made up of multiple hills and valleys, with lower elevations associated with lower cost.</p>\n<p>But Carlone says this can easily confuse an algorithm, especially if there are multiple valleys and no discernible lowest point representing the true, exact match between a particular rotation of an object and the object in a point cloud. Instead, the team developed a “convex relaxation” algorithm that simplifies the topographic map, with one single valley representing the optimal rotation. In this way, the algorithm is able to quickly identify the rotation that defines the orientation of the object in the point cloud.</p>\n<p>With their approach, the team was able to quickly and accurately identify three different objects — a bunny, a dragon, and a Buddha — hidden in point clouds of increasing density. They were also able to identify objects in real-life scenes, including a living room, in which the algorithm quickly was able to spot a cereal box and a baseball hat.</p>\n<p>Carlone says that because the approach is able to work in “polynomial time,” it can be easily scaled up to analyze even denser point clouds, resembling the complexity of sensor data for driverless cars, for example.</p>\n<p>“Navigation, collaborative manufacturing, domestic robots, search and rescue, and self-driving cars is where we hope to make an impact,” Carlone says.</p>\n<p>This research was supported in part by the Army Research Laboratory, the Office of Naval Research, and the Google Daydream Research Program.</p>","descriptionType":"html","publishedDate":"Thu, 20 Jun 2019 03:59:59 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/objectsclutter-1.gif","linkMd5":"3698eaf1b4fc029f5800333810d9d67e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn40@2020_4/2020/08/25/01-57-17-026_50c48d20ee0db479.webp","destWidth":500,"destHeight":281,"sourceBytes":2714554,"destBytes":2297222,"author":"Jennifer Chu | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/objectsclutter-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn40@2020_4/2020/08/25/01-57-17-026_50c48d20ee0db479.webp"},"publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Paul McEuen delivers inaugural Dresselhaus Lecture on cell-sized robots","link":"https://news.mit.edu/2019/paul-mceuen-delivers-inaugural-dresselhaus-lecture-cell-sized-robots-1204","description":"<p>Functional, intelligent robots the size of a single cell are within reach, said Cornell University Professor Paul McEuen at the inaugural Mildred S. Dresselhaus Lecture at MIT on Nov. 13.</p> \n<p>“To build a robot that is on the scale of 100 microns in size, and have it work, that’s a big dream,” said McEuen, the John A. Newman Professor of Physical Science at Cornell University and director of Kavli Institute at Cornell for Nanoscale Science. “One hundred microns is a very special size. It is the border between the visible and the invisible, or microscopic, world.”</p> \n<p>In a talk entitled “Cell-sized Sensors and Robots” in front of a large audience in MIT’s 10-250 lecture hall, McEuen introduced his concept for a new generation of machines that work at the microscale by combining microelectronics, solar cells, and light.&nbsp;The microbots, as he calls them, operate&nbsp;using optical wireless integrated circuits and&nbsp;surface electrochemical actuators.</p> \n<p><strong>Kicking off the Dresselhaus Lectures</strong></p> \n<p>Inaugurated this year to honor MIT professor and physicist Mildred \"Millie\" Dresselhaus, the Dresselhaus Lecture recognizes a significant figure in science and engineering whose&nbsp;leadership and impact echo the late Institute Professor's life, accomplishments, and values. The lecture will be presented annually in November, the month of her birth.</p> \n<p>Dresselhaus spent over 50 years at MIT, where she was a professor in the Department of Electrical Engineering and Computer Science (originally the Department of Electrical Engineering) as well as in the Department of Physics. She was MIT’s first female Institute Professor, co-organizer of the first MIT Women’s Forum, the first solo recipient of a Kavli Prize, and the first woman to win the National Medal of Science in the engineering category.</p> \n<p>Her research into the fundamental properties of carbon earned her the nickname the “Queen of Carbon Science.” She was also nationally known for her work to develop wider opportunities for women in science and engineering.</p> \n<p>“Millie was a physicist, a materials scientist, and an electrical engineer; an MIT professor, researcher, and doctoral supervisor; a prolific author; and a longtime leader in the scientific community,” said&nbsp;Asu Ozdaglar, current EECS department head, in her opening remarks. “Even in her final years, she was active in her field at MIT and in the department, attending EECS faculty meetings and playing an important role in developing the MIT.nano facility.”</p> \n<p><strong>Pushing the boundaries of physics</strong></p> \n<p>McEuen,&nbsp;who first met Dresselhaus when he attended graduate school at Yale University with her son, expressed what a privilege it was to celebrate Millie as the inaugural speaker. “When I think of my scientific heroes, it’s a very, very short list. And I think at the top of it would be Millie Dresselhaus.&nbsp;To be able to give this lecture in her honor means the world to me.”</p> \n<p>After earning his bachelor’s degree in engineering physics from the University of Oklahoma, McEuen continued his research at Yale University, where he completed his PhD in 1990 in applied physics. McEuen spent two years at MIT as a postdoc studying condensed matter physics, and then became a principal investigator at the Lawrence Berkeley National Laboratory. He spent eight years teaching at the University of California at Berkeley before joining the faculty at Cornell as a professor in the physics department in 2001.</p> \n<p>“Paul is a pioneer for our generation, exploring the domain of atoms and molecules to push the frontier even further. It is no exaggeration to say that his discoveries and innovations will help define the Nano Age,” said Vladimir Bulović, the founding faculty director of MIT.nano and the&nbsp;Fariborz Maseeh (1990) Professor in Emerging Technology.</p> \n<p><strong>“</strong><strong>The world is our oyster”</strong></p> \n<p>McEuen joked at the beginning of his talk that speaking of technology measured in microns sounds “so 1950s” in today’s world, in which researchers can manipulate at the scale of nanometers. One micron — an abbreviation for micrometer — is one millionth of a meter; a nanometer is one billionth of a meter.</p> \n<p>“[But] if you want a micro robot, you need nanoscale parts. Just as the birth of the transistor gave rise to all the computational systems we have now,” he said, “the birth of simple, nanoscale mechanical and electronic elements is going to give birth to a robotics technology at the microscopic scale of less than 100 microns.”</p> \n<p>The motto of McEuen and his research group at Cornell is “anything, as long as it’s small.” This focus includes fundamentals of nanostructures, atomically-thin origami for metamaterials and micromachines, and microscale smart phones and optobots. McEuen emphasized the importance of borrowing from other fields, such as microelectronics technology, to build something new. Cornell researchers have used this technology to build an optical wireless integrated circuit (OWIC) — essentially a microscopic cellphone made of solar cells that power it and receive external information, a simple transistor circuit to serve as its brain, and a light-emitting diode to blink out data.</p> \n<p>Why make something so small? The first reason is cost; the second is its wide array of applications. Such tiny devices could measure voltage or temperature, making them useful for microfluidic experiments. In the future, they could be deployed&nbsp;as&nbsp;smart, secure tags for counterfeiting, invisible sensors for the internet of things, or used for neural interfacing to measure electrical activity in the brain.</p> \n<p>Adding a&nbsp;surface electrochemical actuator to these OWICs brings mechanical movement to McEuen’s microbots. By capping a very thin piece of platinum on one side and applying a voltage to the other, “we could make all kinds of cool things.”</p> \n<p>At the end of his talk, McEuen answered audience questions moderated by&nbsp;Bulović, such as how do the microbots communicate with one another and what is their functional lifespan. He closed&nbsp;with a final quote from Millie Dresselhaus: “Follow your interests, get the best available education and training, set your sights high, be persistent, be flexible, keep your options open, accept help when offered, and be prepared to help others.”</p> \n<p>Nominations for the 2020 Dresselhaus lecture can be submitted <a href=\"https://mitnano.mit.edu/dresselhaus-lecture\" target=\"_blank\">on MIT.nano’s website</a>. Any significant figure in science and engineering from anywhere in the world may be considered.</p>","descriptionType":"html","publishedDate":"Wed, 04 Dec 2019 20:30:01 +0000","feedId":12364,"bgimg":"","linkMd5":"244f670452095f1db1bd0d61cbe385d5","bgimgJsdelivr":"","metaImg":"","author":"Amanda Stoll | MIT.nano","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"How to design and control robots with stretchy, flexible bodies","link":"https://news.mit.edu/2019/model-design-control-robots-target-1122","description":"<p>MIT researchers have invented a way to efficiently optimize the control and design of soft robots for target tasks, which has traditionally been a monumental undertaking in computation.</p>\n<p>Soft robots have springy, flexible, stretchy bodies that can essentially move an infinite number of ways at any given moment. Computationally, this represents a highly complex “state representation,” which describes how each part of the robot is moving. State representations for soft robots can have potentially millions of dimensions, making it difficult to calculate the optimal way to make a robot complete complex tasks.</p>\n<p>At the Conference on Neural Information Processing Systems next month, the MIT researchers will present a model that learns a compact, or “low-dimensional,” yet detailed state representation, based on the underlying physics of the robot and its environment, among other factors. This helps the model iteratively co-optimize movement control and material design parameters catered to specific tasks.</p>\n<p>“Soft robots are infinite-dimensional creatures that bend in a billion different ways at any given moment,” says first author Andrew Spielberg, a graduate student in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “But, in truth, there are natural ways soft objects are likely to bend. We find the natural states of soft robots can be described very compactly in a low-dimensional description. We optimize control and design of soft robots by learning a good description of the likely states.”</p>\n<p>In simulations, the model enabled 2D and 3D soft robots to complete tasks — such as moving certain distances or reaching a target spot —more quickly and accurately than current state-of-the-art methods. The researchers next plan to implement the model in real soft robots.</p>\n<p>Joining Spielberg on the paper are CSAIL graduate students Allan Zhao, Tao Du, and Yuanming Hu; Daniela Rus, director of CSAIL and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science; and Wojciech Matusik, an MIT associate professor in electrical engineering and computer science and head of the Computational Fabrication Group.</p>\n<p><strong>“Learning-in-the-loop”</strong></p>\n<p>Soft robotics is a relatively new field of research, but it holds promise for advanced robotics. For instance, flexible bodies could offer safer interaction with humans, better object manipulation, and more maneuverability, among other benefits.</p>\n<p>Control of robots in simulations relies on an “observer,” a program that computes variables that see how the soft robot is moving to complete a task. In previous work, the researchers decomposed the soft robot into hand-designed clusters of simulated particles. Particles contain important information that help narrow down the robot’s possible movements. If a robot attempts to bend a certain way, for instance, actuators may resist that movement enough that it can be ignored. But, for such complex robots, manually choosing which clusters to track during simulations can be tricky.</p>\n<p>Building off that work, the researchers designed a “learning-in-the-loop optimization” method, where all optimized parameters are learned during a single feedback loop over many simulations. And, at the same time as learning optimization —&nbsp;or “in the loop” — the method also learns the state representation.</p>\n<p>The model employs a technique called a material point method (MPM), which simulates the behavior of particles of continuum materials, such as foams and liquids, surrounded by a background grid. In doing so, it captures the particles of the robot and its observable environment into pixels or 3D pixels, known as voxels, without the need of any additional computation. &nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>In a learning phase, this raw particle grid information is fed into a machine-learning component that learns to input an image, compress it to a low-dimensional representation, and decompress the representation back into the input image. If this “autoencoder” retains enough detail while compressing the input image, it can accurately recreate the input image from the compression.</p>\n<p>In the researchers’ work, the autoencoder’s learned compressed representations serve as the robot’s low-dimensional state representation. In an optimization phase, that compressed representation loops back into the controller, which outputs a calculated actuation for how each particle of the robot should move in the next MPM-simulated step.</p>\n<p>Simultaneously, the controller uses that information to adjust the optimal stiffness for each particle to achieve its desired movement. In the future, that material information can be useful for 3D-printing soft robots, where each particle spot may be printed with slightly different stiffness. “This allows for creating robot designs catered to the robot motions that will be relevant to specific tasks,” Spielberg says. “By learning these parameters together, you keep everything as synchronized as much as possible to make that design process easier.”</p>\n<p><strong>Faster optimization</strong></p>\n<p>All optimization information is, in turn, fed back into the start of the loop to train the autoencoder. Over many simulations, the controller learns the optimal movement and material design, while the autoencoder learns the increasingly more detailed state representation. “The key is we want that low-dimensional state to be very descriptive,” Spielberg says.</p>\n<p>After the robot gets to its simulated final state over a set period of time —&nbsp;say, as close as possible to the target destination —&nbsp;it updates a “loss function.” That’s a critical component of machine learning, which tries to minimize some error. In this case, it minimizes, say, how far away the robot stopped from the target. That loss function flows back to the controller, which uses the error signal to tune all the optimized parameters to best complete the task.</p>\n<p>If the researchers tried to directly feed all the raw particles of the simulation into the controller, without the compression step, “running and optimization time would explode,” Spielberg says. Using the compressed representation, the researchers were able to decrease the running time for each optimization iteration from several minutes down to about 10 seconds.</p>\n<p>The researchers validated their model on simulations of various 2D and 3D biped and quadruped robots. They researchers also found that, while robots using traditional methods can take up to 30,000 simulations to optimize these parameters, robots trained on their model took only about 400 simulations.</p> \n<p>\"Our goal is to enable quantum leaps in the way engineers go from specification to design, prototyping, and programming of soft robots. In this paper, we explore the potential of co-optimizing the body and control system of a soft robot can lead the rapid creation of soft bodied robots customized to the tasks they have to do,\" Rus says.</p>\n<p>Deploying the model into real soft robots means tackling issues with real-world noise and uncertainty that may decrease the model’s efficiency and accuracy. But, in the future, the researchers hope to design a full pipeline, from simulation to fabrication, for soft robots.</p>","descriptionType":"html","publishedDate":"Fri, 22 Nov 2019 05:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"839983d2bc456f7e0902c9ea117d34ba","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Self-transforming robot blocks jump, spin, flip, and identify each other","link":"https://news.mit.edu/2019/self-transforming-robot-blocks-jump-spin-flip-identify-each-other-1030","description":"<p>Swarms of simple, interacting robots have the potential to unlock stealthy abilities for accomplishing complex tasks. Getting these robots to achieve a true hive-like mind of coordination, though, has proved to be a hurdle.</p> \n<p>In an effort to change this, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with a surprisingly simple scheme: self-assembling robotic cubes that can climb over and around one another, leap through the air, and roll across the ground.</p> \n<p>Six years after the project’s first iteration, the robots can now “communicate” with each other using a barcode-like system on each face of the block that allows the modules to identify each other. The autonomous fleet of 16 blocks can now accomplish simple tasks or behaviors, such as forming a line, following arrows, or tracking light.</p> \n<p>Inside each modular “M-Block” is a flywheel that moves at 20,000 revolutions per minute, using angular momentum when the flywheel is braked. On each edge and every face are permanent magnets that let any two cubes attach to each other.</p> \n<p>While the cubes can’t be manipulated quite as easily as, say, those from the video game \"Minecraft,\" the team envisions strong applications in inspection, and eventually disaster response. Imagine a burning building where a staircase has disappeared. In the future, you can envision simply throwing M-Blocks on the ground, and watching them build out a temporary staircase for climbing up to the roof, or down to the basement to rescue victims.</p> \n<p>“M stands for motion, magnet, and magic,” says MIT Professor and CSAIL Director Daniela Rus. “'Motion,' because the cubes can move by jumping. 'Magnet,' because the cubes can connect to other cubes using magnets, and once connected they can move together and connect to assemble structures. 'Magic,' because we don’t see any moving parts, and the cube appears to be driven by magic.”</p> \n<p>While the mechanism is quite intricate on the inside, the exterior is just the opposite, which enables more robust connections. Beyond inspection and rescue, the researchers also imagine using the blocks for things like gaming, manufacturing, and health care.</p>\n<p>“The unique thing about our approach is that it’s inexpensive, robust, and potentially easier to scale to a million modules,'' says CSAIL PhD student John Romanishin, lead author on a new paper about the system. “M-Blocks can move in a general way. Other robotic systems have much more complicated movement mechanisms that require many steps, but our system is more scalable.”</p> \n<p>Romanishin wrote the paper alongside Rus and undergraduate student John Mamish of the University of Michigan. They will present the paper on M-blocks at IEEE’s International Conference on Intelligent Robots and Systems in November in Macau.</p> \n<p>Previous modular robot systems typically tackle movement using unit modules with small robotic arms known as external actuators. These systems require a lot of coordination for even the simplest movements, with multiple commands for one jump or hop.</p> \n<p>On the communication side, other attempts have involved the use of infrared light or radio waves, which can quickly get clunky: If you have lots of robots in a small area and they're all trying to send each other signals, it opens up a messy channel of conflict and confusion.</p> \n<p>When a system uses radio signals to communicate, the signals can interfere with each other when there are many radios in a small volume.</p> \n<p>Back in 2013, the team built out their mechanism for M-Blocks. They created six-faced cubes that move about using something called “inertial forces.” This means that, instead of using moving arms that help connect the structures, the blocks have a mass inside of them which they “throw” against the side of the module, which causes the block to rotate and move.</p> \n<p>Each module can move in four cardinal directions when placed on any one of the six faces, which results in 24 different movement directions. Without little arms and appendages sticking out of the blocks, it’s a lot easier for them to stay free of damage and avoid collisions.</p> \n<p>Knowing that the team had tackled the physical hurdles, the critical challenge still persisted: How to make these cubes communicate and reliably identify the configuration of neighboring modules?</p> \n<p>Romanishin came up with algorithms designed to help the robots accomplish simple tasks, or \"behaviors,” which led them to the idea of a barcode-like system where the robots can sense the identity and face of what other blocks they’re connected to.</p> \n<p>In one experiment, the team had the modules turn into a line from a random structure, and they watched if the modules could determine the specific way that they were connected to each other. If they weren’t, they’d have to pick a direction and roll that way until they ended up on the end of the line.</p> \n<p>Essentially, the blocks used the configuration of how they're connected to each other in order to guide the motion that they choose to move — and 90 percent of the M-Blocks succeeded in getting into a line.</p> \n<p>The team notes that building out the electronics was very challenging, especially when trying to fit intricate hardware inside such a small package. To make the M-Block swarms a larger reality, the team wants just that — more and more robots to make bigger swarms with stronger capabilities for various structures.</p> \n<p>The project was supported, in part, by the National Science Foundation and Amazon Robotics.</p>","descriptionType":"html","publishedDate":"Wed, 30 Oct 2019 04:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"76abfeae478447c4206c5da653861b35","bgimgJsdelivr":"","metaImg":"","author":"Rachel Gordon | MIT CSAIL","publishedOrCreatedDate":1598320622358},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Automated system generates robotic parts for novel tasks","link":"https://news.mit.edu/2019/automated-design-print-actuators-robotics-0712","description":"<p>An automated system developed by MIT researchers designs and 3-D prints complex robotic parts called actuators that are optimized according to an enormous number of specifications. In short, the system does automatically what is virtually impossible for humans to do by hand. &nbsp;</p>\n<p>In a paper published today in <em>Science Advances</em>, the researchers demonstrate the system by fabricating actuators — devices that mechanically control robotic systems in response to electrical signals — that show different black-and-white images at different angles. One actuator, for instance, portrays a Vincent van Gogh portrait when laid flat. Tilted an angle when it’s activated, however, it portrays the famous Edvard Munch painting “The Scream.” The researchers also 3-D printed floating water lilies with petals equipped with arrays of actuators and hinges that fold up in response to magnetic fields run through conductive fluids.</p>\n<p>The actuators are made from a patchwork of three different materials, each with a different light or dark color and a property — such as flexibility and magnetization — that controls the actuator’s angle in response to a control signal. Software first breaks down the actuator design into millions of three-dimensional pixels, or “voxels,” that can each be filled with any of the materials. Then, it runs millions of simulations, filling different voxels with different materials. Eventually, it lands on the optimal placement of each material in each voxel to generate two different images at two different angles. A custom 3-D printer then fabricates the actuator by dropping the right material into the right voxel, layer by layer.</p>\n<p>“Our ultimate goal is to automatically find an optimal design for any problem, and then use the output of our optimized design to fabricate it,” says first author Subramanian Sundaram PhD ’18, a former graduate student in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “We go from selecting the printing materials, to finding the optimal design, to fabricating the final product in almost a completely automated way.”</p>\n<p>The shifting images demonstrates what the system can do. But actuators optimized for appearance and function could also be used for biomimicry in robotics. For instance, other researchers are designing underwater robotic skins with actuator arrays meant to mimic denticles on shark skin. Denticles collectively deform to decrease drag for faster, quieter swimming. “You can imagine underwater robots having whole arrays of actuators coating the surface of their skins, which can be optimized for drag and turning efficiently, and so on,” Sundaram says.</p>\n<p>Joining Sundaram on the paper are: Melina Skouras, a former MIT postdoc; David S. Kim, a former researcher in the Computational Fabrication Group; Louise van den Heuvel ’14, SM ’16; and Wojciech Matusik, an MIT associate professor in electrical engineering and computer science and head of the Computational Fabrication Group.</p>\n<p><strong>Navigating the “combinatorial explosion”</strong></p>\n<p>Robotic actuators today are becoming increasingly complex. Depending on the application, they must be optimized for weight, efficiency, appearance, flexibility, power consumption, and various other functions and performance metrics. Generally, experts manually calculate all those parameters to find an optimal design.&nbsp;&nbsp;</p>\n<p>Adding to that complexity, new 3-D-printing techniques can now use multiple materials to create one product. That means the design’s dimensionality becomes incredibly high. “What you’re left with is what’s called a ‘combinatorial explosion,’ where you essentially have so many combinations of materials and properties that you don’t have a chance to evaluate every combination to create an optimal structure,” Sundaram says.</p>\n<p>In their work, the researchers first customized three polymer materials with specific properties they needed to build their actuators: color, magnetization, and rigidity. In the end, they produced a near-transparent rigid material, an opaque flexible material used as a hinge, and a brown nanoparticle material that responds to a magnetic signal. They plugged all that characterization data into a property library.</p>\n<p>The system takes as input grayscale image examples —&nbsp;such as the flat actuator that displays the Van Gogh portrait but tilts at an exact angle to show “The Scream.” It basically executes a complex form of trial and error that’s somewhat like rearranging a Rubik’s Cube, but in this case around 5.5 million voxels are iteratively reconfigured to match an image and meet a measured angle.</p> \n<p><img alt=\"\" src=\"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Actuator-motion.gif\" style=\"width: 380px; height: 214px;\" /></p>\n<p>Initially, the system draws from the property library to randomly assign different materials to different voxels. Then, it runs a simulation to see if that arrangement portrays the two target images, straight on and at an angle. If not, it gets an error signal. That signal lets it know which voxels are on the mark and which should be changed. Adding, removing, and shifting around brown magnetic voxels, for instance, will change the actuator’s angle when a magnetic field is applied. But, the system also has to consider how aligning those brown voxels will affect the image.</p>\n<p><strong>Voxel by voxel</strong></p>\n<p>To compute the actuator’s appearances at each iteration, the researchers adopted a computer graphics technique called “ray-tracing,” which simulates the path of light interacting with objects. Simulated light beams shoot through the actuator at each column of voxels. Actuators can be fabricated with more than 100 voxel layers. Columns can contain more than 100 voxels, with different sequences of the materials that radiate a different shade of gray when flat or at an angle.</p>\n<p>When the actuator is flat, for instance, the light beam may shine down on a column containing many brown voxels, producing a dark tone. But when the actuator tilts, the beam will shine on misaligned voxels. Brown voxels may shift away from the beam, while more clear voxels may shift into the beam, producing a lighter tone. The system uses that technique to align dark and light voxel columns where they need to be in the flat and angled image. After 100 million or more iterations, and anywhere from a few to dozens of hours, the system will find an arrangement that fits the target images.</p>\n<p>“We’re comparing what that [voxel column] looks like when it’s flat or when it’s titled, to match the target images,” Sundaram says. “If not, you can swap, say, a clear voxel with a brown one. If that’s an improvement, we keep this new suggestion and make other changes over and over again.”</p>\n<p>To fabricate the actuators, the researchers built a custom 3-D printer that uses a technique called “drop-on-demand.” Tubs of the three materials are connected to print heads with hundreds of nozzles that can be individually controlled. The printer fires a 30-micron-sized droplet of the designated material into its respective voxel location. Once the droplet lands on the substrate, it’s solidified. In that way, the printer builds an object, layer by layer.</p>\n<p>The work could be used as a stepping stone for designing larger structures, such as airplane wings, Sundaram says. Researchers, for instance, have similarly started breaking down airplane wings into smaller voxel-like blocks to optimize their designs for weight and lift, and other metrics. “We’re not yet able to print wings or anything on that scale, or with those materials. But I think this is a first step toward that goal,” Sundaram says.</p>","descriptionType":"html","publishedDate":"Fri, 12 Jul 2019 17:59:59 +0000","feedId":12364,"bgimg":"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Actuator-motion.gif","linkMd5":"c07103354b409b22f632af6e689ead07","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn9@2020_4/2020/08/25/01-57-15-513_c2e68528593de9bb.webp","destWidth":380,"destHeight":214,"sourceBytes":7020195,"destBytes":295978,"author":"Rob Matheson | MIT News Office","articleImgCdnMap":{"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Actuator-motion.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn9@2020_4/2020/08/25/01-57-15-513_c2e68528593de9bb.webp"},"publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Deep learning with point clouds","link":"https://news.mit.edu/2019/deep-learning-point-clouds-1021","description":"<p>If you’ve ever seen a self-driving car in the wild, you might wonder about that spinning cylinder on top of it.&nbsp;</p> \n<p>It’s a “lidar sensor,” and it’s what allows the car to navigate the world. By sending out pulses of infrared light and measuring the time it takes for them to bounce off objects, the sensor creates a “point cloud” that builds a 3D snapshot of the car’s surroundings.&nbsp;</p> \n<p>Making sense of raw point-cloud data is difficult, and before the age of machine learning it traditionally required highly trained engineers to tediously specify which qualities they wanted to capture by hand. But in a new series of papers out of MIT’s <a href=\"http://csail.mit.edu\">Computer Science and Artificial Intelligence Laboratory</a> (CSAIL), researchers show that they can use deep learning to automatically process point clouds for a wide range of 3D-imaging applications.</p> \n<p>“In computer vision and machine learning today, 90 percent of the advances deal only with two-dimensional images,” says MIT Professor Justin Solomon, who was senior author of the new series of papers spearheaded by PhD student Yue Wang. “Our work aims to address a fundamental need to better represent the 3D world, with application not just in autonomous driving, but any field that requires understanding 3D shapes.”&nbsp;</p> \n<p>Most previous approaches haven’t been especially successful at capturing the patterns from data that are needed to get meaningful information out of a bunch of 3D points in space. But in one of the team’s papers, they showed that their “EdgeConv” method of analyzing point clouds using a type of neural network called a dynamic graph convolutional neural network allowed them to classify and segment individual objects.&nbsp;</p> \n<p>“By building ‘graphs’ of neighboring points, the algorithm can capture hierarchical patterns and therefore infer multiple types of generic information that can be used by a myriad of downstream tasks,” says Wadim Kehl, a machine learning scientist at Toyota Research Institute who was not involved in the work.&nbsp;</p> \n<p>In addition to developing EdgeConv, the team also explored other specific aspects of point-cloud processing. For example, one challenge is that most sensors change perspectives as they move around the 3D world; every time we take a new scan of the same object, its position may be different than the last time we saw it. To merge multiple point clouds together into a single detailed view of the world, you need to align multiple 3D points in a process called “registration.”&nbsp;</p> \n<p>Registration is vital for many forms of imaging, from satellite data to medical procedures. For example, when a doctor has to take multiple magnetic resonance imaging scans of a patient over time, registration is what makes it possible to align the scans to see what’s changed.&nbsp;</p> \n<p>“Registration is what allows us to integrate 3D data from different sources into a common coordinate system,” says Wang. “Without it, we wouldn’t actually be able to get as meaningful information from all these methods that have been developed.”</p> \n<p>Solomon and Wang’s second paper demonstrates a new registration algorithm called “Deep Closest Point” (DCP) that was shown to better find a point cloud’s distinguishing patterns, points, and edges (known as “local features”) in order to align it with other point clouds. This is especially important for such tasks as enabling self-driving cars to situate themselves in a scene (“localization”), as well as for robotic hands to locate and grasp individual objects.</p> \n<p>One limitation of DCP is that it assumes we can see an entire shape instead of just one side. This means it can’t handle the more difficult task of aligning partial views of shapes (known as “partial-to-partial registration”). As a result, in a third paper the researchers presented an improved algorithm for this task that they call the Partial Registration Network (PRNet).&nbsp;</p> \n<p>Solomon says that existing 3D data tends to be “quite messy and unstructured compared to 2D images and photographs.” His team sought to figure out how to get meaningful information out of all that disorganized 3D data without the controlled environment that a lot of machine learning technologies now require.</p> \n<p>A key observation behind the success of DCP and PRNet is the idea that a critical aspect of point-cloud processing is context. The geometric features on point cloud A that suggest the best ways to align it to point cloud B may be different from the features needed to align it to point cloud C. For example, in partial registration, an interesting part of a shape in one point cloud may not be visible in the other — making it useless for registration.</p> \n<p>Wang says that the team’s tools have already been deployed by many researchers in the computer vision community and beyond. Even physicists are using them for an application the CSAIL team had never considered: <a href=\"http://arxiv.org/abs/1902.08570\">particle</a> <a href=\"http://arxiv.org/pdf/1902.07987.pdf\">physics</a>.&nbsp;</p> \n<p>Moving forward, the researchers hope to use the algorithms on real-world data, including data gathered from self-driving cars. Wang says they also plan to explore the potential of training their systems using self-supervised learning, to minimize the amount of human annotation needed.</p> \n<p>Solomon and Wang were the two sole authors of the DCP and PRNet papers. Their co-authors on the EdgeConv paper were research assistant Yongbin Sun and Professor Sanjay Sarma of MIT, alongside postdoc Ziwei Liu of University of California at Berkeley and Professor Michael M. Bronstein of Imperial College London.&nbsp;</p> \n<p>The projects were supported, in part, by the U.S. Air Force, the U.S. Army Research Office, Amazon, Google Research, IBM, the National Science Foundation, the Skoltech-MIT Next Generation Program, and the Toyota Research Institute.</p>","descriptionType":"html","publishedDate":"Mon, 21 Oct 2019 16:10:01 +0000","feedId":12364,"bgimg":"","linkMd5":"5e2e322977c56b2b278e9fdeb38c46d2","bgimgJsdelivr":"","metaImg":"","author":"Adam Conner-Simons | CSAIL","publishedOrCreatedDate":1598320622359},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Giving robots a better feel for object manipulation","link":"https://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417","description":"<p>A new learning system developed by MIT researchers improves robots’ abilities to mold materials into target shapes and make predictions about interacting with solid objects and liquids. The system, known as a learning-based particle simulator, could give industrial robots a more refined touch — and it may have fun applications in personal robotics, such as modelling clay shapes or rolling sticky rice for sushi.</p>\n<p>In robotic planning, physical simulators are models that capture how different materials respond to force. Robots are “trained” using the models, to predict the outcomes of their interactions with objects, such as pushing a solid box or poking deformable clay. But traditional learning-based simulators mainly focus on rigid objects and are unable to handle fluids or softer objects. Some more accurate physics-based simulators can handle diverse materials, but rely heavily on approximation techniques that introduce errors when robots interact with objects in the real world.</p>\n<p>In a paper being presented at the International Conference on Learning Representations in May, the researchers describe a new model that learns to capture how small portions of different materials — “particles” — interact when they’re poked and prodded. The model directly learns from data in cases where the underlying physics of the movements are uncertain or unknown. Robots can then use the model as a guide to predict how liquids, as well as rigid and deformable materials, will react to the force of its touch. As the robot handles the objects, the model also helps to further refine the robot’s control.</p>\n<p>In experiments, a robotic hand with two fingers, called “RiceGrip,” accurately shaped a deformable foam to a desired configuration —&nbsp;such as a “T” shape — that serves as a proxy for sushi rice. In short, the researchers’ model serves as a type of “intuitive physics” brain that robots can leverage to reconstruct three-dimensional objects somewhat similarly to how humans do.</p>\n<p><strong>“</strong>Humans have an intuitive physics model in our heads, where we can imagine how an object will behave if we push or squeeze it. Based on this intuitive model, humans can accomplish amazing manipulation tasks that are far beyond the reach of current robots,” says first author Yunzhu Li, a graduate student in the Computer Science and Artificial Intelligence Laboratory (CSAIL).<strong> “</strong>We want to build this type of intuitive model for robots to enable them to do what humans can do.”</p>\n<p>“When children are 5 months old, they already have different expectations for solids and liquids,” adds co-author Jiajun Wu, a CSAIL graduate student. “That’s something we know at an early age, so maybe that’s something we should try to model for robots.”</p>\n<p>Joining Li and Wu on the paper are: Russ Tedrake, a CSAIL researcher and a professor in the Department of Electrical Engineering and Computer Science (EECS); Joshua Tenenbaum, a professor in the Department of Brain and Cognitive Sciences and a member of CSAIL and the Center for Brains, Minds, and Machines (CBMM); and Antonio Torralba, a professor in EECS and director of the MIT-IBM Watson AI Lab.</p>\n<p><strong>Dynamic graphs</strong></p>\n<p>A key innovation behind the model, called the “particle interaction network” (DPI-Nets), was creating dynamic interaction graphs, which consist of thousands of nodes and edges that can capture complex behaviors of so-called particles. In the graphs, each node represents a particle. Neighboring nodes are connected with each other using directed edges, which represent the interaction passing from one particle to the other. In the simulator, particles are hundreds of small spheres combined to make up some liquid or a deformable object.</p>\n<p>The graphs are constructed as the basis for a machine-learning system called a graph neural network. In training, the model over time learns how particles in different materials react and reshape. It does so by implicitly calculating various properties for each particle — such as its mass and elasticity — to predict if and where the particle will move in the graph when perturbed.</p>\n<p>The model then leverages a “propagation” technique, which instantaneously spreads a signal throughout the graph. The researchers customized the technique for each type of material — rigid, deformable, and liquid —&nbsp;to shoot a signal that predicts particles positions at certain incremental time steps. At each step, it moves and reconnects particles, if needed.</p>\n<p>For example, if a solid box is pushed, perturbed particles will be moved forward. Because all particles inside the box are rigidly connected with each other, every other particle in the object moves the same calculated distance, rotation, and any other dimension. Particle connections remain intact and the box moves as a single unit. But if an area of deformable foam is indented, the effect will be different. Perturbed particles move forward a lot, surrounding particles move forward only slightly, and particles farther away won’t move at all. With liquids being sloshed around in a cup, particles may completely jump from one end of the graph to the other. The graph must learn to predict where and how much all affected particles move, which is computationally complex.</p>\n<p><strong>Shaping and adapting</strong></p>\n<p>In their paper, the researchers demonstrate the model by tasking the&nbsp;two-fingered RiceGrip robot with clamping target shapes out of deformable foam. The robot first uses a depth-sensing camera and object-recognition techniques to identify the foam. The researchers randomly select particles inside the perceived shape to initialize the position of the particles. Then, the model adds edges between particles and reconstructs the foam into a dynamic graph customized for deformable materials.</p>\n<p>Because of the learned simulations, the robot already has a good idea of how each touch, given a certain amount of force, will affect each of the particles in the graph. As the robot starts indenting the foam, it iteratively matches the real-world position of the particles to the targeted position of the particles. Whenever the particles don’t align, it sends an error signal to the model. That signal tweaks the model to better match the real-world physics of the material.</p>\n<p>Next, the researchers aim to improve the model to help robots better predict interactions with partially observable scenarios, such as knowing how a pile of boxes will move when pushed, even if only the boxes at the surface are visible and most of the other boxes are hidden.</p>\n<p>The researchers are also exploring ways to combine the model with an end-to-end perception module by operating directly on images. This will be a joint project with Dan Yamins’s group; Yamin recently completed his postdoc at MIT and is now an assistant professor at Stanford University. “You’re dealing with these cases all the time where there’s only partial information,” Wu says. “We’re extending our model to learn the dynamics of all particles, while only seeing a small portion.”</p>","descriptionType":"html","publishedDate":"Wed, 17 Apr 2019 03:59:59 +0000","feedId":12364,"bgimg":"","linkMd5":"4554e872eaa76a9af042751ca24cf909","bgimgJsdelivr":"","metaImg":"","author":"Rob Matheson | MIT News Office","publishedOrCreatedDate":1598320622360},{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","title":"Letting robots manipulate cables","link":"https://news.mit.edu/2020/letting-robots-manipulate-cables-0713","description":"<p>For humans, it can be challenging to manipulate thin flexible objects like ropes, wires, or cables. But if these problems are hard for humans, they are nearly impossible for robots. As a cable slides between the fingers, its shape is constantly changing, and the robot’s fingers must be constantly sensing and adjusting the cable’s position and motion.</p> \n<p>Standard approaches have used a series of slow and incremental deformations, as well as mechanical fixtures, to get the job done. Recently, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and from the MIT Department of Mechanical Engineering pursued the task from a different angle, in a manner that more closely mimics us humans. The team’s <a href=\"http://www.roboticsproceedings.org/rss16/p029.pdf\" target=\"_blank\">new system</a> uses a pair of soft robotic grippers with high-resolution tactile sensors (and no added mechanical constraints) to successfully manipulate freely moving cables.</p> \n<p>One could imagine using a system like this for both industrial and household tasks, to one day enable robots to help us with things like tying knots, wire shaping, or even surgical suturing.&nbsp;</p> \n<p>The team’s first step was to build a novel two-fingered gripper. The opposing fingers are lightweight and quick moving, allowing nimble, real-time adjustments of force and position. On the tips of the fingers are vision-based <a href=\"http://news.mit.edu/2017/gelsight-robots-sense-touch-0605\" target=\"_self\">“GelSight” sensors</a>, built from soft rubber with embedded cameras. The gripper is mounted on a robot arm, which can move as part of the control system.</p> \n<p>The team’s second step was to create a perception-and-control framework to allow cable manipulation. For perception, they used the GelSight sensors to estimate the pose of the cable between the fingers, and to measure the frictional forces as the cable slides. Two controllers run in parallel: one modulates grip strength, while the other adjusts the gripper pose to keep the cable within the gripper.</p> \n<p>When mounted on the arm, the gripper could reliably follow a USB cable starting from a random grasp position. Then, in combination with a second gripper, the robot can move the cable “hand over hand” (as a human would) in order to find the end of the cable. It could also adapt to cables of different materials and thicknesses.</p> \n<p>As a further demo of its prowess, the robot performed an action that humans routinely do when plugging earbuds into a cell phone. Starting with a free-floating earbud cable, the robot was able to slide the cable between its fingers, stop when it felt the plug touch its fingers, adjust the plug’s pose, and finally insert the plug into the jack.&nbsp;</p> \n<p>“Manipulating soft objects is so common in our daily lives, like cable manipulation, cloth folding, and string knotting,” says Yu She, MIT postdoc and lead author on a new paper about the system. “In many cases, we would like to have robots help humans do this kind of work, especially when the tasks are repetitive, dull, or unsafe.”&nbsp;</p>\n<p><strong>String me along</strong>&nbsp;</p>\n<p>Cable following is challenging for two reasons. First, it requires controlling the “grasp force” (to enable smooth sliding), and the “grasp pose” (to prevent the cable from falling from the gripper’s fingers).&nbsp;&nbsp;</p>\n<p>This information is hard to capture from conventional vision systems during continuous manipulation, because it’s usually occluded, expensive to interpret, and sometimes inaccurate.&nbsp;</p> \n<p>What’s more, this information can’t be directly observed with just vision sensors, hence the team’s use of tactile<em> </em>sensors. The gripper’s joints are also flexible — protecting them from potential impact.&nbsp;</p> \n<p>The algorithms can also be generalized to different cables with various physical properties like material, stiffness, and diameter, and also to those at different speeds.&nbsp;</p> \n<p>When comparing different controllers applied to the team’s gripper, their control policy could retain the cable in hand for longer distances than three others. For example, the “open-loop” controller only followed 36 percent of the total length, the gripper easily lost the cable when it curved, and it needed many regrasps to finish the task.&nbsp;</p> \n<p><strong>Looking ahead&nbsp;</strong></p> \n<p>The team observed that it was difﬁcult to pull the cable back when it reached the edge of the ﬁnger, because of the convex surface of the GelSight sensor. Therefore, they hope to improve the ﬁnger-sensor shape to enhance the overall performance.&nbsp;</p> \n<p>In the future, they plan to study more complex cable manipulation tasks such as cable routing and cable inserting through obstacles, and they want to eventually explore autonomous cable manipulation tasks in the auto industry.</p> \n<p>Yu She wrote the paper alongside MIT PhD students Shaoxiong Wang, Siyuan Dong, and Neha Sunil; Alberto Rodriguez,&nbsp;MIT associate professor of mechanical engineering; and Edward Adelson, the <span class=\"person__info__def\">John and Dorothy Wilson Professor in the MIT Department of Brain and Cognitive Sciences</span>.&nbsp;</p> \n<p>This work was supported by the Amazon Research Awards, the Toyota Research Institute, and the Office of Naval Research.</p>","descriptionType":"html","publishedDate":"Mon, 13 Jul 2020 11:00:00 +0000","feedId":12364,"bgimg":"","linkMd5":"a44ce6a9143d3f33f5fcdafcfb2c92ac","bgimgJsdelivr":"","metaImg":"","author":"Rachel Gordon | MIT CSAIL","publishedOrCreatedDate":1598320622357}],"record":{"createdTime":"2020-08-25 09:57:02","updatedTime":"2020-08-25 09:57:02","feedId":12364,"fetchDate":"Tue, 25 Aug 2020 01:57:02 +0000","fetchMs":489,"handleMs":1326,"totalMs":32017,"newArticles":0,"totalArticles":50,"status":1,"type":0,"ip":"52.30.217.135","hostName":"europe66.herokuapp.com","requestId":"764751a04890464482065007b2c5cf03_12364","contentType":"application/rss+xml; charset=utf-8","totalBytes":42576032,"bgimgsTotal":9,"bgimgsGithubTotal":9,"articlesImgsTotal":16,"articlesImgsGithubTotal":16,"successGithubMap":{"myreaderx25":1,"myreaderx8":1,"myreaderx15":1,"myreaderx7":1,"myreaderx6":1,"myreaderx27":1,"myreaderx10":1,"myreaderx4":1,"myreaderx3":1,"myreaderx33":1,"myreaderx2":1,"myreaderx1":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx29":1,"myreaderx18":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:37:34","updatedTime":"2020-08-25 04:37:34","id":12364,"name":"MIT News - Robotics","url":"http://web.mit.edu/newsoffice/topic/mitrobotics-rss.xml","subscriber":null,"website":null,"icon":"https://news.mit.edu/themes/mit/assets/img/favicon/apple-icon-57x57.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx62/cdn6@2020_5/2020/08/25/01-57-01-472_843542d9c1308f57.png","description":"MIT news feed about: Robotics","weekly":null,"link":"https://news.mit.edu"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":28027446,"tmpBodyImgCdnBytes":14548586,"tmpBgImgCdnBytes":0,"extra4":{"start":1598320620478,"total":0,"statList":[{"spend":556,"msg":"获取xml内容"},{"spend":1326,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":7088,"msg":"正文链接上传到cdn"}]},"extra5":16,"extra6":16,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-020.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-032.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe21.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-1_0.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn20@2020_1/2020/08/25/01-57-15-533_2bb181d23032cc59.webp","sourceBytes":4486017,"destBytes":2431682,"targetWebpQuality":75,"feedId":12364,"totalSpendMs":3477,"convertSpendMs":1414,"createdTime":"2020-08-25 09:57:13","host":"us-040*","referer":"https://news.mit.edu/2020/sensorized-skin-soft-robots-0213","linkMd5ListStr":"33f0571d315efb232788fa019e2672ba,33f0571d315efb232788fa019e2672ba","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.3 MB","destSize":"2.3 MB","compressRate":"54.2%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Actuator-motion.gif","sourceStatusCode":200,"destWidth":380,"destHeight":214,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn9@2020_4/2020/08/25/01-57-15-513_c2e68528593de9bb.webp","sourceBytes":7020195,"destBytes":295978,"targetWebpQuality":60,"feedId":12364,"totalSpendMs":3721,"convertSpendMs":1185,"createdTime":"2020-08-25 09:57:13","host":"europe-25*","referer":"https://news.mit.edu/2019/automated-design-print-actuators-robotics-0712","linkMd5ListStr":"c07103354b409b22f632af6e689ead07,c07103354b409b22f632af6e689ead07","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.7 MB","destSize":"289 KB","compressRate":"4.2%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-1.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_6/2020/08/25/01-57-16-286_8440620f7df6c2f8.webp","sourceBytes":7294189,"destBytes":2829252,"targetWebpQuality":60,"feedId":12364,"totalSpendMs":4314,"convertSpendMs":2108,"createdTime":"2020-08-25 09:57:13","host":"us-016*","referer":"https://news.mit.edu/2019/micro-robots-walk-0702","linkMd5ListStr":"6271779ddbd4c69fa045f9a8358a7655,6271779ddbd4c69fa045f9a8358a7655","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7 MB","destSize":"2.7 MB","compressRate":"38.8%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-1.gif","sourceStatusCode":200,"destWidth":500,"destHeight":615,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn7@2020_6/2020/08/25/01-57-16-045_272b9257fbba6026.webp","sourceBytes":7636396,"destBytes":1583556,"targetWebpQuality":52,"feedId":12364,"totalSpendMs":4784,"convertSpendMs":2014,"createdTime":"2020-08-25 09:57:13","host":"europe-24*","referer":"https://news.mit.edu/2020/bionic-heart-prosthetic-valve-cardiac-0129","linkMd5ListStr":"a453ff748563a20e6aca74e4a66dfbc3,a453ff748563a20e6aca74e4a66dfbc3","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.3 MB","destSize":"1.5 MB","compressRate":"20.7%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/objectsclutter-1.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn40@2020_4/2020/08/25/01-57-17-026_50c48d20ee0db479.webp","sourceBytes":2714554,"destBytes":2297222,"targetWebpQuality":75,"feedId":12364,"totalSpendMs":4886,"convertSpendMs":3212,"createdTime":"2020-08-25 09:57:13","host":"us-54*","referer":"https://news.mit.edu/2019/spotting-objects-cars-robots-0620","linkMd5ListStr":"3698eaf1b4fc029f5800333810d9d67e,3698eaf1b4fc029f5800333810d9d67e","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 MB","destSize":"2.2 MB","compressRate":"84.6%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-1.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn23@2020_1/2020/08/25/01-57-17-085_5e7fa4f859a924ef.webp","sourceBytes":7838286,"destBytes":3801586,"targetWebpQuality":52,"feedId":12364,"totalSpendMs":5061,"convertSpendMs":2904,"createdTime":"2020-08-25 09:57:13","host":"us-028*","referer":"https://news.mit.edu/2019/robots-large-structures-little-pieces-1016","linkMd5ListStr":"fe099c7b7d40176efaf0d171e59d0e8e,fe099c7b7d40176efaf0d171e59d0e8e","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.5 MB","destSize":"3.6 MB","compressRate":"48.5%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/roboats-1.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn36@2020_1/2020/08/25/01-57-16-772_80656559f198c3b4.webp","sourceBytes":8006981,"destBytes":4439350,"targetWebpQuality":52,"feedId":12364,"totalSpendMs":6316,"convertSpendMs":2748,"createdTime":"2020-08-25 09:57:13","host":"europe67*","referer":"https://news.mit.edu/2019/roboats-autonomous-connect-assemble-0829","linkMd5ListStr":"b26d9d53fbfe21bde00a69f1747c70cf,b26d9d53fbfe21bde00a69f1747c70cf","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.6 MB","destSize":"4.2 MB","compressRate":"55.4%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-1.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn16@2020_5/2020/08/25/01-57-17-311_053559e65c44b78a.webp","sourceBytes":6895871,"destBytes":3180834,"targetWebpQuality":60,"feedId":12364,"totalSpendMs":7368,"convertSpendMs":2697,"createdTime":"2020-08-25 09:57:13","host":"europe62*","referer":"https://news.mit.edu/2019/mesh-structure-shape-temperature-changes-0930","linkMd5ListStr":"77ba5259c1b65209944a84af9bf4f01d,77ba5259c1b65209944a84af9bf4f01d","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.6 MB","destSize":"3 MB","compressRate":"46.1%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/MIT-Robot-Gripper.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn32@2020_3/2020/08/25/01-57-23-040_79cf4a8507cc1726.webp","sourceBytes":7742871,"destBytes":7167986,"targetWebpQuality":52,"feedId":12364,"totalSpendMs":11752,"convertSpendMs":9091,"createdTime":"2020-08-25 09:57:13","host":"us-004*","referer":"https://news.mit.edu/2019/robotic-faster-grip-adjust-1017","linkMd5ListStr":"df67e69453094d5de1d32fd338012bff,df67e69453094d5de1d32fd338012bff","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.4 MB","destSize":"6.8 MB","compressRate":"92.6%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/shape-shifter-2.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn56@2020_3/2020/08/25/01-57-26-429_8a0b87dd6580e87f.webp","sourceBytes":323357,"destBytes":310440,"targetWebpQuality":75,"feedId":12364,"totalSpendMs":3028,"convertSpendMs":809,"createdTime":"2020-08-25 09:57:25","host":"europe21*","referer":"https://news.mit.edu/2019/mesh-structure-shape-temperature-changes-0930","linkMd5ListStr":"77ba5259c1b65209944a84af9bf4f01d","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"315.8 KB","destSize":"303.2 KB","compressRate":"96%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/roboats-2.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn51@2020_6/2020/08/25/01-57-28-283_0f7a43e9d5a00deb.webp","sourceBytes":3858595,"destBytes":1430454,"targetWebpQuality":75,"feedId":12364,"totalSpendMs":4228,"convertSpendMs":2288,"createdTime":"2020-08-25 09:57:25","host":"us-008*","referer":"https://news.mit.edu/2019/roboats-autonomous-connect-assemble-0829","linkMd5ListStr":"b26d9d53fbfe21bde00a69f1747c70cf","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.7 MB","destSize":"1.4 MB","compressRate":"37.1%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-2.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn47@2020_1/2020/08/25/01-57-28-167_9c526646fbe25ca5.webp","sourceBytes":6057476,"destBytes":3143876,"targetWebpQuality":67,"feedId":12364,"totalSpendMs":4335,"convertSpendMs":2127,"createdTime":"2020-08-25 09:57:25","host":"us-020*","referer":"https://news.mit.edu/2020/sensorized-skin-soft-robots-0213","linkMd5ListStr":"33f0571d315efb232788fa019e2672ba","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.8 MB","destSize":"3 MB","compressRate":"51.9%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/robotic-assembly-4.gif","sourceStatusCode":200,"destWidth":500,"destHeight":346,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn63@2020_4/2020/08/25/01-57-28-436_bb092c7e3d55ec8f.webp","sourceBytes":2681105,"destBytes":1613960,"targetWebpQuality":75,"feedId":12364,"totalSpendMs":4531,"convertSpendMs":2632,"createdTime":"2020-08-25 09:57:25","host":"us-036*","referer":"https://news.mit.edu/2019/robots-large-structures-little-pieces-1016","linkMd5ListStr":"fe099c7b7d40176efaf0d171e59d0e8e","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 MB","destSize":"1.5 MB","compressRate":"60.2%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/micro-robots-2.gif","sourceStatusCode":200,"destWidth":500,"destHeight":234,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn59@2020_1/2020/08/25/01-57-27-990_2446456c2227d71e.webp","sourceBytes":6253480,"destBytes":1612618,"targetWebpQuality":67,"feedId":12364,"totalSpendMs":4751,"convertSpendMs":2188,"createdTime":"2020-08-25 09:57:25","host":"europe-58*","referer":"https://news.mit.edu/2019/micro-robots-walk-0702","linkMd5ListStr":"6271779ddbd4c69fa045f9a8358a7655","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6 MB","destSize":"1.5 MB","compressRate":"25.8%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/biorobotic-heart-2.gif","sourceStatusCode":200,"destWidth":500,"destHeight":380,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn67@2020_5/2020/08/25/01-57-29-303_03dc07760ab19b8d.webp","sourceBytes":5364426,"destBytes":2712890,"targetWebpQuality":67,"feedId":12364,"totalSpendMs":5193,"convertSpendMs":3328,"createdTime":"2020-08-25 09:57:25","host":"us-024*","referer":"https://news.mit.edu/2020/bionic-heart-prosthetic-valve-cardiac-0129","linkMd5ListStr":"a453ff748563a20e6aca74e4a66dfbc3","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.1 MB","destSize":"2.6 MB","compressRate":"50.6%"},{"code":1,"isDone":false,"source":"https://news.mit.edu/sites/default/files/images/inline/images/sensorized-skin-3.gif","sourceStatusCode":200,"destWidth":500,"destHeight":281,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn43@2020_6/2020/08/25/01-57-30-437_14ab6c28bb72b7f2.webp","sourceBytes":7928315,"destBytes":3724348,"targetWebpQuality":52,"feedId":12364,"totalSpendMs":6930,"convertSpendMs":3883,"createdTime":"2020-08-25 09:57:25","host":"us-032*","referer":"https://news.mit.edu/2020/sensorized-skin-soft-robots-0213","linkMd5ListStr":"33f0571d315efb232788fa019e2672ba","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.6 MB","destSize":"3.6 MB","compressRate":"47%"}],"successGithubMap":{"myreaderx25":1,"myreaderx8":1,"myreaderx15":1,"myreaderx7":1,"myreaderx6":1,"myreaderx27":1,"myreaderx10":1,"myreaderx4":1,"myreaderx3":1,"myreaderx33":1,"myreaderx2":1,"myreaderx1":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx29":1,"myreaderx18":1},"failGithubMap":{}}