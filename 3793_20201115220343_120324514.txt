{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-11-16 06:03:09","updatedTime":"2020-11-16 06:03:09","title":"Announcing the Objectron Dataset","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","description":"<span class=\"byline-author\">Posted by Adel Ahmadyan and Liangkai Zhang, Software Engineers, Google Research</span> <p>The state of the art in machine learning (ML) has achieved exceptional accuracy on many computer vision tasks solely by training models on photos. Building upon these successes and advancing 3D object understanding has great potential to power a wider range of applications, such as augmented reality, robotics, autonomy, and image retrieval. For example, earlier this year we released <a href=\"https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html\">MediaPipe Objectron</a>, a set of real-time 3D object detection models designed for mobile devices, which were trained on a fully annotated, real-world 3D dataset, that can predict objects’ 3D bounding boxes.  </p><p>Yet, understanding objects in 3D remains a challenging task due to the lack of large real-world datasets compared to 2D tasks (e.g., <a href=\"http://image-net.org/\">ImageNet</a>, <a href=\"https://cocodataset.org/#home\">COCO</a>, and <a href=\"https://storage.googleapis.com/openimages/web/index.html\">Open Images</a>).  To empower the research community for continued advancement in 3D object understanding, there is a strong need for the release of object-centric video datasets, which capture more of the 3D structure of an object, while matching the data format used for many vision tasks (i.e., video or camera streams), to aid in the training and benchmarking of machine learning models.  </p><p>Today, we are excited to release the <a href=\"https://github.com/google-research-datasets/Objectron/\">Objectron dataset</a>, a collection of short, object-centric video clips capturing a larger set of common objects from different angles. Each video clip is accompanied by AR session metadata that includes camera poses and sparse point-clouds. The data also contain manually annotated 3D bounding boxes for each object, which describe the object’s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images collected from a geo-diverse sample (covering 10 countries across five continents). </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-ZXTH_Ywi4Ks/X6R3ueMnqdI/AAAAAAAAGwo/CzfKP2_TCSUPlzZBfUVyofRplKNnvQ-ZwCLcBGAsYHQ/s1204/image4.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"804\" data-original-width=\"1204\" src=\"https://1.bp.blogspot.com/-ZXTH_Ywi4Ks/X6R3ueMnqdI/AAAAAAAAGwo/CzfKP2_TCSUPlzZBfUVyofRplKNnvQ-ZwCLcBGAsYHQ/s16000/image4.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Example videos in the Objectron dataset.</td></tr></tbody></table><p><b>A 3D Object Detection Solution</b><br />Along with the dataset, we are also sharing a <a href=\"http://solutions.mediapipe.dev/objectron\">3D object detection solution</a> for four categories of objects — shoes, chairs, mugs, and cameras. These models are released in <a href=\"http://mediapipe.dev\">MediaPipe</a>, Google's open source framework for cross-platform customizable ML solutions for live and streaming media, which also powers <a href=\"http://solutions.mediapipe.dev\">ML solutions</a> like on-device real-time <a href=\"https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html\">hand</a>, <a href=\"https://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html\">iris</a> and <a href=\"https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html\">body pose tracking</a>. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-hNyz0aw6N0k/X6R4CGM51XI/AAAAAAAAGww/l0S8e7cSbSwPRbZhuabL29UsUT1L8HvYwCLcBGAsYHQ/s1440/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"606\" data-original-width=\"1440\" src=\"https://1.bp.blogspot.com/-hNyz0aw6N0k/X6R4CGM51XI/AAAAAAAAGww/l0S8e7cSbSwPRbZhuabL29UsUT1L8HvYwCLcBGAsYHQ/s16000/image3.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Sample results of 3D object detection solution running on mobile.</td></tr></tbody></table><p>In contrast to the previously released <a href=\"https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html\">single-stage Objectron model</a>, these newest versions utilize a two-stage architecture. The first stage employs the <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\">TensorFlow Object Detection</a> model to find the 2D crop of the object. The second stage then uses the image crop to estimate the 3D bounding box while simultaneously computing the 2D crop of the object for the next frame, so that the object detector does not need to run every frame. The second stage 3D bounding box predictor runs at 83 FPS on Adreno 650 mobile GPU. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-cl63GVkiF50/X6R6uZvFD0I/AAAAAAAAGxI/6hUfkTGnZ2oTecEhSQymx270Ttklnf4qQCLcBGAsYHQ/s959/3DObjDetSoln.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"394\" data-original-width=\"959\" src=\"https://1.bp.blogspot.com/-cl63GVkiF50/X6R6uZvFD0I/AAAAAAAAGxI/6hUfkTGnZ2oTecEhSQymx270Ttklnf4qQCLcBGAsYHQ/s16000/3DObjDetSoln.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Diagram of a reference 3D object detection solution.</td></tr></tbody></table><p><b>Evaluation Metric for 3D Object Detection</b><br />With ground truth annotations, we evaluate the performance of 3D object detection models using 3D <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">intersection over union</a> (IoU) similarity statistics, a commonly used metric for computer vision tasks, which measures how close the bounding boxes are to the ground truth.  </p><p>We propose an algorithm for computing accurate 3D IoU values for general 3D-oriented boxes. First, we compute the intersection points between faces of the two boxes using <a href=\"https://en.wikipedia.org/wiki/Sutherland%E2%80%93Hodgman_algorithm\">Sutherland-Hodgman Polygon clipping algorithm</a>. This is similar to <a href=\"https://en.wikipedia.org/wiki/Clipping_(computer_graphics)\">frustum culling</a>, a technique used in computer graphics. The volume of the intersection is computed by the <a href=\"https://en.wikipedia.org/wiki/Convex_hull\">convex hull</a> of all the clipped polygons. Finally, the IoU is computed from the volume of the intersection and volume of the union of two boxes. We are releasing the <a href=\"https://github.com/google-research-datasets/Objectron\">evaluation metrics source code</a> along with the dataset. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-ACxubUnKR4M/X6R49vdskyI/AAAAAAAAGw8/Slr8jlMTONEKNHckpLunBBIMuptrXkXvwCLcBGAsYHQ/s1024/3DIoU.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"399\" data-original-width=\"1024\" src=\"https://1.bp.blogspot.com/-ACxubUnKR4M/X6R49vdskyI/AAAAAAAAGw8/Slr8jlMTONEKNHckpLunBBIMuptrXkXvwCLcBGAsYHQ/s16000/3DIoU.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Compute the 3D intersection over union using the polygon clipping algorithm, Left: Compute the intersection points of each face by clipping the polygon against the box. Right: Compute the volume of intersection by computing the convex hull of all intersection points (green).</td></tr></tbody></table><p><b>Dataset Format</b><br />The technical details of the Objectron dataset, including usage and tutorials, are available on the <a href=\"https://github.com/google-research-datasets/Objectron\">dataset website</a>. The dataset includes bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes, and is stored in the <a href=\"https://storage.googleapis.com/objectron\">objectron bucket</a> on <a href=\"https://cloud.google.com/storage/\">Google Cloud storage</a> with the following assets: </p><ul>  <li>The video sequences</li>  <li>The annotation labels (3D bounding boxes for objects)</li>  <li>AR metadata (such as camera poses, point clouds, and planar surfaces)</li>  <li>Processed dataset: shuffled version of the annotated frames, in tf.example format for images and SequenceExample format for videos.</li>  <li>Supporting scripts to run evaluation based on the metric described above</li>  <li>Supporting scripts to load the data into <a href=\"https://www.tensorflow.org/\">Tensorflow</a>, <a href=\"https://github.com/pytorch/xla\">PyTorch</a>, and&nbsp;<a href=\"https://github.com/google/jax\">Jax</a> and to visualize the dataset, including “Hello World” examples</li></ul><p>With the dataset, we are also open-sourcing a data-pipeline to parse the dataset in popular Tensorflow, PyTorch and Jax frameworks. Example <a href=\"https://github.com/google-research-datasets/Objectron\">colab notebooks</a> are also provided. </p><p>By releasing this Objectron dataset, we hope to enable the research community to push the limits of 3D object geometry understanding. We also hope to foster new research and applications, such as <a href=\"https://www.matthewtancik.com/nerf\">view synthesis</a>, <a href=\"https://arxiv.org/pdf/1912.06126.pdf\">improved 3D representation</a>, and unsupervised learning. Stay tuned for future activities and developments by <a href=\"https://groups.google.com/g/objectron\">joining our mailing list</a> and visiting <a href=\"https://github.com/google-research-datasets/Objectron\">our github page</a>. </p><p><b>Acknowledgements</b><br /><em>The research described in this post was done by Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, Mogan Shieh, Ryan Hickman, Buck Bourdon, Alexander Kanaukou, Chuo-Ling Chang, Matthias Grundmann, ‎and Tom Funkhouser. We thank Aliaksandr Shyrokau, Sviatlana Mialik, Anna Eliseeva, and the annotation team for their high quality annotations. We also would like to thank Jonathan Huang and Vivek Rathod for their guidance on TensorFlow Object Detection API.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=8_otZopCzfU:B-a3Ln8LEh4:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/8_otZopCzfU\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 09 Nov 2020 18:02:00 +0000","feedId":3793,"bgimg":"https://1.bp.blogspot.com/-ZXTH_Ywi4Ks/X6R3ueMnqdI/AAAAAAAAGwo/CzfKP2_TCSUPlzZBfUVyofRplKNnvQ-ZwCLcBGAsYHQ/s16000/image4.gif","linkMd5":"3243f11c065116962e03aff2cd1b5c7a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn98@2020_1/2020/11/15/22-03-34-370_de3839c76fffa3fe.webp","destWidth":1204,"destHeight":804,"sourceBytes":24103024,"destBytes":2631664,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-ZXTH_Ywi4Ks/X6R3ueMnqdI/AAAAAAAAGwo/CzfKP2_TCSUPlzZBfUVyofRplKNnvQ-ZwCLcBGAsYHQ/s16000/image4.gif":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn98@2020_1/2020/11/15/22-03-34-370_de3839c76fffa3fe.webp","https://1.bp.blogspot.com/-hNyz0aw6N0k/X6R4CGM51XI/AAAAAAAAGww/l0S8e7cSbSwPRbZhuabL29UsUT1L8HvYwCLcBGAsYHQ/s16000/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn11@2020_4/2020/11/15/22-03-41-519_b6d63e357d1c3ca9.webp","https://1.bp.blogspot.com/-cl63GVkiF50/X6R6uZvFD0I/AAAAAAAAGxI/6hUfkTGnZ2oTecEhSQymx270Ttklnf4qQCLcBGAsYHQ/s16000/3DObjDetSoln.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn22@2020_3/2020/11/15/22-03-36-499_783f17c82457e613.webp","https://1.bp.blogspot.com/-ACxubUnKR4M/X6R49vdskyI/AAAAAAAAGw8/Slr8jlMTONEKNHckpLunBBIMuptrXkXvwCLcBGAsYHQ/s16000/3DIoU.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn13@2020_3/2020/11/15/22-03-36-619_5305db3247f05764.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn10@2020_2/2020/11/15/22-03-36-499_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/8_otZopCzfU":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn25@2020_1/2020/11/15/22-03-36-508_1dc421f053ca0bfa.webp"},"publishedOrCreatedDate":1605477789327}],"record":{"createdTime":"2020-11-16 06:03:09","updatedTime":"2020-11-16 06:03:09","feedId":3793,"fetchDate":"Sun, 15 Nov 2020 22:03:09 +0000","fetchMs":54,"handleMs":32,"totalMs":33732,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"1bc2e11d54d94d2e8b384096218a63f0","hostName":"us-034*","requestId":"59489964cbd04372b70091cb1290697d_3793","contentType":"text/xml; charset=UTF-8","totalBytes":4503612,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":6,"articlesImgsGithubTotal":6,"successGithubMap":{"myreaderx21":1,"myreaderx11":1,"myreaderx2":1,"myreaderx12":1,"myreaderx1":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:29:31","updatedTime":"2020-09-01 10:35:46","id":3793,"name":"Google AI Blog","url":"http://feeds.feedburner.com/blogspot/gJZg","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn63@2020_3/2020/09/01/02-35-46-379_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":2631664,"tmpBodyImgCdnBytes":1871948,"tmpBgImgCdnBytes":0,"extra4":{"start":1605477789226,"total":0,"statList":[{"spend":70,"msg":"获取xml内容"},{"spend":32,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":6508,"msg":"正文链接上传到cdn"}]},"extra5":6,"extra6":6,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-037.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-ZXTH_Ywi4Ks/X6R3ueMnqdI/AAAAAAAAGwo/CzfKP2_TCSUPlzZBfUVyofRplKNnvQ-ZwCLcBGAsYHQ/s16000/image4.gif","sourceStatusCode":200,"destWidth":1204,"destHeight":804,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn98@2020_1/2020/11/15/22-03-34-370_de3839c76fffa3fe.webp","sourceBytes":24103024,"destBytes":2631664,"targetWebpQuality":4,"feedId":3793,"totalSpendMs":27092,"convertSpendMs":23214,"createdTime":"2020-11-16 06:03:09","host":"us-033*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","linkMd5ListStr":"3243f11c065116962e03aff2cd1b5c7a,3243f11c065116962e03aff2cd1b5c7a","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23 MB","destSize":"2.5 MB","compressRate":"10.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/8_otZopCzfU","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn25@2020_1/2020/11/15/22-03-36-508_1dc421f053ca0bfa.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":788,"convertSpendMs":12,"createdTime":"2020-11-16 06:03:36","host":"us-013*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","linkMd5ListStr":"3243f11c065116962e03aff2cd1b5c7a","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn10@2020_2/2020/11/15/22-03-36-499_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":811,"convertSpendMs":6,"createdTime":"2020-11-16 06:03:36","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","linkMd5ListStr":"3243f11c065116962e03aff2cd1b5c7a","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-cl63GVkiF50/X6R6uZvFD0I/AAAAAAAAGxI/6hUfkTGnZ2oTecEhSQymx270Ttklnf4qQCLcBGAsYHQ/s16000/3DObjDetSoln.png","sourceStatusCode":200,"destWidth":959,"destHeight":394,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn22@2020_3/2020/11/15/22-03-36-499_783f17c82457e613.webp","sourceBytes":54398,"destBytes":23852,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":854,"convertSpendMs":19,"createdTime":"2020-11-16 06:03:36","host":"us-025*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","linkMd5ListStr":"3243f11c065116962e03aff2cd1b5c7a","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"53.1 KB","destSize":"23.3 KB","compressRate":"43.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-ACxubUnKR4M/X6R49vdskyI/AAAAAAAAGw8/Slr8jlMTONEKNHckpLunBBIMuptrXkXvwCLcBGAsYHQ/s16000/3DIoU.png","sourceStatusCode":200,"destWidth":1024,"destHeight":399,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn13@2020_3/2020/11/15/22-03-36-619_5305db3247f05764.webp","sourceBytes":177526,"destBytes":42838,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":1061,"convertSpendMs":73,"createdTime":"2020-11-16 06:03:36","host":"us-038*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","linkMd5ListStr":"3243f11c065116962e03aff2cd1b5c7a","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"173.4 KB","destSize":"41.8 KB","compressRate":"24.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-hNyz0aw6N0k/X6R4CGM51XI/AAAAAAAAGww/l0S8e7cSbSwPRbZhuabL29UsUT1L8HvYwCLcBGAsYHQ/s16000/image3.gif","sourceStatusCode":200,"destWidth":1440,"destHeight":606,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn11@2020_4/2020/11/15/22-03-41-519_b6d63e357d1c3ca9.webp","sourceBytes":13566316,"destBytes":1804876,"targetWebpQuality":15,"feedId":3793,"totalSpendMs":6489,"convertSpendMs":4489,"createdTime":"2020-11-16 06:03:36","host":"us-037*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/8_otZopCzfU/announcing-objectron-dataset.html","linkMd5ListStr":"3243f11c065116962e03aff2cd1b5c7a","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"12.9 MB","destSize":"1.7 MB","compressRate":"13.3%"}],"successGithubMap":{"myreaderx21":1,"myreaderx11":1,"myreaderx2":1,"myreaderx12":1,"myreaderx1":1,"myreaderx":1},"failGithubMap":{}}