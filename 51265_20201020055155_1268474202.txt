{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-10-20 13:50:46","updatedTime":"2020-10-20 13:50:46","title":"层次聚类改进算法之CURE","link":"https://www.biaodianfu.com/?p=11460","description":"\n<p>CURE（Clustering Using Representatives）是一种针对大型数据库的高效的聚类算法。<a href=\"https://www.biaodianfu.com/hierarchical-clustering.html\">基于层次的传统的聚类算法</a>得到的是球状的，相等大小的聚类，对异常数据比较脆弱。CURE采用了用多个点代表一个簇的方法，可以较好的处理以上问题。并且在处理大数据量的时候采用了随机取样，分区的方法，来提高其效率，使得其可以高效的处理大量数据。先看一下基于层次聚类算法的缺陷：</p>\n<p><img class=\"alignnone size-full wp-image-11461\" src=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/Hierarchical-Clustering-problem.png\" alt=\"\" width=\"533\" height=\"187\" srcset=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/Hierarchical-Clustering-problem.png 533w, https://www.biaodianfu.com/wp-content/uploads/2020/05/Hierarchical-Clustering-problem-300x105.png 300w\" sizes=\"(max-width: 533px) 100vw, 533px\" /></p>\n<p>如上图所示，基于层级的聚类算法比如Hierarchical K-means聚类算法，不能够很好地区分尺寸差距大的簇，原因是K-means算法基于“质心”加一定“半径”对数据进行划分，导致最后聚类的簇近似“圆形”。</p>\n<p>再看一下其他聚类算法在聚类结果上可能存在的问题：</p>\n<p><img class=\"alignnone wp-image-11462\" src=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/problem.png\" alt=\"\" width=\"582\" height=\"377\" srcset=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/problem.png 1269w, https://www.biaodianfu.com/wp-content/uploads/2020/05/problem-300x194.png 300w, https://www.biaodianfu.com/wp-content/uploads/2020/05/problem-1024x663.png 1024w, https://www.biaodianfu.com/wp-content/uploads/2020/05/problem-768x497.png 768w\" sizes=\"(max-width: 582px) 100vw, 582px\" /></p>\n<p>上面（b）图使用的是基于“平均连锁”或者基于“质心”的簇间距离计算方式得到的聚类结果，可以看出，聚类结果同基于划分的聚类算法相似、最后聚类的结果呈“圆形”，不能够准确地识别条形的数据；（c）图使用的是基于“单连锁”的簇间距离计算策略，由“单连锁”的定义可知，对于（c）图中最左边两个由一条细线相连的两个簇，会被聚类成一个簇，这也不是我们想要的。</p>\n<p>CURE算法核心的思想是使用一定数量的“分散的”点（scattered points）来代表一个簇（cluster），而不像是其他层次聚类算法中，只使用一个点，使得CURE算法有如下优势：</p>\n<ul>\n<li>准确地识别任意形状的簇；</li>\n<li>准确地识别尺寸差距大的簇；</li>\n<li>很好地处理“噪点”</li>\n</ul>\n<p>所以，CURE算法很好地解决了上面提到的聚类结果的缺陷。</p>\n<h2>CURE算法流程</h2>\n<p>CURE的算法在开始时，每个点都是一个簇，然后将距离最近的簇结合，一直到簇的个数为要求的K。它是一种分裂的层次聚类。算法分为以下6步：</p>\n<ol>\n<li>从源数据对象中抽取一个随机样本S。</li>\n<li>将样本S分割为一组划分。</li>\n<li>对划分局部的聚类。</li>\n<li>通过随机取样提出孤立点。如果一个簇增长得太慢，就去掉它。</li>\n<li>对局部的簇进行聚类。</li>\n<li>用相应的簇标签标记数据。</li>\n</ol>\n<p>CURE算法主流程如下：</p>\n<p><img class=\"alignnone wp-image-11463\" src=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/flow.png\" alt=\"\" width=\"644\" height=\"99\" srcset=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/flow.png 988w, https://www.biaodianfu.com/wp-content/uploads/2020/05/flow-300x46.png 300w, https://www.biaodianfu.com/wp-content/uploads/2020/05/flow-768x118.png 768w\" sizes=\"(max-width: 644px) 100vw, 644px\" /></p>\n<p>首先从原始数据集中随机抽取一部分样本点作为子集，再对该子集进行划分，在这些划分后的集合上运行CURE聚类算法得到每个集合的簇，并删除其中的离群点，然后对这些簇进一步进行CURE层次聚类，并删除其中的离群点，最后对磁盘中剩余的数据集样本点进行划分。</p>\n<h2>CURE算法设计</h2>\n<p><strong>1)基本聚类算法</strong></p><pre class=\"crayon-plain-tag\">procedure cluster(S, k)      /*将数据集S聚类成为k个簇*/\nbegin\n    T := build_kd_tree(S)    /*对应数据集S建立一个K-DTree T*/\n    Q := build_heap(S)       /*对应数据集S建立一个堆Q*/\n    while size(Q) &#62; k do {   /*聚类直至簇的个数为k */\n        u := extract_min(Q)  /*找到最近的两个簇u，v */\n        v := u.cloest\n        delete(Q, v)\n        w := merge(u, v)            /*将u，v合并为簇w */\n        delete_rep(T, u); delete_rep(T, v); insert_rep(T, w)\n        w.cloest := x               /* x is an arbitrary cluster in Q*/\n        for each x∈Q do{            /*调节因合并带来的T和Q的变化*/\n            if (dist(w,x) &#60; dist(w,w.cloest))\n                w.cloest := x\n            if x.cloest is either u or v {\n                if dist(x, x.cloest) &#60; dist(x.w)\n                    x.cloest := cloest_cluster(T, x, dist(x,w))\n                else\n                    x.cloest := w\n                relocate(Q, x)\n            }\n            else if dist(x, x.cloest) &#62; dist(x, w) {\n                x.cloest := w\n                relocate(Q, x)\n            }\n        }\n    insert(Q, w)\n    }\nEnd</pre><p>此程序段用到的数据结构有Heap和KDTree。为了合并距离最短的两个聚类，需要构建一个KDTree来找到空间中的一聚类最近的一个聚类，之后把DTree中的聚类按照其与最近的聚类的距离进行排序（用的是堆排序），找到最近的两个的聚类，将它们合并（对应函数merge()）。</p>\n<p><strong>2)Merge算法</strong></p><pre class=\"crayon-plain-tag\">procedure merge(u, v)                            /*合并两个簇，并确定新簇的中心点和代表点*/\nbegin\n    w := u∪v\n    w.mean := (|u|u.mean+|v|v.mean)/(|u|+|v|)    /* 求新簇w的中心点*/\n    tmpSet := Ø                                  /*用来存c个代表点的集合*/\n    for i := 1 to c do {                         /*选出c个代表点*/\n        maxDist := 0                              \n            if i = 1\n                minDist := dist( p, w.mean )\n            else\n                minDist := min{ dist( p, q ) : q∈tmpSet }\n            if( minDist &#62;= maxDist ) {\n                maxDist := minDist\n                maxPoint := p\n            }\n        }\n        tmpSet := tmpSet∪{ maxPoint }\n    }\n    foreach point p in tmpSet do                 /*按照收缩因子α处理代表点*/\n        w.rep := w.rep∪{ p +α*( w.mean – p )}\n    return w\nend</pre><p>此程序段同时描述了如何选取代表点：对每个簇选择c个分布较好的点，通过系数$\\alpha$向中心收缩，其中$0 &#60;\\alpha &#60;1$。$\\alpha$小，收缩小，可以区分拉长的簇；$\\alpha$大，靠近中心点，得到的簇更紧凑。显然，如果$\\alpha=1$，聚类w的代表点就是w.mean，即其中心点，此时类似于Centroid-base approach，即中心点代表簇，当$\\alpha =0$，此时类似于All-points approach，即所有点代表簇。簇之间的距离定义为：两个簇的代表点之间的最小距离，即：</p>\n<p>$$dist(u,v) = \\min dist(p,q)$$</p>\n<p>点到簇的距离与此类似，是该点到最近的簇的代表点的距离。c个代表点体现了簇的物理几何形状；向中心收缩可以降低异常点的影响。两个簇组合后的新簇，则重新选择c个点作为簇的代表。</p>\n<p><strong>3)数据取样</strong></p>\n<p>在对大规模数据库进行聚类分析时，数据取样是一种常用的提高聚类效率的方法，即对整个数据库进行数据取样，然后对取样数据库进行聚类分析，而对未被取样的数据进行聚类标注。这样，对大规模数据库的聚类分析就转化为对较小规模的取样数据库的聚类分析。由于没有考虑到整个数据库的数据，聚类质量必然会受到影响。但是，只要取样均匀且取样率适当，则取样数据库也可以较好地反映整个数据库状况，从而在保证聚类质量的同时提高聚类效率。</p>\n<p>定理1：对一个簇u，如果取样大小s满足：</p>\n<p>$$s\\geq fN+\\frac{N}{|u|}\\log(\\frac{1}{\\delta })+\\frac{N}{|u|}\\sqrt{(\\log(\\frac{1}{\\delta }))^2+2f|u|\\log(\\frac{1}{\\delta })}$$</p>\n<p>那么，样本中属于簇u的点的个数小于f|u|的概率小于$\\delta$，$0&#60;=\\delta &#60;=1$。因此，采用chernoff bounds来确定的最小的取样数据量：</p>\n<p>$$s_{min}=\\zeta k\\rho +k\\rho\\log(\\frac{1}{\\delta })+\\sqrt{\\log(\\frac{1}{\\delta })^2+2\\zeta \\log(\\frac{1}{\\delta })}$$</p>\n<p>这就表示着如果我们只关心数据点数目大于的聚类，且最小的聚类至少有$\\zeta$个数据点，那么我们只需要一个独立于原始数据点个数的取样数目。</p>\n<p><strong>4)分区方法-减少输入数据，保证内存中存放所有聚类的代表点</strong></p>\n<p>分区过程如下：将所有样本分成p个分区，每个分区大小n/p。每个分区内作聚类，直到分区内的簇的个数为n/pq, q &#62; 1。或者指定一个距离阈值，当最近簇距离大于阈值，则停止。在CURE算法中，</p>\n<ul>\n<li>First pass每个分区： $O(\\frac{n^2}{p^2}(\\frac{q-1}{q})\\log\\frac{n}{p})$</li>\n<li>Second pass总聚类: $O(\\frac{n^2}{p^2}\\log\\frac{n}{p})$</li>\n</ul>\n<p>p,q的最好选值：使n/pq为k的2~3倍。其优点是：减少执行时间；减少输入数据，保证可以在内存中存放所有聚类的代表点。</p>\n<p><strong>5)标记数据所属的簇</strong></p>\n<p>因为CURE用c个点来代表一个聚类，因此在聚类完成后，对未参加聚类的数据或新增的数据进行标注从而计算聚类的可信度时，其可以准确的识别非球状数据集，使得标注更加准确。</p>\n<p><strong>6)异常点的处理</strong></p>\n<p>在进行随机采样时，会过滤掉大部分的离群点，此外，在随机采样得到的数据集中存在的少量离群点由于分布在整个原始数据空间，因而被随机采样进一步隔离了。在进行CURE凝聚层次聚类时，需要将每个点单独初始化为一个簇，并将距离最近的点合并为一个簇，由于离群点往往距离样本中的其它点很远，因此他所代表的簇增长的最为缓慢，以至于簇的大小远远小于正常的簇。</p>\n<p>以上述讨论作为契机，我们将层次聚类中的离群点识别分为两个阶段，第一个阶段是在聚类算法执行到某一阶段（或称当前的簇总数减小到某个值）时，根据簇的增长速度和簇的大小对离群点进行一次识别，需要注意的是，如果这个阶段选择的较早（即簇总数依旧很大）的话，会将一部分本应被合并的簇识别为离群点，如果这个阶段选择的较晚（即簇总数过少）的话，离群点很可能在被识别之前就已经合并到某些簇中，因此原文推荐当前簇的总数为数据集大小的1/3时，进行离群点的识别。第一阶段有一个很明显的问题，就是当随机采样到的离群点分布的比较近时（即使可能性比较小），这些点会被合并为一个簇，而导致无法将他们识别出来，这时就需要第二阶段的来进行处理。由于离群点占的比重很小，而在层次聚类的最后几步中，每个正常簇的粒度都是非常高的，因此很容易将他们识别出来，一般当簇的总数缩减到大约为k时，进行第二阶段的识别。</p>\n<h3>数据取样算法</h3>\n<p>在对大规模数据库进行聚类分析时，数据取样是一种常用的提高聚类效率的方法，即对整个数据库进行数据取样，然后对取样数据库进行聚类分析，而对未被取样的数据进行聚类标注。这样，对大规模数据库的聚类分析就转化为对较小规模的取样数据库的聚类分析。由于没有考虑到整个数据库的数据，聚类质量必然会受到影响。但是，只要取样均匀且取样率适当，则取样数据库也可以较好地反映整个数据库状况，从而在保证聚类质量的同时提高聚类效率。与以前的基于取样的聚类算法相比。</p>\n<p>取样算法：这种算法只需扫描一遍被取样数据库，而且使用恒定的内存空间，便可以从N个记录中随机取出n个取样记录。其基本思想是:从第N-n+1条记录开始，做下列操作。设当前处理的是第t个记录(n+1≤t≤N)，u是产生的一个随机数(u∈〔0，t-1〕)，若u&#60;n，则把第u个记录替换成第t个记录。可以证明该算法能够得到均匀的取样结果。确定取样率很重要。为保证聚类质量，取样数据库应该能够有效地代表原数据库。若取样率太低，取样数据库必然会丢失原数据库的某些特质，导致聚类效果失真。</p>\n<h2><span lang=\"EN-US\">CURE</span>算法<span lang=\"EN-US\">Python</span>实现</h2>\n<p></p><pre class=\"crayon-plain-tag\"># -*- coding: utf-8 -*-\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.spatial.distance as distance\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\n\n\n# Returns the distance between two vectors\ndef dist(vecA, vecB):\n    return np.sqrt(np.power(vecA - vecB, 2).sum())\n\n\n# This class describes the data structure and method of operation for CURE clustering.\nclass CureCluster:\n    def __init__(self, id__, center__):\n        self.points = center__\n        self.rep_points = center__\n        self.center = center__\n        self.index = [id__]\n\n    def __repr__(self):\n        return \"Cluster \" + \" Size: \" + str(len(self.points))\n\n    # Computes and stores the centroid of this cluster, based on its points\n    def compute_centroid(self, clust):\n        total_points_1 = len(self.index)\n        total_points_2 = len(clust.index)\n        self.center = (self.center * total_points_1 + clust.center * total_points_2) / (total_points_1 + total_points_2)\n\n    # Computes and stores representative points for this cluster\n    def generate_rep_points(self, num_rep_points, alpha):\n        temp_set = None\n        for i in range(1, num_rep_points + 1):\n            max_dist = 0\n            max_point = None\n            for p in range(0, len(self.index)):\n                if i == 1:\n                    min_dist = dist(self.points[p, :], self.center)\n                else:\n                    X = np.vstack([temp_set, self.points[p, :]])\n                    tmp_dist = distance.pdist(X)\n                    min_dist = tmp_dist.min()\n                if min_dist &#62;= max_dist:\n                    max_dist = min_dist\n                    max_point = self.points[p, :]\n            if temp_set is None:\n                temp_set = max_point\n            else:\n                temp_set = np.vstack((temp_set, max_point))\n        for j in range(len(temp_set)):\n            if self.rep_points is None:\n                self.rep_points = temp_set[j, :] + alpha * (self.center - temp_set[j, :])\n            else:\n                self.rep_points = np.vstack((self.rep_points, temp_set[j, :] + alpha * (self.center - temp_set[j, :])))\n\n    # Computes and stores distance between this cluster and the other one.\n    def dist_rep(self, clust):\n        dist_rep = float('inf')\n        for repA in self.rep_points:\n            if type(clust.rep_points[0]) != list:\n                repB = clust.rep_points\n                dist_temp = dist(repA, repB)\n                if dist_temp &#60; dist_rep:\n                    dist_rep = dist_temp\n            else:\n                for repB in clust.rep_points:\n                    dist_temp = dist(repA, repB)\n                    if dist_temp &#60; dist_rep:\n                        dist_rep = dist_temp\n        return dist_rep\n\n    # Merges this cluster with the given cluster, recomputing the centroid and the representative points.\n    def merge_with_cluster(self, clust, num_rep_points, alpha):\n        self.compute_centroid(clust)\n        self.points = np.vstack((self.points, clust.points))\n        self.index = np.append(self.index, clust.index)\n        self.rep_points = None\n        self.generate_rep_points(num_rep_points, alpha)\n\n\n# Describe the process of the CURE algorithm\ndef run_CURE(data, num_rep_points, alpha, num_des_cluster):\n    # Initialization\n    clusters = []\n    num_cluster = len(data)\n    num_Pts = len(data)\n    dist_cluster = np.ones([len(data), len(data)])\n    dist_cluster = dist_cluster * float('inf')\n    for id_point in range(len(data)):\n        new_clust = CureCluster(id_point, data[id_point, :])\n        clusters.append(new_clust)\n    for row in range(0, num_Pts):\n        for col in range(0, row):\n            dist_cluster[row][col] = dist(clusters[row].center, clusters[col].center)\n    while num_cluster &#62; num_des_cluster:\n        if np.mod(num_cluster, 50) == 0:\n            print('Cluster count:', num_cluster)\n\n        # Find a pair of closet clusters\n        min_index = np.where(dist_cluster == np.min(dist_cluster))\n        min_index1 = min_index[0][0]\n        min_index2 = min_index[1][0]\n\n        # Merge\n        clusters[min_index1].merge_with_cluster(clusters[min_index2], num_rep_points, alpha)\n        # Update the distCluster matrix\n        for i in range(0, min_index1):\n            dist_cluster[min_index1, i] = clusters[min_index1].dist_rep(clusters[i])\n        for i in range(min_index1 + 1, num_cluster):\n            dist_cluster[i, min_index1] = clusters[min_index1].dist_rep(clusters[i])\n        # Delete the merged cluster and its disCluster vector.\n        dist_cluster = np.delete(dist_cluster, min_index2, axis=0)\n        dist_cluster = np.delete(dist_cluster, min_index2, axis=1)\n        del clusters[min_index2]\n        num_cluster = num_cluster - 1\n\n    print('Cluster count:', num_cluster)\n    # Generate sample labels\n    label = [0] * num_Pts\n    for i in range(0, len(clusters)):\n        for j in range(0, len(clusters[i].index)):\n            label[clusters[i].index[j]] = i + 1\n\n    return label\n\n\nif __name__ == \"__main__\":\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    data, Label_true = make_blobs(n_samples=750, centers=centers,\n                                  cluster_std=0.4, random_state=0)\n\n    num_rep_points = 5  # The number of representative points\n    alpha = 0.1  # Shrink factor\n    num_des_cluster = 3  # Desired cluster number\n\n    Label_pre = run_CURE(data, num_rep_points, alpha, num_des_cluster)\n    nmi = metrics.v_measure_score(Label_true, Label_pre)\n    print(\"NMI =\", nmi)\n\n    scatterColors = ['black', 'blue', 'green', 'yellow', 'red', 'purple', 'orange', 'brown', 'cyan', 'brown',\n                     'chocolate', 'darkgreen', 'darkblue', 'azure', 'bisque']\n\n    plt.subplot(121)\n    for i in range(data.shape[0]):\n        color = scatterColors[Label_true[i]]\n        plt.scatter(data[i, 0], data[i, 1], marker='o', c=color)\n    plt.text(0, 0, \"origin\")\n    \n    plt.subplot(122)\n    for i in range(data.shape[0]):\n        color = scatterColors[Label_pre[i]]\n        plt.scatter(data[i, 0], data[i, 1], marker='o', c=color)\n    plt.text(0, 0, \"clusterResult\")\n    plt.show()</pre><p><img class=\"alignnone size-full wp-image-11464\" src=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/cure.png\" alt=\"\" width=\"640\" height=\"480\" srcset=\"https://www.biaodianfu.com/wp-content/uploads/2020/05/cure.png 640w, https://www.biaodianfu.com/wp-content/uploads/2020/05/cure-300x225.png 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" /></p>\n<p>参考链接：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/qq_40793975/article/details/83574309\">https://blog.csdn.net/qq_40793975/article/details/83574309</a></li>\n</ul>\n<div class='yarpp-related-rss'>\n<p>Related posts:<ol>\n<li><a href=\"https://www.biaodianfu.com/uber-h3-action.html\" rel=\"bookmark\" title=\"Uber H3实战：英国交通事故点聚类\">Uber H3实战：英国交通事故点聚类 </a></li>\n<li><a href=\"https://www.biaodianfu.com/calculate-the-center-point-of-multiple-latitude-longitude-coordinate-pairs.html\" rel=\"bookmark\" title=\"多经纬度坐标的中心点计算方法\">多经纬度坐标的中心点计算方法 </a></li>\n<li><a href=\"https://www.biaodianfu.com/mean-shift.html\" rel=\"bookmark\" title=\"聚类算法之Mean Shift\">聚类算法之Mean Shift </a></li>\n</ol></p>\n</div>\n","descriptionType":"html","publishedDate":"Sun, 24 May 2020 01:43:50 +0000","feedId":51265,"bgimg":"https://www.biaodianfu.com/wp-content/uploads/2020/05/Hierarchical-Clustering-problem.png","linkMd5":"b97ad85ae38030200f4de14ff077d8f0","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn92@2020_5/2020/10/20/05-50-48-517_ac8be52ba130d955.webp","destWidth":533,"destHeight":187,"sourceBytes":69365,"destBytes":23518,"author":"标点符","articleImgCdnMap":{"https://www.biaodianfu.com/wp-content/uploads/2020/05/Hierarchical-Clustering-problem.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn92@2020_5/2020/10/20/05-50-48-517_ac8be52ba130d955.webp","https://www.biaodianfu.com/wp-content/uploads/2020/05/problem.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn3@2020_6/2020/10/20/05-51-54-177_7e3d667652fe3916.webp","https://www.biaodianfu.com/wp-content/uploads/2020/05/flow.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn95@2020_6/2020/10/20/05-50-51-918_6a8a17b835b28d18.webp","https://www.biaodianfu.com/wp-content/uploads/2020/05/cure.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn100@2020_1/2020/10/20/05-50-51-459_76dfa0c3a5eaddf3.webp"},"publishedOrCreatedDate":1603173046091}],"record":{"createdTime":"2020-10-20 13:50:46","updatedTime":"2020-10-20 13:50:46","feedId":51265,"fetchDate":"Tue, 20 Oct 2020 05:50:46 +0000","fetchMs":1870,"handleMs":6297,"totalMs":77580,"newArticles":0,"totalArticles":12,"status":1,"type":0,"ip":"3b520398b65f82a2a83818c2716ca970","hostName":"us-020*","requestId":"56c77c630a5649a9b6fb8d008ff10457_51265","contentType":"application/rss+xml; charset=UTF-8","totalBytes":200898,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":4,"articlesImgsGithubTotal":4,"successGithubMap":{"myreaderx12":1,"myreaderx13":1,"myreaderx5oss":1,"myreaderx29":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 03:59:47","updatedTime":"2020-10-14 10:07:29","id":51265,"name":"标点符","url":"https://www.biaodianfu.com/feed","subscriber":62,"website":null,"icon":"https://www.biaodianfu.com/favicon.ico","icon_jsdelivr":null,"description":"吾生也有涯，而知也无涯。以有涯随无涯，殆已！","weekly":null,"link":"https://www.biaodianfu.com"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":23518,"tmpBodyImgCdnBytes":177380,"tmpBgImgCdnBytes":0,"extra4":{"start":1603173037672,"total":0,"statList":[{"spend":2123,"msg":"获取xml内容"},{"spend":6297,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":65873,"msg":"正文链接上传到cdn"}]},"extra5":4,"extra6":4,"extra7ImgCdnFailResultVector":[null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-002.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-014.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-027.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://www.biaodianfu.com/wp-content/uploads/2020/05/Hierarchical-Clustering-problem.png","sourceStatusCode":200,"destWidth":533,"destHeight":187,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn92@2020_5/2020/10/20/05-50-48-517_ac8be52ba130d955.webp","sourceBytes":69365,"destBytes":23518,"targetWebpQuality":75,"feedId":51265,"totalSpendMs":3273,"convertSpendMs":9,"createdTime":"2020-10-20 13:50:46","host":"us-039*","referer":"https://www.biaodianfu.com/?p=11460","linkMd5ListStr":"b97ad85ae38030200f4de14ff077d8f0,b97ad85ae38030200f4de14ff077d8f0","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"67.7 KB","destSize":"23 KB","compressRate":"33.9%"},{"code":1,"isDone":false,"source":"https://www.biaodianfu.com/wp-content/uploads/2020/05/cure.png","sourceStatusCode":200,"destWidth":640,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn100@2020_1/2020/10/20/05-50-51-459_76dfa0c3a5eaddf3.webp","sourceBytes":42166,"destBytes":36446,"targetWebpQuality":75,"feedId":51265,"totalSpendMs":2913,"convertSpendMs":15,"createdTime":"2020-10-20 13:50:49","host":"us-014*","referer":"https://www.biaodianfu.com/?p=11460","linkMd5ListStr":"b97ad85ae38030200f4de14ff077d8f0","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.2 KB","destSize":"35.6 KB","compressRate":"86.4%"},{"code":1,"isDone":false,"source":"https://www.biaodianfu.com/wp-content/uploads/2020/05/flow.png","sourceStatusCode":200,"destWidth":988,"destHeight":152,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn95@2020_6/2020/10/20/05-50-51-918_6a8a17b835b28d18.webp","sourceBytes":28045,"destBytes":21584,"targetWebpQuality":75,"feedId":51265,"totalSpendMs":3341,"convertSpendMs":28,"createdTime":"2020-10-20 13:50:49","host":"us-027*","referer":"https://www.biaodianfu.com/?p=11460","linkMd5ListStr":"b97ad85ae38030200f4de14ff077d8f0","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"27.4 KB","destSize":"21.1 KB","compressRate":"77%"},{"code":1,"isDone":false,"source":"https://www.biaodianfu.com/wp-content/uploads/2020/05/problem.png","sourceStatusCode":200,"destWidth":1269,"destHeight":822,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn3@2020_6/2020/10/20/05-51-54-177_7e3d667652fe3916.webp","sourceBytes":412681,"destBytes":119350,"targetWebpQuality":75,"feedId":51265,"totalSpendMs":5685,"convertSpendMs":177,"createdTime":"2020-10-20 13:51:49","host":"us-027*","referer":"https://www.biaodianfu.com/?p=11460","linkMd5ListStr":"b97ad85ae38030200f4de14ff077d8f0","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"403 KB","destSize":"116.6 KB","compressRate":"28.9%"}],"successGithubMap":{"myreaderx12":1,"myreaderx13":1,"myreaderx5oss":1,"myreaderx29":1},"failGithubMap":{}}