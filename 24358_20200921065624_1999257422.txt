{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-09-21 14:55:47","updatedTime":"2020-09-21 14:55:47","title":"Multi-Core Machine Learning in Python With Scikit-Learn","link":"https://machinelearningmastery.com/?p=10950","description":"<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-10950 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Multi-Core+Machine+Learning+in+Python+With+Scikit-Learn&url=https://machinelearningmastery.com/multi-core-machine-learning-in-python/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/multi-core-machine-learning-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/cws/share?url=https://machinelearningmastery.com/multi-core-machine-learning-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>Many computationally expensive tasks for machine learning can be made parallel by splitting the work across <strong>multiple CPU cores</strong>, referred to as multi-core processing.</p>\n<p>Common machine learning tasks that can be made parallel include training models like ensembles of decision trees, evaluating models using resampling procedures like k-fold cross-validation, and tuning model hyperparameters, such as grid and random search.</p>\n<p>Using multiple cores for common machine learning tasks can dramatically decrease the execution time as a factor of the number of cores available on your system. A common laptop and desktop computer may have 2, 4, or 8 cores. Larger server systems may have 32, 64, or more cores available, allowing machine learning tasks that take hours to be completed in minutes.</p>\n<p>In this tutorial, you will discover how to configure scikit-learn for multi-core machine learning.</p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>How to train machine learning models using multiple cores.</li>\n<li>How to make the evaluation of machine learning models parallel.</li>\n<li>How to use multiple cores to tune machine learning model hyperparameters.</li>\n</ul>\n<p>Let’s get started.</p>\n<div id=\"attachment_10958\" style=\"width: 810px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-10958\" loading=\"lazy\" class=\"size-full wp-image-10958\" src=\"https://machinelearningmastery.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn.jpg\" alt=\"Multi-Core Machine Learning in Python With Scikit-Learn\" width=\"800\" height=\"450\" srcset=\"http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn.jpg 800w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn-300x169.jpg 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn-768x432.jpg 768w\" sizes=\"(max-width: 800px) 100vw, 800px\" /><p id=\"caption-attachment-10958\" class=\"wp-caption-text\">Multi-Core Machine Learning in Python With Scikit-Learn<br />Photo by <a href=\"https://www.flickr.com/photos/erix-pix/16030189175/\">ER Bauer</a>, some rights reserved.</p></div>\n<h2>Tutorial Overview</h2>\n<p>This tutorial is divided into five parts; they are:</p>\n<ol>\n<li>Multi-Core Scikit-Learn</li>\n<li>Multi-Core Model Training</li>\n<li>Multi-Core Model Evaluation</li>\n<li>Multi-Core Hyperparameter Tuning</li>\n<li>Recommendations</li>\n</ol>\n<h2>Multi-Core Scikit-Learn</h2>\n<p>Machine learning can be computationally expensive.</p>\n<p>There are three main centers of this computational cost; they are:</p>\n<ul>\n<li>Training machine learning models.</li>\n<li>Evaluating machine learning models.</li>\n<li>Hyperparameter tuning machine learning models.</li>\n</ul>\n<p>Worse, these concerns compound.</p>\n<p>For example, evaluating machine learning models using a resampling technique like <a href=\"https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/\">k-fold cross-validation</a> requires that the training process is repeated multiple times.</p>\n<ul>\n<li>Evaluation Requires Repeated Training</li>\n</ul>\n<p>Tuning model hyperparameters compounds this further as it requires the evaluation procedure repeated for each combination of hyperparameters tested.</p>\n<ul>\n<li>Tuning Requires Repeated Evaluation</li>\n</ul>\n<p>Most, if not all, modern computers have multi-core CPUs. This includes your workstation, your laptop, as well as larger servers.</p>\n<p>You can configure your machine learning models to harness multiple cores of your computer, dramatically speeding up computationally expensive operations.</p>\n<p>The scikit-learn Python machine learning library provides this capability via the <a href=\"https://scikit-learn.org/stable/glossary.html#term-n-jobs\">n_jobs argument</a> on key machine learning tasks, such as model training, model evaluation, and hyperparameter tuning.</p>\n<p>This configuration argument allows you to specify the number of cores to use for the task. The default is None, which will use a single core. You can also specify a number of cores as an integer, such as 1 or 2. Finally, you can specify -1, in which case the task will use all of the cores available on your system.</p>\n<ul>\n<li><strong>n_jobs</strong>: Specify the number of cores to use for key machine learning tasks.</li>\n</ul>\n<p>Common values are:</p>\n<ul>\n<li><strong>n_jobs=None</strong>: Use a single core or the default configured by your backend library.</li>\n<li><strong>n_jobs=4</strong>: Use the specified number of cores, in this case 4.</li>\n<li><strong>n_jobs=-1</strong>: Use all available cores.</li>\n</ul>\n<p><strong>What is a core?</strong></p>\n<p>A CPU may have <a href=\"https://en.wikipedia.org/wiki/Multi-core_processor\">multiple physical CPU cores</a>, which is essentially like having multiple CPUs. Each core may also have <a href=\"https://en.wikipedia.org/wiki/Hyper-threading\">hyper-threading</a>, a technology that under many circumstances allows you to double the number of cores.</p>\n<p>For example, my workstation has four physical cores, which are doubled to eight cores due to hyper-threading. Therefore, I can experiment with 1-8 cores or specify -1 to use all cores on my workstation.</p>\n<p>Now that we are familiar with the scikit-learn library’s capability to support multi-core parallel processing for machine learning, let&#8217;s work through some examples.</p>\n<p>You will get different timings for all of the examples in this tutorial; share your results in the comments. You may also need to change the number of cores to match the number of cores on your system.</p>\n<p><strong>Note</strong>: Yes, I am aware of the <a href=\"https://docs.python.org/3/library/timeit.html\">timeit</a> API, but chose against it for this tutorial. We are not profiling the code examples per se; instead, I want you to focus on how and when to use the multi-core capabilities of scikit-learn and that they offer real benefits. I wanted the code examples to be clean and simple to read, even for beginners. I set it as an extension to update all examples to use the timeit API and get more accurate timings. Share your results in the comments.</p>\n<h2>Multi-Core Model Training</h2>\n<p>Many machine learning algorithms support multi-core training via an n_jobs argument when the model is defined.</p>\n<p>This affects not just the training of the model, but also the use of the model when making predictions.</p>\n<p>A popular example is the ensemble of decision trees, such as bagged decision trees, random forest, and gradient boosting.</p>\n<p>In this section we will explore accelerating the training of a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">RandomForestClassifier</a> model using multiple cores. We will use a synthetic classification task for our experiments.</p>\n<p>In this case, we will define a random forest model with 500 trees and use a single core to train the model.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=1)</pre><p>We can record the time before and after the call to the <em>train()</em> function using the <em>time()</em> function. We can then subtract the start time from the end time and report the execution time in the number of seconds.</p>\n<p>The complete example of evaluating the execution time of training a random forest model with a single core is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of timing the training of a random forest model on one core\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=1)\n# record current time\nstart = time()\n# fit the model\nmodel.fit(X, y)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example reports the time taken to train the model with a single core.</p>\n<p>In this case, we can see that it takes about 10 seconds.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">10.702 seconds</pre><p>We can now change the example to use all of the physical cores on the system, in this case, four.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=4)</pre><p>The complete example of multi-core training of the model with four cores is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of timing the training of a random forest model on 4 cores\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=4)\n# record current time\nstart = time()\n# fit the model\nmodel.fit(X, y)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example reports the time taken to train the model with a single core.</p>\n<p>In this case, we can see that the speed of execution more than halved to about 3.151 seconds.</p>\n<p><strong>How long does it take on your system?</strong> Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">3.151 seconds</pre><p>We can now change the number of cores to eight to account for the hyper-threading supported by the four physical cores.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=8)</pre><p>We can achieve the same effect by setting <em>n_jobs</em> to -1 to automatically use all cores; for example:</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=-1)</pre><p>We will stick to manually specifying the number of cores for now.</p>\n<p>The complete example of multi-core training of the model with eight cores is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of timing the training of a random forest model on 8 cores\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=500, n_jobs=8)\n# record current time\nstart = time()\n# fit the model\nmodel.fit(X, y)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example reports the time taken to train the model with a single core.</p>\n<p>In this case, we can see that we got another drop in execution speed from about 3.151 to about 2.521 by using all cores.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">2.521 seconds</pre><p>We can make the relationship between the number of cores used during training and execution speed more concrete by comparing all values between one and eight and plotting the result.</p>\n<p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of comparing number of cores used during training to execution speed\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\nresults = list()\n# compare timing for number of cores\nn_cores = [1, 2, 3, 4, 5, 6, 7, 8]\nfor n in n_cores:\n\t# capture current time\n\tstart = time()\n\t# define the model\n\tmodel = RandomForestClassifier(n_estimators=500, n_jobs=n)\n\t# fit the model\n\tmodel.fit(X, y)\n\t# capture current time\n\tend = time()\n\t# store execution time\n\tresult = end - start\n\tprint('&#62;cores=%d: %.3f seconds' % (n, result))\n\tresults.append(result)\npyplot.plot(n_cores, results)\npyplot.show()</pre><p>Running the example first reports the execution speed for each number of cores used during training.</p>\n<p>We can see a steady decrease in execution speed from one to eight cores, although the dramatic benefits stop after four physical cores.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">&#62;cores=1: 10.798 seconds\n&#62;cores=2: 5.743 seconds\n&#62;cores=3: 3.964 seconds\n&#62;cores=4: 3.158 seconds\n&#62;cores=5: 2.868 seconds\n&#62;cores=6: 2.631 seconds\n&#62;cores=7: 2.528 seconds\n&#62;cores=8: 2.440 seconds</pre><p>A plot is also created to show the relationship between the number of cores used during training and the execution speed, showing that we continue to see a benefit all the way to eight cores.</p>\n<div id=\"attachment_10956\" style=\"width: 1290px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-10956\" loading=\"lazy\" class=\"size-full wp-image-10956\" src=\"https://machinelearningmastery.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed.png\" alt=\"Line Plot of Number of Cores Used During Training vs. Execution Speed\" width=\"1280\" height=\"960\" srcset=\"http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed.png 1280w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed-300x225.png 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed-1024x768.png 1024w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed-768x576.png 768w\" sizes=\"(max-width: 1280px) 100vw, 1280px\" /><p id=\"caption-attachment-10956\" class=\"wp-caption-text\">Line Plot of Number of Cores Used During Training vs. Execution Speed</p></div>\n<p>Now that we are familiar with the benefit of multi-core training of machine learning models, let&#8217;s look at multi-core model evaluation.</p>\n<h2>Multi-Core Model Evaluation</h2>\n<p>The gold standard for model evaluation is <a href=\"https://machinelearningmastery.com/k-fold-cross-validation/\">k-fold cross-validation</a>.</p>\n<p>This is a resampling procedure that requires that the model is trained and evaluated <em>k</em> times on different partitioned subsets of the dataset. The result is an estimate of the performance of a model when making predictions on data not used during training that can be used to compare and select a good or best model for a dataset.</p>\n<p>In addition, it is also a good practice to repeat this evaluation process multiple times, referred to as repeated k-fold cross-validation.</p>\n<p>The evaluation procedure can be configured to use multiple cores, where each model training and evaluation happens on a separate core. This can be done by setting the <em>n_jobs</em> argument on the call to <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\">cross_val_score() function</a>; for example:</p>\n<p>We can explore the effect of multiple cores on model evaluation.</p>\n<p>First, let&#8217;s evaluate the model using a single core.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=1)</pre><p>We will evaluate the random forest model and use a single core in the training of the model (for now).</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=1)</pre><p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of evaluating a model using a single core\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=1)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# record current time\nstart = time()\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=1)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example evaluates the model using 10-fold cross-validation with three repeats.</p>\n<p>In this case, we see that the evaluation of the model took about 6.412 seconds.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">6.412 seconds</pre><p>We can update the example to use all eight cores of the system and expect a large speedup.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=8)</pre><p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of evaluating a model using 8 cores\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=1)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# record current time\nstart = time()\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=8)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example evaluates the model using multiple cores.</p>\n<p>In this case, we can see the execution timing dropped from 6.412 seconds to about 2.371 seconds, giving a welcome speedup.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">2.371 seconds</pre><p>As we did in the previous section, we can time the execution speed for each number of cores from one to eight to get an idea of the relationship.</p>\n<p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># compare execution speed for model evaluation vs number of cpu cores\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\nresults = list()\n# compare timing for number of cores\nn_cores = [1, 2, 3, 4, 5, 6, 7, 8]\nfor n in n_cores:\n\t# define the model\n\tmodel = RandomForestClassifier(n_estimators=100, n_jobs=1)\n\t# define the evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\t# record the current time\n\tstart = time()\n\t# evaluate the model\n\tn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=n)\n\t# record the current time\n\tend = time()\n\t# store execution time\n\tresult = end - start\n\tprint('&#62;cores=%d: %.3f seconds' % (n, result))\n\tresults.append(result)\npyplot.plot(n_cores, results)\npyplot.show()</pre><p>Running the example first reports the execution time in seconds for each number of cores for evaluating the model.</p>\n<p>We can see that there is not a dramatic improvement above four physical cores.</p>\n<p>We can also see a difference here when training with eight cores from the previous experiment. In this case, evaluating performance took 1.492 seconds whereas the standalone case took about 2.371 seconds.</p>\n<p>This highlights the limitation of the evaluation methodology we are using where we are reporting the performance of a single run rather than repeated runs. There is some spin-up time required to load classes into memory and perform any JIT optimization.</p>\n<p>Regardless of the accuracy of our flimsy profiling, we do see the familiar speedup of model evaluation with the increase of cores used during the process.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">&#62;cores=1: 6.339 seconds\n&#62;cores=2: 3.765 seconds\n&#62;cores=3: 2.404 seconds\n&#62;cores=4: 1.826 seconds\n&#62;cores=5: 1.806 seconds\n&#62;cores=6: 1.686 seconds\n&#62;cores=7: 1.587 seconds\n&#62;cores=8: 1.492 seconds</pre><p>A plot of the relationship between the number of cores and the execution speed is also created.</p>\n<div id=\"attachment_10957\" style=\"width: 1290px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-10957\" loading=\"lazy\" class=\"size-full wp-image-10957\" src=\"https://machinelearningmastery.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed.png\" alt=\"Line Plot of Number of Cores Used During Evaluation vs. Execution Speed\" width=\"1280\" height=\"960\" srcset=\"http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed.png 1280w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed-300x225.png 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed-1024x768.png 1024w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed-768x576.png 768w\" sizes=\"(max-width: 1280px) 100vw, 1280px\" /><p id=\"caption-attachment-10957\" class=\"wp-caption-text\">Line Plot of Number of Cores Used During Evaluation vs. Execution Speed</p></div>\n<p>We can also make the model training process parallel during the model evaluation procedure.</p>\n<p>Although this is possible, should we?</p>\n<p>To explore this question, let&#8217;s first consider the case where model training uses all cores and model evaluation uses a single core.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=8)\n...\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=1)</pre><p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of using multiple cores for model training but not model evaluation\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=8)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# record current time\nstart = time()\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=1)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example evaluates the model using a single core, but each trained model uses a single core.</p>\n<p>In this case, we can see that the model evaluation takes more than 10 seconds, much longer than the 1 or 2 seconds when we use a single core for training and all cores for parallel model evaluation.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">10.461 seconds</pre><p>What if we split the number of cores between the training and evaluation procedures?</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=4)\n...\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=4)</pre><p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of using multiple cores for model training and evaluation\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=8)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=4)\n# record current time\nstart = time()\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=4)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example evaluates the model using four cores, and each model is trained using four different cores.</p>\n<p>We can see an improvement over training with all cores and evaluating with one core, but at least for this model on this dataset, it is more efficient to use all cores for model evaluation and a single core for model training.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">3.434 seconds</pre><p></p>\n<h2>Multi-Core Hyperparameter Tuning</h2>\n<p>It is common to tune the hyperparameters of a machine learning model using a grid search or a random search.</p>\n<p>The scikit-learn library provides these capabilities via the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">GridSearchCV</a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">RandomizedSearchCV</a> classes respectively.</p>\n<p>Both of these search procedures can be made parallel by setting the <em>n_jobs</em> argument, assigning each hyperparameter configuration to a core for evaluation.</p>\n<p>The model evaluation itself could also be multi-core, as we saw in the previous section, and the model training for a given evaluation can also be training as we saw in the second before that. Therefore, the stack of potentially multi-core processes is starting to get challenging to configure.</p>\n<p>In this specific implementation, we can make the model training parallel, but we don&#8217;t have control over how each model hyperparameter and how each model evaluation is made multi-core. The documentation is not clear at the time of writing, but I would guess that each model evaluation using a single core hyperparameter configuration is split into jobs.</p>\n<p>Let&#8217;s explore the benefits of performing model hyperparameter tuning using multiple cores.</p>\n<p>First, let&#8217;s evaluate a grid of different configurations of the random forest algorithm using a single core.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define grid search\nsearch = GridSearchCV(model, grid, n_jobs=1, cv=cv)</pre><p>The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of tuning model hyperparameters with a single core\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=1)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['max_features'] = [1, 2, 3, 4, 5]\n# define grid search\nsearch = GridSearchCV(model, grid, n_jobs=1, cv=cv)\n# record current time\nstart = time()\n# perform search\nsearch.fit(X, y)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example tests different values of the <em>max_features</em> configuration for random forest, where each configuration is evaluated using repeated k-fold cross-validation.</p>\n<p>In this case, the grid search on a single core takes about 28.838 seconds.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">28.838 seconds</pre><p>We can now configure the grid search to use all available cores on the system, in this case, eight cores.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define grid search\nsearch = GridSearchCV(model, grid, n_jobs=8, cv=cv)</pre><p>We can then evaluate how long this multi-core grids search takes to execute. The complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of tuning model hyperparameters with 8 cores\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=1)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['max_features'] = [1, 2, 3, 4, 5]\n# define grid search\nsearch = GridSearchCV(model, grid, n_jobs=8, cv=cv)\n# record current time\nstart = time()\n# perform search\nsearch.fit(X, y)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>Running the example reports execution time for the grid search.</p>\n<p>In this case, we see a factor of about four speed up from roughly 28.838 seconds to around 7.418 seconds.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">7.418 seconds</pre><p>Intuitively, we would expect that making the grid search multi-core should be the focus and not model training.</p>\n<p>Nevertheless, we can divide the number of cores between model training and the grid search to see if it offers a benefit for this model on this dataset.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=4)\n...\n# define grid search\nsearch = GridSearchCV(model, grid, n_jobs=4, cv=cv)</pre><p>The complete example of multi-core model training and multi-core hyperparameter tuning is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># example of multi-core model training and hyperparameter tuning\nfrom time import time\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, n_jobs=4)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['max_features'] = [1, 2, 3, 4, 5]\n# define grid search\nsearch = GridSearchCV(model, grid, n_jobs=4, cv=cv)\n# record current time\nstart = time()\n# perform search\nsearch.fit(X, y)\n# record current time\nend = time()\n# report execution time\nresult = end - start\nprint('%.3f seconds' % result)</pre><p>In this case, we do see a decrease in execution speed compared to a single core case, but not as much benefit as assigning all cores to the grid search process.</p>\n<p>How long does it take on your system? Share your results in the comments below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">14.148 seconds</pre><p></p>\n<h2>Recommendations</h2>\n<p>This section lists some general recommendations when using multiple cores for machine learning.</p>\n<ul>\n<li>Confirm the number of cores available on your system.</li>\n<li>Consider using an AWS EC2 instance with many cores to get an immediate speed up.</li>\n<li>Check the API documentation to see if the model/s you are using support multi-core training.</li>\n<li>Confirm multi-core training offers a measurable benefit on your system.</li>\n<li>When using k-fold cross-validation, it is probably better to assign cores to the resampling procedure and leave model training single core.</li>\n<li>When using hyperparamter tuning, it is probably better to make the search multi-core and leave the model training and evaluation single core.</li>\n</ul>\n<p>Do you have any recommendations of your own?</p>\n<h2>Further Reading</h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3>Related Tutorials</h3>\n<ul>\n<li><a href=\"https://machinelearningmastery.com/k-fold-cross-validation/\">A Gentle Introduction to k-fold Cross-Validation</a></li>\n</ul>\n<h3>APIs</h3>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/developers/performance.html\">How to optimize for speed, scikit-learn Documentation</a>.</li>\n<li><a href=\"https://joblib.readthedocs.io/en/latest/\">Joblib: running Python functions as pipeline jobs</a></li>\n<li><a href=\"https://docs.python.org/3/library/timeit.html\">timeit API</a>.</li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">sklearn.ensemble.RandomForestClassifier API</a>.</li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\">sklearn.model_selection.cross_val_score API</a>.</li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">sklearn.model_selection.GridSearchCV API</a>.</li>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">sklearn.model_selection.RandomizedSearchCV API</a>.</li>\n<li><a href=\"https://scikit-learn.org/stable/glossary.html#term-n-jobs\">n_jobs scikit-learn argument</a>.</li>\n</ul>\n<h3>Articles</h3>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Multi-core_processor\">Multi-core processor, Wikipedia</a>.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Hyper-threading\">Hyper-threading, Wikipedia</a>.</li>\n</ul>\n<h2>Summary</h2>\n<p>In this tutorial, you discovered how to configure scikit-learn for multi-core machine learning.</p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>How to train machine learning models using multiple cores.</li>\n<li>How to make the evaluation of machine learning models parallel.</li>\n<li>How to use multiple cores to tune machine learning model hyperparameters.</li>\n</ul>\n<p><strong>Do you have any questions?</strong><br />\nAsk your questions in the comments below and I will do my best to answer.</p>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-10950 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=Multi-Core+Machine+Learning+in+Python+With+Scikit-Learn&url=https://machinelearningmastery.com/multi-core-machine-learning-in-python/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/multi-core-machine-learning-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/cws/share?url=https://machinelearningmastery.com/multi-core-machine-learning-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/multi-core-machine-learning-in-python/\">Multi-Core Machine Learning in Python With Scikit-Learn</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n","descriptionType":"html","publishedDate":"Sun, 20 Sep 2020 19:00:13 +0000","feedId":24358,"bgimg":"https://machinelearningmastery.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn.jpg","linkMd5":"4ee354f969abb07f9fa5a8465142da99","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn3@2020_2/2020/09/21/06-55-51-713_154b242688f61d13.webp","destWidth":800,"destHeight":450,"sourceBytes":128531,"destBytes":69656,"author":"Jason Brownlee","articleImgCdnMap":{"https://machinelearningmastery.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn.jpg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn3@2020_2/2020/09/21/06-55-51-713_154b242688f61d13.webp","https://machinelearningmastery.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn11@2020_1/2020/09/21/06-55-55-210_d41481fcaa9ad31c.webp","https://machinelearningmastery.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn13@2020_5/2020/09/21/06-56-23-325_bb21265c75374b6a.webp"},"publishedOrCreatedDate":1600671347302}],"record":{"createdTime":"2020-09-21 14:55:47","updatedTime":"2020-09-21 14:55:47","feedId":24358,"fetchDate":"Mon, 21 Sep 2020 06:55:47 +0000","fetchMs":96,"handleMs":534,"totalMs":37721,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"fa10cf689958fca83c3f2c65b7f12392","hostName":"us-024*","requestId":"6370d90334dc4ef5be5bdddab2ec1539_24358","contentType":"application/rss+xml; charset=UTF-8","totalBytes":99744,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":3,"articlesImgsGithubTotal":3,"successGithubMap":{"myreaderx25":1,"myreaderx33":1,"myreaderx1":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 02:32:58","updatedTime":"2020-09-07 04:28:49","id":24358,"name":"Machine Learning Mastery","url":"http://machinelearningmastery.com/feed/","subscriber":135,"website":null,"icon":"https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-32x32.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn91@2020_3/2020/09/06/20-28-44-829_3ddce3e18398fd9b.png","description":"Making developers awesome at machine learning","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":69656,"tmpBodyImgCdnBytes":30088,"tmpBgImgCdnBytes":0,"extra4":{"start":1600671346641,"total":0,"statList":[{"spend":134,"msg":"获取xml内容"},{"spend":534,"msg":"解释文章"},{"spend":3,"msg":"上传封面图到cdn"},{"spend":2,"msg":"修正封面图上传失败重新上传"},{"spend":31128,"msg":"正文链接上传到cdn"}]},"extra5":3,"extra6":3,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-011.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://machinelearningmastery.com/wp-content/uploads/2020/09/Multi-Core-Machine-Learning-in-Python-With-Scikit-Learn.jpg","sourceStatusCode":200,"destWidth":800,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn3@2020_2/2020/09/21/06-55-51-713_154b242688f61d13.webp","sourceBytes":128531,"destBytes":69656,"targetWebpQuality":75,"feedId":24358,"totalSpendMs":2475,"convertSpendMs":26,"createdTime":"2020-09-21 14:55:50","host":"europe-59*","referer":"https://machinelearningmastery.com/?p=10950","linkMd5ListStr":"4ee354f969abb07f9fa5a8465142da99,4ee354f969abb07f9fa5a8465142da99","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"125.5 KB","destSize":"68 KB","compressRate":"54.2%"},{"code":1,"isDone":false,"source":"https://machinelearningmastery.com/wp-content/uploads/2020/05/Line-Plot-of-Number-of-Cores-Used-During-Training-vs-Execution-Speed.png","sourceStatusCode":200,"destWidth":1280,"destHeight":960,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn11@2020_1/2020/09/21/06-55-55-210_d41481fcaa9ad31c.webp","sourceBytes":31891,"destBytes":15212,"targetWebpQuality":75,"feedId":24358,"totalSpendMs":1817,"convertSpendMs":154,"createdTime":"2020-09-21 14:55:54","host":"us-011*","referer":"https://machinelearningmastery.com/?p=10950","linkMd5ListStr":"4ee354f969abb07f9fa5a8465142da99","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31.1 KB","destSize":"14.9 KB","compressRate":"47.7%"},{"code":1,"isDone":false,"source":"https://machinelearningmastery.com/wp-content/uploads/2020/09/Line-Plot-of-Number-of-Cores-Used-During-Evaluation-vs-Execution-Speed.png","sourceStatusCode":200,"destWidth":1280,"destHeight":960,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn13@2020_5/2020/09/21/06-56-23-325_bb21265c75374b6a.webp","sourceBytes":32212,"destBytes":14876,"targetWebpQuality":75,"feedId":24358,"totalSpendMs":2297,"convertSpendMs":252,"createdTime":"2020-09-21 14:56:22","host":"us-012*","referer":"https://machinelearningmastery.com/?p=10950","linkMd5ListStr":"4ee354f969abb07f9fa5a8465142da99","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31.5 KB","destSize":"14.5 KB","compressRate":"46.2%"}],"successGithubMap":{"myreaderx25":1,"myreaderx33":1,"myreaderx1":1},"failGithubMap":{}}