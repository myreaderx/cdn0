{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Highlights from Git 2.28","link":"https://github.blog/?p=53590","description":"<p>The open source Git project just <a href=\"https://lore.kernel.org/git/xmqq5za8hpir.fsf@gitster.c.googlers.com/\">released Git 2.28</a> with features and bug fixes from over 58 contributors, 13 of them new. We last caught up with you on the latest in Git back <a href=\"https://github.blog/2020-03-22-highlights-from-git-2-26/\">when 2.26 was released</a>. Here‚Äôs a look at some of the most interesting features and changes introduced since then.</p> \n<h2 id=\"introducing-init-defaultbranch\">Introducing <code>init.defaultBranch</code><a href=\"https://github.blog/?p=53590#introducing-init-defaultbranch\" class=\"heading-link\" aria-label=\"Introducing <code>init.defaultBranch</code>\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>When you initialize a new Git repository from scratch with <code>git init</code>, Git has always created an initial first branch with the name <code>master</code>. In Git 2.28, a new configuration option, <code>init.defaultBranch</code> is being introduced to replace the hard-coded term. (For more background on this change, <a href=\"https://sfconservancy.org/news/2020/jun/23/gitbranchname/\">this statement from the Software Freedom Conservancy</a> is an excellent place to look).</p> \n<p>Starting in Git 2.28, <code>git init</code> will instead look to the value of <code>init.defaultBranch</code> when creating the first branch in a new repository. If that value is unset, <code>init.defaultBranch</code> defaults to <code>master</code>. Here, it‚Äôs important to note that:</p> \n<ol> \n <li>This configuration variable can be set by the user, and overriding the default value is as easy as: <pre><code class=\"language-ShellSession\">$ git config --global init.defaultBranch main\n</code></pre> </li> \n <li>This configuration variable only affects new repositories, and does not cause branches in existing projects to be renamed. <code>git clone</code> will also continue to respect the <code>HEAD</code> of the repository you‚Äôre cloning from, so you won‚Äôt see a change in branch names until a maintainer initiates one.</li> \n</ol> \n<p>This change supports the many communities, both on GitHub and in the wider Git community, who are considering renaming the default branch name of their repository from <code>master</code>.</p> \n<p>To learn more about the complementary changes GitHub is making, see <a href=\"https://github.com/github/renaming\">github/renaming</a>. <a href=\"https://gitlab.com/gitlab-org/gitlab/-/issues/221164\">GitLab</a> and <a href=\"https://bitbucket.org/blog/moving-away-from-master-as-the-default-name-for-branches-in-git\">Bitbucket</a> are also making similar changes.</p> \n<p>[<a href=\"https://github.com/git/git/compare/480e78595e...508fd8e8ba\">source</a>]</p> \n<h2 id=\"changed-path-bloom-filters\">Changed-path Bloom filters<a href=\"https://github.blog/?p=53590#changed-path-bloom-filters\" class=\"heading-link\" aria-label=\"Changed-path Bloom filters\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>In Git 2.27, the <code>commit-graph</code> file format was extended to store changed-path Bloom filters. What does all of that mean? In a sense, this new information helps Git find points in history that touched a given path much more quickly (for example, <code>git log -- &lt;path&gt;</code>, or <code>git blame</code>). Git 2.28 takes advantage of these optimizations to deliver a handful of sizeable performance improvements.</p> \n<p>Before we get into all of that, it‚Äôs worth taking a refresher through commit graphs whether you‚Äôre new to the concept, or familiar with them. (If you are familiar, and want to take a deeper dive, check out <a href=\"https://devblogs.microsoft.com/devops/super-charging-the-git-commit-graph-iv-bloom-filters/\">this blog post explaining all of the juicy technical details</a>).<br /> In the very simplest terms, the <a href=\"https://git-scm.com/docs/commit-graph\"><code>commit-graph</code> file</a> stores information about commits. In essence, the <code>commit-graph</code> acts like a cache for commonly-accessed information about commits: who their parent(s) are, what their root tree is, and things like that. It also stores computed information, too, like a commit‚Äôs <a href=\"https://devblogs.microsoft.com/devops/supercharging-the-git-commit-graph-iii-generations/\">generation number</a>, and changed-path Bloom filters (more on that in just a moment).</p> \n<p>Why store all of this information? To understand the answer to this, it is helpful to have a cursory understanding of how Git stores objects. Git stores objects in one of two ways: either as a <a href=\"https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\">loose object</a> (in which case the object‚Äôs contents are stored in a single file unique to that object on disk), or as a <a href=\"https://git-scm.com/book/en/v2/Git-Internals-Packfiles\">packed object</a> (in which case the object is assembled from a compressed format in a <code>*.pack</code> file). No matter which way a commit is stored, we still have to parse and decompress it before its fields like ‚Äúroot tree‚Äù and ‚Äúparents‚Äù can be accessed.</p> \n<p>With a <code>commit-graph</code> file, all of that information is immediate: for a given commit <code>C</code>, Git knows exactly where to look in a commit-graph file for all of those fields that we store, and can read them off immediately, no decompression or piecing together required. This can shave some time off your usual Git operations by itself, but where the <code>commit-graph</code> really shines is in the computed data it stores.</p> \n<p>Generation numbers are a sort of reachability index that can help Git answer questions about things like reachability and topological ordering very quickly. Since generation numbers aren‚Äôt new in this release (and trying to explain them quickly would lose a lot of the benefit of a more careful exposition), I‚Äôll refer you instead to <a href=\"https://devblogs.microsoft.com/devops/updates-to the-git-commit-graph-feature/\">this blog post</a> by freshly-minted Hubber <a href=\"https://github.com/derrickstolee\">Derrick Stolee</a> on the matter.</p> \n<h3 id=\"whats-new-in-2-28\">What‚Äôs new in 2.28?<a href=\"https://github.blog/?p=53590#whats-new-in-2-28\" class=\"heading-link\" aria-label=\"What‚Äôs new in 2.28?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>OK, if you‚Äôve made it this far, you‚Äôve got a pretty good handle on what commit graphs are, and what they‚Äôre useful for. Now, let‚Äôs get to the juicy details. In Git 2.27, the <code>commit-graph</code> file learned how to store changed-path Bloom filters. What are changed-path Bloom filters, you ask? A <a href=\"https://en.wikipedia.org/wiki/Bloom_filter\">Bloom filter</a> is a <em>probabilistic set</em>; that is it‚Äôs a set of items, but querying that set for the presence of some item <code>x</code> returns either ‚Äú<code>x</code> is definitely not in this set‚Äù or ‚Äú<code>x</code> might be in this set‚Äù, but never ‚Äú<code>x</code> is definitely in this set‚Äù. The <code>commit-graph</code> stores one of these Bloom filters for commits that reside in the <code>commit-graph</code>, and it populates that Bloom filter with a list of paths changed between that commit and its first parent.</p> \n<p>These Bloom filters are a huge boon for performance in lots of Git commands. The general pattern is something like: if you have a Git command that computes diffs (which can sometimes be proportionally expensive), then having Bloom filters allows Git to compute far fewer diffs by skipping the computation for certain commits when their Bloom filters return ‚Äúdefinitely not‚Äù for paths of interest.</p> \n<p>Take <code>git log -- /path/to/file</code>, for example. Prior to Git 2.27, <code>git log</code> would have to compute a diff over every revision in its walk before determining whether or not to show it (i.e., whether or not that diff has any entries for <code>/path/to/file</code>). In Git 2.27 and newer, Git can skip computing many of those diffs altogether by consulting each commit <code>C</code>‚Äòs changed-path Bloom filter and querying it for <code>/path/to/file</code>. Again: if querying returns ‚Äúdefinitely not‚Äù, then Git knows that computing that diff is strictly uninteresting.</p> \n<p>Because computing diffs between commits can be expensive (at least, relative to the complexity of the algorithm for which they are being generated), reducing the number of diffs computed overall can greatly improve performance.</p> \n<p>To try this for yourself, you can run the command:</p> \n<pre><code class=\"language-ShellSession\">$ git commit-graph write --reachable --changed-paths\n</code></pre> \n<p>This generates a <code>commit-graph</code> file with changed path Bloom filters enabled.<sup>[<a href=\"https://github.blog/?p=53590#footnote-1\">1</a>]</sup> You should be able to see performance improvements in commands like <code>git log -- &lt;path&gt;</code>, <code>git log -L</code>, <code>git blame</code>, and anything else that computes first-parent diffs against a given pathspec.</p> \n<p>[<a href=\"https://github.com/git/git/compare/cf054f817a...caf388caa1\">source</a>, <a href=\"https://github.com/git/git/compare/9b6606f43d...1b4c57fa87\">source</a>, <a href=\"https://github.com/git/git/compare/20514004dd...f32dde8c12\">source</a>]</p> \n<h2 id=\"tidbits\">Tidbits<a href=\"https://github.blog/?p=53590#tidbits\" class=\"heading-link\" aria-label=\"Tidbits\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Now that we‚Äôve talked about a few of the headlining changes from the past couple of releases, let‚Äôs look at a few more new features <img src=\"https://s.w.org/images/core/emoji/13.0.0/72x72/1f50e.png\" alt=\"üîé\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p> \n<ul> \n <li>Have you ever been looking for the parts of history that changed some path? Maybe you just want to know about the commits that have modified some file, and that can be found easily enough by running <code>git log -- &lt;path&gt;</code>.Sometimes, you might be interested not only in which commits touched <code>&lt;path&gt;</code>, but which merge commits brought <em>those</em> commits into the main line of developement. Have you ever found those merges difficult to find? You‚Äôre not alone. In most cases, Git will skip showing you those kind of merges with <code>git log -- &lt;path&gt;</code>, since those commits don‚Äôt modify the <code>&lt;path&gt;</code> by themselves.Now you can bring those merges back into view with Git‚Äôs new <code>--show-pulls</code> flag to revision walking commands, like <code>git log</code> and <code>git rev-list</code>. For a particularly informative view, try: <pre><code class=\"language-ShellSession\">$ git log --oneline --graph --show-pulls -- &lt;path&gt;\n</code></pre> <p>[<a href=\"https://github.com/git/git/compare/82fa169d55...8d049e182e\">source</a>]</p></li> \n</ul> \n<ul> \n <li>When you run <code>git pull</code> in a repository when you‚Äôre tracking a remote branch, one of four things can happen: there might be no changes, changes on the server, client, or both. As long as there aren‚Äôt changes in both directions, resolving the difference is straightforward: when there are no changes at all, there‚Äôs nothing to do. When the server is strictly ahead of the client, the client fast-forwards to the state on the server.But, when there are change both on the client and on the server: what happens? That depends on whether not you have <a href=\"https://git-scm.com/docs/git-config/2.28.0#Documentation/git-config.txt-pullrebase\">the <code>pull.rebase</code> configuration</a> set. If you do, your branch is rebased on top of where you‚Äôre pulling from, and otherwise, a merge is performed.These merges can clutter your history and be tricky to back out of without starting over your pull from scratch. Git 2.28 now warns you of this case (specifically, when <code>pull.rebase</code> is unset, and you didn‚Äôt explicitly specify <code>--[no-]rebase</code> as an argument to <code>git pull</code>). <p>[<a href=\"https://github.com/git/git/compare/369ae7567a...d18c950a69\">source</a>]</p></li> \n</ul> \n<ul> \n <li>Git now includes a <a href=\"https://github.com/features/actions\">GitHub Actions</a> <a href=\"https://github.com/git/git/blob/master/.github/workflows/main.yml\">workflow</a> which you can use to run Git‚Äôs own integration tests on a variety of platforms and compilers. There‚Äôs no extra effort required on your part: if you have a fork of <code>git/git</code> on GitHub, each push will be run through the array of tests necessary to validate your change. But wait: doesn‚Äôt Git use a mailing list for development? Yes, it does, but now you can use <a href=\"https://gitgitgadget.github.io/\">GitGitGadget</a> on the <code>git/git</code> repository. This means that you can open a pull request, and have GitGitGadget send it to the mailing list on your behalf. So, if you‚Äôre more comfortable contributing to Git like that instead of composing emails manually, you can now contribute to Git from start to finish using GitHub. <p>[<a href=\"https://github.com/git/git/compare/af986863c1...f72f328bc5\">source</a>]</p></li> \n</ul> \n<ul> \n <li>On the other hand, if you don‚Äôt mind sending an email or two, it‚Äôs now much easier to interact with the <a href=\"https://lore.kernel.org/git/\">Git mailing list</a> when you encounter a bug by running <code>git bugreport</code>. Running this new command will open your <code>$EDITOR</code> with a pre-populated form of questions that will be useful in debugging your issue. It also includes some helpful information about your system, like your CPU architecture, what version of Git you‚Äôre running, and so on.When you‚Äôre done, you can send that file as the body of an email to the Git mailing list, and rest assured that you‚Äôve opened a helpful bug report. <p>[<a href=\"https://github.com/git/git/compare/6d6b412da3...8f0e9843bd\">source</a>]</p></li> \n</ul> \n<ul> \n <li>We‚Äôve talked a number of times about Git‚Äôs <a href=\"https://git-scm.com/docs/gitattributes#_filter\"><code>clean</code> and <code>smudge</code> filters</a> and the corresponding <a href=\"https://git-scm.com/docs/long-running-process-protocol\"><code>process</code> filter</a> (which simulates multiple <code>clean</code> and <code>smudge</code> filters in a single process). Up until recently, the protocol for these filters has been relatively straightforward: Git supplies one end of the content, and the filter produces the other.In Git 2.27, more information is supplied over the protocol, like metadata about the branch being checked out in the case of <code>git checkout</code>, or the remote that was contacted in case of a <code>git fetch</code>. This new information could be used in tools like, for eg., <a href=\"https://git-lfs.github.com\">Git LFS</a> in order to figure out which remote to contact for extra data. <p>[<a href=\"https://github.com/git/git/compare/fa82be982d...0c0f8a7f28\">source</a>]</p></li> \n</ul> \n<ul> \n <li>Last but not least, <code>git status</code> learned some new tricks, too. You might recall from a recent blog post that we talked how <a href=\"https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/\">sparse checkouts can shrink the size of your monorepo</a>. Now, <code>git status</code> can remind you of when you are in a sparse checkout by telling you what percentage of files you have checked out.For fans of <a href=\"https://github.com/git/git/blob/v2.28.0/contrib/completion/git-prompt.sh\"><code>git-prompt.sh</code></a>, the prompt will now display <code>SPARSE</code> if you are in a sparse checkout, too. <p>[<a href=\"https://github.com/git/git/compare/33a22c1a88...afda36dbf3\">source</a>]</p></li> \n</ul> \n<h2 id=\"the-rest-of-the-iceberg\">The rest of the iceberg<a href=\"https://github.blog/?p=53590#the-rest-of-the-iceberg\" class=\"heading-link\" aria-label=\"The rest of the iceberg\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>That‚Äôs just a sample of changes from the latest couple of releases. For more, check out the release notes for <a href=\"https://github.com/git/git/blob/v2.28.0/Documentation/RelNotes/2.27.0.txt\">2.27</a> and <a href=\"https://github.com/git/git/blob/v2.28.0/Documentation/RelNotes/2.28.0.txt\">2.28</a>, or <a href=\"https://github.com/git/git/tree/v2.28.0/Documentation/RelNotes\">any previous version</a> in the Git repository.</p> \n<p><sup>[1]</sup>: Note that since Bloom filters are not persisted automatically (that is, you have to pass <code>--changed-paths</code> explicitly on each subsequent write), it is a good idea to disable configuration that automatically generates <code>commit-graph</code>s, like <code>fetch.writeCommitGraph</code> and <code>gc.writeCommitGraph</code>.</p>","descriptionType":"html","publishedDate":"Mon, 27 Jul 2020 16:50:05 +0000","feedId":10014,"bgimg":"https://s.w.org/images/core/emoji/13.0.0/72x72/1f50e.png","linkMd5":"b81328087625f26fe46688594131e001","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn18@2020_5/2020/08/25/00-41-38-596_7bb77e3d12a3390a.webp","destWidth":72,"destHeight":72,"sourceBytes":855,"destBytes":1204,"author":"Taylor Blau","articleImgCdnMap":{"https://s.w.org/images/core/emoji/13.0.0/72x72/1f50e.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn18@2020_5/2020/08/25/00-41-38-596_7bb77e3d12a3390a.webp"},"publishedOrCreatedDate":1598316098351},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Using GitHub Actions for MLOps &#038; Data Science","link":"https://github.blog/?p=53097","description":"<h2 id=\"background\">Background<a href=\"https://github.blog/?p=53097#background\" class=\"heading-link\" aria-label=\"Background\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p><a href=\"https://en.wikipedia.org/wiki/MLOps\">Machine Learning Operations</a> (or MLOps) enables Data Scientists to work in a more collaborative fashion, by providing testing, lineage, versioning, and historical information in an automated way.&nbsp; Because the landscape of MLOps is nascent, data scientists are often forced to implement these tools from scratch. The closely related discipline of <a href=\"https://en.wikipedia.org/wiki/DevOps\">DevOps</a> offers some help, however many <a href=\"https://github.com/learn/devops\">DevOps</a> tools are generic and require the implementation of ‚ÄúML awareness‚Äù through custom code. Furthermore, these platforms often require disparate tools that are decoupled from your code leading to poor debugging and reproducibility.</p> \n<p>To mitigate these concerns, we have created a series of GitHub Actions that integrate parts of the data science and machine learning workflow with a software development workflow. Furthermore, we provide components and examples that automate common tasks.</p> \n<h2 id=\"an-example-of-mlops-using-github-actions\">An Example Of MLOps Using GitHub Actions<a href=\"https://github.blog/?p=53097#an-example-of-mlops-using-github-actions\" class=\"heading-link\" aria-label=\"An Example Of MLOps Using GitHub Actions\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Consider the below example of how an experiment tracking system can be integrated with <a href=\"https://github.com/features/actions\">GitHub Actions</a> to enable MLOps. In the below example, we demonstrate how you can orchestrate a machine learning pipeline to run on the infrastructure of your choice, collect metrics using an experiment tracking system, and report the results back to a pull request.</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53100\" src=\"https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125\" alt=\"Screenshot of a pull request\" width=\"1024\" height=\"1125\" srcset=\"https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125?w=1506 1506w, https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125?w=273 273w, https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125?w=768 768w, https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125?w=932 932w, https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125?w=1398 1398w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></p> \n<p style=\"text-align: center\">A screenshot of this <a href=\"https://github.com/machine-learning-apps/actions-ml-cicd/pull/34\">pull request</a>.</p> \n<p>For a live demonstration of the above example, please see <a href=\"https://www.youtube.com/watch?v=Ll50l3fsoYs&amp;feature=youtu.be\">this talk</a>.</p> \n<p>MLOps is not limited to the above example. Due to the composability of GitHub Actions, you can stack workflows in many ways that can help data scientists. Below is a concrete example of a very simple workflow that adds links to <a href=\"https://mybinder.org/\">mybinder.org</a> on pull requests:</p> \n<pre><code class=\"code-none\">name: Binder\non: \n  pull_request:\n    types: [opened, reopened]\n\njobs:\n  Create-Binder-Badge:\n    runs-on: ubuntu-latest\n    steps:\n\n    - name: checkout pull request branch\n      uses: actions/checkout@v2\n      with:\n        ref: ${{ github.event.pull_request.head.sha }}\n\n    - name: comment on PR with Binder link\n      uses: actions/github-script@v1\n      with:\n        github-token: ${{secrets.GITHUB_TOKEN}}\n        script: |\n          var BRANCH_NAME = process.env.BRANCH_NAME;\n          github.issues.createComment({\n            issue_number: context.issue.number,\n            owner: context.repo.owner,\n            repo: context.repo.repo,\n            body: `[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/${context.repo.owner}/${context.repo.repo}/${BRANCH_NAME}) :point_left: Launch a binder notebook on this branch`\n          }) \n      env:\n        BRANCH_NAME: ${{ github.event.pull_request.head.ref }}\n</code></pre> \n<p>When the above YAML file is added to a repository‚Äôs <code>.github/workflow</code> directory, pull requests can be annotated with a useful link as illustrated below [1]:</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53101\" src=\"https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676\" alt=\"Screenshot showing Actions commenting with a Binder link\" width=\"1024\" height=\"676\" srcset=\"https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676?w=1060 1060w, https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676?w=300 300w, https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676?w=768 768w, https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676?w=1024 1024w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></p> \n<h2 id=\"a-growing-ecosystem-of-mlops-data-science-actions\">A Growing Ecosystem of MLOps &amp; Data Science Actions<a href=\"https://github.blog/?p=53097#a-growing-ecosystem-of-mlops-data-science-actions\" class=\"heading-link\" aria-label=\"A Growing Ecosystem of MLOps &amp; Data Science Actions\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>There is a growing number of Actions available for machine learning ops and data science. Below are some concrete examples that are in use today, categorized by topic.</p> \n<p><b>Orchestrating Machine Learning Pipelines</b>:</p> \n<ul> \n <li><a href=\"https://github.com/marketplace/actions/submit-argo-workflows-from-github\">Submit Argo Workflows</a> ‚Äì allows you to orchestrate machine learning pipelines that run on Kubernetes.</li> \n <li><a href=\"https://github.com/marketplace/actions/kubeflow-compile-deploy-and-run\">Publish Kubeflow Pipelines to GKE</a> ‚Äì Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.</li> \n</ul> \n<p><b>Jupyter Notebooks</b>:</p> \n<ul> \n <li><a href=\"https://github.com/yaananth/run-notebook\">Run parameterized Notebooks</a> ‚Äì run notebooks programmatically using papermill.</li> \n <li><a href=\"https://github.com/machine-learning-apps/repo2docker-action\">Repo2Docker Action</a> ‚Äì Automatically turn data-science repositories into Jupyter-enabled Docker containers using repo2docker.</li> \n <li><a href=\"https://github.com/fastai/fastpages\">fastai/fastpages</a> ‚Äì share information from Jupyter notebooks as blog posts using GitHub Actions &amp; GitHub Pages.</li> \n</ul> \n<p><b>End-To-End Workflow Orchestration</b>:</p> \n<ul> \n <li><a href=\"https://techcommunity.microsoft.com/t5/azure-ai/using-github-actions-amp-azure-machine-learning-for-mlops/ba-p/1419027\">Examples</a> and <a href=\"https://github.com/Azure/aml-template\">templates</a> for utilizing Azure Machine Learning from GitHub Actions.</li> \n</ul> \n<p><b>Experiment Tracking</b></p> \n<ul> \n <li><a href=\"https://github.com/marketplace/actions/get-runs-from-weights-biases\">Fetch runs from Weights &amp; Biases</a> ‚Äì W&amp;B is an experiment tracking and logging system for machine learning and is free for open-source projects.</li> \n</ul> \n<p>This is by no means an exhaustive list of the things you might want to automate with GitHub Actions with respect to data science and machine learning. &nbsp; You can follow our progress towards this goal <a href=\"http://mlops-github.com/\">on our page</a>, which contains links to <a href=\"http://mlops-github.com/blog\">blog posts</a>, <a href=\"http://mlops-github.com/actions\">GitHub Actions</a>, <a href=\"http://mlops-github.com/talks\">talks</a>, and <a href=\"http://mlops-github.com/examples\">examples</a> that are relevant to this topic.</p> \n<p>We invite the community to create other Actions that might be useful for the community. Some ideas for getting started include data and model versioning, model deployment, data validation, as well as expanding upon some of the areas mentioned above. A great place to start is the documentation for GitHub Actions, particularly on how <a href=\"https://help.github.com/en/actions/creating-actions\">to build Actions for the community</a>!</p> \n<h2 id=\"related-materials\">Related Materials<a href=\"https://github.blog/?p=53097#related-materials\" class=\"heading-link\" aria-label=\"Related Materials\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<ul> \n <li><a href=\"http://mlops-github.com/\">Our page</a> with relevant materials.</li> \n <li>GitHub Actions official <a href=\"https://help.github.com/en/actions\">documentation</a>.</li> \n <li><a href=\"https://github.com/actions/hello-world-docker-action\">Hello world Docker Action</a>: A template to demonstrate how to build a Docker Action for other people to use</li> \n <li>Using <a href=\"https://help.github.com/en/actions/hosting-your-own-runners/about-self-hosted-runners\">self-hosted runners</a>.</li> \n <li>This talk <a href=\"https://youtu.be/S-kn4mmlxFU\">introducing Actions for data science</a>, including some live-coding!</li> \n <li><a href=\"https://github.com/sdras/awesome-actions\">Awesome Actions</a>: A curated list of interesting GitHub Actions by topic</li> \n <li>Useful GitHub Actions to know about when getting started: \n  <ul> \n   <li><a href=\"https://github.com/actions/checkout\">actions/checkout</a>: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository‚Äôs files into downstream Docker containers.</li> \n   <li><a href=\"https://github.com/mxschmitt/action-tmate\">mxschmitt/action-tmate</a>: Provides a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this.</li> \n   <li><a href=\"https://github.com/actions/github-script\">actions/github-script</a>: Gives you a pre-authenticated <a href=\"https://octokit.github.io/rest.js/\">ocotokit.js</a> client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only <a href=\"https://help.github.com/en/actions/configuring-and-managing-workflows/authenticating-with-the-github_token#permissions-for-the-github_token\">these endpoints</a> are supported.</li> \n  </ul> </li> \n</ul> \n<p>Footnotes:</p> \n<p>[1] This example workflow will not work on pull requests from forks.&nbsp; To enable this, you have to trigger a PR comment to occur via a <a href=\"https://help.github.com/en/actions/reference/events-that-trigger-workflows\">different event.</a></p>","descriptionType":"html","publishedDate":"Wed, 17 Jun 2020 15:00:29 +0000","feedId":10014,"bgimg":"https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125","linkMd5":"bb439890785b1d30230e7b3dca98580f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_1/2020/08/25/00-41-38-844_c989bfd2b85bfc5c.webp","destWidth":1024,"destHeight":1125,"sourceBytes":139262,"destBytes":139262,"author":"Hamel Husain","articleImgCdnMap":{"https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_1/2020/08/25/00-41-38-844_c989bfd2b85bfc5c.webp","https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn57@2020_4/2020/08/25/00-41-40-213_cf3f14d4d372df9a.webp"},"publishedOrCreatedDate":1598316098351},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Introducing GitHub Super Linter: one linter to rule them all","link":"https://github.blog/?p=53124","description":"<p>Setting up a new repository with all the right linters for the different types of code can be time consuming and tedious. So many tools and configurations to choose from and often more than one linter is needed to cover all the languages used.</p> \n<p>The <a href=\"https://github.com/github/super-linter\">GitHub Super Linter</a> was built out of necessity by the GitHub Services DevOps Engineering team to maintain consistency in our documentation and code while making communication and collaboration across the company a more productive experience. Now we are open sourcing that so everyone can use and improve it!</p> \n<p>The <a href=\"https://github.com/github/super-linter\">Super Linter</a>&nbsp;solves many of these requirements through automation. Some included features:</p> \n<ul> \n <li>Prevent broken code from being uploaded to master branches</li> \n <li>Help establish coding best practices across multiple languages</li> \n <li>Build guidelines for code layout and format</li> \n <li>Automate the process to help streamline code reviews</li> \n <li>With these basic criteria, we should be shipping better, cleaner, and more stable code internally and to our customers and partners</li> \n</ul> \n<h2 id=\"what-is-it\">What is it?<a href=\"https://github.blog/?p=53124#what-is-it\" class=\"heading-link\" aria-label=\"What is it?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>The Super Linter is a <em>source code repository</em>&nbsp;that is packaged into a Docker container and called by GitHub Actions. This allows for any repository on GitHub.com to call the Super Linter and start utilizing its benefits.</p> \n<p>The Super Linter will currently support a lot of languages and more coming in the future. For details on languages, check out the <a href=\"https://github.com/github/super-linter/blob/master/README.md\"><code>README.md</code></a>.</p> \n<h2 id=\"how-it-works\">How it works<a href=\"https://github.blog/?p=53124#how-it-works\" class=\"heading-link\" aria-label=\"How it works\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>When you‚Äôve set your repository to start running this action, any time you open a pull request, it will start linting the code case and return via the Status API. It will let you know if any of your code changes passed successfully, or if any errors were detected, where they are, and what they are. This then allows the developer to go back to their branch, fix any issues, and create a new push to the open pull request. At that point, the Super Linter will run again and validate the updated code and repeat the process. You can configure your branch protection rules to make sure all code must pass before being able to merge as an additional measure.</p> \n<p>There‚Äôs a ton of customization with flags and templates that can help you customize the Super Linter to your individual repository. Just follow the detailed directions at the <a href=\"https://github.com/github/super-linter/\">Super Linter repository</a> and the <a href=\"https://github.com/github/super-linter/wiki\">Super Linter wiki</a>.</p> \n<p>This tool can also be helpful for any repository where multiple types of code and/or documentation all live together (monorepo).</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53134\" src=\"https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819\" alt=\"GitHub Super Linter in action\" width=\"1024\" height=\"819\" srcset=\"https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819?w=1990 1990w, https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819?w=300 300w, https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819?w=768 768w, https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819?w=1024 1024w, https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></p> \n<h2 id=\"default-rules\">Default rules<a href=\"https://github.blog/?p=53124#default-rules\" class=\"heading-link\" aria-label=\"Default rules\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Standardizing a rule set across the Super Linter has been an interesting challenge as each developer is unique in how they code. This is why we allow users to use any rules for the linter as they see fit for their repository. But, if no ruleset is defined, we must default to a certain standard.</p> \n<p>The rule set for Ruby and Rails are pulled from the Ruby gem: <a href=\"https://github.com/github/rubocop-github\"><code>rubocop-github</code></a> and follow the same rules and versioning we use on GitHub.com.</p> \n<p>For other languages, we choose what is the default when installing the linter such as: <a href=\"https://github.com/clutchski/coffeelint\"><code>coffeelint</code></a> or <a href=\"https://github.com/adrienverge/yamllint\"><code>yamllint</code></a>. For others, we try to find a happy middle ground that lays the simple groundwork and helps establish some best practices like: <a href=\"https://github.com/igorshubovych/markdownlint-cli\"><code>Markdownlint</code></a> or <a href=\"https://github.com/PyCQA/pylint/\"><code>pylint</code></a>.</p> \n<p>The beauty of this is, out of the box you will start establishing the framework, and your team can decide at any point, if additional customization is needed, you have all the ability to do so.</p> \n<p>Just navigate to the Super Linter and copy templates from the <code>TEMPLATES</code> folder to your local repository.</p> \n<h2 id=\"join-in-the-fun\">Join in the fun<a href=\"https://github.blog/?p=53124#join-in-the-fun\" class=\"heading-link\" aria-label=\"Join in the fun\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>We encourage you to set up this action and start the process of cleaning up your codebase and building your team‚Äôs standards and best practices.</p> \n<h2 id=\"how-can-i-contribute\">How can I contribute?<a href=\"https://github.blog/?p=53124#how-can-i-contribute\" class=\"heading-link\" aria-label=\"How can I contribute?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>We‚Äôre always looking to update best practices, add additional languages, and make the tool easier for consumption. If you‚Äôd like to help contribute to this action, check out our <a href=\"https://github.com/github/super-linter/blob/master/.github/CONTRIBUTING.md\">contributing guide</a>.</p> \n<p><a href=\"https://github.com/github/super-linter/\">Learn more about our Super Linter</a></p>","descriptionType":"html","publishedDate":"Thu, 18 Jun 2020 15:00:19 +0000","feedId":10014,"bgimg":"https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819","linkMd5":"aca63aee24bc985d37005e319ace6d95","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn21@2020_5/2020/08/25/00-41-38-515_d7757c61ef55f70b.webp","destWidth":1024,"destHeight":819,"sourceBytes":102122,"destBytes":102122,"author":"Lucas Gravley","articleImgCdnMap":{"https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn21@2020_5/2020/08/25/00-41-38-515_d7757c61ef55f70b.webp"},"publishedOrCreatedDate":1598316098351},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Introducing GitHub&#8217;s OpenAPI Description","link":"https://github.blog/?p=53620","description":"<p>The GitHub REST API has been through three major revisions since it was first released, <a href=\"https://github.blog/2008-03-12-the-api/\">only a month after the site was launched</a>. We often receive feedback that our REST API is an inspiration to many for design, and that it‚Äôs an industry reference for what an API should look like. Today, we‚Äôre excited to announce an improvement to how developers can interact with the API. GitHub has open sourced an <a href=\"https://github.com/github/rest-api-description\">OpenAPI description</a> of the REST API.</p> \n<h2 id=\"openapi\">OpenAPI<a href=\"https://github.blog/?p=53620#openapi\" class=\"heading-link\" aria-label=\"OpenAPI\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>The <a href=\"https://github.com/OAI/OpenAPI-Specification\">OpenAPI specification</a> is a programming language agnostic standard that lets providers describe the interface of their HTTP APIs. This allows both humans and machines to discover the capabilities of an API without needing to first read documentation or understand the implementation. OpenAPI is a widely adopted industry standard and GitHub is proud to be part of the community and help push the standard forward.</p> \n<h2 id=\"try-it-out\">Try it Out<a href=\"https://github.blog/?p=53620#try-it-out\" class=\"heading-link\" aria-label=\"Try it Out\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>The GitHub OpenAPI description contains more than 600 operations exposed in our API. For visual exploration of the API, you can load the description as a <a href=\"https://www.postman.com/collection/\">Postman Collection</a>. Programmatically, the description can be used to generate mock servers, test suites, and bindings for languages not supported by <a href=\"https://github.com/octokit\">Octokit</a>.</p> \n<p>The description is provided under two formats. The <b>bundled </b>version is preferred for most use cases as it makes use of <a href=\"https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#componentsObject\">OpenAPI components</a> for reuse and readability. For tooling that has poor support for inline references to components, we also provide a fully <b>dereferenced</b> version.</p> \n<h2 id=\"active-development\">Active Development<a href=\"https://github.blog/?p=53620#active-development\" class=\"heading-link\" aria-label=\"Active Development\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>The description is currently in <i>beta</i>. Describing a 12-year-old API is no easy task. We‚Äôve built this description using a mix of existing JSON schemas, documented examples, contract testing, and love. We expect to make the description even more complete and accurate as we go forward and as OpenAPI becomes central to our developer experience ‚Äî internally and externally.</p> \n<p>Quarterly releases of the description are available for GitHub Enterprise Server and GitHub Private Instances, with versions like <i>v2.21</i>. More frequent updates to the description will be available for GitHub.com.</p> \n<h2 id=\"how-can-i-contribute\">How Can I Contribute?<a href=\"https://github.blog/?p=53620#how-can-i-contribute\" class=\"heading-link\" aria-label=\"How Can I Contribute?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>We‚Äôre always looking to make our OpenAPI description more complete and accurate as well as making it easier to consume. If you‚Äôd like to help contribute to the description, <a href=\"https://github.com/github/rest-api-description/blob/master/CONTRIBUTING.md\">check out our contributing guide</a>. If something is not working for you, please file an Issue on the repository.</p> \n<p>Building a complete OpenAPI description for the GitHub API was no easy task and could not have been possible without a great team. Thanks to <a href=\"https://github.blog/2020-04-09-from-48k-lines-of-code-to-10-the-story-of-githubs-javascript-sdk/\">Gregor Martynus for his initial work on describing the API</a>, the <a href=\"https://github.blog/2020-07-02-how-we-launched-docs-github-com/\">Docs Engineering team for their amazing work around OpenAPI and documentation</a>, <a href=\"https://github.com/WillAbides\">Will Roden</a> for his help validating the description with <a href=\"https://github.com/WillAbides/octo-go\">octo-go</a>, as well as the folks at <a href=\"https://redoc.ly/\">Redoc.ly</a> who helped along the way.</p> \n<p>Learn more about our <a href=\"https://github.com/github/rest-api-description\">REST API OpenAPI Description</a></p> \n<p><i>*&nbsp; The OpenAPI Initiative logo is a trademark of The Linux Foundation</i></p>","descriptionType":"html","publishedDate":"Mon, 27 Jul 2020 16:00:07 +0000","feedId":10014,"bgimg":"","linkMd5":"326ee6bc78e5005457b89b7f05cbc53a","bgimgJsdelivr":"","metaImg":"","author":"Marc-Andre Giroux","publishedOrCreatedDate":1598316098350},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Introducing the GitHub Availability Report","link":"https://github.blog/?p=53375","description":"<h2 id=\"what-is-the-availability-report\">What is the Availability Report?<a href=\"https://github.blog/?p=53375#what-is-the-availability-report\" class=\"heading-link\" aria-label=\"What is the Availability Report?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Historically, GitHub has published post-incident reviews for major incidents that impact service availability. Whether we‚Äôre sharing new investments to infrastructure or detailing site downtimes, our belief is that we can collectively grow as an industry by learning from one another. This month, we‚Äôre excited to introduce the GitHub Availability Report.</p> \n<h2 id=\"what-can-you-expect\">What can you expect?<a href=\"https://github.blog/?p=53375#what-can-you-expect\" class=\"heading-link\" aria-label=\"What can you expect?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>On the first Wednesday of each month, we‚Äôll publish a report describing GitHub‚Äôs availability, including a description of any incidents that may have occurred and update you on how we are evolving our engineering systems and practices in response. You should expect these updates to include a summary of what happened, as well as a technical explanation for incidents where we believe the occurrence was novel and contains information that helps engineers around the world learn how to improve product operations at scale.</p> \n<h2 id=\"why-are-we-doing-this\">Why are we doing this?<a href=\"https://github.blog/?p=53375#why-are-we-doing-this\" class=\"heading-link\" aria-label=\"Why are we doing this?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Availability and performance are a core feature, including how GitHub responds to service disruptions. We strive to engineer systems that are highly available and fault-tolerant and we expect that most of these monthly updates will recap periods of time where GitHub was &gt;99% available. When things don‚Äôt go as planned, rather than waiting to share information about particularly interesting incidents, we want to describe all of the events that may impact you. Our hope is that by increasing our transparency and sharing what we‚Äôve learned, rather than simply reporting minutes of downtime on a status page, everyone can learn from our experiences. At GitHub, we take the trust you place in us very seriously, and we hope this is a way for you to help hold us accountable for continuously improving our operational excellence as well as our product functionality.</p> \n<h1 id=\"availability-report-for-may-and-june\">Availability Report for May and June<a href=\"https://github.blog/?p=53375#availability-report-for-may-and-june\" class=\"heading-link\" aria-label=\"Availability Report for May and June\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h1> \n<p>In May and June, we experienced four distinct incidents resulting in a lack of availability or degraded service for GitHub.com.</p> \n<h3 id=\"may-5-0045-utc-lasting-for-two-hours-and-24-minutes\">May 5 00:45 UTC (lasting for two hours and 24 minutes)<a href=\"https://github.blog/?p=53375#may-5-0045-utc-lasting-for-two-hours-and-24-minutes\" class=\"heading-link\" aria-label=\"May 5 00:45 UTC (lasting for two hours and 24 minutes)\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>During the incident, a shared database table‚Äôs auto-incrementing ID column exceeded the size that can be represented by the MySQL Integer type (Rails int(11)): 2147483647. When we attempted to insert larger integers into the column, the database rejected the value and Rails raised an ActiveModel::RangeError, which resulted in 500s from our API endpoint.</p> \n<p>This impacted GitHub apps that rely on getting installation tokens. The top affected GitHub apps internally included Actions, Pages, and Dependabot.</p> \n<p>GitHub‚Äôs monitoring systems currently alert when tables hit 70% of the primary key size used. We are now extending our test frameworks to include a linter in place for int / bigint foreign key mismatches.</p> \n<h3 id=\"may-22-1641-utc-lasting-for-five-hours-and-nine-minutes\">May 22 16:41 UTC (lasting for five hours and nine minutes)<a href=\"https://github.blog/?p=53375#may-22-1641-utc-lasting-for-five-hours-and-nine-minutes\" class=\"heading-link\" aria-label=\"May 22 16:41 UTC (lasting for five hours and nine minutes)\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>During a planned maintenance operation (failing over a MySQL primary instance) we experienced a novel crash in the mysqld process on the newly promoted MySQL primary server. To mitigate the impact of the crash, we manually redirected traffic back to the original primary. However, the crashed MySQL primary had already served approximately six seconds of write traffic. At this point, a restore of replicas from the new primary was initiated which took approximately four hours with a further hour for cluster reconfiguration to re-enable full read capacity. For a period of approximately five hours, users may have observed delays before data written to the affected database cluster were visible in the web interface and API.</p> \n<p>We‚Äôve run multiple internal gameday exercises in response to ensure a higher degree of preparedness for similar topology inconsistencies and will continue to exercise our automated failover systems to reduce recovery time.</p> \n<h3 id=\"june-19-0852-utc-lasting-for-51-minutes\">June 19 08:52 UTC (lasting for 51 minutes)<a href=\"https://github.blog/?p=53375#june-19-0852-utc-lasting-for-51-minutes\" class=\"heading-link\" aria-label=\"June 19 08:52 UTC (lasting for 51 minutes)\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>Changes to better instrument A/B experimentation for UI improvements introduced an unknown dependency on the presence of a specific, dynamically generated file that is served by a separate application.</p> \n<p>During an application deployment, the file failed to be generated on a significant proportion of the application deployments due to a high retrieval rate being rate limited by the upstream application. This resulted in site-wide application errors for a percentage of users enrolled in the experiment. Upon detection, we were able to disable the requirement on this file which restored service to all users.</p> \n<p>Going forward, configuration for A/B and multivariate experiments will be cached internally to ensure successful propagation of dependencies.</p> \n<h3 id=\"june-29-1203-utc-lasting-for-two-hours-and-29-minutes\">June 29 12:03 UTC (lasting for two hours and 29 minutes)<a href=\"https://github.blog/?p=53375#june-29-1203-utc-lasting-for-two-hours-and-29-minutes\" class=\"heading-link\" aria-label=\"June 29 12:03 UTC (lasting for two hours and 29 minutes)\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>As part of maintenance, the database team rolled out an updated version of ProxySQL on Monday, June 22. A week later, the primary MySQL node on one of our main database clusters failed and was replaced automatically by a new host. Within seconds, the newly promoted primary crashed. <a href=\"https://github.com/openark/orchestrator\">Orchestrator</a>‚Äôs anti-flapping mechanism prevented a subsequent automatic failover. After we recovered service manually, the new primary became CPU starved and crashed again. A new primary was promoted which also crashed shortly thereafter. To recover, we rolled back to the previous version of ProxySQL and disabled a change in our application that had required the new ProxySQL version. When this completed, we were able to allow writes on the primary node without it crashing.</p> \n<p>We are analyzing application logs, MySQL core dumps, and our internal telemetry as part of continued investigation into the CPU starvation issue to avoid similar failure modes going forward.</p> \n<h1 id=\"in-summary\">In summary<a href=\"https://github.blog/?p=53375#in-summary\" class=\"heading-link\" aria-label=\"In summary\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h1> \n<p>As an organization we continue to invest heavily in reliability. We treat each incident discussed here as an invaluable opportunity from which to learn and grow. Our systems and processes continue to evolve based on these learnings and we look forward to sharing our progress in future updates.</p> \n<p>Please follow our <a href=\"https://githubstatus.com\">status page</a> for real time updates and watch our blog for next month‚Äôs availability report.</p>","descriptionType":"html","publishedDate":"Wed, 08 Jul 2020 20:00:37 +0000","feedId":10014,"bgimg":"","linkMd5":"6358d8b522dee88934d894ba97e8e256","bgimgJsdelivr":"","metaImg":"","author":"Keith Ballinger","publishedOrCreatedDate":1598316098351},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Introducing the Rally + GitHub integration","link":"https://github.blog/?p=53970","description":"<p>GitHub&#8217;s Professional Services Engineering team has decided to open source another project:¬†<a href=\"https://github.com/github/rally\">Rally + GitHub</a>. You may have seen our most recent open source project,¬†<a href=\"https://github.blog/2020-06-18-introducing-github-super-linter-one-linter-to-rule-them-all/\" rel=\"nofollow\">Super Linter</a>. Well, the team has done it again, this time to help users ensure that Rally stays up to date with the latest development in GitHub!¬†<img src=\"https://s.w.org/images/core/emoji/13.0.0/72x72/1f389.png\" alt=\"üéâ\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n<h2><a id=\"user-content-rally--github\" class=\"anchor\" href=\"https://gist.github.com/leereilly/ee1a09585ef81f282048b813b7e41434#rally--github\" aria-hidden=\"true\"></a>Rally + GitHub</h2>\n<p>This project integrates GitHub Enterprise Server (and cloud, if you host it yourself) with Broadcom&#8217;s Rally project management.</p>\n<p>Every time a pull request is created or updated,¬†<a href=\"https://github.com/github/rally\">Rally + GitHub</a>¬†will check for the existence of a¬†<strong>Rally User Story</strong>¬†or¬†<strong>Defect</strong>¬†in the¬†<code>title</code>,¬†<code>body</code>, or¬†<code>commit messages</code>, and then validate that they both¬†<em>exist</em>¬†and are in the correct¬†<em>state</em>¬†within¬†<strong>Rally</strong>.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53973\" src=\"https://github.blog/wp-content/uploads/2020/08/demo.gif?resize=1024%2C576\" alt=\"Animation showing a pull request being created\" width=\"1024\" height=\"576\" data-recalc-dims=\"1\" /></p>\n<h2><a id=\"user-content-why-was-it-created\" class=\"anchor\" href=\"https://gist.github.com/leereilly/ee1a09585ef81f282048b813b7e41434#why-was-it-created\" aria-hidden=\"true\"></a>Why was it created?</h2>\n<p>GitHub Enterprise Server had a legacy Services integration with Rally. The deprecation of legacy Services for GitHub was¬†<a href=\"https://developer.github.com/changes/2018-04-25-github-services-deprecation\">announced in 2018</a>, and the release of GitHub Enterprise Server 2.20¬†<a href=\"https://enterprise.github.com/releases/2.20.0/notes\">officially removed this functionality</a>. As a result, many GitHub Enterprise users will be left without the ability to integrate the two platforms when upgrading to recent releases of GitHub Enterprise Server.</p>\n<p>While Broadcom¬†<a href=\"https://rally1.rallydev.com/apps/github-app/home\" rel=\"nofollow\">created a new integration for github.com</a>, this functionality does not extend to GitHub Enterprise Server environments.</p>\n<h2><a id=\"user-content-get-started\" class=\"anchor\" href=\"https://gist.github.com/leereilly/ee1a09585ef81f282048b813b7e41434#get-started\" aria-hidden=\"true\"></a>Get Started</h2>\n<p>We encourage you to check out this¬†<a href=\"https://github.com/github/rally\">project</a>¬†and set it up with your existing Rally instance. A good place to start getting set up is the¬†<a href=\"https://github.com/github/rally/blob/main/.github/README.md#get-started\">Get Started</a>¬†guide in the project&#8217;s¬†<code>README.md</code></p>\n<p>We invite you to join us in developing this project! Come engage with us by opening up an¬†<a href=\"https://github.com/github/rally/issues\">issue</a>¬†even just to share your experience with the project.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53975\" src=\"https://github.blog/wp-content/uploads/2020/08/demo2.gif?resize=1024%2C576\" alt=\"Animation showing Rally and GitHub integration\" width=\"1024\" height=\"576\" data-recalc-dims=\"1\" /></p>\n","descriptionType":"html","publishedDate":"Tue, 18 Aug 2020 15:00:11 +0000","feedId":10014,"bgimg":"https://s.w.org/images/core/emoji/13.0.0/72x72/1f389.png","linkMd5":"422e103183b7eeff6dd32d4c3057c4fd","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn14@2020_5/2020/08/25/00-41-38-588_41448c516a847944.webp","destWidth":72,"destHeight":72,"sourceBytes":1376,"destBytes":2694,"author":"Jared Murrell","articleImgCdnMap":{"https://s.w.org/images/core/emoji/13.0.0/72x72/1f389.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn14@2020_5/2020/08/25/00-41-38-588_41448c516a847944.webp","https://github.blog/wp-content/uploads/2020/08/demo.gif?resize=1024%2C576":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn45@2020_5/2020/08/25/00-42-05-727_202eb8df93ba2457.webp","https://github.blog/wp-content/uploads/2020/08/demo2.gif?resize=1024%2C576":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn50@2020_5/2020/08/25/00-41-56-573_88d6c6591ab12fd2.webp"},"publishedOrCreatedDate":1598316098351},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"How we launched docs.github.com","link":"https://github.blog/?p=53324","description":"<p>ICYMI: <a href=\"https://github.blog/2020-07-01-launching-docs-github-com\">docs.github.com is the new place to discover all of GitHub‚Äôs product documentation</a>!</p> \n<p>We recently completed a major overhaul of GitHub‚Äôs documentation websites. When you visit docs.github.com today, you‚Äôll see content from the former help.github.com and developer.github.com sites in a unified experience.</p> \n<p>Our engineering goals were two-fold: 1) improve the reading and navigation experience for GitHub‚Äôs users; 2) improve the suite of tools that GitHub‚Äôs writers use to create and publish documentation.</p> \n<p>Combining the content was the last of several complex projects we completed to reach these goals. Here‚Äôs the story behind this years-long effort, undertaken in collaboration with GitHub‚Äôs Product Documentation team and many other contributors.</p> \n<h2 id=\"a-brief-history-of-the-docs-sites\">&nbsp;A brief history of the docs sites<a href=\"https://github.blog/?p=53324#a-brief-history-of-the-docs-sites\" class=\"heading-link\" aria-label=\"&nbsp;A brief history of the docs sites\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Providing separate docs sites for different audiences was the right choice for us for many years. But our plans evolved along with GitHub‚Äôs products. Over time, we aspired to help an international audience use GitHub by:</p> \n<ul> \n <li>Offering multi-language support for all content</li> \n <li>Scaling docs for new products</li> \n <li>Autogenerating API reference docs</li> \n <li>Providing interactive experiences</li> \n <li>Allowing anyone to easily contribute documentation</li> \n</ul> \n<p>We couldn‚Äôt do these things when we had two static sites, each with its own codebase, its own way of organizing content, and its own markup conventions. Efforts were made to streamline the tooling over the years, but they were limited by the nature of static builds.</p> \n<p>To achieve our goals, we determined we needed to write a custom dynamic backend, and eventually, combine the content.</p> \n<h2 id=\"only-fixing-what-was-broken\">Only fixing what was broken<a href=\"https://github.blog/?p=53324#only-fixing-what-was-broken\" class=\"heading-link\" aria-label=\"Only fixing what was broken\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Our docs sites were <a href=\"https://github.blog/2015-01-06-how-github-uses-github-to-document-github/\">previously hosted on GitHub Pages</a> using <a href=\"https://jekyllrb.com/\">Jekyll</a>&nbsp; practices: a <code>content</code> directory full of Markdown files and a <code>data</code> directory full of YAML files. This is a great setup for simple sites, and it worked for us for a long time. Although we outgrew Jekyll tooling, the writing conventions based in Markdown and YAML worked well. So we kept them, and we built the dynamic site around them.</p> \n<p>Keeping these conventions let us alleviate pain points in the tooling without introducing a new paradigm for technical writing and asking writers to learn it. It also meant that writers could continue publishing content that helps people use GitHub in the old system while we built the new one.</p> \n<h2 id=\"what-was-broken\">What <em>was</em> broken?<a href=\"https://github.blog/?p=53324#what-was-broken\" class=\"heading-link\" aria-label=\"What <em>was</em> broken?\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>We outgrew static site tooling for a number of reasons. A big factor was the complexity of versioning content for our GitHub Enterprise Server product.</p> \n<p>We release a new version of GitHub Enterprise Server every three months, and we support docs for each version for one year before we deprecate them. At any time, we provide docs for <strong>four</strong> versions of GitHub Enterprise Server.</p> \n<p>We handle this complexity by <em>single-sourcing</em> our docs. This means we provide multiple versions of each article, and a dropdown on the site lets users switch between versions. Here‚Äôs how it looked in the old help.github.com:</p> \n<p><img src=\"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-7.48.43-PM.png\" alt=\"Screenshot showing versioned articles\" /></p> \n<p>Versioning can be hard. Some articles are available in all versions. Some are GitHub.com-only. Some are Enterprise Server-only. Some have lots of internal conditionals, where a single paragraph or even a word may be relevant in some versions but not others. We also have workflows and tools for releasing new versions and deprecating old versions.</p> \n<p>What does single-sourcing look like under the hood? Writers use the Liquid templating language (<a href=\"https://jekyllrb.com/docs/liquid/\">another Jekyll holdover</a>) to render version-specific content using <code>if</code> / <code>else</code> statements:</p> \n<pre><code class=\"text\">{% if page.version == 'dotcom' or page.version ver_gt '2.20' %}\n\nContent relevant to new versions\n\n{% else %}\n\nContent relevant to old versions\n\n{% endif %}</code></pre> \n<p>Statements like this are all over the <code>content</code> and <code>data</code> files.</p> \n<p>Static site generators are designed to do <em>one</em> build. They don‚Äôt build multiple versions of pages. To support our single-source approach in the Jekyll days, we had to create a <strong>backport</strong>&nbsp;process, in which writers would build Enterprise Server versions separately from building GitHub.com docs. Backport pull requests had to be reviewed, deployed to staging, and published as a separate process. Over the years, as we released new Enterprise Server versions, the tooling started to fray around the edges. Backports took a long time to build, did weird things, or got forgotten entirely. Ultimately, backports became a liability.</p> \n<h2 id=\"launching-a-dynamic-help-github-com\">Launching a dynamic help.github.com<a href=\"https://github.blog/?p=53324#launching-a-dynamic-help-github-com\" class=\"heading-link\" aria-label=\"Launching a dynamic help.github.com\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>When we set out to create a new dynamic site, we started with help.github.com. We built it over six months and carefully coordinated with the writers to swap out the backend, while mostly leaving the content alone. In February 2019, we launched the new Node.js site backed by Express. On the frontend, there is just vanilla JavaScript and CSS using <a href=\"https://primer.style/css/tools/prototyping\">Primer</a>.</p> \n<p>It was a big improvement:</p> \n<ul> \n <li><strong>No more build</strong> \n  <ul> \n   <li>The app loads metadata for all pages at server startup, but the contents are rendered dynamically at page load.</li> \n   <li>Shaved ~10 minutes off deploy times.</li> \n  </ul> </li> \n <li><strong>Dynamic version rendering</strong> \n  <ul> \n   <li>No more backports! Enterprise Server content is loaded at the same time as everything else.</li> \n  </ul> </li> \n <li><strong>Fastly CDN</strong> \n  <ul> \n   <li>Serves as a global edge cache to keep things fast.</li> \n  </ul> </li> \n <li><strong>Better search using Algolia</strong></li> \n <li><strong>Less chatops, more GitHub flow</strong> \n  <ul> \n   <li>Staging and production deployments happen automatically.</li> \n  </ul> </li> \n</ul> \n<h2 id=\"internationalized-docs\">Internationalized docs<a href=\"https://github.blog/?p=53324#internationalized-docs\" class=\"heading-link\" aria-label=\"Internationalized docs\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Within a few months, the dynamic backend allowed us to reach our next major milestone: internationalization. We launched the <a href=\"https://github.blog/2019-06-27-making-github-more-accessible\">Japanese and simplified Chinese versions of the site in June 2019</a> and added support for <a href=\"https://github.blog/2019-09-16-product-documentation-now-available-in-spanish\">Spanish</a> and <a href=\"https://github.blog/2019-10-23-help-documentation-now-available-in-portuguese\">Portuguese</a> by the end of the year. (Look for a deep dive post into the internationalization process coming soon!)</p> \n<p>This was progress. But developer.github.com was still running on the old static build, and parts of it were starting to break down. We needed to bring the developer content into the new codebase.</p> \n<h2 id=\"supporting-multiple-products\">Supporting multiple products<a href=\"https://github.blog/?p=53324#supporting-multiple-products\" class=\"heading-link\" aria-label=\"Supporting multiple products\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>First, we needed to more robustly support the idea of products.</p> \n<p>When we originally launched the new help site, the homepage did allow users to choose a product:</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53338\" src=\"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-8.19.12-PM.png?resize=689%2C294\" alt=\"Old help site\" width=\"689\" height=\"294\" srcset=\"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-8.19.12-PM.png?resize=689%2C294?w=689 689w, https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-8.19.12-PM.png?resize=689%2C294?w=300 300w\" sizes=\"(max-width: 689px) 100vw, 689px\" data-recalc-dims=\"1\" />But the <em>content</em> for these products was organized in wildly different ways. For example, a directory called <code>content/dotcom/articles</code> contained nearly a thousand Markdown files with no hierarchy. URLs looked like <code>help.github.com/articles/&lt;article&gt;</code>, with no indication of which product they belonged to. Writers and contributors had a hard time navigating the repository. It all ‚Äúworked,‚Äù but it wouldn‚Äôt scale.</p> \n<p>So we created a new product-centric structure that would be consistent across the site: <code>content/&lt;product&gt;/&lt;category&gt;/&lt;article&gt;</code>, with URLs that matched. To support this change, we developed a new TOC system and refactored product handling on the backend. Once again, we coordinated the changes with writers who were still actively writing, and once again, we left the core Jekyll conventions untouched. We also added support for redirects from the legacy article URLs to the new product-based ones.</p> \n<p>In 2019, we released <a href=\"https://docs.github.com/actions\">GitHub Actions</a> as the first new product on the help site.</p> \n<p>With a more scalable content organization in place, we were ready to <em>start</em> thinking about how to get developer content into the codebase.</p> \n<h2 id=\"autogenerating-api-documentation\">Autogenerating API documentation<a href=\"https://github.blog/?p=53324#autogenerating-api-documentation\" class=\"heading-link\" aria-label=\"Autogenerating API documentation\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Historically, developer.github.com hosted documentation for integrators, including docs for GitHub‚Äôs two APIs: REST and GraphQL.</p> \n<h3 id=\"rest-docs-via-openapi\">REST docs via openAPI<a href=\"https://github.blog/?p=53324#rest-docs-via-openapi\" class=\"heading-link\" aria-label=\"REST docs via openAPI\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>From the time GitHub‚Äôs REST docs were first released roughly a decade ago, they were handwritten and updated by humans using unstructured formats. This workflow was sustainable at first, but as the API grew, it became a big drain on writers‚Äô time. Updating REST input and response parameters manually was hard enough, but versioning them for GitHub.com and Enterprise Server was almost impossibly complex. Readers had long asked for code samples and other standard features of API documentation we were unable to provide. We‚Äôd dreamed of autogenerating REST docs from a structured schema, but this seemed unattainable for years.</p> \n<p>With the new codebase in place, it opened the door for us to think about autogeneration for real. And we happened into some lucky timing.</p> \n<p><a href=\"https://github.blog/2020-04-09-from-48k-lines-of-code-to-10-the-story-of-githubs-javascript-sdk/\">Octokit maintainer Gregor Martynus</a> had already started the process of generating an OpenAPI schema that described how GitHub‚Äôs API works. This schema happened to be exactly what we needed. Rather than reinventing the wheel, we invested in that existing schema effort and enlisted the services of <a href=\"https://redoc.ly\">Redoc.ly</a>, a small firm that specializes in OpenAPI schema design and implementation. We worked with them to get the work-in-progress OpenAPI over the finish line and ready for production use, and created a pipeline to consume and render the docs from OpenAPI.</p> \n<p>Check out the new REST docs: <a href=\"http://docs.github.com/rest/reference\">http://docs.github.com/rest/reference</a></p> \n<h3 id=\"graphql-docs\">GraphQL docs<a href=\"https://github.blog/?p=53324#graphql-docs\" class=\"heading-link\" aria-label=\"GraphQL docs\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>GraphQL is a different story from REST. Since GitHub first released its GraphQL API in 2017, we‚Äôve had a pipeline for autogenerating docs from a schema using <a href=\"https://github.com/gjtorikian/graphql-docs\">https://github.com/gjtorikian/graphql-docs</a>.</p> \n<p>This tooling worked well, but it was written in Ruby, and with the new Node.js backend, we needed something more JavaScript-friendly. We looked for existing JavaScript GraphQL docs generators but didn‚Äôt find any that fit our specific needs. So we rolled our own.</p> \n<p>We wrote a script that takes a GraphQL schema as input, does some sanitization, and outputs JSON files containing only the data needed for rendering documentation. Our HTML files loop over that JSON data and render it on page load.</p> \n<p>The script runs via a <a href=\"https://help.github.com/en/actions/reference/workflow-syntax-for-github-actions#onschedule\">scheduled GitHub Actions workflow</a>&nbsp;and automatically opens and merges PRs with the updates. This means writers never have to touch GraphQL documentation; it publishes itself.</p> \n<p>Check out the new GraphQL docs: <a href=\"http://docs.github.com/graphql/reference\">http://docs.github.com/graphql/reference </a></p> \n<h2 id=\"scripting-the-content-migration\">Scripting the content migration<a href=\"https://github.blog/?p=53324#scripting-the-content-migration\" class=\"heading-link\" aria-label=\"Scripting the content migration\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>In addition to API docs, developer.github.com contained content about GitHub and OAuth apps, GitHub Marketplace, and webhooks. The majority of this content is vanilla Markdown, so the project requirements were (finally) straightforward: import the files, process them, run tests. We wrote scripts to do this in a repeatable process.</p> \n<p>The content strategist on the docs team created a comprehensive spreadsheet that mapped all the old developer content to their new product-based locations, with titles and intros for each. This formed the basis of our scripted efforts. We ran the scripts several times, doing reviews and making changes each time, before unveiling the final documentation.</p> \n<p>Check out the new docs: <a href=\"http://docs.github.com/developers\">http://docs.github.com/developers</a></p> \n<h2 id=\"redirecting-all-the-things\">Redirecting all the things<a href=\"https://github.blog/?p=53324#redirecting-all-the-things\" class=\"heading-link\" aria-label=\"Redirecting all the things\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Getting a 404 on a documentation site is an abrupt end to a learning experience. GitHub‚Äôs docs team makes a commitment to prevent 404s when content files get renamed or moved around. There are a number of ways we support 20,000+ redirects in the codebase, and they can get complex.</p> \n<p>For example, if you go to an Enterprise URL without a version, such as https://docs.github.com/enterprise, the site redirects you to the latest version by injecting the number in the URL. But we have to be careful ‚Äì what if the URL happens to include <code>enterprise</code> somewhere but is not a real Enterprise Server path? No number should be injected in those cases.</p> \n<p>We also redirect any URL without a language code to use the <code>/en</code> prefix. And we have special link rewriting under the hood to make sure that if you are on a Japanese page, all links to other GitHub articles take you to <code>/ja</code> versions of those pages instead of <code>/en</code> versions.</p> \n<p>Soon we will enable blanket redirects to point most <code>https://developer.github.com</code> links to <code><a href=\"https://docs.github.com\">https://docs.github.com</a></code>. That step won‚Äôt be too hard, but in the course of the migration, we changed the names and locations of much of the developer content. For example, <code>/v3</code> became <code>/rest/reference</code>, <code>/apps</code> became <code>/developers/apps</code>, and so on. To support all these redirects, we worked from a list of the top few hundred developer.github.com URLs from Google Analytics to whittle down dead links, path by path.</p> \n<p>These redirects will help GitHub‚Äôs users arrive at the content they want when they navigate docs.github.com or follow legacy bookmarks or links.</p> \n<h2 id=\"whats-next\">What‚Äôs next<a href=\"https://github.blog/?p=53324#whats-next\" class=\"heading-link\" aria-label=\"What‚Äôs next\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>This is an exciting time for content at GitHub. With the foundation we built for docs.github.com, we can‚Äôt wait to continue improving the experience for people creating and using GitHub‚Äôs content. Keep an eye out for more behind-the-scenes posts about docs.github.com!</p>","descriptionType":"html","publishedDate":"Thu, 02 Jul 2020 19:00:54 +0000","feedId":10014,"bgimg":"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-7.48.43-PM.png","linkMd5":"1e30dff8bcd58e8b68043a491f96592e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn5@2020_2/2020/08/25/00-41-38-472_c0f3752b662e5e19.webp","destWidth":221,"destHeight":141,"sourceBytes":26701,"destBytes":7154,"author":"Sarah Schneider","articleImgCdnMap":{"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-7.48.43-PM.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn5@2020_2/2020/08/25/00-41-38-472_c0f3752b662e5e19.webp","https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-8.19.12-PM.png?resize=689%2C294":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn33@2020_5/2020/08/25/00-41-40-236_80bc40489c42395a.webp"},"publishedOrCreatedDate":1598316098351},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"Why Write ADRs","link":"https://github.blog/?p=53941","description":"<p>Architecture decision records, also known as ADRs, are a great way to document how and why a decision was reached within a codebase. We&#8217;ve started to adopt them within the mobile team here at GitHub, documenting decisions that affect the iOS codebase and Android codebase, as well as decisions that affect both mobile clients.</p>\n<p>ADRs are not the most common within open source codebases, but have gained more popularity <a href=\"https://www.thoughtworks.com/radar/techniques/lightweight-architecture-decision-records\" rel=\"nofollow\">since ~2017 within long-lived, &#8220;evolutionary&#8221; codebases</a>¬†like those in more enterprise-y settings.</p>\n<p>So why write an ADR? Why spend time documenting something when a decision has already been made?</p>\n<h2><a id=\"user-content-theyre-not-for-you-theyre-for-future-you\" class=\"anchor\" href=\"https://github.com/github/blog/blob/32a963b224bada16b77eac114f531aa0069ecc00/_posts/2020/2020-08-04-why-write-adrs.md#theyre-not-for-you-theyre-for-future-you\" aria-hidden=\"true\"></a>They&#8217;re not for you, they&#8217;re for future you</h2>\n<p>ADRs are not meant to be a self-discovery, reflection process on what you decided. ADRs will help you 6-12 months from now recall what your mindset was when you decided upon that architecture.</p>\n<p><strong>ADRs capture the decision at the time it&#8217;s being made.</strong>¬†ADRs are the culmination of all those minutes and hours you spent in meetings, on Zoom, in Slack, or jamming through various proof-of-concepts in Xcode. All of the context that&#8217;s in your head has a chance to get out into words so that when you&#8217;re revisiting the architecture down the road, you can put that context right back into your head.</p>\n<p>The real bonus comes when someone¬†<code>git blame</code>s you some months from now and asks you how the¬†<code>GitHubAPIClient</code>¬†module works. Better than setting up a 30-minute pairing call to walk them through the code, you can now link to the ADR you wrote to explain some more about the decisions made while building the¬†<code>GitHubAPIClient</code>¬†module.</p>\n<h2><a id=\"user-content-theyre-not-for-you-theyre-for-your-peers\" class=\"anchor\" href=\"https://github.com/github/blog/blob/32a963b224bada16b77eac114f531aa0069ecc00/_posts/2020/2020-08-04-why-write-adrs.md#theyre-not-for-you-theyre-for-your-peers\" aria-hidden=\"true\"></a>They&#8217;re not for you, they&#8217;re for your peers</h2>\n<p>ADRs force you to write more than a one-liner &#8220;this ships the feature for #3128&#8221;. They are¬†a longer form of prose to help your teammates understand why the feature is built the way it is, and not built some other way (see: &#8220;Alternatives Considered&#8221; and &#8220;Pros/Cons&#8221; within ADRs themselves).</p>\n<p>Something that may be simple for you might be complicated for your teammates. Taking the time to write down what your thought process was as you made decisions gives your teammates the chance to get inside your head. Writing ADRs allows for <a href=\"https://www.ozimmer.ch/practices/2020/05/22/ADDefinitionOfDone.html\" rel=\"nofollow\">‚Äúdecision socialization‚Äù</a>, where your¬†<em>team</em>¬†comes to a decision that the¬†<em>team</em>¬†is responsible for maintaining, rather than decisions made in isolation.</p>\n<p>By¬†<em>expanding</em> upon what you write in your pull request titles and descriptions (and you are still writing quality pull request descriptions, right?), <strong>you give your teammates more information about how a patch or diff works in a larger system.</strong></p>\n<p>Better yet, by writing an ADR¬†<em>ahead</em> of putting your pull requests up, you&#8217;ll get better pull request reviews from the team reviewing it. No longer do you need to explain how line 387 in <code>APIClient+Caching.swift</code>¬†will affect data fetching and caching architecture, because your teammates already understand how you&#8217;re changing the system from the ADR you wrote about &#8220;Adding cache support to¬†<code>E-Tag</code>¬†entities&#8221;.</p>\n<h2><a id=\"user-content-theyre-not-for-you-theyre-for-your-future-peers\" class=\"anchor\" href=\"https://github.com/github/blog/blob/32a963b224bada16b77eac114f531aa0069ecc00/_posts/2020/2020-08-04-why-write-adrs.md#theyre-not-for-you-theyre-for-your-future-peers\" aria-hidden=\"true\"></a>They&#8217;re not for you, they&#8217;re for your future peers</h2>\n<p>ADRs are not meant you to show off how smart you are or for folks to fawn over the architecture you built. ADRs <em>are</em>¬†for helping onboard new teammates as they work to understand the codebase and how it has evolved over time.</p>\n<p>As teams scale and grow, the number of lines of communication between teammates increases. A team of three individuals only has three lines of communication (<code>A &#60;&#62; B</code>,¬†<code>A &#60;&#62; C</code>,¬†<code>B &#60;&#62; C</code>). A team of four has six (<code>A &#60;&#62; B</code>,¬†<code>A &#60;&#62; C</code>,¬†<code>A &#60;&#62; D</code>,¬†<code>B &#60;&#62; C</code>,¬†<code>B &#60;&#62; D</code>,¬†<code>C &#60;&#62; D</code>). Wanna do the math for a team of five or six? How about 14 engineers, two designers, two PMs, and three EMs?</p>\n<p>Writing down decisions made helps communicate to your current teammates, but also those who join your team as it scales and grows. <strong>By informing your team how and why a decision was made in an asynchronous fashion, you no longer need to &#8220;hop on a Zoom call&#8221; to onboard each new teammate on a per-architectural-decision basis.</strong></p>\n<p>In the best-case scenario, you&#8217;ll have your teammates writing new ADRs for¬†<em>you</em>,¬†<a href=\"https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/#revisit-a-past-decision--a-new-adr\" rel=\"nofollow\">superseding the decisions</a> past you made, so that you can learn from your teammates in the future.</p>\n<h2><a id=\"user-content-write-more-adrs\" class=\"anchor\" href=\"https://github.com/github/blog/blob/32a963b224bada16b77eac114f531aa0069ecc00/_posts/2020/2020-08-04-why-write-adrs.md#write-more-adrs\" aria-hidden=\"true\"></a>Write more ADRs</h2>\n<p>I hope this has convinced you to document the decisions that you&#8217;re making as we build software that (hopefully) millions of folks use! As our team grows larger and our codebases grow more entangled and intertwined, architecture decision records are a great way to help future us, our current teammates, and future teammates.</p>\n<p><img src=\"https://s.w.org/images/core/emoji/13.0.0/72x72/1f44b.png\" alt=\"üëã\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />¬†Thanks for coming to my TED talk.</p>\n<h2><a id=\"user-content-more-info\" class=\"anchor\" href=\"https://github.com/github/blog/blob/32a963b224bada16b77eac114f531aa0069ecc00/_posts/2020/2020-08-04-why-write-adrs.md#more-info\" aria-hidden=\"true\"></a>More Info</h2>\n<ul>\n<li><a href=\"https://github.com/joelparkerhenderson/architecture_decision_record\"><code>joelparkerhenderson/architecture_decision_record</code>: ADR examples and documentation</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=rwfXkSjFhzc\" rel=\"nofollow\">David Ayers from #LeadDevNewYork: &#8220;Communicating and documenting architectural decisions&#8221;</a></li>\n<li><a href=\"https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs/\" rel=\"nofollow\">Understand Legacy Code: &#8220;Earn future maintainers esteem by writing simple ADRs&#8221;</a></li>\n<li><a href=\"https://personal.utdallas.edu/~chung/SA/zz-Impreso-architecture_decisions-tyree-05.pdf\" rel=\"nofollow\">IEEE Software: &#8220;Architecture Decisions: Demystifying Architecture&#8221;</a></li>\n</ul>\n","descriptionType":"html","publishedDate":"Thu, 13 Aug 2020 17:00:30 +0000","feedId":10014,"bgimg":"https://s.w.org/images/core/emoji/13.0.0/72x72/1f44b.png","linkMd5":"203c3ee9a4f9f4bc7b42a707c65d3090","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn10@2020_3/2020/08/25/00-41-38-486_9c8dd5939bfa653e.webp","destWidth":72,"destHeight":72,"sourceBytes":1285,"destBytes":2206,"author":"Eli Perkins","articleImgCdnMap":{"https://s.w.org/images/core/emoji/13.0.0/72x72/1f44b.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn10@2020_3/2020/08/25/00-41-38-486_9c8dd5939bfa653e.webp"},"publishedOrCreatedDate":1598316098350},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"GitHub Availability Report: July 2020","link":"https://github.blog/?p=53820","description":"<p><span style=\"font-weight: 400;\">In July we experienced one specific incident resulting in a degraded state of availability for GitHub.com. We‚Äôd like to share our learnings from this incident with the community in the spirit of being transparent about our service disruptions, and helping other services improve their own operations.</span></p> \n<h3 id=\"july-13-0818-utc-lasting-for-four-hours-25-minutes\"><span style=\"font-weight: 400;\">July 13 08:18 UTC (lasting for four hours, 25 minutes)</span><a href=\"https://github.blog/?p=53820#july-13-0818-utc-lasting-for-four-hours-25-minutes\" class=\"heading-link\" aria-label=\"<span style=&quot;font-weight: 400;&quot;>July 13 08:18 UTC (lasting for four hours, 25 minutes)</span>\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p><span style=\"font-weight: 400;\">The incident started when our production Kubernetes Pods started getting marked as unavailable. This cascaded through our clusters resulting in a reduction in capacity, which ultimately brought down our services. Investigation into the Pods revealed that a single container within the Pod was exceeding its defined memory limits and being terminated. Even though that container is not required for production traffic to be processed, the nature of Kubernetes requires that all containers be healthy for a Pod to be marked as available.</span></p> \n<p><span style=\"font-weight: 400;\">Normally when a Pod runs into this failure mode, the cluster will recover within a minute or so. In this case, the container in the Pod was configured with an ImagePullPolicy of Always, which instructed Kubernetes to fetch a new container image every time. However, due to a routine DNS maintenance operation that had been completed earlier, our clusters were unable to successfully reach our registry resulting in Pods failing to start. This issue impact was increased when a redeploy was triggered in an attempt to mitigate</span><span style=\"font-weight: 400;\">,</span><span style=\"font-weight: 400;\"> and we saw the failure start to propagate across our production clusters. It wasn‚Äôt until we restarted the process with the cached DNS records that we were able to successfully fetch container images, redeploy, and recover our services.&nbsp;</span></p> \n<p><span style=\"font-weight: 400;\">Moving forward, we‚Äôve identified a number of areas to address this quarter:&nbsp;</span></p> \n<ul> \n <li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Enhancing monitoring ensuring Pod restarts would not fail again based on this same pattern</span></li> \n <li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Minimizing our dependency on the image registry</span></li> \n <li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Expanding validation during DNS changes</span></li> \n <li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Reevaluating all the existing Kubernetes deployment policies</span></li> \n</ul> \n<p><span style=\"font-weight: 400;\">In parallel, we have an ongoing workstream to improve our approach to progressive deployments that will provide the ability to carefully evaluate the impact of deployments in a more incremental fashion. This is part of a broader engineering initiative focused on reliability that we will have more details on in the coming months.</span></p> \n<h3 id=\"in-summary\"><span style=\"font-weight: 400;\">In summary</span><a href=\"https://github.blog/?p=53820#in-summary\" class=\"heading-link\" aria-label=\"<span style=&quot;font-weight: 400;&quot;>In summary</span>\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p><span style=\"font-weight: 400;\">We place great importance in the reliability of our service along with the trust that our users place in us every day. We look forward to continuing to share more details of our journey and hope you can learn from our experiences along the way.&nbsp;</span></p>","descriptionType":"html","publishedDate":"Wed, 05 Aug 2020 16:00:54 +0000","feedId":10014,"bgimg":"","linkMd5":"ab7ca7a424cb1a446b52abd0d334f5ab","bgimgJsdelivr":"","metaImg":"","author":"Keith Ballinger","publishedOrCreatedDate":1598316098350},{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","title":"CodeGen: Semantic&#8217;s improved language support system","link":"https://github.blog/?p=53743","description":"<p>The Semantic Code team shipped a massive improvement to the language support system that powers&nbsp;<a href=\"https://help.github.com/en/github/managing-files-in-a-repository/navigating-code-on-github\">code navigation</a>. Code navigation features only scratch the surface of possibilities that start to open up when we combine&nbsp;<a href=\"https://github.com/github/semantic\">Semantic</a>‚Äòs program analysis potential with GitHub‚Äôs scale.</p> \n<p>GitHub is home to over 50 million developers worldwide. Our team‚Äôs mission is to analyze code on our platform and surface insights that empower users to feel more productive. Ideally, this analysis should work for all users, regardless of which programming language they use. Until now, however, we‚Äôve only been able to support a handful of languages due to the high cost of adding and maintaining them. Our new language support system, <a href=\"https://github.com/github/semantic/blob/df03b302fa7e2408e93ba923a54a9f05a8520d52/docs/codegen.md\">CodeGen</a>, cuts that cost dramatically by automating a significant portion of the pipeline, in addition to making it more resilient. The result is that it is now easier than ever to add new programming languages that get parsed by our library.</p> \n<h2 id=\"language-support-is-mission-critical\">Language Support is mission-critical<a href=\"https://github.blog/?p=53743#language-support-is-mission-critical\" class=\"heading-link\" aria-label=\"Language Support is mission-critical\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Before Semantic can compute diffs or perform abstract interpretation, we need to transform code into a representation that is amenable to our analysis goals. For this reason, a significant portion of our pipeline deals with processing source code from files on GitHub.com into an appropriate representation‚Äîwe call this our ‚Äúlanguage support system‚Äù.</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53760\" src=\"https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576\" alt=\"Diagram showing semantic architecture\" width=\"1024\" height=\"576\" srcset=\"https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=3840 3840w, https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=300 300w, https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=768 768w, https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=1024 1024w, https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=1536 1536w, https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=2048 2048w, https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576?w=3000 3000w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></p> \n<h2 id=\"adding-languages-has-been-difficult\">Adding languages has been difficult<a href=\"https://github.blog/?p=53743#adding-languages-has-been-difficult\" class=\"heading-link\" aria-label=\"Adding languages has been difficult\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h2> \n<p>Zooming into the part of Semantic that achieves language support, we see that it involved several development phases, including two parsing steps that required writing and maintaining two separate grammars per language.</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53761\" src=\"https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576\" alt=\"Diagram showing language support pipeline\" width=\"1024\" height=\"576\" srcset=\"https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=3840 3840w, https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=300 300w, https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=768 768w, https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=1024 1024w, https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=1536 1536w, https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=2048 2048w, https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576?w=3000 3000w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></p> \n<p>Reading the diagram from left to right, our historic language support pipeline:</p> \n<ol> \n <li><strong>Parsed source code into ASTs.</strong>&nbsp;A grammar is hand-written for a given language using&nbsp;<a href=\"https://github.com/tree-sitter/tree-sitter\"><code>tree-sitter</code></a>, an incremental GLR parsing library for programming tools.</li> \n <li><strong>Read&nbsp;<code>tree-sitter</code>&nbsp;ASTs into&nbsp;<code>Semantic</code>.</strong>&nbsp;Connecting Semantic to&nbsp;<code>tree-sitter</code>‚Äòs C library requires providing an interface to the C source. We achieve this through our&nbsp;<a href=\"https://github.com/tree-sitter/haskell-tree-sitter\"><code>haskell-tree-sitter</code></a>&nbsp;library, which has Haskell bindings to&nbsp;<code>tree-sitter</code>.</li> \n <li><strong>Parsed ASTs into a generalized representation of syntax.</strong>&nbsp;For these ASTs to be consumable by our Haskell project, we had to translate the&nbsp;<code>tree-sitter</code>&nbsp;parse trees into an appropriate representation. This required: \n  <ul> \n   <li><strong><em>√Ä la carte syntax types:</em>&nbsp;generalization across programming languages</strong>&nbsp;Many constructs, such as&nbsp;<code>If</code>&nbsp;statements, occur in several languages. Instead of having different representations of&nbsp;<code>If</code>&nbsp;statements for each language, could we reduce duplication by creating a generalized representation of syntax that could be shared across languages, such as a datatype modeling the semantics of conditional logic? This was the reasoning behind creating our hand-written, generalized √† la carte syntaxes based on Wouter Swierstra‚Äôs&nbsp;<a href=\"http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf\" rel=\"nofollow\">Data types √† la carte</a>&nbsp;approach, allowing us to represent those shared semantics across languages. For example,&nbsp;<a href=\"https://github.com/github/semantic/blob/621696f5bc523a651f1cf9fc2ac58c557ea02d07/src/Data/Syntax/Expression.hs\">this file</a>&nbsp;captures a subset of √† la carte datatypes that model expression syntaxes across languages.</li> \n   <li><strong><em>Assignment:</em>&nbsp;turning&nbsp;<code>tree-sitter</code>‚Äòs representation into a Haskell datatype representation</strong>&nbsp;We had to translate&nbsp;<code>tree-sitter</code>&nbsp;AST nodes to be represented by the new generalized √† la carte syntax. To do this, a second grammar was written in Haskell to&nbsp;<em>assign</em>&nbsp;the nodes of the&nbsp;<code>tree-sitter</code>&nbsp;ASTs onto a generalized representation of syntax modeled by the √† la carte datatypes. As an example,&nbsp;<a href=\"https://github.com/github/semantic/blob/621696f5bc523a651f1cf9fc2ac58c557ea02d07/src/Language/Ruby/Assignment.hs\">here</a>&nbsp;is the&nbsp;<code>Assignment</code>&nbsp;grammar written for Ruby.</li> \n  </ul> </li> \n <li><strong>Performed Evaluation.</strong>&nbsp;Next, we captured what it meant to interpret the syntax datatypes. To do so, we wrote a polymorphic type class called&nbsp;<code>Evaluatable</code>, which defined the necessary interface for a term to be evaluated.&nbsp;<code>Evaluatable</code>&nbsp;instances were added for each of the √† la carte syntaxes.</li> \n <li><strong>Handled Effects.</strong>&nbsp;In addition to evaluation semantics, describing the control flow of a given program also necessitates modeling effects. This helps ensure we can represent things like the file system, state, non-determinism, and other effectful computations.</li> \n <li><strong>Validated via tests.</strong>&nbsp;Tests for diffing, tagging, graphing, and evaluating source code written in that language were added along the process.</li> \n</ol> \n<h3 id=\"challenges-posed-by-the-system\">Challenges posed by the system<a href=\"https://github.blog/?p=53743#challenges-posed-by-the-system\" class=\"heading-link\" aria-label=\"Challenges posed by the system\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>The process described had several obstacles. Not only was it very technically involved, but it had additional limitations.</p> \n<ol> \n <li><strong>The system was brittle.</strong>&nbsp;Each language‚Äôs&nbsp;<code>Assignment</code>&nbsp;code was tightly coupled to the language‚Äôs&nbsp;<code>tree-sitter</code>&nbsp;grammar. This meant it could break at runtime if we changed the structure of the grammar, without any compile-time error. To prevent such errors required tracking ongoing changes in tree-sitter, which was also tedious, manual, and error-prone. Each time a grammar changed, assignment changes had to be made to accommodate new tree-structures, such as nodes that changed names or shifted positions. Because improvements to the underlying grammars required changes to&nbsp;<code>Assignment</code>‚Äîwhich were costly in terms of time and risky in terms of the possibility of introducing bugs‚Äî, our system had inadvertently become incentivized&nbsp;<em>against</em>&nbsp;iterative improvement.</li> \n <li><strong>There were no named child nodes.</strong>&nbsp;<code>tree-sitter</code>‚Äòs syntax nodes didn‚Äôt provide us with named child nodes. Instead, child nodes were structured as ordered-lists, without any name indicating the role of each child. This didn‚Äôt match Semantic‚Äôs internal representation of syntax nodes, where each type of node has a specific set of named children. This meant more&nbsp;<code>Assignment</code>&nbsp;work was necessary to compensate for the discrepancy. One concern, for example, was about how we represented comments, which could be any arbitrary node attached to any part of the AST. But if we had named child nodes, this would allow us to associate comments relative to their parent nodes (like if a comment appeared in an&nbsp;<code>if</code>&nbsp;statement, it could be the first child for that&nbsp;<code>if</code>&nbsp;statement node). This would also apply to any other nodes that could appear anywhere within the tree, such as Ruby heredocs.</li> \n <li style=\"padding-bottom: 0px;\"><strong>Evaluation and √† la carte sum types were sub-optimal.</strong>&nbsp;Taking a step back to examine language support also gave us an opportunity to rethink our √† la carte datatypes and the evaluation machinery. √Ä la carte syntax types were motivated by a desire to better share tooling in evaluating common fragments of languages. However, the introduction of these syntax types (and the design of the&nbsp;<code>Evaluatable</code>&nbsp;typeclass) did not make our analysis sensitive to minor linguistic differences, or even to relate different pieces of syntax together. We could overcome this by adding language-specific syntax datatypes to be used with&nbsp;<code>Assignment</code>, along with their accompanying&nbsp;<code>Evaluatable</code>&nbsp;instances‚Äîbut this would defeat the purpose of a generalized representation. This is because √† la carte syntax was essentially untyped; it enforced only a minimal structure on the tree. As a result, any given subterm could be any element of the syntax, and not some limited subset. This meant that many&nbsp;<code>Evaluatable</code>&nbsp;instances had to deal with error conditions that in practice can‚Äôt occur. To make this idea more concrete, consider examples showcasing a before and after syntax type transformation: <pre><code>-- former system: √† la carte syntax\n\ndata Op a = Op { ann :: a, left :: Expression a, right :: Expression a }</code></pre> <pre><code>-- new system: auto-generated, precisely typed syntax\n\ndata Op a = Op { ann :: a, left :: Err (Expression a), right :: Err (Expression a) }</code></pre> <p>The shape of a syntax type in our √† la carte paradigm has polymorphic children, compared with the monomorphic children of our new ‚Äúprecisely-typed‚Äù syntax, which offers better guarantees of what we could expect.</p></li> \n <li style=\"padding-top: 0px;\"><strong>Infeasible time and effort was required.</strong>&nbsp;A two-step parsing process required writing two separate language-specific grammars by hand. This was time-consuming, engineering-intensive, error-prone, and tedious. The&nbsp;<code>Assignment</code>&nbsp;grammar used parser combinators in Haskell mirroring the tree-sitter grammar specification, which felt like a lot of duplicated work. For a long time, this work‚Äôs mechanical nature begged the question of whether we could automate parts of it. While we‚Äôve open-sourced Semantic, leveraging community support for adding languages has been difficult because, until recently, it was backed by such a grueling process.</li> \n</ol> \n<h3 id=\"designing-a-new-system\">Designing a new system<a href=\"https://github.blog/?p=53743#designing-a-new-system\" class=\"heading-link\" aria-label=\"Designing a new system\" data-anchorjs-icon=\"#\" style=\"padding-left: 0.375em;\"></a></h3> \n<p>To address challenges, we introduced a few changes:</p> \n<ol> \n <li><strong>Add named child nodes.</strong>&nbsp;To address the issue of not having named child nodes, we modified the&nbsp;<code>tree-sitter</code>&nbsp;library by adding a new function called&nbsp;<code>field</code>&nbsp;to the grammar API and resultantly updating every language grammar. When parsing, you can retrieve a nodes‚Äô children based on their field name. Here is an example of what a Python&nbsp;<code>if_statement</code>&nbsp;looks like in the old and new&nbsp;<code>tree-sitter</code>&nbsp;grammar APIs:<br /> <img loading=\"lazy\" class=\"aligncenter size-full wp-image-53762\" src=\"https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165\" alt=\"Screenshot of a diff highlighting an example of what a Python if_statement looks like in the old and new tree-sitter grammar APIs\" width=\"1024\" height=\"165\" srcset=\"https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165?w=1576 1576w, https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165?w=300 300w, https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165?w=768 768w, https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165?w=1024 1024w, https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165?w=1536 1536w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></li> \n <li><strong>Generate a Node Interface File.</strong>&nbsp;Once a grammar has this way of associating child references, the parser generation code also produces a&nbsp;<code>node-types.json</code>&nbsp;file that indicates what kinds of children references you can expect for each node type. This JSON file provides static information about nodes‚Äô fields based on the grammar. Using this JSON, applications like ours can use meta-programming to generate specific datatypes for each kind of node.&nbsp;<a href=\"https://github.com/tree-sitter/tree-sitter-python/blob/2a7098abcfa2197854589f8e2dd557a15658b138/src/node-types.json#L1365-L1403\">Here</a>&nbsp;is an example of the JSON generated from the grammar definition of an&nbsp;<code>if</code>&nbsp;statement. This file provided a schema for a language‚Äôs ASTs and introduced additional improvements, such as the way we specify highlighting.</li> \n <li><strong>Auto-generate language-specific syntax datatypes.</strong>&nbsp;Using the structure provided by the&nbsp;<code>node-types.json</code>&nbsp;file, we can auto-generate syntax types instead of writing them by hand. First, we deserialize the JSON file to capture the structure we want to represent in the desired shape of datatypes. Specifically, we have four distinct shapes that the nodes in our node-types JSON file take on:&nbsp;<em>sums</em>,&nbsp;<em>products</em>,&nbsp;<em>named leaves</em>, and&nbsp;<em>anonymous leaves</em>. We then use Template Haskell to generate syntax datatypes for each of the language constructs represented by the Node Interface File. This means that our hand-written √† la carte syntax types get replaced with auto-generated language-specific types, saving all of the developer time historically spent writing them.&nbsp;<a href=\"https://gist.github.com/aymannadeem/26669d51cc322a4ada90a289ba6b05a9\">Here</a>&nbsp;is an example of an auto-generated datatype representing a Python&nbsp;<code>if</code>&nbsp;statement derived from the JSON snippet provided above, which is structurally a product type.</li> \n <li><strong>Build ASTs generically.</strong>&nbsp;Once we have an exhaustive set of language-specific datatypes, we need to have a mechanism that can map appropriate auto-generated datatypes onto the ASTs representing the source code being parsed. Historically, this was accomplished by manually writing an&nbsp;<code>Assignment</code>&nbsp;grammar. To obviate the need for a second grammar, we have created an API that uses Haskell‚Äôs&nbsp;<a href=\"http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Generics.html\" rel=\"nofollow\">generic metaprogramming framework</a>&nbsp;to unmarshal tree-sitter‚Äôs parse trees automatically. We iterate over&nbsp;<code>tree-sitter</code>‚Äòs parse trees using its&nbsp;<a href=\"https://github.com/tree-sitter/tree-sitter/blob/bfeec63d60e64ecbf8d6cbb324fd3f3d32ee2295/lib/src/tree_cursor.c\">tree cursor API</a>&nbsp;and produce Haskell ASTs, where each node is represented by a Template Haskell generated datatype described by the previous step. This allows us to parse a particular set of nodes according to their structure, and return an AST with meta-data (such as range and span information).&nbsp;<a href=\"https://gist.github.com/aymannadeem/6002edb23e25cc55811150d756a8a93f\">Here is an example</a>&nbsp;of the AST generated if the Python source code is simply&nbsp;<code>1</code><br /> <img loading=\"lazy\" class=\"aligncenter size-full wp-image-53763\" src=\"https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576\" alt=\"Screenshot of CodeGen language support pipeline\" width=\"1024\" height=\"576\" srcset=\"https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=3840 3840w, https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=300 300w, https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=768 768w, https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=1024 1024w, https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=1536 1536w, https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=2048 2048w, https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576?w=3000 3000w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></li> \n</ol> \n<p>The final result is a set of language-specific, strongly-typed, TH-generated datatypes represented as the sum of syntax possible at a given point in the grammar. Strongly-typed trees give us the ability to indicate only the subset of the syntax that can occur at a given position. For example, a function‚Äôs name would be strongly typed as an&nbsp;<code>identifier</code>; a&nbsp;<code>switch</code>&nbsp;statement would contain&nbsp;<code>case</code>&nbsp;statements; and so on. This provides better guarantees about where syntax can occur, and strong compile-time guarantees about both correctness and completeness.</p> \n<p>The new system bypasses a significant part of the engineering effort historically required; it cuts code from our pipeline in addition to addressing the technical limitations described above. The diagram below provides a visual ‚Äúdiff‚Äù of the old and new systems.</p> \n<p><img loading=\"lazy\" class=\"aligncenter size-full wp-image-53764\" src=\"https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576\" alt=\"Diagram showing language support pipeline\" width=\"1024\" height=\"576\" srcset=\"https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=3840 3840w, https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=300 300w, https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=768 768w, https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=1024 1024w, https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=1536 1536w, https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=2048 2048w, https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576?w=3000 3000w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" data-recalc-dims=\"1\" /></p> \n<p>A big testament to our approach‚Äôs success was that we were able to&nbsp;<a href=\"https://github.com/github/semantic/pull/577\">remove our √† la carte syntaxes</a>&nbsp;completely. In addition, we were also able to ship two new languages,&nbsp;<a href=\"https://github.com/github/semantic/pull/323\">Java</a>&nbsp;and&nbsp;<a href=\"https://github.com/github/semantic/pull/507\">CodeQL</a>, using precise ASTs generated by the new system.</p> \n<h3><a id=\"user-content-contributions-welcome\" class=\"anchor\" href=\"https://github.com/github/blog/blob/dfc2f5eae907d3ad4b5afd688de0e93311638d07/_posts/2020-07-23-New-Semantic-Language-Support.md#contributions-welcome\" aria-hidden=\"true\"></a>Contributions welcome!</h3> \n<p>To learn more about how you can help, check out our documentation&nbsp;<a href=\"https://github.com/github/semantic/blob/df03b302fa7e2408e93ba923a54a9f05a8520d52/docs/codegen.md\">here</a>.</p>","descriptionType":"html","publishedDate":"Tue, 04 Aug 2020 14:45:41 +0000","feedId":10014,"bgimg":"https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576","linkMd5":"208070edfd99425e57c442df9c3e3f73","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn26@2020_4/2020/08/25/00-41-38-490_cea2327281858c22.webp","destWidth":1024,"destHeight":576,"sourceBytes":46712,"destBytes":46712,"author":"Ayman Nadeem","articleImgCdnMap":{"https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn26@2020_4/2020/08/25/00-41-38-490_cea2327281858c22.webp","https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn35@2020_6/2020/08/25/00-41-40-218_069f8d48d77baad6.webp","https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn42@2020_3/2020/08/25/00-41-40-088_9d7681c17a2a003b.webp","https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn62@2020_6/2020/08/25/00-41-40-109_2fa9b013e46bfe41.webp","https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn53@2020_5/2020/08/25/00-41-40-087_c0c553f1e0009898.webp"},"publishedOrCreatedDate":1598316098350}],"record":{"createdTime":"2020-08-25 08:41:38","updatedTime":"2020-08-25 08:41:38","feedId":10014,"fetchDate":"Tue, 25 Aug 2020 00:41:38 +0000","fetchMs":160,"handleMs":861,"totalMs":29678,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"54.209.71.245","hostName":"us-011.herokuapp.com","requestId":"71613376350a4974a23205546cbdd375_10014","contentType":"application/atom+xml; charset=UTF-8","totalBytes":2067666,"bgimgsTotal":7,"bgimgsGithubTotal":7,"articlesImgsTotal":15,"articlesImgsGithubTotal":15,"successGithubMap":{"myreaderx25":1,"myreaderx8":1,"myreaderx14":1,"myreaderx7":1,"myreaderx32":1,"myreaderx21":1,"myreaderx4":1,"myreaderx11":1,"myreaderx23":1,"myreaderx2":1,"myreaderx1":1,"myreaderx24":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx29":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:35:34","updatedTime":"2020-08-25 04:35:34","id":10014,"name":"Engineering ‚Äì The GitHub Blog","url":"https://githubengineering.com/atom.xml","subscriber":null,"website":null,"icon":"https://github.blog/wp-content/uploads/2019/01/cropped-github-favicon-512.png?fit=32%2C32","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn57@2020_1/2020/08/25/00-41-37-554_4666d2dbb4c1a638.webp","description":"Updates, ideas, and inspiration from GitHub to help developers build and design software.","weekly":null,"link":"https://github.blog"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":301354,"tmpBodyImgCdnBytes":1766312,"tmpBgImgCdnBytes":0,"extra4":{"start":1598316097325,"total":0,"statList":[{"spend":165,"msg":"Ëé∑ÂèñxmlÂÜÖÂÆπ"},{"spend":861,"msg":"Ëß£ÈáäÊñáÁ´†"},{"spend":0,"msg":"‰∏ä‰º†Â∞ÅÈù¢ÂõæÂà∞cdn"},{"spend":0,"msg":"‰øÆÊ≠£Â∞ÅÈù¢Âõæ‰∏ä‰º†Â§±Ë¥•ÈáçÊñ∞‰∏ä‰º†"},{"spend":27050,"msg":"Ê≠£ÊñáÈìæÊé•‰∏ä‰º†Âà∞cdn"}]},"extra5":15,"extra6":15,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{"https://github.blog/?p=53324_#what-was-broken":"https://github.blog/?p=53324#what-was-broken","https://github.blog/?p=53324_#scripting-the-content-migration":"https://github.blog/?p=53324#scripting-the-content-migration","https://github.blog/?p=53324_#rest-docs-via-openapi":"https://github.blog/?p=53324#rest-docs-via-openapi","https://github.blog/?p=53590_#whats-new-in-2-28":"https://github.blog/?p=53590#whats-new-in-2-28","https://github.blog/?p=53324_#only-fixing-what-was-broken":"https://github.blog/?p=53324#only-fixing-what-was-broken","https://github.blog/?p=53324_#graphql-docs":"https://github.blog/?p=53324#graphql-docs","https://github.blog/?p=53324_#whats-next":"https://github.blog/?p=53324#whats-next","https://github.blog/?p=53375_#what-can-you-expect":"https://github.blog/?p=53375#what-can-you-expect","https://github.blog/?p=53375_#in-summary":"https://github.blog/?p=53375#in-summary","https://github.blog/?p=53820_#in-summary":"https://github.blog/?p=53820#in-summary","https://github.blog/?p=53124_#how-can-i-contribute":"https://github.blog/?p=53124#how-can-i-contribute","https://github.blog/?p=53743_#adding-languages-has-been-difficult":"https://github.blog/?p=53743#adding-languages-has-been-difficult","https://github.blog/?p=53743_#designing-a-new-system":"https://github.blog/?p=53743#designing-a-new-system","https://github.blog/?p=53375_#may-22-1641-utc-lasting-for-five-hours-and-nine-minutes":"https://github.blog/?p=53375#may-22-1641-utc-lasting-for-five-hours-and-nine-minutes","https://github.blog/?p=53097_#related-materials":"https://github.blog/?p=53097#related-materials","https://github.blog/?p=53324_#supporting-multiple-products":"https://github.blog/?p=53324#supporting-multiple-products","https://github.blog/?p=53620_#openapi":"https://github.blog/?p=53620#openapi","https://github.blog/?p=53375_#june-19-0852-utc-lasting-for-51-minutes":"https://github.blog/?p=53375#june-19-0852-utc-lasting-for-51-minutes","https://github.blog/?p=53620_#try-it-out":"https://github.blog/?p=53620#try-it-out","https://github.blog/?p=53375_#what-is-the-availability-report":"https://github.blog/?p=53375#what-is-the-availability-report","https://github.blog/?p=53124_#default-rules":"https://github.blog/?p=53124#default-rules","https://github.blog/?p=53324_#redirecting-all-the-things":"https://github.blog/?p=53324#redirecting-all-the-things","https://github.blog/?p=53124_#join-in-the-fun":"https://github.blog/?p=53124#join-in-the-fun","https://github.blog/?p=53124_#what-is-it":"https://github.blog/?p=53124#what-is-it","https://github.blog/?p=53124_#how-it-works":"https://github.blog/?p=53124#how-it-works","https://github.blog/?p=53620_#how-can-i-contribute":"https://github.blog/?p=53620#how-can-i-contribute","https://github.blog/?p=53590_#footnote-1":"https://github.blog/?p=53590#footnote-1","https://github.blog/?p=53375_#may-5-0045-utc-lasting-for-two-hours-and-24-minutes":"https://github.blog/?p=53375#may-5-0045-utc-lasting-for-two-hours-and-24-minutes","https://github.blog/?p=53375_#june-29-1203-utc-lasting-for-two-hours-and-29-minutes":"https://github.blog/?p=53375#june-29-1203-utc-lasting-for-two-hours-and-29-minutes","https://github.blog/?p=53324_#internationalized-docs":"https://github.blog/?p=53324#internationalized-docs","https://github.blog/?p=53590_#introducing-init-defaultbranch":"https://github.blog/?p=53590#introducing-init-defaultbranch","https://github.blog/?p=53590_#tidbits":"https://github.blog/?p=53590#tidbits","https://github.blog/?p=53743_#language-support-is-mission-critical":"https://github.blog/?p=53743#language-support-is-mission-critical","https://github.blog/?p=53375_#availability-report-for-may-and-june":"https://github.blog/?p=53375#availability-report-for-may-and-june","https://github.blog/?p=53097_#background":"https://github.blog/?p=53097#background","https://github.blog/?p=53324_#launching-a-dynamic-help-github-com":"https://github.blog/?p=53324#launching-a-dynamic-help-github-com","https://github.blog/?p=53097_#an-example-of-mlops-using-github-actions":"https://github.blog/?p=53097#an-example-of-mlops-using-github-actions","https://github.blog/?p=53620_#active-development":"https://github.blog/?p=53620#active-development","https://github.blog/?p=53590_#the-rest-of-the-iceberg":"https://github.blog/?p=53590#the-rest-of-the-iceberg","https://github.blog/?p=53375_#why-are-we-doing-this":"https://github.blog/?p=53375#why-are-we-doing-this","https://github.blog/?p=53097_#a-growing-ecosystem-of-mlops-data-science-actions":"https://github.blog/?p=53097#a-growing-ecosystem-of-mlops-data-science-actions","https://github.blog/?p=53820_#july-13-0818-utc-lasting-for-four-hours-25-minutes":"https://github.blog/?p=53820#july-13-0818-utc-lasting-for-four-hours-25-minutes","https://github.blog/?p=53743_#challenges-posed-by-the-system":"https://github.blog/?p=53743#challenges-posed-by-the-system","https://github.blog/?p=53590_#changed-path-bloom-filters":"https://github.blog/?p=53590#changed-path-bloom-filters","https://github.blog/?p=53324_#a-brief-history-of-the-docs-sites":"https://github.blog/?p=53324#a-brief-history-of-the-docs-sites","https://github.blog/?p=53324_#autogenerating-api-documentation":"https://github.blog/?p=53324#autogenerating-api-documentation"},"extra111_proxyServerAndStatMap":{"http://us-54.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe61.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-7.48.43-PM.png","sourceStatusCode":200,"destWidth":221,"destHeight":141,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn5@2020_2/2020/08/25/00-41-38-472_c0f3752b662e5e19.webp","sourceBytes":26701,"destBytes":7154,"targetWebpQuality":75,"feedId":10014,"totalSpendMs":822,"convertSpendMs":5,"createdTime":"2020-08-25 08:41:38","host":"us-020*","referer":"https://github.blog/?p=53324","linkMd5ListStr":"1e30dff8bcd58e8b68043a491f96592e,1e30dff8bcd58e8b68043a491f96592e","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"26.8%","sourceSize":"26.1 KB","destSize":"7 KB"},{"code":1,"isDone":false,"source":"https://s.w.org/images/core/emoji/13.0.0/72x72/1f44b.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn10@2020_3/2020/08/25/00-41-38-486_9c8dd5939bfa653e.webp","sourceBytes":1285,"destBytes":2206,"targetWebpQuality":75,"feedId":10014,"totalSpendMs":831,"convertSpendMs":4,"createdTime":"2020-08-25 08:41:38","host":"us-007*","referer":"https://github.blog/?p=53941","linkMd5ListStr":"203c3ee9a4f9f4bc7b42a707c65d3090,203c3ee9a4f9f4bc7b42a707c65d3090","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"171.7%","sourceSize":"1.3 KB","destSize":"2.2 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/1-semantic-architecture.png?resize=1024%2C576","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn26@2020_4/2020/08/25/00-41-38-490_cea2327281858c22.webp","sourceBytes":46712,"destBytes":46712,"feedId":10014,"totalSpendMs":983,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:38","host":"us-024*","referer":"https://github.blog/?p=53743","linkMd5ListStr":"208070edfd99425e57c442df9c3e3f73,208070edfd99425e57c442df9c3e3f73","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"45.6 KB","destSize":"45.6 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/06/super-linter-in-action-2.png?resize=1024%2C819","sourceStatusCode":200,"destWidth":1024,"destHeight":819,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn21@2020_5/2020/08/25/00-41-38-515_d7757c61ef55f70b.webp","sourceBytes":102122,"destBytes":102122,"feedId":10014,"totalSpendMs":1071,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:38","host":"us-036*","referer":"https://github.blog/?p=53124","linkMd5ListStr":"aca63aee24bc985d37005e319ace6d95,aca63aee24bc985d37005e319ace6d95","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"99.7 KB","destSize":"99.7 KB"},{"code":1,"isDone":false,"source":"https://s.w.org/images/core/emoji/13.0.0/72x72/1f389.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn14@2020_5/2020/08/25/00-41-38-588_41448c516a847944.webp","sourceBytes":1376,"destBytes":2694,"targetWebpQuality":75,"feedId":10014,"totalSpendMs":974,"convertSpendMs":4,"createdTime":"2020-08-25 08:41:38","host":"europe70*","referer":"https://github.blog/?p=53970","linkMd5ListStr":"422e103183b7eeff6dd32d4c3057c4fd,422e103183b7eeff6dd32d4c3057c4fd","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"195.8%","sourceSize":"1.3 KB","destSize":"2.6 KB"},{"code":1,"isDone":false,"source":"https://s.w.org/images/core/emoji/13.0.0/72x72/1f50e.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn18@2020_5/2020/08/25/00-41-38-596_7bb77e3d12a3390a.webp","sourceBytes":855,"destBytes":1204,"targetWebpQuality":75,"feedId":10014,"totalSpendMs":1060,"convertSpendMs":6,"createdTime":"2020-08-25 08:41:38","host":"europe-58*","referer":"https://github.blog/?p=53590","linkMd5ListStr":"b81328087625f26fe46688594131e001,b81328087625f26fe46688594131e001","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"140.8%","sourceSize":"855 B","destSize":"1.2 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/06/update-train-py.png?resize=1024%2C1125","sourceStatusCode":200,"destWidth":1024,"destHeight":1125,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_1/2020/08/25/00-41-38-844_c989bfd2b85bfc5c.webp","sourceBytes":139262,"destBytes":139262,"feedId":10014,"totalSpendMs":1517,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:38","host":"us-012*","referer":"https://github.blog/?p=53097","linkMd5ListStr":"bb439890785b1d30230e7b3dca98580f,bb439890785b1d30230e7b3dca98580f","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"136 KB","destSize":"136 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/3-new-system-diff.png?resize=1024%2C165","sourceStatusCode":200,"destWidth":1024,"destHeight":165,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn42@2020_3/2020/08/25/00-41-40-088_9d7681c17a2a003b.webp","sourceBytes":22430,"destBytes":22430,"feedId":10014,"totalSpendMs":879,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:40","host":"us-040*","referer":"https://github.blog/?p=53743","linkMd5ListStr":"208070edfd99425e57c442df9c3e3f73","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"21.9 KB","destSize":"21.9 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/5-new-language-support-pipeline.png?resize=1024%2C576","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn53@2020_5/2020/08/25/00-41-40-087_c0c553f1e0009898.webp","sourceBytes":54788,"destBytes":54788,"feedId":10014,"totalSpendMs":988,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:40","host":"us-003*","referer":"https://github.blog/?p=53743","linkMd5ListStr":"208070edfd99425e57c442df9c3e3f73","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"53.5 KB","destSize":"53.5 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/4-codegen-language-support-pipeline.png?resize=1024%2C576","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn62@2020_6/2020/08/25/00-41-40-109_2fa9b013e46bfe41.webp","sourceBytes":77368,"destBytes":77368,"feedId":10014,"totalSpendMs":1112,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:40","host":"us-54*","referer":"https://github.blog/?p=53743","linkMd5ListStr":"208070edfd99425e57c442df9c3e3f73","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"75.6 KB","destSize":"75.6 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-01-at-8.19.12-PM.png?resize=689%2C294","sourceStatusCode":200,"destWidth":689,"destHeight":294,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn33@2020_5/2020/08/25/00-41-40-236_80bc40489c42395a.webp","sourceBytes":22202,"destBytes":22202,"feedId":10014,"totalSpendMs":1195,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:40","host":"europe61*","referer":"https://github.blog/?p=53324","linkMd5ListStr":"1e30dff8bcd58e8b68043a491f96592e","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"21.7 KB","destSize":"21.7 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/2-language-support-pipeline.png?resize=1024%2C576","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn35@2020_6/2020/08/25/00-41-40-218_069f8d48d77baad6.webp","sourceBytes":49038,"destBytes":49038,"feedId":10014,"totalSpendMs":1393,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:40","host":"europe62*","referer":"https://github.blog/?p=53743","linkMd5ListStr":"208070edfd99425e57c442df9c3e3f73","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"47.9 KB","destSize":"47.9 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/06/binder-demo.png?resize=1024%2C676","sourceStatusCode":200,"destWidth":1024,"destHeight":676,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn57@2020_4/2020/08/25/00-41-40-213_cf3f14d4d372df9a.webp","sourceBytes":59054,"destBytes":59054,"feedId":10014,"totalSpendMs":1421,"convertSpendMs":0,"createdTime":"2020-08-25 08:41:40","host":"europe66*","referer":"https://github.blog/?p=53097","linkMd5ListStr":"bb439890785b1d30230e7b3dca98580f","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"100%","sourceSize":"57.7 KB","destSize":"57.7 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/demo2.gif?resize=1024%2C576","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn50@2020_5/2020/08/25/00-41-56-573_88d6c6591ab12fd2.webp","sourceBytes":2353196,"destBytes":650732,"targetWebpQuality":75,"feedId":10014,"totalSpendMs":17826,"convertSpendMs":16422,"createdTime":"2020-08-25 08:41:40","host":"us-016*","referer":"https://github.blog/?p=53970","linkMd5ListStr":"422e103183b7eeff6dd32d4c3057c4fd","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"27.7%","sourceSize":"2.2 MB","destSize":"635.5 KB"},{"code":1,"isDone":false,"source":"https://github.blog/wp-content/uploads/2020/08/demo.gif?resize=1024%2C576","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn45@2020_5/2020/08/25/00-42-05-727_202eb8df93ba2457.webp","sourceBytes":3264627,"destBytes":830700,"targetWebpQuality":75,"feedId":10014,"totalSpendMs":26993,"convertSpendMs":25587,"createdTime":"2020-08-25 08:41:40","host":"us-028*","referer":"https://github.blog/?p=53970","linkMd5ListStr":"422e103183b7eeff6dd32d4c3057c4fd","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"compressRate":"25.4%","sourceSize":"3.1 MB","destSize":"811.2 KB"}],"successGithubMap":{"myreaderx25":1,"myreaderx8":1,"myreaderx14":1,"myreaderx7":1,"myreaderx32":1,"myreaderx21":1,"myreaderx4":1,"myreaderx11":1,"myreaderx23":1,"myreaderx2":1,"myreaderx1":1,"myreaderx24":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx29":1},"failGithubMap":{}}