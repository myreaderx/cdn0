{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-09-28 01:12:41","updatedTime":"2021-09-28 01:12:41","title":"为何Transformer在计算机视觉中如此受欢迎？","link":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","description":"<div><div><div id=\"media\" class=\"rich_media_thumb_wrp\">\n\n            <img class=\"rich_media_thumb\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaLJ4uzS5iaaP3FSXdAbTPnFciarBhTh8QNYsN754ricpt0FibKcRkkDkqSQ/0?wx_fmt=jpeg?imageView2/1/w/600\">\n        </div>\n    \n\n    \n\n    <div class=\"rich_media_content\" id=\"js_content\">\n                    \n\n                    \n                    \n                    \n                    <section style=\"margin-left: 8px;margin-right: 8px;\" data-mpa-powered-by=\"yiban.io\"><img class=\"__bg_gif\" data-cropselx1=\"0\" data-cropselx2=\"578\" data-cropsely1=\"0\" data-cropsely2=\"116\" data-ratio=\"0.17896389324960754\" data-type=\"gif\" data-w=\"637\" style=\"outline: 0px; box-sizing: border-box; letter-spacing: 0.544px; white-space: normal; background-color: rgb(255, 255, 255); color: rgb(62, 62, 62); visibility: visible !important; width: 637px !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif\"></section><p style=\"text-align: center;\"><span style=\"font-size: 12px;background-color: rgb(255, 255, 255);letter-spacing: 0.544px;text-align: center;\">（本文阅读时间：24分钟）</span></p><section style=\"margin: 10px 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);\"><section style=\"outline: 0px;border-width: 2px;border-style: dashed;border-color: rgb(160, 219, 239);\"><section style=\"margin: 15px;outline: 0px;\"><section style=\"outline: 0px;line-height: 1.75em;\"><span style=\"outline: 0px;font-size: 14px;color: rgb(136, 136, 136);\">编者按：近一年来，Transformer 在计算机视觉领域所带来的革命性提升，引起了学术界的广泛关注，有越来越多的研究人员投入其中。Transformer 的特点和优势是什么？为什么在计算机领域中 Transformer 可以频频出圈？让我们通过今天的文章来一探究竟吧！</span></section></section></section></section><p style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">“统一性”是很多学科共同追求的目标，例如在物理学领域，科学家们追求的大统一，就是希望用单独一种理论来解释力与力之间的相互作用。人工智能领域自然也存在着关于“统一性”的目标。在深度学习的浪潮中，人工智能领域已经朝着统一性的目标前进了一大步。比如，一个新的任务基本都会遵循同样的流程对新数据进行预测：收集数据，做标注，定义网络结构，训练网络参数。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">但是，在人工智能的不同子领域中，基本建模的方式各种各样，并不统一，例如：在自然语言处理（NLP）领域目前的主导建模网络是 Transformer；计算机视觉（CV）领域很长一段时间的主导网络是卷积神经网络（CNN）；社交网络领域目前的主导网络则是图网络等。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">尽管如此，从2020年年底开始，Transformer 还是在 CV 领域中展现了革命性的性能提升。这就表明 CV 和 NLP 有望统一在 Transformer 结构之下。这一趋势对于两个领域的发展来说有很多好处：1）使视觉和语言的联合建模更容易；2）两个领域的建模和学习经验可以深度共享，从而加快各自领域的进展。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;outline: 0px;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);text-align: center;line-height: 1.75em;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"40\" data-cropsely1=\"0\" data-cropsely2=\"40\" data-ratio=\"1.118421052631579\" data-s=\"300,640\" data-type=\"png\" data-w=\"76\" style=\"outline: 0px; box-sizing: border-box !important; visibility: visible !important; width: 31px !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png\"></section><section data-id=\"93765\" data-tools=\"135编辑器\" style=\"white-space: normal;outline: 0px;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);\"><section data-width=\"100%\" style=\"outline: 0px;text-align: center;\"><section style=\"outline: 0px;display: inline-block;\"><section style=\"padding-right: 0.8em;padding-left: 0.8em;outline: 0px;letter-spacing: 2px;font-size: 18px;font-weight: bold;\">Transformer 在视觉任务中的优异性能</section><section data-width=\"100%\" style=\"margin-top: -10px;outline: 0px;background: rgb(160, 219, 239);border-radius: 20px;height: 10px;overflow: hidden;\"><br style=\"letter-spacing: 0.544px;outline: 0px;\"></section></section></section></section><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;line-height: 1.75em;\"><br></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">视觉 Transformer 的先驱工作是谷歌在 ICLR 2021 上发表的 ViT [1]，该工作把图像分成多个图像块（例如16x16像素大小），并把这些图像块比作 NLP 中的 token。然后直接将 NLP 中的标准 Transformer 编码器应用于这些 “token”，并据此进行图像分类。该工作结合了海量的预训练数据（如谷歌内部3亿图片分类训练库 JFT-300M），在 ImageNet-1K 的 validation 评测集上取得了88.55%的准确率，刷新了该榜单上的纪录。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">ViT 应用 Transformer 比较简单直接，因为其没有仔细考虑视觉信号本身的特点，所以它主要适应于图像分类任务，对于区域级别和像素级别的任务并不是很友好，例如物体检测和语义分割等。为此，学术界展开了大量的改进工作。其中，<strong>Swin Transformer 骨干网络 [2] 在物体检测和语义分割任务中大幅刷新了此前的纪录，让学术界更加确信 Transformer 结构将会成为视觉建模的新主流</strong>。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">具体而言，在物体检测的重要评测集 COCO 上，Swin Transformer 取得了单模型58.7的 box mAP 和51.1的 mask mAP，分别比此前最好的、没有扩充数据的单模型方法高出了+2.7个点和+2.6个点。此后，通过改进检测框架以及更好地利用数据，基于 Swin Transformer 网络的方法性能进一步取得了61.3的 box mAP 和53.0的 mask mAP，累计提升达+5.3 box mAP 和+5.5 mask mAP。在语义分割的重要评测数据集 ADE20K 上，Swin Transformer 也取得了显著的性能提升，达到了53.5 mIoU，比此前最好的方法高出+3.2 mIoU，此后随着分割框架和训练方法的进一步改进，目前已达到57.0 mIoU 的性能。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-ratio=\"0.829733163913596\" data-s=\"300,640\" data-type=\"png\" data-w=\"787\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriacn2flFc3v9Er8q7fhLQCJvvZfBtcmNI27hpIFefcAh5nYYt6LyiacJQ/640?wx_fmt=png\"></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图1：历年 COCO 物体检测评测集上的纪录</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">除了在物体检测和语义分割任务上表现亮眼外，基于 Swin Transformer 骨干网络的方法在众多视觉任务中也取得了优异的成绩，如视频动作识别 [3]、视觉自监督学习 [4][5]、图像复原 [6]、行人 Re-ID [7]、医疗图像分割 [8]等。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Swin Transformer 的主要思想是将具有很强建模能力的 Transformer 结构和重要的视觉信号先验结合起来。这些先验具有层次性（Hierarchy）、局部性（locality）以及平移不变性的特点（translation invariance）。Swin Transformer 的一个重要设计是移位的不重叠窗口（shifted windows），不同于传统的滑动窗，不重叠窗口的设计对硬件实现更加友好，从而具有更快的实际运行速度。如图2（左）所示，在滑动窗口设计中，不同的点采用了不同的邻域窗口来计算相互关系，这种计算对硬件并不友好。而如图2（右）所示，Swin Transformer 使用的不重叠窗口中，统一窗口内的点将采用相同的邻域来进行计算，对速度更友好。实际测试表明，非重叠窗口方法的速度比滑动窗口方法快了2倍左右。在两个连续的层中还做了移位的操作。在 L 层中，窗口分区从图像的左上角开始；在 L+1 层中，窗口划分则往右下移动了半个窗口。这样的设计保证了不重叠的窗口间可以有信息的交换。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-ratio=\"0.40794223826714804\" data-s=\"300,640\" data-type=\"png\" data-w=\"1108\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriazlUe47TibKMXkbCZ3KNbD51HJDoqXLtVJLy3YbpicUlFicBwX6nalz8fg/640?wx_fmt=png\"></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图2：传统的滑动窗口方法（左），由于不同的查询所用到的关键字集合不同，其对存储的访问不太友好，实际运行速度较慢。移位的不重叠窗口方法（右），由于不同的查询共享关键字集合，所以实际运行速度更快，从而更实用。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">在过去的大半年中，学术界视觉 Transformer 还涌现了大量变种，包括 DeiT [9]，LocalViT [10]，Twins [11]，PvT [12]，T2T-ViT [13], ViL [14]，CvT [15]，CSwin [16]，Focal Transformer [17]，Shuffle Transformer [18] 等。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;outline: 0px;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);text-align: center;line-height: 1.75em;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"40\" data-cropsely1=\"0\" data-cropsely2=\"40\" data-ratio=\"1.118421052631579\" data-s=\"300,640\" data-type=\"png\" data-w=\"76\" style=\"outline: 0px; box-sizing: border-box !important; visibility: visible !important; width: 31px !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png\"></section><section data-id=\"93765\" data-tools=\"135编辑器\" style=\"white-space: normal;outline: 0px;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);\"><section data-width=\"100%\" style=\"outline: 0px;text-align: center;\"><section style=\"outline: 0px;display: inline-block;\"><section style=\"padding-right: 0.8em;padding-left: 0.8em;outline: 0px;letter-spacing: 2px;font-size: 18px;font-weight: bold;\">拥抱 Transformer 的五个理由</section><section data-width=\"100%\" style=\"margin-top: -10px;outline: 0px;background: rgb(160, 219, 239);border-radius: 20px;height: 10px;overflow: hidden;\"><br style=\"letter-spacing: 0.544px;outline: 0px;\"></section></section></section></section><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;line-height: 1.75em;\"><br></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">除了刷新很多视觉任务的性能纪录以外，视觉 Transformer 还拥有诸多好处。事实上，过去4年间学术界不断挖掘出了 Transformer 建模的各种优点，可以总结为图3所示的五个方面。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-ratio=\"0.30699300699300697\" data-s=\"300,640\" data-type=\"png\" data-w=\"1430\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaXbfRib9r2sb6rdClboy7bv85oqsDSPKLkltT9NztWPM8IngZ5NKt1cQ/640?wx_fmt=png\"></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图3：过去4年学术界不断挖掘出的 Transformer 建模的五个优点</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">理由1：通用的建模能力</span></strong></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Transformer 的通用建模能力来自于两个方面：一方面 Transformer 可以看作是一种图建模方法。图是全连接的，节点之间的关系通过数据驱动的方式来学习得到。由于任意概念（无论具体或抽象）都可以用图中的节点来表示，且概念之间的关系可以用图上的边来刻画，因此 Transformer 建模具有很强的通用性。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">另一方面，Transformer 通过验证的哲学来建立图节点之间的关系，具有较好的通用性：无论节点多么异构，它们之间的关系都可以通过投影到一个可以比较的空间里计算相似度来建立。如图4（右）所示，节点可以是不同尺度的图像块，也可以是“运动员”的文本输入，Transformer 均可以刻画这些异构节点之间的关系。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-ratio=\"0.35758651286601595\" data-s=\"300,640\" data-type=\"png\" data-w=\"1127\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaZf7638vFa7pzUDcjrFUPBeQSiciaL3FHjrgvkYW6ooVuMOA2EnIjqpPQ/640?wx_fmt=png\"></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图4：促成 Transformer 通用建模能力的两大原因：图建模（左）和验证哲学（右）</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">正是因为具备这样的通用建模能力，Transformer 中的注意力单元可以被应用到各种各样的视觉任务中。具体而言，计算机视觉处理的对象主要涉及两个层次的基本元素：像素和物体。而计算机视觉所涉及到的任务主要就囊括了这些基本元素之间的关系，包括像素-像素，物体-像素和物体-物体的关系建模。此前，前两种关系建模主要是分别由卷积和 RoIAlign 来实现的，最后一种关系通常没有很好的建模方法。但是，Transformer 中的注意力单元因其通用的建模能力，可以被应用到所有这些基本关系的建模中。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">近些年，在这个领域中已经出现了很多代表性的工作，例如：1） 非局部网络 [19]。王小龙等人将注意力单元用于建模像素-像素的关系，证明了 Transformer 可以帮助视频动作分类和物体检测等任务。元玉慧等人将其应用于语义分割问题，也取得了显著的性能提升[20]。2）物体关系网络 [21]。注意力单元用于物体检测中的物体关系建模，这一模块也被广泛应用于视频物体分析中 [22, 23, 24]。3）物体和像素的关系建模，典型的工作包括 DETR [25]，LearnRegionFeat [26]，以及 RelationNet++ [27]等。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-ratio=\"0.46420323325635104\" data-s=\"300,640\" data-type=\"png\" data-w=\"866\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriabIztpNIfQyKEMLmEXGrYDfXfAcbeNibQOCia9TQJfmu06ynxKlibTgvAw/640?wx_fmt=png\"></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图5：Transformer 能被应用于各种视觉基本元素之间的关系建模，包括像素-像素（左），物体-像素（中），物体-物体（右）</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">理由2：和卷积形成互补</span></strong></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">卷积是一种局部操作，一个卷积层通常只会建模邻域像素之间的关系。Transformer 则是全局操作，一个 Transformer 层能建模所有像素之间的关系，双方可以很好地进行互补。最早将这种互补性联系起来的是非局部网络 [19]，在这个工作中，少量 Transformer 自注意单元被插入到了原始网络的几个地方，作为卷积网络的补充，并被证明其在物体检测、语义分割和视频动作识别等问题中广泛有效。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">此后，也有工作发现非局部网络在视觉中很难真正学到像素和像素之间的二阶关系 [28]，为此，有研究员们也提出了一些针对这一模型的改进，例如解耦非局部网络 [29]。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">理由3：更强的建模能力</span></strong></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">卷积可以看作是一种模板匹配，图像中不同位置采用相同的模板进行滤波。而 Transformer 中的注意力单元则是一种自适应滤波，模板权重由两个像素的可组合性来决定，这种自适应计算模块具有更强的建模能力。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">最早将 Transformer 这样一种自适应计算模块应用于视觉骨干网络建模的方法是局部关系网络 LR-Net [30] 和 SASA [31]，它们都将自注意的计算限制在一个局部的滑动窗口内，在相同理论计算复杂度的情况下取得了相比于 ResNet 更好的性能。然而，虽然理论上与 ResNet 的计算复杂度相同，但在实际使用中它们却要慢得多。一个主要原因是不同的查询（query）使用不同的关键字（key）集合，如图2（左）所示，对内存访问不太友好。 </span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Swin Transformer 提出了一种新的局部窗口设计——移位窗口（shifted windows）。这一局部窗口方法将图像划分成不重叠的窗口，这样在同一个窗口内部，不同查询使用的关键字集合将是相同的，进而可以拥有更好的实际计算速度。在下一层中，窗口的配置会往右下移动半个窗口，从而构造了前一层中不同窗口像素间的联系。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">理由4：对大模型和大数据的可扩展性</span></strong></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">在 NLP 领域，Transformer 模型在大模型和大数据方面展示了强大的可扩展性。图6中，蓝色曲线显示近年来 NLP 的模型大小迅速增加。大家都见证了大模型的惊人能力，例如微软的 Turing 模型、谷歌的 T5 模型以及 OpenAI 的 GPT-3 模型。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">视觉 Transformer 的出现为视觉模型的扩大提供了重要的基础，目前最大的视觉模型是谷歌的150亿参数 ViT-MoE 模型 [32]，这些大模型在 ImageNet-1K 分类上刷新了新的纪录。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-ratio=\"0.6502564102564102\" data-s=\"300,640\" data-type=\"png\" data-w=\"975\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriatSjjPlB512JNibBPl9niaopcnk3y2r35dJDkMFia2LB3DC50QLI1QWoGg/640?wx_fmt=png\"></section><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图6：NLP 领域和计算机视觉领域模型大小的变迁</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">理由5：更好地连接视觉和语言</span></strong></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">在以前的视觉问题中，科研人员通常只会处理几十类或几百类物体类别。例如 COCO 检测任务中包含了80个物体类别，而 ADE20K 语义分割任务包含了150个类别。视觉 Transformer 模型的发明和发展，使视觉领域和 NLP 领域的模型趋同，有利于联合视觉和 NLP 建模，从而将视觉任务与其所有概念联系起来。这方面的先驱性工作主要有 OpenAI 的 CLIP [33] 和 DALL-E 模型 [34]。</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">考虑到上述的诸多优点，相信视觉 Transformer 将开启计算机视觉建模的新时代，我们也期待学术界和产业界共同努力，进一步挖掘和探索这一新的建模方法给视觉领域带来的全新机遇和挑战。</span></strong></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\">参考文献：</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. ICCV 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[3] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu. Video Swin Transformer. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[4] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, Han Hu. Self-Supervised Learning with Swin Transformers. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[5] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, Jianfeng Gao. Efficient Self-supervised Vision Transformers for Representation Learning. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[6] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte. SwinIR: Image Restoration Using Swin Transformer. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[7] https://github.com/layumi/Person_reID_baseline_pytorch</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[8] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, Manning Wang. Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[9] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[10] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool. LocalViT: Bringing Locality to Vision Transformers. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen. Twins: Revisiting the Design of Spatial Attention in Vision Transformers. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[12] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions. ICCV 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[13] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[14] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao. Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[15] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang. CvT: Introducing Convolutions to Vision Transformers. ICCV 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[16] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo. CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[17] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao. Focal Self-attention for Local-Global Interactions in Vision Transformers. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[18] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu. Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[19] Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He. Non-local Neural Networks. CVPR 2018</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[20] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong Wang. OCNet: Object Context for Semantic Segmentation. IJCV 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[21] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei. Relation Networks for Object Detection. CVPR 2018</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[22] Jiarui Xu, Yue Cao, Zheng Zhang, Han Hu. Spatial-Temporal Relation Networks for Multi-Object Tracking. ICCV 2019</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[23] Yihong Chen, Yue Cao, Han Hu, Liwei Wang. Memory Enhanced Global-Local Aggregation for Video Object Detection. CVPR 2020</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[24] Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao Mei. Relation distillation networks for video object detection. ICCV 2019</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[25] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko. End-to-End Object Detection with Transformers. ECCV 2020</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[26] Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai. Learning Region Features for Object Detection. ECCV 2018</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[27] Cheng Chi, Fangyun Wei, Han Hu. RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder. NeurIPS 2020</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[28] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu. GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond. ICCV workshop 2019</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[29] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu. Disentangled Non-Local Neural Networks. ECCV 2020</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[30] Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin. Local Relation Networks for Image Recognition. ICCV 2019</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[31] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens. Stand-Alone Self-Attention in Vision Models. NeurIPS 2019</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[32] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, Neil Houlsby. Scaling Vision with Sparse Mixture of Experts. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. Learning Transferable Visual Models from Natural Language Supervision. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">[34] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. Zero-Shot Text-to-Image Generation. Tech report 2021</span></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p style=\"margin-left: 8px;margin-right: 8px;line-height: 1.75em;\"><br></p><p> </p><section style=\"margin-top: 10px;margin-bottom: 10px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><section style=\"margin-bottom: -3px;padding-top: 3px;outline: 0px;width: 677px;border-bottom: 1px dotted rgb(160, 160, 160);\"><br style=\"outline: 0px;\"></section><section style=\"outline: 0px;height: 5px;line-height: 5px;\"><section style=\"outline: 0px;vertical-align: top;display: inline-block;\"><span style=\"margin-left: 5px;outline: 0px;width: 5px;height: 5px;float: left;border-radius: 50%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);\"><br style=\"outline: 0px;\"></span></section><section style=\"outline: 0px;clear: both;\"><br style=\"outline: 0px;\"></section><section style=\"outline: 0px;clear: both;\"><br style=\"outline: 0px;\"></section><section style=\"outline: 0px;clear: both;\"><br style=\"outline: 0px;\"></section></section></section><p style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);\"><br style=\"outline: 0px;\"></p><p style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);\"><strong style=\"outline: 0px;color: rgb(46, 87, 151);font-size: 15px;letter-spacing: 1px;\">你也许还想看</strong><strong style=\"outline: 0px;color: rgb(46, 87, 151);font-size: 15px;letter-spacing: 1px;\">：</strong></p><p style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><br style=\"outline: 0px;\"></p><p style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649467499&amp;idx=1&amp;sn=809c616b985b981c855c423ec9ca972a&amp;chksm=82c043efb5b7caf9da4e88c529203c7a29674f16e4854afc98039910cdb8912d2c009e7a04f9&amp;scene=21#wechat_redirect\" textvalue=\"你已选中了添加链接的内容\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"1\"><span class=\"js_jump_icon h5_image_link\" data-positionback=\"static\" style=\"outline: 0px;line-height: 0;vertical-align: bottom;inset: 0px;\"><img class=\"rich_pages\" data-backh=\"186\" data-backw=\"562\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"186\" data-ratio=\"0.3317167798254122\" data-s=\"300,640\" data-type=\"png\" data-w=\"3093\" style=\"outline: 0px; border-width: 0px; border-style: initial; border-color: initial; inset: 0px; box-sizing: border-box !important; visibility: visible !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaNWJyVCqCEPJHQ5lNe3AibII49vqibicY8tkvzWaiagibT4iat9sDLDSpJcNQ/640?wx_fmt=png\"></span></a></p><section style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649471207&amp;idx=1&amp;sn=354ddda71dc0c75803fecd4fb8c8a76c&amp;chksm=82c05163b5b7d8752469ff327abcc1e66a945b062cabd0abd1c7fd3e56401718b56906bbb898&amp;scene=21#wechat_redirect\" textvalue=\"你已选中了添加链接的内容\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"1\"><span class=\"js_jump_icon h5_image_link\" data-positionback=\"static\" style=\"outline: 0px;line-height: 0;vertical-align: bottom;inset: auto;\"><img class=\"rich_pages\" data-backh=\"170\" data-backw=\"513\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"186\" data-ratio=\"0.3317167798254122\" data-s=\"300,640\" data-type=\"png\" data-w=\"3093\" style=\"outline: 0px; border-width: 0px; border-style: initial; border-color: initial; box-sizing: border-box !important; visibility: visible !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaYsmOdtFLekloc3qGs7CvXltaxiayvqq2t8tHPeIYmYrtO7DpcPSLtibw/640?wx_fmt=png\"></span></a></section><section style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649464566&amp;idx=1&amp;sn=04c63181831b101ea1613529f3c17e1c&amp;chksm=82c07772b5b7fe64725559d0fc00dbb00830ed2414f53c724324f713fff36d4f7ee379547aa2&amp;scene=21#wechat_redirect\" textvalue=\"你已选中了添加链接的内容\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"1\"><span class=\"js_jump_icon h5_image_link\" data-positionback=\"static\" style=\"outline: 0px;line-height: 0;vertical-align: bottom;inset: auto;\"><img class=\"rich_pages wxw-img\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"186\" data-galleryid=\"\" data-ratio=\"0.3317167798254122\" data-s=\"300,640\" data-type=\"png\" data-w=\"3093\" style=\"outline: 0px; border-width: 0px; border-style: initial; border-color: initial; box-sizing: border-box !important; visibility: visible !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriamkADWGJqgERCrb4ibI3CMIxqGlCr6rfV4exGmF09j6SNNn9w0BevRiaQ/640?wx_fmt=png\"></span></a></section><section style=\"margin-right: 8px;margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><br style=\"outline: 0px;\"></section><section style=\"margin: 20px 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);display: flex;justify-content: center;align-items: center;\"><section data-width=\"90%\" style=\"outline: 0px;background: rgb(137, 137, 137);width: 562px;height: 2px;\"><br style=\"outline: 0px;\"></section><section data-width=\"90%\" style=\"outline: 0px;background: rgb(137, 137, 137);width: 562px;height: 2px;\"><br style=\"outline: 0px;\"></section></section><p style=\"margin-left: 8px;outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 1.75em;text-align: center;\"><span style=\"outline: 0px;color: rgb(0, 0, 0);font-size: 15px;letter-spacing: 1px;\"><img class=\"__bg_gif\" data-copyright=\"0\" data-ratio=\"0.5\" data-type=\"gif\" data-w=\"750\" style=\"outline: 0px; box-sizing: border-box; color: rgb(51, 51, 51); visibility: visible !important; width: 677px !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNLsud1BZ6dFjDQiaG01NGT1z7llzgFU2U91l2cXywRJmR0FZ8SrqEpzuibSgI078eLPTHIlPElysjQ/640?wx_fmt=gif\"></span></p><p style=\"outline: 0px;letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);text-align: center;\"><img class=\"rich_pages __bg_gif\" data-ratio=\"0.41379310344827586\" data-type=\"gif\" data-w=\"638\" style=\"outline: 0px; box-sizing: border-box !important; visibility: visible !important; width: 638px !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNMMrKApwKS4eEP6EC9NIpYiaLvqtet8icWZqrHsFqWkWrN99RVkkGGEOOCCj9XPXW5H1ZhwOQulrxZg/640?wx_fmt=gif\"></p>\n                </div>\n\n    \n    <br>\n\n    \n        <a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649474855&amp;idx=1&amp;sn=8645351c8581ad52f177a280f0bbb865&amp;chksm=82c02ea3b5b7a7b55dee97fe29e7d45de9962cce828f6a3adb8ae26167a6175c264a8521cc9c#rd\" style=\"color: blue\" class=\"media_tool_meta meta_primary\">原文</a>\n        <br>\n    \n\n    \n\n    <img alt=\"\" width=\"1px\" height=\"1px\" class=\"\" style=\"width:1px;height:1px;display:none\" src=\"http://www.jintiankansha.me/rss_static/5416/aVSGqqpIX9\"></div></div>","descriptionType":"html","publishedDate":"Fri, 24 Sep 2021 07:37:00 +0000","feedId":1700,"bgimg":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaLJ4uzS5iaaP3FSXdAbTPnFciarBhTh8QNYsN754ricpt0FibKcRkkDkqSQ/0?wx_fmt=jpeg?imageView2/1/w/600","linkMd5":"9058cc7cf70d3dfb8d9ae265b257d74c","destWidth":1080,"destHeight":459,"sourceBytes":43631,"destBytes":25760,"author":"","articleImgCdnMap":{"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaLJ4uzS5iaaP3FSXdAbTPnFciarBhTh8QNYsN754ricpt0FibKcRkkDkqSQ/0?wx_fmt=jpeg?imageView2/1/w/600":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn16@2020_4/2021/09/27/17-12-54-012_d8f757d4d88b52d6.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn20@2020_1/2021/09/27/17-12-55-489_f61698ba2e51c308.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriacn2flFc3v9Er8q7fhLQCJvvZfBtcmNI27hpIFefcAh5nYYt6LyiacJQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn95@2020_2/2021/09/27/17-13-02-843_24cbde88e6566963.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriazlUe47TibKMXkbCZ3KNbD51HJDoqXLtVJLy3YbpicUlFicBwX6nalz8fg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn3@2020_5/2021/09/27/17-12-59-835_a14f990fdaccac7b.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaXbfRib9r2sb6rdClboy7bv85oqsDSPKLkltT9NztWPM8IngZ5NKt1cQ/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaZf7638vFa7pzUDcjrFUPBeQSiciaL3FHjrgvkYW6ooVuMOA2EnIjqpPQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_2/2021/09/27/17-13-06-103_6131375edd4a1754.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriabIztpNIfQyKEMLmEXGrYDfXfAcbeNibQOCia9TQJfmu06ynxKlibTgvAw/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriatSjjPlB512JNibBPl9niaopcnk3y2r35dJDkMFia2LB3DC50QLI1QWoGg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn92@2020_5/2021/09/27/17-12-54-807_72acd930ed9d0d40.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaNWJyVCqCEPJHQ5lNe3AibII49vqibicY8tkvzWaiagibT4iat9sDLDSpJcNQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn97@2020_5/2021/09/27/17-13-18-673_44e1797b21d765c7.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaYsmOdtFLekloc3qGs7CvXltaxiayvqq2t8tHPeIYmYrtO7DpcPSLtibw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn40@2020_1/2021/09/27/17-13-21-137_e0fe134c4403affe.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriamkADWGJqgERCrb4ibI3CMIxqGlCr6rfV4exGmF09j6SNNn9w0BevRiaQ/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNLsud1BZ6dFjDQiaG01NGT1z7llzgFU2U91l2cXywRJmR0FZ8SrqEpzuibSgI078eLPTHIlPElysjQ/640?wx_fmt=gif":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNMMrKApwKS4eEP6EC9NIpYiaLvqtet8icWZqrHsFqWkWrN99RVkkGGEOOCCj9XPXW5H1ZhwOQulrxZg/640?wx_fmt=gif":null,"http://www.jintiankansha.me/rss_static/5416/aVSGqqpIX9":null},"publishedOrCreatedDate":1632762761425}],"record":{"createdTime":"2021-09-28 01:12:41","updatedTime":"2021-09-28 01:12:41","feedId":1700,"fetchDate":"Mon, 27 Sep 2021 17:12:41 +0000","fetchMs":576,"handleMs":17,"totalMs":134223,"newArticles":0,"totalArticles":5,"status":1,"type":0,"ip":"af6ca12eefe249e94a3f077d15d7c168","hostName":"europe62*","requestId":"0d6b547d4d854dc2b2ccbfe022de10a4_1700","contentType":"application/rss+xml","totalBytes":225044,"bgimgsTotal":1,"bgimgsGithubTotal":0,"articlesImgsTotal":15,"articlesImgsGithubTotal":8,"successGithubMap":{"myreaderx21":1,"myreaderx11":1,"myreaderx22":1,"myreaderx2":1,"myreaderx1":1,"myreaderx29":1,"myreaderx18":1,"myreaderx":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}},"feed":{"createdTime":"2020-08-24 21:31:32","updatedTime":"2020-09-01 09:51:25","id":1700,"name":"微软研究院AI头条","url":"http://feedmaker.kindle4rss.com/feeds/MSRAsia.weixin.xml","subscriber":null,"website":null,"icon":"http://www.sogou.com/images/logo/new/favicon.ico?v=4","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn60@2020_1/2020/09/01/01-51-26-423_d24121c9beed1de6.ico","description":"专注科研18年，盛产黑科技","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-09-28 01:14:54","updatedTime":"2021-09-28 01:14:54","id":null,"feedId":1700,"linkMd5":"9058cc7cf70d3dfb8d9ae265b257d74c"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":225044,"tmpBgImgCdnBytes":0,"extra4":{"start":1632762760586,"total":0,"statList":[{"spend":822,"msg":"获取xml内容"},{"spend":17,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":1,"msg":"修正封面图上传失败重新上传"},{"spend":122696,"msg":"正文链接上传到cdn"}]},"extra5":15,"extra6":11,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaLJ4uzS5iaaP3FSXdAbTPnFciarBhTh8QNYsN754ricpt0FibKcRkkDkqSQ/0?wx_fmt=jpeg?imageView2/1/w/600","sourceStatusCode":200,"destWidth":1080,"destHeight":459,"sourceBytes":43631,"destBytes":25760,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":5248,"convertSpendMs":21,"createdTime":"2021-09-28 01:12:41","host":"us-013*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c,9058cc7cf70d3dfb8d9ae265b257d74c","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn82/contents/2021/09/27/17-12-46-757_1deb35b2dc72c7e8.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 27 Sep 2021 17:12:46 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["977C:0C25:2DC7E2D:89FD3AD:6151FB8E"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1632765307"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn82/contents/2021/09/27/17-12-46-757_1deb35b2dc72c7e8.webp","historyStatusCode":[],"spendMs":35},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"42.6 KB","destSize":"25.2 KB","compressRate":"59%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaLJ4uzS5iaaP3FSXdAbTPnFciarBhTh8QNYsN754ricpt0FibKcRkkDkqSQ/0?wx_fmt=jpeg?imageView2/1/w/600","sourceStatusCode":200,"destWidth":1080,"destHeight":459,"sourceBytes":43631,"destBytes":25760,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":5211,"convertSpendMs":16,"createdTime":"2021-09-28 01:12:46","host":"us-013*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c,9058cc7cf70d3dfb8d9ae265b257d74c","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn82/contents/2021/09/27/17-12-52-043_1deb35b2dc72c7e8.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 27 Sep 2021 17:12:52 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["977C:0C25:2DC808C:89FD8BC:6151FB94"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1632765307"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn82/contents/2021/09/27/17-12-52-043_1deb35b2dc72c7e8.webp","historyStatusCode":[],"spendMs":33},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"42.6 KB","destSize":"25.2 KB","compressRate":"59%"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5416/aVSGqqpIX9","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":408,"convertSpendMs":0,"createdTime":"2021-09-28 01:12:52","host":"europe-59*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif","sourceStatusCode":403,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":999,"convertSpendMs":0,"createdTime":"2021-09-28 01:12:52","host":"europe-56*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","extra22GetBytesInfo":"2、Referer字段 ： http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","extra23historyStatusCode":[403,403],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png","sourceStatusCode":403,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":934,"convertSpendMs":0,"createdTime":"2021-09-28 01:12:52","host":"us-034*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c,9058cc7cf70d3dfb8d9ae265b257d74c","extra22GetBytesInfo":"2、Referer字段 ： http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","extra23historyStatusCode":[403,403],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5416/aVSGqqpIX9","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":722,"convertSpendMs":0,"createdTime":"2021-09-28 01:12:52","host":"us-027*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png","sourceStatusCode":403,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":1232,"convertSpendMs":0,"createdTime":"2021-09-28 01:12:53","host":"us-023*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c,9058cc7cf70d3dfb8d9ae265b257d74c","extra22GetBytesInfo":"2、Referer字段 ： http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","extra23historyStatusCode":[403,403],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaXbfRib9r2sb6rdClboy7bv85oqsDSPKLkltT9NztWPM8IngZ5NKt1cQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":332,"sourceBytes":72822,"destBytes":23382,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":6407,"convertSpendMs":17,"createdTime":"2021-09-28 01:12:52","host":"us-006*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn8/contents/2021/09/27/17-12-58-627_2b459db59f47883b.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 27 Sep 2021 17:12:58 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["896E:48A5:3216ABC:9175DD9:6151FB9A"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1632765310"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["64"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn8/contents/2021/09/27/17-12-58-627_2b459db59f47883b.webp","historyStatusCode":[],"spendMs":33},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"71.1 KB","destSize":"22.8 KB","compressRate":"32.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaXbfRib9r2sb6rdClboy7bv85oqsDSPKLkltT9NztWPM8IngZ5NKt1cQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":332,"sourceBytes":72822,"destBytes":23382,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":7319,"convertSpendMs":22,"createdTime":"2021-09-28 01:12:58","host":"us-022*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn8/contents/2021/09/27/17-13-05-956_2b459db59f47883b.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 27 Sep 2021 17:13:06 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["DF1E:6B90:4119AA:174F463:6151FBA1"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1632765310"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["64"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn8/contents/2021/09/27/17-13-05-956_2b459db59f47883b.webp","historyStatusCode":[],"spendMs":93},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"71.1 KB","destSize":"22.8 KB","compressRate":"32.1%"},null,null,null,null,null,null,null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://europe-56.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[403]},"http://us-018.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://europe65.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-55.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-002.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-034.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[403]},"http://us-022.herokuapp.com/":{"failCount":2,"successCount":2,"resultList":[200,200,null,null]},"http://us-030.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-60.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-006.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe69.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe-59.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]},"http://us-010.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-023.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[403]},"http://us-029.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-027.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif","sourceStatusCode":200,"destWidth":637,"destHeight":114,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn16@2020_4/2021/09/27/17-12-54-012_d8f757d4d88b52d6.webp","sourceBytes":21989,"destBytes":14032,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":1091,"convertSpendMs":29,"createdTime":"2021-09-28 01:12:53","host":"us-022*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.5 KB","destSize":"13.7 KB","compressRate":"63.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriatSjjPlB512JNibBPl9niaopcnk3y2r35dJDkMFia2LB3DC50QLI1QWoGg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":975,"destHeight":634,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn92@2020_5/2021/09/27/17-12-54-807_72acd930ed9d0d40.webp","sourceBytes":73042,"destBytes":39766,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":3099,"convertSpendMs":25,"createdTime":"2021-09-28 01:12:52","host":"europe65*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"71.3 KB","destSize":"38.8 KB","compressRate":"54.4%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":76,"destHeight":85,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn20@2020_1/2021/09/27/17-12-55-489_f61698ba2e51c308.webp","sourceBytes":1776,"destBytes":2060,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":1009,"convertSpendMs":2,"createdTime":"2021-09-28 01:12:54","host":"us-55*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c,9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 KB","destSize":"2 KB","compressRate":"116%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriazlUe47TibKMXkbCZ3KNbD51HJDoqXLtVJLy3YbpicUlFicBwX6nalz8fg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":441,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn3@2020_5/2021/09/27/17-12-59-835_a14f990fdaccac7b.webp","sourceBytes":361655,"destBytes":32066,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":7929,"convertSpendMs":39,"createdTime":"2021-09-28 01:12:52","host":"us-018*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"353.2 KB","destSize":"31.3 KB","compressRate":"8.9%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriacn2flFc3v9Er8q7fhLQCJvvZfBtcmNI27hpIFefcAh5nYYt6LyiacJQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":787,"destHeight":653,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn95@2020_2/2021/09/27/17-13-02-843_24cbde88e6566963.webp","sourceBytes":286279,"destBytes":43764,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":10966,"convertSpendMs":27,"createdTime":"2021-09-28 01:12:52","host":"us-029*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"279.6 KB","destSize":"42.7 KB","compressRate":"15.3%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaZf7638vFa7pzUDcjrFUPBeQSiciaL3FHjrgvkYW6ooVuMOA2EnIjqpPQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":386,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_2/2021/09/27/17-13-06-103_6131375edd4a1754.webp","sourceBytes":103016,"destBytes":25580,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":14245,"convertSpendMs":19,"createdTime":"2021-09-28 01:12:52","host":"us-010*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"100.6 KB","destSize":"25 KB","compressRate":"24.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaNWJyVCqCEPJHQ5lNe3AibII49vqibicY8tkvzWaiagibT4iat9sDLDSpJcNQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":358,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn97@2020_5/2021/09/27/17-13-18-673_44e1797b21d765c7.webp","sourceBytes":274739,"destBytes":30464,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":26712,"convertSpendMs":25,"createdTime":"2021-09-28 01:12:52","host":"us-030*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"268.3 KB","destSize":"29.8 KB","compressRate":"11.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMEF3ayn1gOp9Lj9kkcYHriaYsmOdtFLekloc3qGs7CvXltaxiayvqq2t8tHPeIYmYrtO7DpcPSLtibw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":358,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn40@2020_1/2021/09/27/17-13-21-137_e0fe134c4403affe.webp","sourceBytes":375930,"destBytes":37312,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":29199,"convertSpendMs":28,"createdTime":"2021-09-28 01:12:52","host":"us-038*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+%E4%B8%BA%E4%BD%95Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E5%A6%82%E6%AD%A4%E5%8F%97%E6%AC%A2%E8%BF%8E%EF%BC%9F","linkMd5ListStr":"9058cc7cf70d3dfb8d9ae265b257d74c","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"367.1 KB","destSize":"36.4 KB","compressRate":"9.9%"}],"successGithubMap":{"myreaderx21":1,"myreaderx11":1,"myreaderx22":1,"myreaderx2":1,"myreaderx1":1,"myreaderx29":1,"myreaderx18":1,"myreaderx":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}}