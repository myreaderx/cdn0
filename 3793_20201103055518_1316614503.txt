{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-11-03 13:54:52","updatedTime":"2020-11-03 13:54:52","title":"Experimenting with Automatic Video Creation from a Web Page","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","description":"<span class=\"byline-author\">Posted by Peggy Chi, Senior Research Scientist, and Irfan Essa, Senior Staff Research Scientist, Google Research</span> <p>At Google, we're actively exploring how people can use creativity tools powered by machine learning and computational methods when producing multimedia content, from <a href=\"https://magenta.tensorflow.org/\">creating music</a> and <a href=\"https://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html\">reframing videos</a>, to <a href=\"https://ai.googleblog.com/2017/04/teaching-machines-to-draw.html\">drawing</a> and more. One creative process in particular, video production, can especially benefit from such tools, as it requires a series of decisions about what content is best suited to a target audience, how to position the available assets within the field of view, and what temporal arrangement will yield the most compelling narrative. But what if one could leverage existing assets, such as a website, to get a jump-start on video creation? Businesses commonly host websites that contain rich visual representations about their services or products, all of which could be repurposed for other multimedia formats, such as videos, potentially enabling those without extensive resources the ability to reach a broader audience. </p><p>In “<a href=\"https://research.google/pubs/pub49618/\">Automatic Video Creation From a Web Page</a>”, published at <a href=\"https://uist.acm.org/uist2020/\">UIST 2020</a>, we introduce URL2Video, a research prototype pipeline to automatically convert a web page into a short video, given temporal and visual constraints provided by the content owner. URL2Video extracts assets (text, images, or videos) and their design styles (including fonts, colors, graphical layouts, and hierarchy) from HTML sources and organizes the visual assets into a sequence of shots, while maintaining a look-and-feel similar to the source page. Given a user-specified aspect ratio and duration, it then renders the repurposed materials into a video that is ideal for product and service advertising. </p><div style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-9vM8BsGVXNY/X5r3-ZYwX-I/AAAAAAAAGvQ/ai5-yIIg4gcQNBQGvzfwrLghPSc9bgTogCLcBGAsYHQ/s900/image3.gif\"><img border=\"0\" data-original-height=\"506\" data-original-width=\"900\" src=\"https://1.bp.blogspot.com/-9vM8BsGVXNY/X5r3-ZYwX-I/AAAAAAAAGvQ/ai5-yIIg4gcQNBQGvzfwrLghPSc9bgTogCLcBGAsYHQ/s16000/image3.gif\" /></a></div><p><b>URL2Video Overview</b><br>Assume a user provides an URL to a web page that illustrates their business. The URL2Video pipeline automatically selects key content from the page and decides the temporal and visual presentation of each asset, based on a set of heuristics derived from an interview study with designers who were familiar with web design and video ad creation. These designer-informed heuristics capture common video editing styles, including content hierarchy, constraining the amount of information in a shot and its time duration, providing consistent color and style for branding, and more. Using this information, the URL2Video pipeline parses a web page, analyzing the content and selecting visually salient text or images while preserving their design styles, which it organizes according to the video specifications provided by the user.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-0TU46rroWzo/X5r4HZDiWSI/AAAAAAAAGvU/lcqd_ao3jec3J4HAG2MK8FZOT4U53-GbACLcBGAsYHQ/s900/image4.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"413\" data-original-width=\"900\" src=\"https://1.bp.blogspot.com/-0TU46rroWzo/X5r4HZDiWSI/AAAAAAAAGvU/lcqd_ao3jec3J4HAG2MK8FZOT4U53-GbACLcBGAsYHQ/s16000/image4.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">By extracting the structural content and design from the input web page, URL2Video makes automatic editing decisions to present key messages in a video. It considers the temporal (e.g., the duration in seconds) and spatial (e.g., the aspect ratio) constraints of the output video defined by users.</td></tr></tbody></table><p><em>Webpage Analysis</em><br>Given a webpage URL, URL2Video extracts <a href=\"https://en.wikipedia.org/wiki/Document_Object_Model\">document object model</a> (DOM) information and multimedia materials. For the purposes of our research prototype, we limited the domain to static web pages that contain salient assets and headings preserved in an HTML hierarchy that follows recent <a href=\"https://material.io/design/usability/accessibility.html#hierarchy\">web design principles</a>, which encourage the use of prominent elements, distinct sections, and an order of visual focus that guides readers in perceiving information. URL2Video identifies such visually-distinguishable elements as a candidate list of asset groups, each of which may contain a heading, a product image, detailed descriptions, and call-to-action buttons, and  captures both the raw assets (text and multimedia files) and detailed design specifications (HTML tags, CSS styles, and rendered locations) for each element. It then ranks the asset groups by assigning each a priority score based on their visual appearance and annotations, including their HTML tags, rendered sizes, and ordering shown on the page. In this way, an asset group that occupies a larger area at the top of the page receives a higher score. </p><p><em>Constraints-Based Asset Selection</em><br>We consider two goals when composing a video: (1) each video shot should provide concise information, and (2) the visual design should be consistent with the source page. Based on these goals and the video constraints provided by the user, including the intended video duration (in seconds) and aspect ratio (commonly 16:9, 4:3, 1:1, etc.), URL2Video automatically selects and orders the asset groups to optimize the total priority score. To make the content concise, it presents only dominant elements from a page, such as a headline and a few multimedia assets. It constrains the duration of each visual element for viewers to perceive the content. In this way, a short video highlights the most salient information from the top of the page, and a longer video contains more campaigns or products.  </p><p><em>Scene Composition &amp; Video Rendering</em><br>Given an ordered list of assets based on the DOM hierarchy, URL2Video follows the design heuristics obtained from interview studies to make decisions about both the <em>temporal</em> and <em>spatial</em> arrangement to present the assets in individual shots. It transfers the graphical layout of elements into the video’s aspect ratio, and applies the style choices including fonts and colors. To make a video more dynamic and engaging, it adjusts the presentation timing of assets. Finally, it renders the content into a video in the MPEG-4 container format. </p><p><em>User Control</em><br>The interface to the research prototype allows the user to review the design attributes in each video shot extracted from the source page, reorder the materials, change the detailed design, such as colors and fonts, and adjust the constraints to generate a new video. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-5NG1bgFx2oI/X5r4RA8k4NI/AAAAAAAAGvc/59QzCbjMAMIMkSkCzWWu6eCURGKtRDR2ACLcBGAsYHQ/s1999/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"930\" data-original-width=\"1999\" src=\"https://1.bp.blogspot.com/-5NG1bgFx2oI/X5r4RA8k4NI/AAAAAAAAGvc/59QzCbjMAMIMkSkCzWWu6eCURGKtRDR2ACLcBGAsYHQ/s16000/image1.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">In URL2Video's authoring  interface (<b>left</b>), users specify the input URL to a source page, size of the target page view, and the output video parameters. URL2Video analyzes the web page and extracts major visual components. It composes a series of scenes and visualizes the key frames as a storyboard. These components are rendered into an output video that satisfies the input temporal and spatial constraints. Users can playback the video, examine the design attributes (<b>bottom-right</b>), and make adjustments to generate video variation, such as reordering the scenes (<b>top-right</b>).</td></tr></tbody></table><p><b>URL2Video Use Cases</b><br>We demonstrate the performance of the end-to-end URL2Video pipeline on a variety of existing web pages. Below we highlight an example result where URL2Video converts a page that embeds multiple short video clips into a 12-second output video. Note how the pipeline makes automatic editing decisions on font and color choices, timing, and content ordering in a video captured from the source page. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XmSTfFgfZBE/X5r4buwezaI/AAAAAAAAGvg/2REw-HvcQeITvtJyMRYa6_SMxIh-MlCcgCLcBGAsYHQ/s800/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"747\" data-original-width=\"800\" src=\"https://1.bp.blogspot.com/-XmSTfFgfZBE/X5r4buwezaI/AAAAAAAAGvg/2REw-HvcQeITvtJyMRYa6_SMxIh-MlCcgCLcBGAsYHQ/s16000/image2.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">URL2Video identifies key content from our <a href=\"https://www.google.com/search/howsearchworks/\">Google Search</a> introduction page (<b>top</b>), including headings and video assets. It converts them into a video by considering the presentation flow, the source design and the output constraints (a 12-second landscape video; <b>bottom</b>).</td></tr></tbody></table><p>The video below provides further demonstration: </p><div class=\"separator\" style=\"clear: both; text-align: center;\"><iframe class=\"BLOG_video_class\" allowfullscreen=\"\" youtube-src-id=\"3yFYc-Wet8k\" width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/3yFYc-Wet8k\"></iframe></div><p>To evaluate the automatically-generated videos, we conducted a user study with designers at Google. Our results show that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.  </p><p><b>Next steps</b><br>While this current research focuses on the visual presentation, we are developing new techniques that support the audio track and a voiceover in video editing. All in all, we envision a future where creators focus on making high-level decisions and an ML model interactively suggests detailed temporal and graphical edits for a final video creation on multiple platforms. </p><p><b>Acknowledgments</b><br><em>We greatly thank our paper co-authors, Zheng Sun (Research) and Katrina Panovich (YouTube). We would also like to thank our colleagues who contributed to URL2Video, (in alphabetical order of last name) Jordan Canedy, Brian Curless, Nathan Frey, Madison Le, Alireza Mahdian, Justin Parra, Emily Ryan, Mogan Shieh, Sandor Szego, and Weilong Yang. We are grateful to receive the support from our leadership, Tomas Izo, Rahul Sukthankar, and Jay Yagnik.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=74zfVk2YBt4:pYvJ-doj310:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/74zfVk2YBt4\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Thu, 29 Oct 2020 17:28:00 +0000","feedId":3793,"bgimg":"https://1.bp.blogspot.com/-9vM8BsGVXNY/X5r3-ZYwX-I/AAAAAAAAGvQ/ai5-yIIg4gcQNBQGvzfwrLghPSc9bgTogCLcBGAsYHQ/s16000/image3.gif","linkMd5":"2512b6b0d02eb9bf3b6b64be4a14cc02","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn72@2020_3/2020/11/03/05-54-57-313_5817cbba928c9298.webp","destWidth":900,"destHeight":506,"sourceBytes":463619,"destBytes":258512,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-9vM8BsGVXNY/X5r3-ZYwX-I/AAAAAAAAGvQ/ai5-yIIg4gcQNBQGvzfwrLghPSc9bgTogCLcBGAsYHQ/s16000/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn72@2020_3/2020/11/03/05-54-57-313_5817cbba928c9298.webp","https://1.bp.blogspot.com/-0TU46rroWzo/X5r4HZDiWSI/AAAAAAAAGvU/lcqd_ao3jec3J4HAG2MK8FZOT4U53-GbACLcBGAsYHQ/s16000/image4.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn80@2020_2/2020/11/03/05-55-06-010_ff1ec667c11e7673.webp","https://1.bp.blogspot.com/-5NG1bgFx2oI/X5r4RA8k4NI/AAAAAAAAGvc/59QzCbjMAMIMkSkCzWWu6eCURGKtRDR2ACLcBGAsYHQ/s16000/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn92@2020_3/2020/11/03/05-54-58-774_0ed7b922ad377743.webp","https://1.bp.blogspot.com/-XmSTfFgfZBE/X5r4buwezaI/AAAAAAAAGvg/2REw-HvcQeITvtJyMRYa6_SMxIh-MlCcgCLcBGAsYHQ/s16000/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn88@2020_5/2020/11/03/05-55-16-610_6aa3422ec8426614.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn75@2020_5/2020/11/03/05-54-58-543_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/74zfVk2YBt4":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn83@2020_2/2020/11/03/05-54-58-435_16b51d39f1ef1a3c.webp"},"publishedOrCreatedDate":1604382892856}],"record":{"createdTime":"2020-11-03 13:54:52","updatedTime":"2020-11-03 13:54:52","feedId":3793,"fetchDate":"Tue, 03 Nov 2020 05:54:52 +0000","fetchMs":21,"handleMs":42,"totalMs":25132,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"6923885023856883326b4ebb589fbdad","hostName":"us-017*","requestId":"1ab5a2d8161e4ffabfa741c4d9802214_3793","contentType":"text/xml; charset=UTF-8","totalBytes":1596418,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":6,"articlesImgsGithubTotal":6,"successGithubMap":{"myreaderx15":1,"myreaderx27":1,"myreaderx12":1,"myreaderx5oss":1,"myreaderx31":1,"myreaderx19":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:29:31","updatedTime":"2020-09-01 10:35:46","id":3793,"name":"Google AI Blog","url":"http://feeds.feedburner.com/blogspot/gJZg","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn63@2020_3/2020/09/01/02-35-46-379_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":258512,"tmpBodyImgCdnBytes":1337906,"tmpBgImgCdnBytes":0,"extra4":{"start":1604382892766,"total":0,"statList":[{"spend":49,"msg":"获取xml内容"},{"spend":42,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":19501,"msg":"正文链接上传到cdn"}]},"extra5":6,"extra6":6,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-006.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-53.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-031.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe65.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-019.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-9vM8BsGVXNY/X5r3-ZYwX-I/AAAAAAAAGvQ/ai5-yIIg4gcQNBQGvzfwrLghPSc9bgTogCLcBGAsYHQ/s16000/image3.gif","sourceStatusCode":200,"destWidth":900,"destHeight":506,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn72@2020_3/2020/11/03/05-54-57-313_5817cbba928c9298.webp","sourceBytes":463619,"destBytes":258512,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":5523,"convertSpendMs":4295,"createdTime":"2020-11-03 13:54:52","host":"us-002*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","linkMd5ListStr":"2512b6b0d02eb9bf3b6b64be4a14cc02,2512b6b0d02eb9bf3b6b64be4a14cc02","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"452.8 KB","destSize":"252.5 KB","compressRate":"55.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/74zfVk2YBt4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn83@2020_2/2020/11/03/05-54-58-435_16b51d39f1ef1a3c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":808,"convertSpendMs":3,"createdTime":"2020-11-03 13:54:58","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","linkMd5ListStr":"2512b6b0d02eb9bf3b6b64be4a14cc02","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn75@2020_5/2020/11/03/05-54-58-543_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":890,"convertSpendMs":4,"createdTime":"2020-11-03 13:54:58","host":"europe65*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","linkMd5ListStr":"2512b6b0d02eb9bf3b6b64be4a14cc02","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-5NG1bgFx2oI/X5r4RA8k4NI/AAAAAAAAGvc/59QzCbjMAMIMkSkCzWWu6eCURGKtRDR2ACLcBGAsYHQ/s16000/image1.png","sourceStatusCode":200,"destWidth":1999,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn92@2020_3/2020/11/03/05-54-58-774_0ed7b922ad377743.webp","sourceBytes":572474,"destBytes":95476,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":1285,"convertSpendMs":215,"createdTime":"2020-11-03 13:54:58","host":"us-006*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","linkMd5ListStr":"2512b6b0d02eb9bf3b6b64be4a14cc02","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"559.1 KB","destSize":"93.2 KB","compressRate":"16.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-0TU46rroWzo/X5r4HZDiWSI/AAAAAAAAGvU/lcqd_ao3jec3J4HAG2MK8FZOT4U53-GbACLcBGAsYHQ/s16000/image4.gif","sourceStatusCode":200,"destWidth":900,"destHeight":413,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn80@2020_2/2020/11/03/05-55-06-010_ff1ec667c11e7673.webp","sourceBytes":390038,"destBytes":235196,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":8752,"convertSpendMs":7140,"createdTime":"2020-11-03 13:54:58","host":"us-53*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","linkMd5ListStr":"2512b6b0d02eb9bf3b6b64be4a14cc02","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"380.9 KB","destSize":"229.7 KB","compressRate":"60.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-XmSTfFgfZBE/X5r4buwezaI/AAAAAAAAGvg/2REw-HvcQeITvtJyMRYa6_SMxIh-MlCcgCLcBGAsYHQ/s16000/image2.gif","sourceStatusCode":200,"destWidth":800,"destHeight":747,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn88@2020_5/2020/11/03/05-55-16-610_6aa3422ec8426614.webp","sourceBytes":3483615,"destBytes":1006852,"targetWebpQuality":75,"feedId":3793,"totalSpendMs":19482,"convertSpendMs":17996,"createdTime":"2020-11-03 13:54:58","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/74zfVk2YBt4/experimenting-with-automatic-video.html","linkMd5ListStr":"2512b6b0d02eb9bf3b6b64be4a14cc02","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.3 MB","destSize":"983.3 KB","compressRate":"28.9%"}],"successGithubMap":{"myreaderx15":1,"myreaderx27":1,"myreaderx12":1,"myreaderx5oss":1,"myreaderx31":1,"myreaderx19":1},"failGithubMap":{}}