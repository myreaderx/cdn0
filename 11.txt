{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-07-26 00:59:44","updatedTime":"2020-07-25 16:59:44","title":"MySQL Query Performance Troubleshooting: Resource-Based Approach","link":"https://www.percona.com/blog/?p=69859","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL Query Performance Troubleshooting\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69885\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-300x168.png\" alt=\"MySQL Query Performance Troubleshooting\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />When I speak about MySQL performance troubleshooting (or frankly any other database), I <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/04/03/18-things-you-can-do-to-remove-mysql-bottlenecks-caused-by-high-traffic-part-one/\">tend to speak</a> about four primary resources which typically end up being a bottleneck and limiting system performance: CPU, Memory, Disk, and Network.</p>\n<p>It would be great if when seeing what resource is a bottleneck, we could also easily see what queries contribute the most to its usage and optimize or eliminate them. Unfortunately, it is not as easy as it may seem.</p>\n<p>First, MySQL does not really provide very good instrumentation in those terms, and it is not easy to get information on how much CPU usage, Disk IO, or Memory a given query caused.  Second, direct attribution is not even possible in a lot of cases. For example, disk writes from flushing data from the InnoDB buffer pool in the background aggregates writes from multiple queries, or disk what technically is Disk IO &#8211; temporary files, etc. may, in the end, cause major memory load due to caching.</p>\n<p>While exact science on this matter is hard to impossible with MySQL, I think we can get close enough for it to be useful… and this is what I’ll try to do as explained in the rest of this blog post.</p>\n<p><em>This is work in progress so your feedback is very much appreciated!</em></p>\n<p>I have created the dashboard for <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) v2, which combines the System Performance Metrics with MySQL Performance Metrics and Query Execution information to estimate the top queries for each class.</p>\n<p>In addition to the top resources mentioned, I also add locking, which while is not really a “physical” resource but still often a cause of database performance problems.</p>\n<p><img class=\"aligncenter wp-image-69868 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/1-1024x293.png\" alt=\"MySQL Query Performance Troubleshooting\" width=\"900\" height=\"258\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/1-1024x293.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/1-300x86.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/1-200x57.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/1-1536x440.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/1-367x105.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/1.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h2>CPU Analysis</h2>\n<p><img class=\"aligncenter wp-image-69869 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/2-1024x275.png\" alt=\"CPU Analysis\" width=\"900\" height=\"242\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/2-1024x275.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/2-300x80.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/2-200x54.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/2-1536x412.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/2-367x98.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/2.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>In the first part, we see the performance graphs; the first line is system metrics which show utilization and saturation (as in Brendan Gregg’s <a target=\"_blank\" href=\"http://www.brendangregg.com/usemethod.html\">USE Method</a>).  Additionally, to overall CPU, I also included the information about max core utilization, which is rather important for MySQL where you will often have a bottleneck caused one CPU core saturated with a single thread workload rather than running out of CPU capacity for multi-core CPU.</p>\n<p>The second row contains the data which is relevant for high-level CPU usage in MySQL: Connections, Queries, and Handlers. A large number of new connections,  especially TLS, can cause significant CPU usage.  A large number of queries, even trivial ones, can load up CPU, and finally, there are MySQL “Handlers” which correspond to row-level operations. When there is a lot of data being crunched, CPU usage tends to be high.</p>\n<p><span>The purpose of those graphs is to easily allow you to find a problem spot when resource utilization was high. You can zoom in to it by selecting a given interval on the graph and see query activity for that particular activity.</span></p>\n<p>The next section includes the mentioned CPU Intensive Queries:</p>\n<p><img class=\"aligncenter wp-image-69870 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/3-1024x214.png\" alt=\"CPU Intensive Queries\" width=\"900\" height=\"188\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/3-1024x214.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/3-300x63.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/3-200x42.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/3-1536x320.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/3-367x77.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/3.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><span>You can see queries are sorted by CPU Score &#8211; the metric computed based on the number of rows a query examines and changes (there is no direct CPU usage data easily available). We also have additional columns that typically are of interest for CPU intensive queries: number of rows this query kind crunches per second,  the number of such queries, query avg latency, etc.</span></p>\n<p>If you click on QueryID, Query Analytics will open with a focus on this particular query and let you see more details:</p>\n<p><img class=\"aligncenter wp-image-69871 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/4-1024x345.png\" alt=\"Query Analytics\" width=\"900\" height=\"303\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/4-1024x345.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/4-300x101.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/4-200x67.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/4-1536x517.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/4-367x124.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/4.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>This particular query examines 12K rows for each row send; not particularly healthy. If we check out EXPLAIN, we can see it is using FULL table scan because index is missing.</p>\n<p><img class=\"aligncenter size-large wp-image-69872\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/5-1024x141.png\" alt=\"\" width=\"900\" height=\"124\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/5-1024x141.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/5-300x41.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/5-200x28.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/5-1536x212.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/5-367x51.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/5.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h2>Disk IO Analysis</h2>\n<p>For Disk IO, we’re also looking at Disk Utilization and Saturation as Disk IO errors would simply cause a database crash.</p>\n<p><img class=\"aligncenter wp-image-69873 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/6-1024x274.png\" alt=\"Disk IO Analysis\" width=\"900\" height=\"241\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/6-1024x274.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/6-300x80.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/6-200x54.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/6-1536x411.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/6-367x98.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/6.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>We’re looking at both system-level Disk IO Bandwidth as well as “Load” &#8211; how many IO requests, on average, are in flight, as well as Disk IO Latency, which is compared to the medium-term average. If something is wrong with storage, or if it is overloaded, latency tends to spike compared to the norm.</p>\n<p>We&#8217;re also looking at the most important disk consumers on MySQL Level &#8211; InnoDB Data IO, Log IO, and Fsyncs. As you pick the period you’re looking to analyze, you can look at the queries scored by IOScore.</p>\n<p><img class=\"aligncenter wp-image-69874 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/7-1024x216.png\" alt=\"IOScore\" width=\"900\" height=\"190\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/7-1024x216.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/7-300x63.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/7-200x42.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/7-1536x323.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/7-367x77.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/7.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><span>You can also see the columns most relevant for IO intensive queries &#8211; how many reads from the disk query are required, how much load is generated, as well what portion of response time was waiting for disk IO versus total query execution time.</span></p>\n<p>The IO Score (same as all scores) is not super scientific; I just came up with something which looked like it works reasonably well, but the great thing is you can easily customize the scoring to be more meaningful for your environment:</p>\n<p><img class=\"aligncenter wp-image-69875 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/8-1024x233.png\" alt=\"ClickHouse Query\" width=\"900\" height=\"205\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/8-1024x233.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/8-300x68.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/8-200x45.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/8-1536x349.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/8-367x83.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/8.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h2>Network Analysis</h2>\n<p><img class=\"aligncenter wp-image-69876 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/9-1024x348.png\" alt=\"Network Analysis\" width=\"900\" height=\"306\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/9-1024x348.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/9-300x102.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/9-200x68.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/9-1536x522.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/9-367x125.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/9.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>You now get the drill; we see the network usage on the system, the most common errors which can cause the problem, as well as MySQL network-level data traffic, number of concurrent connections, and new connections created (or failed connection attempts).</p>\n<p>And queries with network-specific information:</p>\n<p><img class=\"aligncenter size-large wp-image-69878\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/10-1024x268.png\" alt=\"\" width=\"900\" height=\"236\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/10-1024x268.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/10-300x78.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/10-200x52.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/10-1536x402.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/10-367x96.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/10.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><span>This query generates network bandwidth utilization, how large the average query result set is, etc.</span></p>\n<h2>Memory Analysis</h2>\n<p>At the system level, Physical Memory and Virtual Memory are resources you need to be tracking as well as swap activity, as serious swapping is a performance killer.  For MySQL, I currently only show the most important memory buffers as configured; it would be much better to integrate with memory instrumentation in MySQL 5.7 Performance Schema, which I have not done yet&#8230;</p>\n<p><img class=\"aligncenter wp-image-69879 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/11-1024x404.png\" alt=\"Memory Analysis\" width=\"900\" height=\"355\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/11-1024x404.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/11-300x118.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/11-200x79.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/11-1536x606.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/11-367x145.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/11.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>And when we can get queries showing which are heavy memory users:</p>\n<p><img class=\"aligncenter size-large wp-image-69880\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/12-1024x273.png\" alt=\"\" width=\"900\" height=\"240\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/12-1024x273.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/12-300x80.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/12-200x53.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/12-1536x409.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/12-367x98.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/12.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>I’m using indirect measures here such as the use of temporary tables and filesort to guesstimate memory-intensive queries.</p>\n<h2>Locking Analysis</h2>\n<p>Locking is a database phenomenon so it does not show on the system level in some very particular way. It may show itself as an increased number of context switches, which is why I include the graph here. The other two graphs show us the Load (or the number of average active sessions which were blocked waiting for row-level locks), as well as what portion of the total running session it is, basically to show how much of overall response time is due to locks rather than Disk IO, CPU, etc.</p>\n<p><img class=\"aligncenter wp-image-69881 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/13-1024x189.png\" alt=\"Locking Analysis\" width=\"900\" height=\"166\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/13-1024x189.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/13-300x55.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/13-200x37.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/13-1536x283.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/13-367x68.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/13.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>And here we have the queries:</p>\n<p><img class=\"aligncenter size-large wp-image-69882\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/14-1024x266.png\" alt=\"\" width=\"900\" height=\"234\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/14-1024x266.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/14-300x78.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/14-200x52.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/14-1536x399.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/14-367x95.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/14.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>When it comes to locking in MySQL, you typically have to take care of two kinds of locks. There are InnoDB row locks, and there are other locks &#8211; Tables locks, DML locks, etc. which I show here as Row locks and other locks, highlighting how much load each of these generates.</p>\n<p>We can also see what percent of the particular query response time was taken by waiting on lock, which can give a lot of ideas for optimization.</p>\n<p>Note:  Queries that spent a lot of time waiting on locks may be blocked by other instances of the same queries… or might be blocked by entirely different types of queries, which is much harder to catch. Still, this view will be helpful to troubleshoot many simple locking cases.</p>\n<p><em>Like what you see?</em>  <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Install Percona Monitoring and Management v2</a>  (if you have not done it already) and add the <a target=\"_blank\" href=\"https://grafana.com/grafana/dashboards/12630\">MySQL Query Performance Troubleshooting dashboard</a> from the Grafana Dashboards library!</p>\n<p>Looking for more cool extension dashboards for Percona Monitoring and Management v2? Check out my blog post on using <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/06/17/red-method-for-mysql-performance-analyses/\">RED Method with MySQL</a>.</p>\n<hr />\n<p>Our solution brief &#8220;Get Up and Running with Percona Server for MySQL&#8221; outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/get-and-running-percona-server-mysql?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=mysqlresource&#38;utm_content=solutionbrief\" rel=\"noopener\">Download PDF</a></p>\n","descriptionType":"html","publishedDate":"Wed, 15 Jul 2020 17:00:15 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting.png","linkMd5":"0a9450e2db848cd68f619a8a2a7108d3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_6/2020-07-25/1595696397740_5bfc001ef96e9f02.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting.png","author":"Peter Zaitsev","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_5/2020-07-25/1595696399993_adc3ca8aa5c9be29.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Query-Performance-Troubleshooting-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_3/2020-07-25/1595696401266_c6e72dca6e563acd.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/1-1024x293.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_1/2020-07-25/1595696404033_ea2e56da2e2c3bb9.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/2-1024x275.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_6/2020-07-25/1595696405220_99efaf48f284f564.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/3-1024x214.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_1/2020-07-25/1595696400355_18cb0bc709faa80a.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/4-1024x345.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_2/2020-07-25/1595696399790_fbba55dbed01a90e.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/5-1024x141.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_2/2020-07-25/1595696401779_d2be93f136841459.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/6-1024x274.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_2/2020-07-25/1595696400249_af8635d5d5ccf97a.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/7-1024x216.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_3/2020-07-25/1595696402048_99e9ea6492976a91.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/8-1024x233.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_3/2020-07-25/1595696401582_2665ea2b69c202f7.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/9-1024x348.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_6/2020-07-25/1595696405577_35fad986b081293e.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/10-1024x268.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_2/2020-07-25/1595696405577_3542289d65e0f3af.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/11-1024x404.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_5/2020-07-25/1595696405455_5b947bbdb9c9ec13.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/12-1024x273.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_4/2020-07-25/1595696401952_00bad8e012f659f3.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/13-1024x189.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_6/2020-07-25/1595696404195_d6aaf098eae77948.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/14-1024x266.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_6/2020-07-25/1595696399840_cb1bde3364998e05.webp"}},{"createdTime":"2020-07-26 00:59:50","updatedTime":"2020-07-25 16:59:50","title":"ProxySQL Behavior in the Percona Kubernetes Operator for Percona XtraDB Cluster","link":"https://www.percona.com/blog/?p=69617","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"ProxySQL Percona Kubernetes Operator\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span><img class=\"alignright size-medium wp-image-69627\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-300x168.png\" alt=\"ProxySQL Percona Kubernetes Operator\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />The <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Percona Kubernetes Operator for Percona XtraDB Cluster</a>(PXC) comes with ProxySQL as part of the deal. And to be honest, the behavior of ProxySQL is pretty much the same as in a regular non-k8s deployment of it. So why bother to write a blog about it? Because what happens around ProxySQL in the context of the operator is actually interesting.</span></p>\n<p><span>ProxySQL is deployed on its own POD (that can be scaled as well as the PXC Pods can). Each ProxySQL Pod has its own ProxySQL Container and a sidecar container. If you are curious, you can find out which node holds the pod by running</span></p><pre class=\"crayon-plain-tag\">kubectl describe pod cluster1-proxysql-0 | grep Node:\nNode: ip-192-168-37-111.ec2.internal/192.168.37.111</pre><p><span>Login into and ask for the running containers. You will see something like this:</span></p><pre class=\"crayon-plain-tag\">[root@ip-192-168-37-111 ~]# docker ps | grep -i proxysql\nd63c55d063c5        percona/percona-xtradb-cluster-operator                            \"/entrypoint.sh /usr…\"   2 hours ago         Up 2 hours                              k8s_proxysql-monit_cluster1-proxys\nql-0_pxc_631a2c34-5de2-4c0f-b02e-cb077df4ee13_0\nd75002a3847e        percona/percona-xtradb-cluster-operator                            \"/entrypoint.sh /usr…\"   2 hours ago         Up 2 hours                              k8s_pxc-monit_cluster1-proxysql-0_\npxc_631a2c34-5de2-4c0f-b02e-cb077df4ee13_0\ne34d551594a8        percona/percona-xtradb-cluster-operator                            \"/entrypoint.sh /usr…\"   2 hours ago         Up 2 hours                              k8s_proxysql_cluster1-proxysql-0_p\nxc_631a2c34-5de2-4c0f-b02e-cb077df4ee13_0</pre><p><span>Now, what’s the purpose of the sidecar container in this case? To find out if there are new PXC nodes (pods) or on the contrary, PXC pods have been removed (due to scale down) and configure ProxySQL accordingly.</span></p>\n<h2>Adding and Removing PXC Nodes (Pods)</h2>\n<p><span>Let’s see it in action. A regular PXC kubernetes deployment with 3 PXC pods, like this:</span></p><pre class=\"crayon-plain-tag\">kubectl get pod\nNAME                                                   READY   STATUS      RESTARTS   AGE\ncluster1-proxysql-0                                    3/3     Running     0          106m\ncluster1-proxysql-1                                    3/3     Running     0          106m\ncluster1-proxysql-2                                    3/3     Running     0          106m\ncluster1-pxc-0                                         1/1     Running     0          131m\ncluster1-pxc-1                                         1/1     Running     0          128m\ncluster1-pxc-2                                         1/1     Running     0          129m</pre><p>Will have the mysql_server information as following:</p><pre class=\"crayon-plain-tag\">mysql&#62; select hostgroup_id,hostname,status, weight from runtime_mysql_servers;\n+--------------+---------------------------------------------------+--------+--------+\n| hostgroup_id | hostname                                          | status | weight |\n+--------------+---------------------------------------------------+--------+--------+\n| 11           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n+--------------+---------------------------------------------------+--------+--------+\n5 rows in set (0.00 sec)</pre><p><span>What do we have?</span></p>\n<ul>\n<li><span>3 PXC pods</span></li>\n<li><span>3 ProxySQL POD</span></li>\n<li><span>The 3 PXC pods (or nodes) registered inside ProxySQL</span></li>\n<li><span>And several host groups.</span></li>\n</ul>\n<p><span>What are those host groups?</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select * from runtime_mysql_galera_hostgroups\\G\n*************************** 1. row ***************************\n       writer_hostgroup: 11\nbackup_writer_hostgroup: 12\n       reader_hostgroup: 10\n      offline_hostgroup: 13\n                 active: 1\n            max_writers: 1\n  writer_is_also_reader: 2\nmax_transactions_behind: 100\n                comment: NULL\n1 row in set (0.01 sec)</pre><p><span>ProxySQL is using the native galera support and has defined a writer hg, a backup writer hg, and a reader hg. Looking back at the server configuration we have 1 writer, 2 readers, and those same 2 readers are also backup writers.</span></p>\n<p><span>And what are the query rules?</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select rule_id, username, match_digest, active, destination_hostgroup from runtime_mysql_query_rules;\n+---------+--------------+---------------------+--------+-----------------------+\n| rule_id | username     | match_digest        | active | destination_hostgroup |\n+---------+--------------+---------------------+--------+-----------------------+\n| 1       | clustercheck | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 2       | clustercheck | ^SELECT             | 1      | 10                    |\n| 3       | monitor      | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 4       | monitor      | ^SELECT             | 1      | 10                    |\n| 5       | root         | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 6       | root         | ^SELECT             | 1      | 10                    |\n| 7       | xtrabackup   | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 8       | xtrabackup   | ^SELECT             | 1      | 10                    |\n+---------+--------------+---------------------+--------+-----------------------+\n8 rows in set (0.00 sec)</pre><p><span>Now, let’s scale up the deployment and add 2 more PXC pods:</span></p><pre class=\"crayon-plain-tag\">kubectl patch pxc cluster1 --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/pxc/size\", \"value\": 5 }]'</pre><p>And let&#8217;s check the PODs</p><pre class=\"crayon-plain-tag\">kubectl get pods\nNAME                                                   READY   STATUS      RESTARTS   AGE\ncluster1-proxysql-0                                    3/3     Running     0          124m\ncluster1-proxysql-1                                    3/3     Running     0          124m\ncluster1-proxysql-2                                    3/3     Running     0          124m\ncluster1-pxc-0                                         1/1     Running     0          149m\ncluster1-pxc-1                                         1/1     Running     0          146m\ncluster1-pxc-2                                         1/1     Running     0          147m\ncluster1-pxc-3                                         1/1     Running     0          2m53s\ncluster1-pxc-4                                         1/1     Running     0          2m10s</pre><p><span>And now the transition inside ProxySQL:</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select hostgroup_id,hostname,status, weight from runtime_mysql_servers;\n+--------------+---------------------------------------------------+--------+--------+\n| hostgroup_id | hostname                                          | status | weight |\n+--------------+---------------------------------------------------+--------+--------+\n| 11           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n+--------------+---------------------------------------------------+--------+--------+\n5 rows in set (0.00 sec)\n\nmysql&#62; select hostgroup_id,hostname,status, weight from runtime_mysql_servers;\n+--------------+---------------------------------------------------+--------------+--------+\n| hostgroup_id | hostname                                          | status       | weight |\n+--------------+---------------------------------------------------+--------------+--------+\n| 11           | cluster1-pxc-3.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 13           | cluster1-pxc-4.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 10           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 10           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 10           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 12           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 12           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 12           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE       | 1000   |\n| 11           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | OFFLINE_HARD | 1000   |\n+--------------+---------------------------------------------------+--------------+--------+\n9 rows in set (0.00 sec)\n\nmysql&#62; select hostgroup_id,hostname,status, weight from runtime_mysql_servers;\n+--------------+---------------------------------------------------+--------+--------+\n| hostgroup_id | hostname                                          | status | weight |\n+--------------+---------------------------------------------------+--------+--------+\n| 11           | cluster1-pxc-4.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-3.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 10           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-3.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-0.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-2.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n| 12           | cluster1-pxc-1.cluster1-pxc.pxc.svc.cluster.local | ONLINE | 1000   |\n+--------------+---------------------------------------------------+--------+--------+\n9 rows in set (0.00 sec)</pre><p><span>What happened? The new PXC nodes were added to ProxySQL and are ready to handle the traffic. We can also see that the previous master node, which was cluster1-pxc-2.cluster1 is now assigned to the reader hg and the new master is cluster1-pxc-4.cluster1</span></p>\n<p><span>And what about the query rules?</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select rule_id, username, match_digest, active, destination_hostgroup from runtime_mysql_query_rules;\n+---------+--------------+---------------------+--------+-----------------------+\n| rule_id | username     | match_digest        | active | destination_hostgroup |\n+---------+--------------+---------------------+--------+-----------------------+\n| 1       | clustercheck | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 2       | clustercheck | ^SELECT             | 1      | 10                    |\n| 3       | monitor      | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 4       | monitor      | ^SELECT             | 1      | 10                    |\n| 5       | root         | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 6       | root         | ^SELECT             | 1      | 10                    |\n| 7       | xtrabackup   | ^SELECT.*FOR UPDATE | 1      | 11                    |\n| 8       | xtrabackup   | ^SELECT             | 1      | 10                    |\n+---------+--------------+---------------------+--------+-----------------------+\n8 rows in set (0.00 sec)</pre><p><span>Same as before. Query rules are not modified when adding/removing PXC pods. </span></p>\n<p>But what happens when rules are modified?</p>\n<h2>Adding and Removing ProxySQL Query Rules</h2>\n<p>In our Operator, we use the <a target=\"_blank\" href=\"https://proxysql.com/blog/proxysql-cluster/\">ProxySQL Native Clustering</a>. What does that means? It means that if the user made configuration changes (via admin port) on one instance, it is automatically distributed to all members. For example, adding a rule:</p>\n<p>We use Pod 0 for adding the rule:</p><pre class=\"crayon-plain-tag\">kubectl exec -it cluster1-proxysql-0 -- mysql -h127.0.0.1 -P6032 -uproxyadmin -padmin_password\nDefaulting container name to proxysql.\nUse 'kubectl describe pod/cluster1-proxysql-0 -n pxc' to see all of the containers in this pod.\nmysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 2270\nServer version: 8.0.18 (ProxySQL Admin Module)\n\nCopyright (c) 2009-2020 Percona LLC and/or its affiliates\nCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&#62; select rule_id, username, match_digest, active, destination_hostgroup from runtime_mysql_query_rules;\nEmpty set (0.00 sec)\n\nmysql&#62; INSERT INTO mysql_query_rules (active, match_digest, destination_hostgroup, cache_ttl,username) VALUES (1, \"^SELECT.*table-dani.*\", 10, NULL, \"root\");\nQuery OK, 1 row affected (0.00 sec)\n\nmysql&#62; LOAD MYSQL QUERY RULES TO RUNTIME;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&#62; select rule_id, username, match_digest, active, destination_hostgroup from runtime_mysql_query_rules;\n+---------+----------+-----------------------+--------+-----------------------+\n| rule_id | username | match_digest          | active | destination_hostgroup |\n+---------+----------+-----------------------+--------+-----------------------+\n| 13      | root     | ^SELECT.*table-dani.* | 1      | 10                    |\n+---------+----------+-----------------------+--------+-----------------------+\n1 row in set (0.00 sec)</pre><p>And the rule is immediately replicated to the other pods, for example, POD 1:</p><pre class=\"crayon-plain-tag\">kubectl exec -it cluster1-proxysql-1 -- mysql -h127.0.0.1 -P6032 -uproxyadmin -padmin_password\nDefaulting container name to proxysql.\nUse 'kubectl describe pod/cluster1-proxysql-1 -n pxc' to see all of the containers in this pod.\nmysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 1977\nServer version: 8.0.18 (ProxySQL Admin Module)\n\nCopyright (c) 2009-2020 Percona LLC and/or its affiliates\nCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&#62; select rule_id, username, match_digest, active, destination_hostgroup from runtime_mysql_query_rules;\n+---------+----------+-----------------------+--------+-----------------------+\n| rule_id | username | match_digest          | active | destination_hostgroup |\n+---------+----------+-----------------------+--------+-----------------------+\n| 13      | root     | ^SELECT.*table-dani.* | 1      | 10                    |\n+---------+----------+-----------------------+--------+-----------------------+\n1 row in set (0.00 sec)</pre><p></p>\n<h3>In Conclusion:</h3>\n<ul>\n<li><span>The <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Percona Kubernetes Operator for Percona XtraDB Cluster</a> is aware of the changes in the pods</span></li>\n<li><span>Modifications are automatically made in order to dispatch traffic to the new pods</span></li>\n<li><span>Query rules are not modified</span></li>\n<li>ProxySQL Cluster is enabled in order to maintain all the PODs in sync</li>\n</ul>\n<hr />\n<p>Percona XtraDB Cluster is a cost-effective and robust clustering solution created to support your business-critical data. It gives you the benefits and features of MySQL along with the added enterprise features of Percona Server for MySQL.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/datasheets/percona-xtradb-cluster?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=proxybehavior&#38;utm_content=datasheet\" rel=\"noopener\">Download Percona XtraDB Cluster Datasheet</a></p>\n","descriptionType":"html","publishedDate":"Fri, 03 Jul 2020 14:24:40 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator.png","linkMd5":"fed856c3b803aa31ea463289810637fe","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_5/2020-07-25/1595696398136_1d86ef668836987a.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator.png","author":"Daniel Guzmán Burgos","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_2/2020-07-25/1595696402205_1db41cb07d123626.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/ProxySQL-Percona-Kubernetes-Operator-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_6/2020-07-25/1595696400052_496c00fa29f6a9cd.webp"}},{"createdTime":"2020-07-26 00:59:40","updatedTime":"2020-07-25 16:59:40","title":"Percona Monitoring Plugins End of Life Notification","link":"https://www.percona.com/blog/?p=69838","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Monitoring Plugins End of Life\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69901\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-300x168.png\" alt=\"Percona Monitoring Plugins End of Life\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Effective August 1, 2020, Percona is moving the Percona Monitoring Plugins to end of life status. This means that no new versions, enhancements, bug fixes, or security updates will be released. The software will continue to be available at our <a target=\"_blank\" href=\"https://www.percona.com/downloads/percona-monitoring-plugins/LATEST/\">download site</a>.</p>\n<p>The Monitoring Plugins provide a suite of predefined monitoring tools for use with other monitoring products. Since the release of <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a>, the Monitoring Plugins became redundant.</p>\n<p>We thank all of the users of Percona Monitoring Plugins for their support over the years and are happy to discuss alternatives. Please contact <a target=\"_blank\" href=\"mailto:info@percona.com\">info@percona.com</a> for more information.</p>\n","descriptionType":"html","publishedDate":"Thu, 16 Jul 2020 15:59:22 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life.png","linkMd5":"553be31e76f0b24de4207c24ccbbe649","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_3/2020-07-25/1595696397468_8b6c322a0c4cf0e1.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life.png","author":"Rick Golba","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_5/2020-07-25/1595696403019_27db5a56c44fdf57.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Monitoring-Plugins-End-of-Life-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_6/2020-07-25/1595696403939_70684094020ef391.webp"}},{"createdTime":"2020-07-26 00:59:53","updatedTime":"2020-07-25 16:59:53","title":"Security Threat Tool Design in Percona Monitoring and Management","link":"https://www.percona.com/blog/?p=68925","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Security Threat Tool Percona Monitoring and Management\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/platform/stt.html\">Security Threat Tool</a> (STT) is the new <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) feature. It provides the ability to run automated security checks across all of your databases and detect potential security problems. For example: empty or default passwords, weak security settings, outdated database versions, misconfiguration, but also more complex and tricky issues.</p>\n<p><img class=\"aligncenter wp-image-69287 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/STT_design.png\" alt=\"Percona Monitoring and Management Security Threat Tool\" width=\"921\" height=\"522\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/STT_design.png 921w, https://www.percona.com/blog/wp-content/uploads/2020/06/STT_design-300x170.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/STT_design-200x113.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/STT_design-367x208.png 367w\" sizes=\"(max-width: 921px) 100vw, 921px\" /></p>\n<p>Security checks developed independently from PMM releases, therefore PMM server downloads, via HTTPS, a list of available security checks from Percona servers each time before starting the STT cycle. Downloaded data is a single, signed YAML file. We use <a target=\"_blank\" href=\"https://github.com/jedisct1/minisign\" target=\"_blank\" rel=\"noopener\">minisign</a> &#8211; <a target=\"_blank\" href=\"https://doc.libsodium.org/\" target=\"_blank\" rel=\"noopener\">libsodium</a> based tool for singing and verification. PMM server contains embedded public Percona keys for signature verification.</p>\n<p>STT check itself is a database dependent query and a script that processes the query result.</p><pre class=\"crayon-plain-tag\">---\nchecks:\n  - version: 1\n    name: mongodb_version\n    type: MONGODB_BUILDINFO\n    script: &#60;script text&#62;\n\n  - version: 1\n    name: mysql_empty_password\n    type: MYSQL_SELECT\n    query: '* FROM mysql.user'\n    script: &#60;script text&#62;</pre><p>&#160;</p>\n<p>The check type field describes the target database type and query statement. For example: <strong>MYSQL_SHOW</strong>, <strong>POSTGRESQL_SELECT</strong>, <strong>MONGODB_GETPARAMETER</strong>, etc. Some checks have an empty query field. In that case, PMM server uses a hardcoded query, which typically returns a wide range of rows. For checks that require specific data to be selected, the query field contains an incomplete query. For security reasons, it doesn&#8217;t contain statements, such as <strong>SELECT</strong>, <strong>SHOW</strong>, etc. PMM concatenates these statements to the content of the query field.  For instance, for type <strong>MYSQL_SELECT</strong>, a query field can contain:</p>\n<p style=\"text-align: center;\"><code>* FROM mysql.user.</code></p>\n<p>And after concatenation the resulting query will be like:</p>\n<p style=\"text-align: center;\"><code>SELECT * FROM mysql.user.</code></p>\n<p>This approach prevents the execution of queries that could harm the user’s data. STT uses only queries that don’t modify or somehow affect any data and this restriction is made on the PMM side.</p>\n<p>Relevant PMM agents execute the resulting query on connected databases and return a serialized set of rows to the PMM server. After that, the server processes the response with the script that detects security threats.</p>\n<p>The script takes a set of rows as a single parameter and returns alerts in case it detects any problem. We use <a target=\"_blank\" href=\"https://github.com/bazelbuild/starlark\" target=\"_blank\" rel=\"noopener\">Starlark</a> as a scripting language, which was originally designed by Google for its build system Basel. Technically Starlark is a Python dialect and it has great <a target=\"_blank\" href=\"https://github.com/google/starlark-go\" target=\"_blank\" rel=\"noopener\">integration with Go</a>, the main language of the PMM server. The server provides some prepared functions for different check versions. For example, the first version of the STT environment provides a function for parsing <a target=\"_blank\" href=\"https://semver.org/\" target=\"_blank\" rel=\"noopener\">semantic version</a>, that is used for detecting outdated database versions. STT scripts can’t do any input/output, can’t access the host machine, and can’t do any interaction with the environment. They can operate only on a query result.</p>\n<p>As a final step, alerts are published to Prometheus Alertmanager, one of the PMM server components. PMM UI uses that data to show the number of problems and detailed information about each of them. For now, any configuration or integration with that built-in alertmanager is not supported. However, we have big plans for alerting, and some features already in our roadmap.</p>\n","descriptionType":"html","publishedDate":"Mon, 29 Jun 2020 17:17:09 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management.png","linkMd5":"e17de17b2815fe2493a96f563369175c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_4/2020-07-25/1595696397569_9abc15e38859f59d.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management.png","author":"Artem Gavrilov","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Security-Threat-Tool-Percona-Monitoring-and-Management-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_3/2020-07-25/1595696404374_8b21eef05c20e8b4.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/STT_design.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_1/2020-07-25/1595696401242_a188a60eafade720.webp"}},{"createdTime":"2020-07-26 00:59:43","updatedTime":"2020-07-25 16:59:43","title":"Backup and Restore of MongoDB Deployment on Kubernetes","link":"https://www.percona.com/blog/?p=69817","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"backup MongoDB Kubernetes\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-70011\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-300x157.png\" alt=\"backup MongoDB Kubernetes\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Every database environment should require a robust backup strategy as its a fundamental requirement to running a successful and robust database. No matter the size of the database, the function of the application, or how technologically advanced a company is, backups are now a requirement for everyone.</p>\n<p>As a Solutions Engineers, we speak to database users from all types of companies, ranging from startups to the most complex database environments being used today. Interestingly enough, while talking about backups, we hear several concerning statements such as, “We never used backups in the past, so we don’t need them in this new environment”, “cloud services never fail” (hint &#8211; they do), and “this cluster is too big to fail.” It&#8217;s never an issue until something happens to your environment and you are unable to recover data and put your entire company at risk.</p>\n<p><span>The adoption of the databases on Kubernetes (K8s) and other cloud-native platforms is definitely on the rise. There are multiple tools and approaches to deploying MongoDB on K8s. Assuming that you already have your MongoDB up and running on K8s, how do you implement a backup strategy? It could be a healthy amount of error-prone, manual work. You&#8217;d need to configure K8s Jobs from scratch, configure mongodump and all its parameters, setup a PersistentVolume, accompanying Claims. Additionally, you would need to take care about your PersistentVolume durability. What about streaming backup to some remote storage or, dealing with the backup of a complex, sharded cluster?</span></p>\n<p><span>There are free tools that help you to streamline the entire MongoDB deployment process on K8s, including backups. The critical nature of backups doesn’t have to cost you any money to implement neither. Percona offers the following free enterprise-grade solutions for MongoDB:</span></p>\n<ul>\n<li><a target=\"_blank\" href=\"https://www.percona.com/software/mongodb\">Percona Distribution for MongoDB</a>:\n<ul>\n<li><a target=\"_blank\" href=\"https://www.percona.com/software/mongodb/percona-server-for-mongodb\"><span>Percona Server for MongoDB (PSMDB)</span></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/software/mongodb/percona-backup-for-mongodb\"><span>Percona Backup for MongoDB (PBM)</span></a></li>\n</ul>\n</li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/software/percona-kubernetes-operators\"><span>Percona Kubernetes Operator for MongoDB (PSMDB Operator)</span></a></li>\n</ul>\n<p><span>Let’s explore how to take and restore backups using PSMDB Operator deployed on AWS Elastic Kubernetes Service (AWS EKS).</span></p>\n<h2>Architecture</h2>\n<p><span>When deployed outside of K8s, PBM requires a running pbm-agent process on each node (next to the mongod instance) in the cluster/replica set. PBM uses it&#8217;s own &#8216;control collections&#8217; in the admin database to store config and relay commands from the user (who uses the &#8220;pbm&#8221; CLI) to the pbm-agent processes. PBM&#8217;s backups are &#8216;logical&#8217; style, the same as mongodump. This means the data is copied using a database driver connection rather than copying the underlying data files on disk.</span></p>\n<p><span>When you deploy PSMDB cluster using PSMDB Operator, pbm-agent is automatically deployed in each pod as a sidecar container next to the mongod container. PSMDB Operator writes commands to the PBM control collections directly (in a way, replacing what outside of the K8s deployments pbm CLI does) controlling the entire backup process. PSMDB Operator supports two types of backups: on-demand and scheduled backups, both being controlled entirely by PSMDB Operator.</span></p>\n<p><span>Backups taken by the PSMDB Operator can be stored in any S3 compatible storage, be it AWS S3, Google Cloud Storage, or locally deployed cloud-native MinIO storage. The backup contains a metadata file, a dump of all collections from your database, and an oplog dump covering the timespan of the backup.</span></p>\n<h2>Configuration</h2>\n<p><span>We have a running PSMDB replica set deployed with PSMDB Operator in AWS EKS K8s cluster. Please note that we use PSMDB Operator v1.4.0 (the newest release at the moment of writing of this article). To deploy it, we followed </span><a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-psmongodb/kubernetes.html\"><span>these instructions</span></a><span> and used all default settings. </span></p>\n<p>Check if your PSMDB cluster is running correctly:</p><pre class=\"crayon-plain-tag\">$ kubectl get pods\n\nNAME                                               READY   STATUS    RESTARTS   AGE\nmy-cluster-name-rs0-0                              2/2     Running   0          113s\nmy-cluster-name-rs0-1                              2/2     Running   2          72s\nmy-cluster-name-rs0-2                              2/2     Running   1          42s\npercona-server-mongodb-operator-568f85969c-5hqbw   1/1     Running   0          22m</pre><p></p>\n<h2>Access secrets: deploy/backup-s3.yaml</h2>\n<p><span>Let’s start by adding our AWS access and secret access keys. The operator will use these keys to access your S3 bucket (all cloud providers have different methods of distributing these keys). The keys that you put into K8s secrets must be base64 encoded. You can encode your keys by running <pre class=\"crayon-plain-tag\">echo -n ‘YOUR_KEY’ | base64</pre>  in bash CLI:</span></p><pre class=\"crayon-plain-tag\">$ cat deploy/backup-s3.yaml\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-cluster-name-backup-s3\ntype: Opaque\ndata:\n  AWS_ACCESS_KEY_ID: #############\n  AWS_SECRET_ACCESS_KEY: ##############</pre><p><span>Create a secret in K8s cluster with the following command:</span></p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/backup-s3.yaml\n\nsecret/my-cluster-name-backup-s3 created</pre><p></p>\n<h2>S3 bucket: deploy/cr.yaml</h2>\n<p><span>Next, we need to edit <pre class=\"crayon-plain-tag\">storages</pre></span><span>section in the deploy/cr.yaml file so we can send our backups to an S3 bucket.</span></p><pre class=\"crayon-plain-tag\">$ cat deploy/cr.yaml \n\nstorages:\n  s3-us-east: #(backup name)\n    type: s3\n    s3:\n      bucket: psmdb-operator-backup   #(bucket name)\n      credentialsSecret: my-cluster-name-backup-s3   #(reference to credentials set in backup-s3.yaml)\n      region: us-east-1</pre><p>Apply changes:</p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/cr.yaml\n\nperconaservermongodb.psmdb.percona.com/my-cluster-name configured</pre><p><span>We are ready to take backups now!</span></p>\n<h2>On-Demand Backup</h2>\n<p><span>On-demand backup can be taken at any point in time. Pbm-control tool is distributed together with the operator code. Based on the requested details it will use pre-configured storage to store the on-demand backup.</span></p>\n<p><span>Let’s start with editing deploy/backup/backup.yaml to ensure we are all set. psmdbCluster should match our cluster name and <pre class=\"crayon-plain-tag\">storageName</pre> the storage defined in previous steps.</span></p><pre class=\"crayon-plain-tag\">$ cat deploy/backup/backup.yaml \n\napiVersion: psmdb.percona.com/v1\nkind: PerconaServerMongoDBBackup\nmetadata:\n  name: backup1\nspec:\n  psmdbCluster: my-cluster-name\n  storageName: s3-us-east</pre><p><span>Run backup by the following command:</span></p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/backup/backup.yaml\n\nperconaservermongodbbackup.psmdb.percona.com/backup1 created</pre><p><span>If we set up everything correctly, our backup should be uploaded to the S3 bucket. To check its status run:</span><span><br />\n</span></p><pre class=\"crayon-plain-tag\">$ kubectl describe perconaservermongodbbackup.psmdb.percona.com/backup1</pre><p><span>If you look at your S3 AWS console, you should see backup files there:</span></p>\n<p><img class=\"size-large wp-image-69823 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-1024x157.png\" alt=\"\" width=\"900\" height=\"138\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-1024x157.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-300x46.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-200x31.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-1536x235.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-367x56.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed.png 1600w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h2>Automated Backup</h2>\n<p><span>The second type of backup is a scheduled backup. The backup:tasks section of the deploy/cr.yaml file can be edited to schedule fully automatically executed backups. We can configure backups using the UNIX cron string format. Let’s say we want to take backups every day at midnight. Let’s edit the deploy/cr.yaml file:</span></p><pre class=\"crayon-plain-tag\">$ cat deploy/cr.yaml \n\ntasks:\n  - name: daily-backup\n    enabled: true\n    schedule: \"0 0 * * *\"\n    storageName: s3-us-west</pre><p>Apply changes:</p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/cr.yaml\n\nperconaservermongodb.psmdb.percona.com/my-cluster-name configured</pre><p></p>\n<h2>Restoring Backups</h2>\n<p><span>To restore a backup, we need to find the backup name. We can obtain a list of all backups by using the following command:</span></p><pre class=\"crayon-plain-tag\">$ kubectl get psmdb-backup\n\nNAME      CLUSTER           STORAGE      DESTINATION            STATUS   COMPLETED   AGE\nbackup1   my-cluster-name   s3-us-east   2020-06-11T07:52:27Z   ready    28m         28m</pre><p><span>Backup restoration configuration is in deploy/backup/restore.yaml. Let’s ensure there’s the appropriate backup name specified:</span></p><pre class=\"crayon-plain-tag\">$ cat deploy/backup/restore.yaml \n\napiVersion: psmdb.percona.com/v1\nkind: PerconaServerMongoDBRestore\nmetadata:\n  name: restore1\nspec:\n  clusterName: my-cluster-name\n  backupName: backup1</pre><p><span>To restore the backup, we execute the following command:</span></p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/backup/restore.yaml\n\nperconaservermongodbrestore.psmdb.percona.com/restore1 created</pre><p><span>To check the backup status use: </span></p><pre class=\"crayon-plain-tag\">$ kubectl describe perconaservermongodbrestore.psmdb.percona.com/restore1</pre><p>&#160;</p>\n<p><span>Percona Kubernetes Operator for MongoDB utilizing Percona Backup for MongoDB is a Kubernetes-idiomatic way to run, backup, and restore a MongoDB replica set. If you’d like to learn more about these tools, check out our </span><a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-psmongodb/index.html\"><span>documentation.</span></a></p>\n<hr />\n<p>Learn more about the history of Oracle, the growth of MongoDB, and what really qualifies software as open source. If you are a DBA, or an executive looking to adopt or renew with MongoDB, this is a must-read!</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/white-papers/mongodb-new-oracle?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=mongokubernetes&#38;utm_content=whitepaper\">Download &#8220;Is MongoDB the New Oracle?&#8221;</a></p>\n","descriptionType":"html","publishedDate":"Mon, 20 Jul 2020 16:01:48 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes.png","linkMd5":"cddda986ff8ef6512ff4ba4cfed1018c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_5/2020-07-25/1595696397808_4bd8ea20467b19a1.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes.png","author":"Michal Nosek","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_6/2020-07-25/1595696404406_ebb2eeac54fb37c9.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/backup-MongoDB-Kubernetes-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_5/2020-07-25/1595696399558_bb52e15f7464861c.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/unnamed-1024x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_6/2020-07-25/1595696401543_07e140b3cc4aff73.webp"}},{"createdTime":"2020-07-26 00:59:41","updatedTime":"2020-07-25 16:59:41","title":"Scaling the Percona Kubernetes Operator for Percona XtraDB Cluster","link":"https://www.percona.com/blog/?p=69714","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Scaling Percona Kubernetes Operator\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span><img class=\"alignright size-medium wp-image-69750\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-300x168.png\" alt=\"Scaling Percona Kubernetes Operator\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />You got yourself a Kubernetes cluster and are now testing our <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Percona Kubernetes Operator for Percona XtraDB Cluster</a>. Everything is working great and you decided that you want to increase the number of Percona XtraDB Cluster (PXC) pods from the default 3, to let’s say, 5 pods. </span></p>\n<p><span>It&#8217;s just a matter of running the following command:</span></p><pre class=\"crayon-plain-tag\">kubectl patch pxc cluster1 --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/pxc/size\", \"value\": 5 }]'</pre><p>Good, you run the command without issues and now you will have 5 pxc pods! Right? Let&#8217;s check out how the pods are being replicated:</p><pre class=\"crayon-plain-tag\">kubectl get pods | grep pxc\ncluster1-pxc-0                                     1/1     Running   0          25m\ncluster1-pxc-1                                     1/1     Running   0          23m\ncluster1-pxc-2                                     1/1     Running   0          22m\ncluster1-pxc-3                                     0/1     Pending   0          13m</pre><p><span>You not only see 4 pods instead of 5 but also the one new pod is stuck in the “Pending” state. </span><span>Further info shows that the kube scheduler wasn’t able to find a node to deploy the pod:</span></p><pre class=\"crayon-plain-tag\">kubectl describe pod cluster1-pxc-3\nName:           cluster1-pxc-3\nNamespace:      pxc\nPriority:       0\n…\n…\n…\nEvents:\n  Type     Reason            Age                  From               Message\n  ----     ------            ----                 ----               -------\n  Warning  FailedScheduling  69s (x5 over 5m28s)  default-scheduler  0/3 nodes are available: 3 node(s) didn't match pod affinity/anti-affinity, 3 node(s) didn't satisfy existing pods anti-affinity rules.</pre><p><span>From that output, we can see what&#8217;s the issue: Affinity. Or more specifically: Anti-affinity</span></p>\n<p><span>Affinity defines eligible pods that can be scheduled (can run) on the node which already has pods with specific labels. Anti-affinity defines pods that are not eligible.</span></p>\n<p><span>The operator provides an option called </span><b>“antiAffinityTopologyKey”</b><span> which can have several values:</span></p>\n<ul>\n<li><b>kubernetes.io/hostname</b><span> &#8211; Pods will avoid residing within the same host.</span></li>\n<li><b>failure-domain.beta.kubernetes.io/zone</b><span> &#8211; Pods will avoid residing within the same zone.</span></li>\n<li><b>failure-domain.beta.kubernetes.io/region</b><span> &#8211; Pods will avoid residing within the same region.</span></li>\n<li><b>none</b><span> &#8211; No constraints are applied. It means that all PXC pods can be scheduled on one Node and you can lose all your cluster because of one Node failure.</span></li>\n</ul>\n<p><span>The default value is kubernetes.io/hostname which pretty much means: “only one Pod per node”</span></p>\n<p><span>In this case, the kubernetes cluster is running on top of 3 aws instances, hence when one tries to increase the number of pods, the scheduler will have trouble finding where to put that new pod.</span></p>\n<h2>Alternatives?</h2>\n<p><span>There are several options. One plain and simple (and obvious) one can be to add new nodes to the k8s cluster. </span></p>\n<p>Another option is to set the anti-affinity to “none”. Now, why one would want to remove the guarantee of having POD distribute among the available nodes? Well, think about lower environments like QA or Staging, where the HA requirements are not hard and you just need to deploy the operator in a couple of nodes (control plane/worker).</p>\n<p><span>Now, here’s how the affinity setting can be changed:</span></p>\n<p><span>Edit the cluster configuration. My cluster is called “cluster1” so the command is:</span></p><pre class=\"crayon-plain-tag\">kubectl edit pxc/cluster1</pre><p><span>Find the line where “antiAffinityTopologyKey” is defined and change “kubernetes.io/hostname” to “none” and save the changes. This modification will be applied immediately.</span></p>\n<p><span>Delete the old pods ONE BY ONE. Kubernetes will spawn a new one, so don’t worry about it. For example, to delete the pod named “cluster1-pxc-0”, run:</span></p><pre class=\"crayon-plain-tag\">kubectl delete pod cluster1-pxc-0</pre><p><span>You will see how the Pods are recreated and the one that was on “pending” moves on:</span></p><pre class=\"crayon-plain-tag\">kubectl get pod\nNAME                                               READY   STATUS              RESTARTS   AGE\ncluster1-pxc-0                                     0/1     ContainerCreating   0          8s\ncluster1-pxc-1                                     1/1     Running             0          4h55m\ncluster1-pxc-2                                     1/1     Running             0          4h53m\ncluster1-pxc-3                                     0/1     ContainerCreating   0          8m25s</pre><p>Finally, the goal of having 5 pods is achieved:</p><pre class=\"crayon-plain-tag\">kubectl get pod\nNAME                                               READY   STATUS    RESTARTS   AGE\ncluster1-pxc-0                                     1/1     Running   0          39m\ncluster1-pxc-1                                     1/1     Running   0          36m\ncluster1-pxc-2                                     1/1     Running   0          37m\ncluster1-pxc-3                                     1/1     Running   0          47m\ncluster1-pxc-4                                     1/1     Running   0          38m</pre><p>But what if one needs a more sophisticated option? One with some degree of guarantee that HA will be met? For those cases, the operator Affinity can use an advanced approach, by using the NodeAffinity with <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity\" target=\"_blank\" rel=\"noopener\">&#8220;preferredDuringSchedulingIgnoredDuringExecution&#8221;</a></p>\n<p>The whole description and configuration is available in the Operator documentation<a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/constraints.html#affinity-and-anti-affinity\" target=\"_blank\" rel=\"noopener\"> &#8220;Binding Percona XtraDB Cluster components to Specific Kubernetes/OpenShift Nodes&#8221;</a></p>\n<p>And also, in the future, the operator will make use of the <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods\" target=\"_blank\" rel=\"noopener\">&#8220;topologySpreadConstraints&#8221;</a> spec to control the degree to which Pods may be unevenly distributed.</p>\n<p>Thanks to Ivan Pylypenko and Mykola Marzhan from the Percona Engineer Team for the guidance.</p>\n<hr />\n<p>We understand that choosing open source software for your business can be a potential minefield. You need to select the best available options, which fully support and adapt to your changing needs. In this white paper, we discuss the key features that make open source software attractive, and why Percona&#8217;s software might be the best option for your business.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/white-papers/when-percona-software-right-choice?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=scalingkubernetes&#38;utm_content=whitepaper\" rel=\"noopener\">Download &#8220;When is Percona Software the Right Choice?&#8221;</a></p>\n","descriptionType":"html","publishedDate":"Thu, 09 Jul 2020 13:59:56 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator.png","linkMd5":"b091df5a4a91f4219b70a0a28aa7eaa9","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_1/2020-07-25/1595696397955_42db5c682f7d2cc8.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator.png","author":"Daniel Guzmán Burgos","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_5/2020-07-25/1595696403570_da18491b1078a15f.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Scaling-Percona-Kubernetes-Operator-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_2/2020-07-25/1595696400422_92247fd6ea085254.webp"}},{"createdTime":"2020-07-26 00:59:41","updatedTime":"2020-07-25 16:59:41","title":"Percona Audit Log Plugin and the Percona Monitoring and Management Security Threat Tool","link":"https://www.percona.com/blog/?p=69858","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Audit Log Plugin\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span><img class=\"alignright size-medium wp-image-70084\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-300x168.png\" alt=\"Percona Audit Log Plugin\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />The Security Threat Tool has been available since <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> 2.6.0 (PMM) and starting from that first release it is proving to be a severely-needed feature. If you&#8217;re not familiar with the Security Threat Tool (STT), it helps you to ensure compliance as you have the ability to run checks daily, get alerts when non-compliance is found, and audit your security check history. Instructions on how to set up the STT can be found in this excellent post called</span> <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/05/19/running-security-threat-tool-in-percona-monitoring-and-management-for-the-first-time/\"><span>Running Security Threat Tool In Percona Monitoring and Management For the First Time</span></a><span>.</span></p>\n<p><span>Now, one of the promises of PMM is the ability to be customizable and that is still true with the STT, and this blog is proof. If you attended our webinar</span> <a target=\"_blank\" href=\"https://www.percona.com/resources/webinars/how-percona-monitoring-and-management-pmm-improves-database-security\"><span>&#8220;How Percona Monitoring and Management (PMM) Improves Database Security&#8221;</span></a><span> you might have seen that we demonstrated a new dynamic check. Here&#8217;s the process of how we made it work.</span></p>\n<p><span>While thinking about ways to improve database security, the obvious data source was clear: The</span><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/management/audit_log_plugin.html\"> <span>Percona Audit Log Plugin</span></a><span>. Our plugin is an alternative to the MySQL Enterprise Audit Log Plugin and provides detailed monitoring and logging about the activity performed on a specific database server.</span></p>\n<p>And what exactly is the info that would be checked by the STT? Access attempts. Let&#8217;s talk about that briefly:</p>\n<h2>Failed Access Attempts</h2>\n<p><span>By failed access attempts I mean every connection that didn&#8217;t make it through the authentication stage (a great indication that bad things could be happening), like:</span></p>\n<ul>\n<li>Wrong user, and whatever the password is</li>\n<li>Correct user, wrong password</li>\n<li>Correct user, no password</li>\n</ul>\n<p>One of the logs produced by the Audit Log is the &#8220;Connect/Disconnect&#8221; which will add a record with the NAME value of &#8220;Connect&#8221; for every user logged in or login failed. Perfect info for our purposes! Let&#8217;s create a custom check!&#8230;.or not so fast.</p>\n<p>The STT checks for MySQL in particular (remember that the STT works not only with MySQL but also for MongoDB, PostgreSQL, and DBaaS like RDS) can only be queries of the kind SHOW (like SHOW STATUS) or read queries (SELECTs) and the Audit Log Plugin writes to either a file or SysLog.</p>\n<p><span>So what can we do to use the valuable info inside the Audit Log file? One can think of several options, but the one we liked was to stream the records to a MySQL table. Here&#8217;s how:</span></p>\n<h2>Audit Log Stream to MySQL</h2>\n<p><span>Before actually thinking on how to ingest a table, the question to answer is “How do we want to store that data?” The audit log plugin supports four log formats: OLD, NEW, JSON, and CSV. OLD and NEW formats are based on XML, where the former outputs log record properties as XML attributes and the latter as XML tags. Information logged is the same in all four formats.</span></p>\n<p><span>Now, JSON is a language both MySQL (since 5.7) and the Audit Log plugin speak natively. So JSON it is! A simple table was created with a JSON type field:</span></p><pre class=\"crayon-plain-tag\">mysql&#62; show create table audit_data\\G\n*************************** 1. row ***************************\n       Table: audit_data\nCreate Table: CREATE TABLE `audit_data` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `info` json DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n1 row in set (0.00 sec)</pre><p>And the way to instruct the plugin to use JSON as the output format is by changing the value of the variable <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/management/audit_log_plugin.html#audit_log_format\" target=\"_blank\" rel=\"noopener\">audit_log_format</a>.</p>\n<p><span>Next step: data ingestion.</span></p>\n<h2>Streaming</h2>\n<p><span>The idea is to insert data into the table as soon as it’s available in the log file. The solution applied was to use something called named pipes for linux (not to be confused with the named pipes used on Windows to connect to MySQL). A</span> <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Named_pipe\"><span>named pipe</span></a><span> (also known as a FIFO) is just a way to communicate with processes using the filesystem.</span></p>\n<p>The whole process is this:</p>\n<ul>\n<li>Create a fifo file</li>\n<li>Tail the audit log file and send the output to the fifo file</li>\n<li>Using a simple script, read from the fifo file and insert the record into MySQL</li>\n</ul>\n<p>Steps:</p><pre class=\"crayon-plain-tag\">mkfifo /root/danipipe  #Create the named pipe\nexec 3&#60;&#62;/root/danipipe #Open the file in a non-blocking way so it doesn't close when the pipe is read\ntail -f /var/lib/mysql/audit.log 1&#62;/root/danipipe 2&#62;&#38;1   #Send the log file contents to the pipe as soon as is written to the log file</pre><p>The script to continuously read the pipe is as follows:</p><pre class=\"crayon-plain-tag\">#/bin/bash\npipe=/root/danipipe\n\nwhile true; do\n    if read line &#60;$pipe; then if [[ \"$line\" == 'quit' ]]; then break fi query=\"INSERT INTO percona.audit_data (info) VALUES(\\\"${line//\\\"/\\\\\\\"}\\\")\" echo $query &#62; /tmp/dani\n        eval mysql -uaudit_user -pXXXXXX &#60; /tmp/dani\n    fi\ndone</pre><p>Run the script and it will be executed until is explicitly terminated. As an alternative, you can also daemonize it with SystemD to guarantee re spawn. And now we have the audit data into a MySQL table. But before continuing, here&#8217;s the full audit log config used:</p><pre class=\"crayon-plain-tag\">mysql&#62; show variables like 'audit_log%';\n+-----------------------------+------------------------------------+\n| Variable_name               | Value                              |\n+-----------------------------+------------------------------------+\n| audit_log_buffer_size       | 1048576                            |\n| audit_log_exclude_accounts  | pmm@localhost,audit_user@localhost |\n| audit_log_exclude_commands  |                                    |\n| audit_log_exclude_databases |                                    |\n| audit_log_file              | audit.log                          |\n| audit_log_flush             | OFF                                |\n| audit_log_format            | JSON                               |\n| audit_log_handler           | FILE                               |\n| audit_log_include_accounts  |                                    |\n| audit_log_include_commands  |                                    |\n| audit_log_include_databases |                                    |\n| audit_log_policy            | LOGINS                             |\n| audit_log_rotate_on_size    | 104857600                          |\n| audit_log_rotations         | 10                                 |\n| audit_log_strategy          | SYNCHRONOUS                        |\n| audit_log_syslog_facility   | LOG_USER                           |\n| audit_log_syslog_ident      | percona-audit                      |\n| audit_log_syslog_priority   | LOG_INFO                           |\n+-----------------------------+------------------------------------+</pre><p>The relevant variables for this case are:</p>\n<ul>\n<li>audit_log_format = JSON.</li>\n<li>audit_log_handler = FILE (instead of SysLog)</li>\n<li>audit_log_policy = LOGINS</li>\n<li>audit_log_exclude_accounts = pmm@localhost,audit_user@localhost (To avoid records related to the PMM user and the user that inserts data from the audit log)</li>\n</ul>\n<p><span>And now, we need a query. Here&#8217;s what a record for a failed login looks like:</span></p><pre class=\"crayon-plain-tag\">{\"audit_record\": {\"db\": \"\", \"ip\": \"\", \"host\": \"localhost\", \"name\": \"Connect\", \"user\": \"root\", \"record\": \"77248798_2020-07-13T15:20:46\", \"status\": 1045, \"os_login\": \"\", \"priv_user\": \"root\", \"timestamp\": \"2020-07-14T18:36:42 UTC\", \"proxy_user\": \"\", \"connection_id\": \"112777\"}}</pre><p>We need to look for records on that the status value is different than zero. Another condition that we added is that it&#8217;s gotta be records from the last 24 hours. So this is the query that would retrieve that data:</p><pre class=\"crayon-plain-tag\">SELECT \n\tconcat(json_extract(info, \"$.audit_record.user\"),\"@\",json_extract(info, \"$.audit_record.host\"),\" - \",json_extract(info, \"$.audit_record.timestamp\")) AS name\nFROM \n\taudit_data\nWHERE \n\tinfo-&#62;&#62;'$.audit_record.name' = 'Connect'\n\tAND info-&#62;&#62;'$.audit_record.status' != '0'\n\tAND DATE(info-&#62;&#62;'$.audit_record.timestamp') &#62; DATE(DATE_SUB(NOW(), INTERVAL 24 HOUR))\nORDER BY id DESC</pre><p>Finally, we can write a custom check.</p>\n<h2>STT Custom Checks</h2>\n<p><span>Now I need to level with you, this part isn’t quite ready for public consumption yet as the PMM development team is still finalizing how end-users will be able to create their own checks, but this is meant to show you what will be possible and get you thinking of how YOU can create your own checks to ensure your company isn’t featured on the front page of “<em>I Didn’t Take Security Seriously</em> <em>Weekly</em>.&#8221;</span></p>\n<p><span>For writing custom checks, when using docker as the PMM Server deploy method, one needs to login to the pmm-server container and edit the custom checks file.  </span></p>\n<p>With the help from the VP of Engineering &#8211; Platform, Steve Hoffman, who wrote the actual check code, we ended up with the following lines of code:</p><pre class=\"crayon-plain-tag\">- version: 1\n  name: mysql_failed_login\n  type: MYSQL_SELECT\n  query: json_extract(info, '$.audit_record.user') as name, json_extract(info, \"$.audit_record.host\") as host, json_extract(info, \"$.audit_record.timestamp\") as timestamp from percona.audit_data where info-&#62;&#62;'$.audit_record.name' = 'Connect' and info-&#62;&#62;'$.audit_record.status' != '0' and DATE(info-&#62;&#62;'$.audit_record.timestamp') &#62; DATE(DATE_SUB(NOW(), INTERVAL 24 HOUR)) order by id desc \n  script: |\n    def check(rows):\n        \"\"\"\n        This check returns a warning of a failed login attempt.\n        \"\"\"\n        results = []\n        for row in rows:\n            results.append(\"{}@{} - {}\".format(row.get(\"name\"), row.get(\"host\"), row.get(\"timestamp\")))\n        count = len(results)\n        if count:\n            desc = \"1 user\"\n            if count &#62; 1:\n                desc = \"{} users\".format(count)\n            return [{\n                \"summary\": \"Trivial - Failed Login Detected\",\n                \"description\": \"{} - {}\".format(desc, results),\n                \"severity\": \"notice\",\n                \"labels\": {\n                    \"count\": str(count),\n                },\n            }]\n        return []</pre><p><span>We are ready to test the check and, in the next release of PMM, you’ll be able to click a button on the Check Results page to run the checks immediately.</span></p>\n<h2>Tests</h2>\n<p>Let&#8217;s deliberately execute some fail to connect attempts:</p><pre class=\"crayon-plain-tag\">[root@ip-192-168-1-200 ~]# mysql -uiexists\nERROR 1045 (28000): Access denied for user 'iexists'@'localhost' (using password: YES)\n[root@ip-192-168-1-200 ~]# mysql -uroot -pbadpass\nmysql: [Warning] Using a password on the command line interface can be insecure.\nERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)\n[root@ip-192-168-1-200 ~]# mysql -uroot -p\nEnter password:\nERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)</pre><p><span>We have a user that doesn&#8217;t exist, a user that exists with a wrong password, and a user that exists with an empty password. All three failed. And the audit log recorded those actions, and through the streaming process, ended up in the mysql table:</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select concat(json_extract(info, \"$.audit_record.user\"),\"@\",json_extract(info, \"$.audit_record.host\"),\" - \",json_extract(info, \"$.audit_record.timestamp\")) as name from audit_data where info-&#62;&#62;'$.audit_record.name' = 'Connect' and info-&#62;&#62;'$.audit_record.status' != '0' and DATE(info-&#62;&#62;'$.audit_record.timestamp') &#62; DATE(DATE_SUB(NOW(), INTERVAL 24 HOUR)) order by id desc\\G\n*************************** 1. row ***************************\nname: \"root\"@\"localhost\" - \"2020-07-17T02:55:56 UTC\"\n*************************** 2. row ***************************\nname: \"root\"@\"localhost\" - \"2020-07-17T02:55:45 UTC\"\n*************************** 3. row ***************************\nname: \"iexists\"@\"localhost\" - \"2020-07-17T02:55:34 UTC\"\n3 rows in set, 6 warnings (0.00 sec)</pre><p><span>Great! Streaming works, the query works, but did the STT check work? To find out, check the dashboard and yes, you will see that the check worked:</span></p>\n<p><img class=\"size-large wp-image-69931 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-1024x187.png\" alt=\"\" width=\"900\" height=\"164\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-1024x187.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-300x55.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-200x37.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-1536x281.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-2048x375.png 2048w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-367x67.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><span>A failed database check informing that three failed login attempts were recorded in the last 24 hours. Works!</span></p>\n<h3>In Conclusion</h3>\n<p><span>The Security Threat Tool, like everything else in PMM, is highly flexible and can be completely customized. By integrating the tool with other available security measures, you take control of an incredibly powerful security solution. <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Download Percona Monitoring and Management</a> for free today!</span></p>\n","descriptionType":"html","publishedDate":"Wed, 22 Jul 2020 17:59:45 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin.png","linkMd5":"0d53aca1d4bcc7b3cd88863ea4773bb7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_4/2020-07-25/1595696398125_e59acfdece9f79e6.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin.png","author":"Daniel Guzmán Burgos","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_4/2020-07-25/1595696399997_070d6f873c42be5a.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Audit-Log-Plugin-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_5/2020-07-25/1595696401521_40d1fcae0a4a4129.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-07-16-at-22.01.24-1024x187.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_4/2020-07-25/1595696400142_bd81184eb85f4ecf.webp"}},{"createdTime":"2020-07-26 00:59:40","updatedTime":"2020-07-25 16:59:40","title":"New MySQL 8.0.21 and Percona XtraBackup 8.0.13 Issues","link":"https://www.percona.com/blog/?p=69913","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQl Issue\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-70030\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-300x168.png\" alt=\"MySQl Issue\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />On Monday, July 13, 2020, Oracle released MySQL 8.0.21.  This release contained a few new changes that cause issues with Percona XtraBackup.</p>\n<p>First, this release introduced the ability to temporarily disable InnoDB redo logging (see the <a target=\"_blank\" href=\"https://dev.mysql.com/worklog/task/?id=13795\">work log</a> and <a target=\"_blank\" href=\"https://dev.mysql.com/doc/refman/8.0/en/alter-instance.html\">documentation</a>).  If you love your data, this feature should ONLY be used to speed up an initial logical data import and should never be used under a production workload.</p>\n<p>When used, this new feature creates some interesting complications for Percona XtraBackup that you need to be aware of.  The core requirement for XtraBackup to be able to make consistent hot backups is to have some form of redo or write ahead log to copy and read from.  Disabling the InnoDB redo log will prevent XtraBackup from making a consistent backup.  As of XtraBackup 8.0.13, the process of making a backup with the InnoDB redo log disabled will succeed, but the backup will be useless.  We are working towards adding some validation to a future release of XtraBackup that will cause it to detect and fail earlier in the process.</p>\n<p>XtraBackup has the <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-xtrabackup/LATEST/xtrabackup_bin/xbk_option_reference.html#cmdoption-lock-ddl\">&#8211;lock-ddl</a> option that can help prevent a backup from failing due to a DDL operation occurring during the backup.  This option can also be used to help to prevent the toggling of InnoDB redo logs during the backup process.  Here is the current truth table that explains the potential outcome based on the various combinations of starting states and &#8211;lock-ddl:</p>\n<ul>\n<li>With &#8211;lock-ddl\n<ul>\n<li>With REDO enabled at start &#8211; good backup</li>\n<li>With REDO disabled at start &#8211; bad backup, prepare may even succeed</li>\n<li>Can not disable or enable redo during backup</li>\n</ul>\n</li>\n<li>Without &#8211;lock-ddl\n<ul>\n<li>With REDO enabled &#8211; good backup</li>\n<li>With REDO disabled &#8211; bad backup, prepare may even succeed</li>\n<li>Can disable and enable redo during backup, if so, bad backup, prepare may even succeed</li>\n</ul>\n</li>\n</ul>\n<p>&#160;</p>\n<p>Second, <a target=\"_blank\" href=\"https://dev.mysql.com/worklog/task/?id=11819\">this worklog</a> modified the handling of the <code>ALTER UNDO TABLESPACE <em>tablespace_name</em> SET INACTIVE</code> operation and will cause XtraBackup to hang during the backup phase if this operation is executed and exists within the InnoDB redo log.</p>\n<p>Look out for the upcoming release of Percona Server 8.0.21 and Percona XtraBackup 8.0.14 which will have more advanced detection and locking to ensure that you are making good, clean, consistent backups.</p>\n","descriptionType":"html","publishedDate":"Tue, 21 Jul 2020 13:28:41 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue.png","linkMd5":"64ffbee1451e1bc31dac01118ece86e6","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue.png","author":"George O. Lorch III","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_1/2020-07-25/1595696402024_1d522a861b203253.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/MySQl-Issue-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_6/2020-07-25/1595696399628_6b9248b4dfa3a34e.webp"}},{"createdTime":"2020-07-26 00:59:49","updatedTime":"2020-07-25 16:59:49","title":"Beyond Relational Databases: A Focus on Redis, MongoDB, and ClickHouse","link":"https://www.percona.com/blog/?p=70068","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Beyond Relational Databases\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-70074\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-300x157.png\" alt=\"Beyond Relational Databases\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Many of us use and love relational databases… until we try and use them for purposes that aren’t their strong point. Queues, caches, catalogs, unstructured data, counters, and many other use cases, can be solved with relational databases, but are better served by alternative options.</p>\n<p>Recently, Marcos Albe, Support Engineer at Percona, examined the goals, pros and cons, and the good and bad use cases of the most popular alternatives on the market, and looked into some modern open source implementations.</p>\n<h2>Beyond Relational Databases</h2>\n<p>Developers frequently choose the backend store for the applications they produce. Amidst dozens of options, buzzwords, industry preferences, and vendor offers, it&#8217;s not always easy to make the right choice… Even with a map!</p>\n<div id=\"attachment_70069\" style=\"width: 910px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-70069\" class=\"wp-image-70069 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-1024x667.png\" alt=\"Relational Databases\" width=\"900\" height=\"586\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-1024x667.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-300x196.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-200x130.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-1536x1001.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-367x239.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /><p id=\"caption-attachment-70069\" class=\"wp-caption-text\">Image credit: 451 Research (https://451research.com/state-of-the-database-landscape)</p></div>\n<p>Relational databases are generally the go-to option as they are flexible and well-known. But, when used for the wrong purpose, they can end up being a bottleneck down the road.</p>\n<p>Download the full analysis to help you make the right choice for your business. With focus on the three most popular database alternatives: Key-value, Document, and Columnar, get an overview of situations when relational databases might be a poor choice and the alternative options you have to relational databases.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/white-papers/beyond-relational-databases?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=beyondrelational&#38;utm_content=whitepaper\">Download Now</a></p>\n","descriptionType":"html","publishedDate":"Wed, 22 Jul 2020 14:52:31 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases.png","linkMd5":"1e96265ec09eab6d53e7dcab225830d3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_1/2020-07-25/1595696397800_3cfca370ed68b347.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases.png","author":"Rachel Pescador","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_2/2020-07-25/1595696403214_f36c89ebd7dbe568.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Beyond-Relational-Databases-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_5/2020-07-25/1595696400137_5627db9b9d071c42.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Relational-Databases-1024x667.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_5/2020-07-25/1595696403897_bd06d10d5cb31618.webp"}},{"createdTime":"2020-07-26 00:59:41","updatedTime":"2020-07-25 16:59:41","title":"Announcing Percona Live ONLINE: October 20-21, 2020","link":"https://www.percona.com/blog/?p=70042","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-200x105.jpg\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"percona live\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live.jpg 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-70043\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-300x157.jpg\" alt=\"percona live\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-300x157.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-1024x536.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-200x105.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-1140x595.jpg 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-367x192.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live.jpg 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />We’re moving Percona Live Europe ONLINE, and we’re going global!</p>\n<p>After much deliberation and consideration of the ever-changing situation around COVID-19 we have decided it would be the best decision to take this year’s event ONLINE!</p>\n<p>Percona Live ONLINE will take place on October 20-21, 2020 starting at 10:00 am EDT. Last time we streamed 24 hours of nonstop content and it went great — so this time, we decided to boost it and go for 28 hours! Now we can deliver content across the globe in every time zone!</p>\n<p>In May, we had over 6,500 individuals tune into Percona Live ONLINE and we hope even more will join us in October!</p>\n<h2>Speak at Percona Live ONLINE</h2>\n<p>We are also extending the Call for Papers by two weeks. We are seeking talks of 30 minutes, 1 hour, and filler content of up to 5 mins for lightning talks — <a target=\"_blank\" href=\"http://cfp.percona.com\">learn more</a>.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"http://cfp.percona.com\">Submit your Talk</a></p>\n<h2>Registration</h2>\n<p>Registration is now open and FREE for anyone to attend! Pre-registration gives you access to the agenda sneak peeks, links for the live stream, and access to the Percona Live ONLINE virtual viewing and chat room on Slack.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/live/conferences\">Register for Free</a></p>\n<p>We look forward to seeing everyone ONLINE!</p>\n<p><strong>Please note</strong>: <em>Tickets that have been purchased for Percona Live Europe in Amsterdam will be fully refunded.</em></p>\n","descriptionType":"html","publishedDate":"Tue, 21 Jul 2020 18:37:23 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live.jpg","linkMd5":"494e2a33b2f27e00a0019dd2df77f29b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_6/2020-07-25/1595696397649_b90161d43022ec4f.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live.jpg","author":"Bronwyn Campbell","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-200x105.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_1/2020-07-25/1595696401145_fc77286159191750.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/percona-live-300x157.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_6/2020-07-25/1595696403721_9e759f444aeaaf83.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"Adding eBPF-Based Metrics to Percona Monitoring and Management","link":"https://www.percona.com/blog/?p=69350","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"ebpf percona monitoring and management\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69682\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-300x168.png\" alt=\"ebpf percona monitoring and management\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />I wanted to start this post with the words “eBPF is the hot new thing”, but I think it’s already too late to write that. Running eBPF programs in production is becoming less of a peculiarity and more of a normal way to operate. Famously, <a target=\"_blank\" href=\"https://lwn.net/Articles/801871/\" target=\"_blank\" rel=\"noopener\">Facebook runs about 40 BPF programs on each server</a>. There are multiple things you can do with BPF, but as a support engineer here in Percona, I’m mostly interested in the performance observability side of things. Running tools from the <a target=\"_blank\" href=\"https://github.com/iovisor/bcc\" target=\"_blank\" rel=\"noopener\">bcc project</a> and writing short <a target=\"_blank\" href=\"https://github.com/iovisor/bpftrace/\" target=\"_blank\" rel=\"noopener\">bpftrace</a> programs is a great way to get insight into some particular performance problem. However, that’s still mostly not monitoring, at least not in a continuous way. Recently, I became curious to see if it’s possible to add metrics from BPF programs to <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM), and in this blog post, I’ll show that it’s surprisingly easy.</p>\n<p>This post will not cover what’s BPF or means of writing BPF programs. You may watch Peter Zaitsev’s recent webinar “<a target=\"_blank\" href=\"https://www.percona.com/resources/webinars/using-ebpf-linux-performance-analysis\">Using eBPF for Linux Performance Analysis</a>” or read “<a target=\"_blank\" href=\"http://www.brendangregg.com/blog/2019-01-01/learn-ebpf-tracing.html\" target=\"_blank\" rel=\"noopener\">Learn eBPF Tracing: Tutorial and Examples</a>” by Brendan Gregg to understand the basics.</p>\n<h2>Technologies</h2>\n<p>PMM is built on top of Prometheus, and thus the first thing that will be required is an exporter capable of converting BPF program’s output into metrics. Luckily, there’s already <a target=\"_blank\" href=\"https://github.com/cloudflare/ebpf_exporter\" target=\"_blank\" rel=\"noopener\">ebpf_exporter from Cloudflare</a>. Next, we’ll need a reasonably modern kernel (ebpf_exporter’s readme mentions at least 4.14). It would probably be easier to use Ubuntu 20.04, which ships with the 5.4 LTS kernel, but I habitually went for CentOS, specifically version 8. On a side note, it’s possible to use the same approach for CentOS 7, but not all example BPF programs work. Even with CentOS 8, there are some issues along the way. Finally, we’ll need a BPF program. ebpf_exporter project comes with a <a target=\"_blank\" href=\"https://github.com/cloudflare/ebpf_exporter/tree/master/examples\" target=\"_blank\" rel=\"noopener\">set of examples</a>, mimicking the aforementioned bcc tools, and I decided to go with a <a target=\"_blank\" href=\"https://github.com/cloudflare/ebpf_exporter/blob/master/examples/bio-tracepoints.yaml\" target=\"_blank\" rel=\"noopener\">biolatency equivalent based on Kernel tracepoints</a>.</p>\n<h2>Setting up the Environment</h2>\n<p>I used an extremely simple environment based on two CentOS 8 VMs run with Vagrant. <a target=\"_blank\" href=\"https://raw.githubusercontent.com/arronax/scratch/master/pmm-ebpf/Vagrantfile\" target=\"_blank\" rel=\"noopener\">The Vagrantfile used</a> includes installation of the PMM2 server and setting up a monitored node with pmm-admin. Once the VMs are up, the other steps are manual to better show the process.</p>\n<p>There’s just one pre-requisite for ebpf_exporter, and that’s the bcc package with its own dependencies. Unfortunately, at the time of writing this blog, CentOS 8.1 has only the 0.8.0 version of bcc, a rather old one, even though CentOS 7 provides version 0.10.0. Fortunately, in the <a target=\"_blank\" href=\"https://wiki.centos.org/Manuals/ReleaseNotes/CentOSStream#Presenting_CentOS_Stream\">“Stream” version of the OS</a>, you can get the 0.11.0 version. I hope that the non-rolling versions of the OS will get updated packages soon, but for now, that’s how things are.</p>\n<p>To change the version of CentOS 8 to Stream, a single command should be run:</p><pre class=\"crayon-plain-tag\"># dnf install centos-release-stream</pre><p>With the OS side of things sorted out, we can proceed to more interesting stuff.</p>\n<p>1. We’ll need to actually install the bcc package and its dependencies.</p><pre class=\"crayon-plain-tag\"># dnf install bcc\n# dnf info bcc | grep Version\nVersion      : 0.11.0</pre><p>2. Once that’s done, we can test that the tools actually work. We should see PMM’s node_exporter monitoring the system.</p><pre class=\"crayon-plain-tag\"># /usr/share/bcc/tools/opensnoop\nPID    COMM               FD ERR PATH\n/var/run/utmp\n1474   vminfo              5   0 /usr/local/share/dbus-1/system-services\n885    dbus-daemon        -1   2 /usr/share/dbus-1/system-services\n885    dbus-daemon         6   0 /lib/dbus-1/system-services\n885    dbus-daemon        -1   2 /proc/stat\n6030   node_exporter       8   0 /usr/local/percona/pmm2/collectors/textfile-collector/medium-resolution\n6030   node_exporter       8   0 /usr/local/percona/pmm2/collectors/textfile-collector/medium-resolution/example.prom\n6030   node_exporter       8   0 /sys/class/hwmon\n6030   node_exporter       8   0 /proc/stat\n6030   node_exporter       8   0 /proc/loadavg</pre><p><em>A common issue with tools is a kernel and kernel-devel package version mismatch. Just make sure that you have a kernel-devel of the same version as your running kernel.</em></p>\n<p>3. Install the ebpf_exporter. You can build it, but it’s easier to <a target=\"_blank\" href=\"https://github.com/cloudflare/ebpf_exporter/releases\" target=\"_blank\" rel=\"noopener\">get the release version</a>.</p><pre class=\"crayon-plain-tag\"># wget https://github.com/cloudflare/ebpf_exporter/releases/download/v1.2.2/ebpf_exporter-1.2.2.tar.gz\n# tar xf ebpf_exporter-1.2.2.tar.gz\n# cp -ip ebpf_exporter-1.2.2/ebpf_exporter /usr/local/bin/ebpf_exporter\n# /usr/local/bin/ebpf_exporter --help\nusage: ebpf_exporter [&#60;flags&#62;]\n\nFlags:\n  -h, --help                     Show context-sensitive help (also try --help-long and --help-man).\n      --web.listen-address=\":9435\"\n                                 The address to listen on for HTTP requests\n      --config.file=config.yaml  Config file path\n      --debug                    Enable debug\n      --version                  Show application version.</pre><p>4. We’ll also need the <a target=\"_blank\" href=\"https://github.com/cloudflare/ebpf_exporter/blob/master/examples/bio-tracepoints.yaml\" target=\"_blank\" rel=\"noopener\">bio-tracepoints.yaml</a> file from the examples. You can get it alone, but I recommend cloning the whole repo so that you can explore the other examples as well.</p><pre class=\"crayon-plain-tag\"># git clone https://github.com/cloudflare/ebpf_exporter</pre><p>5. Run the exporter and test its output.</p><pre class=\"crayon-plain-tag\"># /usr/local/bin/ebpf_exporter --config.file=ebpf_exporter/examples/bio-tracepoints.yaml\n2020/06/02 12:35:42 Starting with 1 programs found in the config\n2020/06/02 12:35:42 Listening on :9435\n\n# curl -s localhost:9435/metrics | grep enabled_programs\n# HELP ebpf_exporter_enabled_programs The set of enabled programs\n# TYPE ebpf_exporter_enabled_programs gauge\nebpf_exporter_enabled_programs{name=\"bio\"} 1</pre><p><em>It’s pretty easy to set up ebpf_exporter as a systemd service by modifying <a target=\"_blank\" href=\"https://github.com/prometheus/node_exporter/tree/master/examples/systemd\">example files provided with node_exporter</a>. Note that you’ll either need to run the program from root, or set up capabilities, as ebpf_exporter will need CAP_SYS_ADMIN capability.</em></p>\n<p>6. Register newly-added exporter with a local pmm agent.</p><pre class=\"crayon-plain-tag\"># pmm-admin add external --listen-port=9435\nExternal Service added.\nService ID  : /service_id/28c36115-a07c-4520-aca2-9cf0586fa588\nService name: node1-external</pre><p>7. We don’t have any dashboards for the data yet, but we can check raw metrics in Prometheus. Navigate to <a target=\"_blank\" href=\"http://pmm-url/prometheus/\">http://pmm-url/prometheus/</a> to access its UI.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0.png\"><img class=\"aligncenter wp-image-69351 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-1024x509.png\" alt=\"prometheus dashboard showing newly-added ebpf metrics\" width=\"900\" height=\"447\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-1024x509.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-300x149.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-200x99.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-1536x763.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-367x182.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0.png 1600w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<h2>Setting up Grafana Dashboard</h2>\n<p>Now that we have the data, we need to represent it in a clear and convenient way. PMM’s Grafana allows you to set up custom dashboards, so that’s what we’ll need to do. I’ve prepared a very simple dashboard that has panels for BPF-based metrics alongside panels taken from the existing PMM dashboards, based on node_exporter. Dashboard’s <a target=\"_blank\" href=\"https://raw.githubusercontent.com/arronax/scratch/master/pmm-ebpf/eBPF_exporter-panels.json\" target=\"_blank\" rel=\"noopener\">JSON source</a> is available alongside the Vagrantfile.</p>\n<p>Metrics are provided by ebpf_exporter when running a bio program in the form of a <a target=\"_blank\" href=\"https://prometheus.io/docs/concepts/metric_types/#histogram\" target=\"_blank\" rel=\"noopener\">histogram</a>, which data can be used to calculate percentiles and averages, or used raw. Grafana has built-in support for <a target=\"_blank\" href=\"https://grafana.com/docs/grafana/latest/getting-started/intro-histograms/\" target=\"_blank\" rel=\"noopener\">histograms and heatmaps</a>, and in this case, we’re going to be using heatmap panels. Heatmaps allow us to view the distribution over time, unlike the histogram that shows an instant representation of distribution. Looking at distribution changes over time can add more detail into otherwise “flat” data (like 95th percentile), and potentially show some otherwise hidden discontinuities. For example, on the next screenshot, you can see steady streams of 512KiB, 8kb, and 4kb write requests, on the “bio bytes write” panel. Percentile and average values are based on the same prometheus histogram data.</p>\n<p>The screenshot below shows IO characteristics with sysbench running a very basic rw load.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05.png\"><img class=\"aligncenter wp-image-69352 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-1024x1006.png\" alt=\"Grafana Dashboard\" width=\"900\" height=\"884\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-1024x1006.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-300x295.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-153x150.png 153w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-1536x1510.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-32x32.png 32w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-367x361.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-50x50.png 50w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05.png 1920w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<h2>Testing to See We Got The Right Data</h2>\n<p>No monitoring can be considered valid until it has been checked for correctness. As I’ve mentioned, the dashboard includes existing metrics, which we can treat as a source of truth. Thus, we need to test if the new metrics are showing the same load profile without skew. Inherently there will be some misalignment, because of different scraping intervals, but it shouldn’t be significant.</p>\n<p>We can use a <a target=\"_blank\" href=\"https://fio.readthedocs.io/en/latest/fio_doc.html\" target=\"_blank\" rel=\"noopener\">Flexible I/O tester</a> program (fio for short) to generate an IO load profile, and then compare performance observations made by fio itself, node_exporter, and ebpf_exporter. This is not a benchmark, just a way to generate some load with actual performance metrics.</p><pre class=\"crayon-plain-tag\"># fio --name=randrw --rw=randrw -direct=1 --ioengine=libaio --bs=16k --numjobs=4 --rwmixread=30 --size=1G --runtime=1200 --group_reporting --time_based\nrandrw: (g=0): rw=randrw, bs=(R) 16.0KiB-16.0KiB, (W) 16.0KiB-16.0KiB, (T) 16.0KiB-16.0KiB, ioengine=libaio, iodepth=1\n...\nfio-3.19\nStarting 4 processes\nJobs: 4 (f=4): [m(4)][100.0%][r=60.0MiB/s,w=140MiB/s][r=3842,w=8960 IOPS][eta 00m:00s]\nrandrw: (groupid=0, jobs=4): err= 0: pid=4501: Thu Jun 25 12:35:54 2020\n  read: IOPS=3508, BW=54.8MiB/s (57.5MB/s)(64.2GiB/1200001msec)\n    slat (usec): min=4, max=15528, avg=35.06, stdev=37.44\n    clat (nsec): min=1079, max=38747k, avg=726766.24, stdev=988979.33\n     lat (usec): min=38, max=38781, avg=762.07, stdev=990.54\n    clat percentiles (usec):\n     |  1.00th=[  200],  5.00th=[  241], 10.00th=[  273], 20.00th=[  322],\n     | 30.00th=[  363], 40.00th=[  404], 50.00th=[  445], 60.00th=[  498],\n     | 70.00th=[  578], 80.00th=[  742], 90.00th=[ 1319], 95.00th=[ 2212],\n     | 99.00th=[ 5538], 99.50th=[ 6849], 99.90th=[10028], 99.95th=[11207],\n     | 99.99th=[15401]\n   bw (  KiB/s): min=15744, max=89248, per=100.00%, avg=56193.09, stdev=2343.37, samples=9572\n   iops        : min=  984, max= 5578, avg=3511.18, stdev=146.52, samples=9572\n  write: IOPS=8182, BW=128MiB/s (134MB/s)(150GiB/1200001msec); 0 zone resets\n    slat (usec): min=4, max=24021, avg=36.07, stdev=41.72\n    clat (nsec): min=1493, max=26888k, avg=119918.94, stdev=200327.65\n     lat (usec): min=30, max=30857, avg=156.24, stdev=212.02\n    clat percentiles (usec):\n     |  1.00th=[    9],  5.00th=[   33], 10.00th=[   41], 20.00th=[   48],\n     | 30.00th=[   53], 40.00th=[   61], 50.00th=[   73], 60.00th=[   88],\n     | 70.00th=[  111], 80.00th=[  147], 90.00th=[  235], 95.00th=[  347],\n     | 99.00th=[  734], 99.50th=[ 1004], 99.90th=[ 2114], 99.95th=[ 2868],\n     | 99.99th=[ 6652]\n   bw (  KiB/s): min=36640, max=204800, per=100.00%, avg=131068.14, stdev=5343.27, samples=9572\n   iops        : min= 2290, max=12800, avg=8190.75, stdev=334.01, samples=9572\n  lat (usec)   : 2=0.01%, 4=0.07%, 10=0.69%, 20=0.05%, 50=16.74%\n  lat (usec)   : 100=28.54%, 250=19.59%, 500=20.76%, 750=6.96%, 1000=2.05%\n  lat (msec)   : 2=2.75%, 4=1.15%, 10=0.62%, 20=0.03%, 50=0.01%\n  cpu          : usr=0.92%, sys=8.04%, ctx=14104000, majf=2, minf=77\n  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &#62;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &#62;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &#62;=64=0.0%\n     issued rwts: total=4209757,9818981,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=1\nRun status group 0 (all jobs):\n   READ: bw=54.8MiB/s (57.5MB/s), 54.8MiB/s-54.8MiB/s (57.5MB/s-57.5MB/s), io=64.2GiB (68.0GB), run=1200001-1200001msec\n  WRITE: bw=128MiB/s (134MB/s), 128MiB/s-128MiB/s (134MB/s-134MB/s), io=150GiB (161GB), run=1200001-1200001msec\nDisk stats (read/write):\n    dm-0: ios=4212124/9818871, merge=0/0, ticks=3530914/824074, in_queue=4354988, util=84.77%, aggrios=4215345/9819613, aggrmerge=2291/44598, aggrticks=2897415/758011, aggrin_queue=1170573, aggrutil=84.78%\n  sda: ios=4215345/9819613, merge=2291/44598, ticks=2897415/758011, in_queue=1170573, util=84.78%</pre><p>Looking at what FIO showed, we should see on our panels IO load with around 134MB/s write, 57.5MB/s read. Read latency 95% of 2.2ms and write latency 95% of 0.347ms. Let’s inspect the dashboard.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11.png\"><img class=\"aligncenter wp-image-69353 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-800x1024.png\" alt=\"eBPF dashboard\" width=\"800\" height=\"1024\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-800x1024.png 800w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-234x300.png 234w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-117x150.png 117w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-1200x1536.png 1200w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-1600x2048.png 1600w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-367x470.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11.png 1920w\" sizes=\"(max-width: 800px) 100vw, 800px\" /></a></p>\n<p>We can see that the measurements match across the board. While the tail latency of IO can be seen on the non-heatmap panel, as 95% is quite much higher than the average latency, I think that the heatmap panel gives a useful insight into the actual distribution.</p>\n<p>This test doesn’t really show anything about IO size distribution since FIO generates a steady stream of 16kb-sized requests. However, it does show that accounting matches between eBPF-based data, and regular node_exporter data. Note that the “Disk IO Size” panel in PMM has a bug currently, so 16kb requests turned into 8mb ones. The bug for this issue is here: <a target=\"_blank\" href=\"https://jira.percona.com/browse/PMM-6189\">https://jira.percona.com/browse/PMM-6189</a>. Even with FIO pushing the same 16kb requests, you can see there are fluctuations on the average and percentile graphs, which can be easily cross-checked with heatmap-based panels to see what kind of IO load changed the profile.</p>\n<h3>Summary</h3>\n<p>This is not an in-depth blog about writing BPF programs or making sense of their output. However, I believe that it’s now simpler than ever to get more insight into your system’s performance using “advanced tools”, and I tried to show that simplicity with off-the-shelf tools. For now, the main barrier seems to be a prevalence of legacy kernels and possible difficulties obtaining fresher packages. <a target=\"_blank\" href=\"https://github.com/cloudflare/ebpf_exporter/tree/master/benchmark\" target=\"_blank\" rel=\"noopener\">Performance overhead</a> is also a concern, but in most situations, the added value of more insight outweighs the performance hit. In the future, I hope, we’ll see more BPF programs used routinely for constant monitoring.</p>\n<hr />\n<p>Correctly understanding the true cause of database performance problems allows for a quick and efficient resolution – yet enterprises often lack this crucial information. Without it, your solution could require more time and resources than necessary, or inefficiently address the issue. And contrary to popular belief, the problem is not always the database itself!</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/top-5-causes-poor-database-performance?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=ebpfpmm&#38;utm_content=solutionbrief\" rel=\"noopener\">Download: Top 5 Causes of Poor Database Performance</a></p>\n","descriptionType":"html","publishedDate":"Mon, 06 Jul 2020 16:11:13 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management.png","linkMd5":"0eac0eae0e0bf38bbc970d30d5272241","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_4/2020-07-25/1595696397963_d7142a817d8191a2.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management.png","author":"Sergey Kuzmichev","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_4/2020-07-25/1595696402716_4893e96692adee60.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/ebpf-percona-monitoring-and-management-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_6/2020-07-25/1595696399323_56bff12ae49bf053.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/pasted-image-0-1024x509.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_1/2020-07-25/1595696404705_639e8f635f5a43d7.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-14_26_05-1024x1006.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_2/2020-07-25/1595696403534_ec272a4848ddf46b.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/screencapture-192-168-70-110-graph-d-eaDORvzMz-ebpf-exporter-panels-2020-06-25-15_49_11-800x1024.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_3/2020-07-25/1595696401507_836c6fdab96c64e8.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"Preventing MySQL Error 1040: Too Many Connections","link":"https://www.percona.com/blog/?p=69592","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"mysql error 1040\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>One of the most common errors encountered in the MySQL world at large is the infamous <strong>Error 1040</strong>:</p><pre class=\"crayon-plain-tag\">ERROR 1040 (00000): Too many connections</pre><p>What this means in practical terms is that a MySQL instance has reached its maximum allowable limit for client connections.  Until connections are closed, no new connection will be accepted by the server.</p>\n<p>I’d like to discuss some practical advice for preventing this situation, or if you find yourself in it, how to recover.</p>\n<h2>Accurately Tune the max_connections Parameter</h2>\n<p>This setting defines the maximum number of connections that a MySQL instance will accept.  Considerations on “why” you would want to even have a max number of connections are based on resources available to the server and application usage patterns.  Allowing uncontrolled connections can crash a server, which may be considered “worse” than preventing further connections.  Max_connections is a value designed to protect your server, not fix problems related to whatever is hijacking the connections.</p>\n<p>Each connection to the server will consume both a fixed amount of overhead for things like the “thread” managing the connection and the memory used to manage it, as well as variable resources (for instance memory used to create an in-memory table.  It is important to measure the application’s resource patterns and find the point at which exceeding that number of connections will become dangerous.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management (PMM) can help you find these values.</a> Look at the memory usage patterns, threads running, and correlate these with the number of connections.  PMM can also show you spikes in connection activity, letting you know how close to the threshold you’re coming.  Tune accordingly, keeping in mind the resource constraints of the server.</p>\n<p>Seen below is a server with a very steady connection pattern and there is a lot of room between Max Used and Max Connections.</p>\n<p><img class=\"aligncenter wp-image-69594 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-1024x381.png\" alt=\"Preventing MySQL Error 1040\" width=\"900\" height=\"335\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-1024x381.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-300x111.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-200x74.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-1536x571.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-367x136.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM.png 1706w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h2>Avoiding Common Scenarios Resulting in Overuse of Connections</h2>\n<p>Having worked in the Percona Managed Services team for years, I’ve had the first-hand opportunity to see where many businesses get into “trouble” from opening too many connections.  Conventional wisdom says that it will usually be a bad code push where an application will behave badly by not closing its open connections or by opening too many quickly for frivolous reasons.</p>\n<p>There are other scenarios that I’ve seen that will cause this too even if the application is performing “as expected”.  Consider an application stack that utilizes a cache.  Over time the application has scaled up and grown.  Now consider the behavior under load if the cache is completely cleared.  The workers in the application might try to repopulate the cache in mass generating a spike that will overwhelm a server.</p>\n<p>It is important to consider the systems that use the MySQL server and prevent these sorts of edge case behaviors or it might lead to problems.  If possible, it is a good idea to trap errors in the application and if you run into “Too many connections” have the application back off and slip for a bit before a retry to reduce the pressure on the connection pool.</p>\n<h2>Safeguard Yourself From Being Locked Out</h2>\n<p>MySQL actually gives you “breathing” room from being locked out.  In versions 5.xx the SUPER user has a +1 always available connection and in versions 8.xx there is a +1 for users with CONNECTION_ADMIN privileges.  However, many times a system has lax privilege assignments and maybe an application user is granted these permissions and consumes this extra emergency connection.  It is a good idea to audit users and be sure that only true administrators have access to these privileges so that if a server does consume all its available connections, an administrator can step in and take action.  There are other benefits to being strict on permissions.  Remember that the minimum privilege policy is often a best practice for good reason!  And not always just “security”.</p>\n<p>MySQL 8.0.14+ also allows us to specify <a target=\"_blank\" href=\"https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_admin_address\" target=\"_blank\" rel=\"noopener\">admin_address</a> and admin_port to provide for a completely different endpoint, bypassing the primary endpoint and establishing a dedicated admin connection.  If you’re running a lower version but are using Percona Server for MySQL, you’ll have the option of using <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/performance/threadpool.html#extra_port\">extra_port</a> and extra_max_connections to achieve another way of connecting.</p>\n<p>If you are able to log in as an admin account, you may be able to kill connections, use pt-kill to kill open connections, adjust timeouts, ban offending accounts, or raise the max_connections to free up the server.</p>\n<p>If you are unable to log in, you may try to adjust the max_connection value on the fly as a last resort.  Please see <a target=\"_blank\" href=\"https://www.percona.com/blog/2010/03/23/too-many-connections-no-problem/\">Too many connections? No problem!</a></p>\n<h2>Use a Proxy</h2>\n<p>Another way to alleviate connection issues (or move the issue to a different layer in the stack), is to adopt the user of a proxy server, such as ProxySQL to handle multiplexing.  See <a target=\"_blank\" href=\"https://www.percona.com/blog/2019/09/27/multiplexing-mux-in-proxysql/\">Multiplexing (Mux) in ProxySQL: Use Case</a>.</p>\n<h2>Limits Per User</h2>\n<p>Another variable that MySQL can use to determine if a connection should be allowed is max_user_connections.  By setting this value, it puts a limit on the number of connections for any given user.  If you have a smaller number of application users that can stand some limit on their connection usage, you can set this value appropriately to prevent total server connection maximum.</p>\n<p>For instance, if we know we have 3 application users and we expect those 3 users to never individually exceed 300 connections, we could set max_user_connections to 300.  Between the 3 application users, only a total of 900 connections would be allowed.  If max_connections was set to 1000, we’d still have 100 open slots.</p>\n<p>Another approach in this same vein that is even more granular is to limit connections PER USER account.  To achieve this you can create an account like this:</p><pre class=\"crayon-plain-tag\">CREATE USER 'user'@'localhost' IDENTIFIED BY 'XXXXXXXX' WITH MAX_USER_CONNECTIONS 10;</pre><p>It is a good idea to limit connections to tools/applications/monitoring that are newly being introduced in your environment and make sure they do not “accidentally” consume too many connections.</p>\n<h2>Close Unused Connections</h2>\n<p>MySQL provides the <a target=\"_blank\" href=\"https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout\" target=\"_blank\" rel=\"noopener\">wait_timeout variable</a>.  If you observe connections climbing progressively over time and not in a spike (and your application can handle it), you may want to reduce this variable from its default of 28800 seconds to something more reasonable.  This will essentially ask the server to close sleeping connections.</p>\n<p>These are just a few considerations when dealing with “Too many connections”.  I hope they help you.  You may also consider further reading on the topic in this previous Percona blog post, <a target=\"_blank\" href=\"https://www.percona.com/blog/2013/11/28/mysql-error-too-many-connections/\">MySQL Error: Too many connections</a>.</p>\n","descriptionType":"html","publishedDate":"Wed, 01 Jul 2020 18:19:35 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040.png","linkMd5":"703fc73912d6b5e0d301457c791da870","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_2/2020-07-25/1595696397958_f8828b07790c6e1c.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040.png","author":"Tate McDaniel","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/mysql-error-1040-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_2/2020-07-25/1595696402004_eaebd9b24aebbd4e.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Screen-Shot-2020-06-30-at-2.26.15-PM-1024x381.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_5/2020-07-25/1595696402552_321c18b3f072e405.webp"}},{"createdTime":"2020-07-26 00:59:48","updatedTime":"2020-07-25 16:59:48","title":"MySQL 101: How to Find and Tune a Slow SQL Query","link":"https://www.percona.com/blog/?p=69394","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"tune a slow sql query\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69455\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-300x157.png\" alt=\"tune a slow sql query\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />One of the most common support tickets we get at Percona is the infamous “<strong>database is running slower</strong>” ticket.  While this can be caused by a multitude of factors, it is more often than not caused by a bad query.  While everyone always hopes to recover through some quick config tuning, the real fix is to identify and fix the problem query.  Sure, we can generally alleviate some pain by throwing more resources at the server.  But this is almost always a short term bandaid and not the proper fix.</p>\n<h2>With Percona Monitoring and Management</h2>\n<p>So how do we find the queries causing problems and fix them?  If you have <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) installed, the identification process is swift.  With the Query Analytics enabled (QAN) in PMM, you can simply look at the table to identify the top query:</p>\n<p><img class=\"aligncenter wp-image-69395 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-1024x349.png\" alt=\"Percona Monitoring and Management queries\" width=\"900\" height=\"307\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-1024x349.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-300x102.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-200x68.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-1536x524.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-2048x698.png 2048w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-367x125.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>When you click on the query in the table, you should see some statistics about that query and also (in most cases), an example:</p>\n<p><img class=\"aligncenter wp-image-69396 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-1024x374.png\" alt=\"Percona Monitoring and Management queries\" width=\"900\" height=\"329\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-1024x374.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-300x110.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-200x73.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-1536x561.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-367x134.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM.png 2014w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h3><img class=\"aligncenter size-large wp-image-69397\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-1024x320.png\" alt=\"\" width=\"900\" height=\"281\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-1024x320.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-300x94.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-200x63.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-1536x481.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-367x115.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM.png 1758w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></h3>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\" rel=\"noopener\">View a Demo of Percona Monitoring and Management</a></p>\n<h2>Without Percona Monitoring and Management</h2>\n<p>Now, let’s assume that you don’t have PMM installed yet (I’m sure that is being worked on as you read this).  To find the problem queries, you’ll need to do some manual collection and processing that PMM does for you.  The following is the best process for collecting and aggregating the top queries:</p>\n<ol>\n<li>Set long_query_time = 0 (in some cases, you may need to rate limit to not flood the log)</li>\n<li>Enable the slow log and collect for some time (slow_query_log = 1)</li>\n<li>Stop collection and process the log with <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-toolkit/LATEST/pt-query-digest.html\">pt-query-digest</a></li>\n<li>Begin reviewing the top queries in times of resource usage</li>\n</ol>\n<p>Note &#8211; you can also use the performance schema to identify queries, but setting that up is outside the scope of this post.  Here is a good reference on <a target=\"_blank\" href=\"https://www.percona.com/blog/2015/10/13/mysql-query-digest-with-performance-schema/\">how to use P_S to find suboptimal queries</a>.</p>\n<p>When looking for bad queries, one of the top indicators is a large discrepancy between <strong>rows_examined</strong> and <strong>rows_sent</strong>.  In cases of suboptimal queries, the rows examined will be very large compared with a small number of rows sent.</p>\n<p>Once you have identified your query, it is time to start the optimization process.  The odds are that the queries at the top of your list (either in PMM or the digest report) lack indices.  Indexes allow the optimizer to target the rows you need rather than scanning everything and discarding non-matching values.   Let’s take the following sample query as an example:</p><pre class=\"crayon-plain-tag\">SELECT * \nFROM user\nWHERE username = \"admin1\" \nORDER BY last_login DESC;</pre><p>This looks like a straightforward query that should be pretty simple.  However, it is showing up as a resource hog and is bogging down the server.  Here is how it showed up in the pt-query-digest output:</p><pre class=\"crayon-plain-tag\"># Profile\n# Rank Query ID           Response time Calls R/Call V/M   Item\n# ==== ================== ============= ===== ====== ===== ===========\n#    1 0xA873BB85EEF9B3B9  0.4011 98.7%     2 0.2005  0.40 SELECT user\n# MISC 0xMISC              0.0053  1.3%     7 0.0008   0.0 &#60;7 ITEMS&#62;</pre><p></p><pre class=\"crayon-plain-tag\"># Query 1: 0.18 QPS, 0.04x concurrency, ID 0xA873BB85EEF9B3B9 at byte 3391\n# This item is included in the report because it matches --limit.\n# Scores: V/M = 0.40\n# Time range: 2018-08-30T21:38:38 to 2018-08-30T21:38:49\n# Attribute    pct   total     min     max     avg     95%  stddev  median\n# ============ === ======= ======= ======= ======= ======= ======= =======\n# Count         22       2\n# Exec time     98   401ms    54us   401ms   201ms   401ms   284ms   201ms\n# Lock time     21   305us       0   305us   152us   305us   215us   152us\n# Rows sent      6       1       0       1    0.50       1    0.71    0.50\n# Rows examine  99 624.94k       0 624.94k 312.47k 624.94k 441.90k 312.47k\n# Rows affecte   0       0       0       0       0       0       0       0\n# Bytes sent    37     449      33     416  224.50     416  270.82  224.50\n# Query size    47     142      71      71      71      71       0      71\n# String:\n# Databases    plive_2017\n# Hosts        localhost\n# Last errno   0\n# Users        root\n# Query_time distribution\n#   1us\n#  10us  ################################################################\n# 100us\n#   1ms\n#  10ms\n# 100ms  ################################################################\n#    1s\n#  10s+\n# Tables\n#    SHOW TABLE STATUS FROM `plive_2017` LIKE 'user'\\G\n#    SHOW CREATE TABLE `plive_2017`.`user`\\G\n# EXPLAIN /*!50100 PARTITIONS*/\nSELECT *  FROM user  WHERE username = \"admin1\" ORDER BY last_login DESC\\G</pre><p>We can see right away the high number of rows examined vs. the rows sent, as highlighted above.  So now that we’ve identified the problem query let’s start optimizing it.  Step 1 in optimizing the query would be to run an EXPLAIN plan:</p><pre class=\"crayon-plain-tag\">mysql&#62; EXPLAIN SELECT *  FROM user  WHERE username = \"admin1\" ORDER BY last_login DESC\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: user\n   partitions: NULL\n         type: ALL\npossible_keys: NULL\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 635310\n     filtered: 10.00\n        Extra: Using where; Using filesort\n1 row in set, 1 warning (0.00 sec)</pre><p>The EXPLAIN output is the first clue that this query is not properly indexed.  The <strong>type: ALL</strong> indicates that the entire table is being scanned to find a single record.  In many cases, this will lead to I/O pressure on the system if your dataset exceeds memory.  The <strong>Using filesort</strong> indicates that once it goes through the entire table to find your rows, it has to then sort them (a common symptom of CPU spikes).</p>\n<h3>Limiting Rows Examined</h3>\n<p>One thing that is critical to understand is that query tuning is an iterative process.  You won’t always get it right the first time and data access patterns may change over time.  In terms of optimization, the first thing we want to do is get this query using an index and not using a full scan.  For this, we want to look at the WHERE clause: <strong>where username = “admin1”</strong>.</p>\n<p>With this column theoretically being selective, an index on username would be a good start.  Let’s add the index and re-run the query:</p><pre class=\"crayon-plain-tag\">mysql&#62; ALTER TABLE user ADD INDEX idx_name (username);\nQuery OK, 0 rows affected (6.94 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql&#62; EXPLAIN SELECT *  FROM user  WHERE username = \"admin1\" ORDER BY last_login DESC\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: user\n   partitions: NULL\n         type: ref\npossible_keys: idx_name\n          key: idx_name\n      key_len: 131\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: Using index condition; Using filesort\n1 row in set, 1 warning (0.01 sec)</pre><p></p>\n<h3>Optimizing Sorts</h3>\n<p>So we are halfway there!  The <strong>type: ref</strong> indicates we are now using an index, and you can see the rows dropped from 635k down to 1.  This example isn’t the best as this finds one row, but the next thing we want to address is the filesort.  For this, we’ll need to change our username index to be a composite index (multiple columns).  The rule of thumb for a composite index is to work your way from the most selective to the least selective columns, and then if you need sorting, keep that as the last field.  Given that premise, let’s modify the index we just added to include the last_login field:</p><pre class=\"crayon-plain-tag\">mysql&#62; ALTER TABLE user DROP INDEX idx_name, ADD INDEX idx_name_login (username, last_login);\nQuery OK, 0 rows affected (7.88 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql&#62; EXPLAIN SELECT *  FROM user  WHERE username = \"admin1\" ORDER BY last_login DESC\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: user\n   partitions: NULL\n         type: ref\npossible_keys: idx_name_login\n          key: idx_name_login\n      key_len: 131\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)</pre><p>And there we have it!  Even if this query scanned more than one row, it would read them in sorted order, so the extra CPU needed for the sorting is eliminated.  To show this, let&#8217;s do this same index on a non-unique column (I left email as non-unique for this demo):</p><pre class=\"crayon-plain-tag\">mysql&#62; select count(1) from user where email = \"SGCRGCTOPGLNGR@RLCDLWD.com\";\n+----------+\n| count(1) |\n+----------+\n|       64 |\n+----------+\n1 row in set (0.23 sec)\n\nmysql&#62; ALTER TABLE user ADD INDEX idx_email (email, last_login);\nQuery OK, 0 rows affected (8.08 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql&#62; EXPLAIN SELECT *  FROM user  WHERE email = \"SGCRGCTOPGLNGR@RLCDLWD.com\" ORDER BY last_login DESC\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: user\n   partitions: NULL\n         type: ref\npossible_keys: idx_email\n          key: idx_email\n      key_len: 131\n          ref: const\n         rows: 64\n     filtered: 100.00\n        Extra: Using where\n1 row in set, 1 warning (0.00 sec)</pre><p>In summary, the general process to tune a SQL query follows this process:</p>\n<ol>\n<li>Identify the query (either manually or with a tool like PMM)</li>\n<li>Check the EXPLAIN plan of the query</li>\n<li>Review the table definition</li>\n<li>Create indexes\n<ol>\n<li>Start with columns in the WHERE clause</li>\n<li>For composite indexes, start with the most selective column and work to the least selective column</li>\n<li>Ensure sorted columns are at the end of the composite index</li>\n</ol>\n</li>\n<li>Review the updated explain plan and revise as needed</li>\n<li>Continue to review the server to identify changes in access patterns that require new indexing</li>\n</ol>\n<p>While query optimization can seem daunting, using a process can make it much easier to achieve.  Naturally, optimizing complex queries isn’t trivial like the above example, but is definitely possible when broken down.  And remember that Percona engineers are always available to help you when you get stuck!  Happy optimizing!</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\" rel=\"noopener\">View a Demo of Percona Monitoring and Management</a></p>\n<hr />\n<p>Our solution brief &#8220;Get Up and Running with Percona Server for MySQL&#8221; outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/get-and-running-percona-server-mysql?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=findandtune&#38;utm_content=solutionbrief\" rel=\"noopener\">Download PDF</a></p>\n","descriptionType":"html","publishedDate":"Fri, 26 Jun 2020 16:46:51 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query.png","linkMd5":"c46782b6d719ee015f6eedb908dc03ff","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_3/2020-07-25/1595696398019_56ea75d82a644a2a.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query.png","author":"Mike Benshoof","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_2/2020-07-25/1595696403642_74f7fafb4d3f5c87.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/tune-a-slow-sql-query-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_5/2020-07-25/1595696401838_fdfa2b0d9873db5d.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.52.02-PM-1024x349.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_1/2020-07-25/1595696400298_7cfe613e7323872c.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.53.04-PM-1024x374.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_6/2020-07-25/1595696399494_a2bf5b261a25bb62.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-15-at-2.54.17-PM-1024x320.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_4/2020-07-25/1595696401846_b71979548dfd620d.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"MongoDB Checkpointing Woes","link":"https://www.percona.com/blog/?p=69372","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MongoDB Checkpointing Woes\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>In my recent post <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/06/24/evaluating-mongodb-under-python-tpcc-1000w-workload/\">Evaluating MongoDB Under Python TPCC 1000W Workload</a> with MongoDB benchmarks, I showed an average throughput for a prolonged period of time (900sec or 1800sec), and the average throughput tended to smooth and hide problems.</p>\n<p>But if we zoom in to <strong>1-sec</strong> resolution for WiredTiger dashboard (Available in the <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> MongoDB dashboards), we can see the following:</p>\n<p><img class=\"aligncenter wp-image-69380 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2-1024x364.png\" alt=\"WiredTiger dashboard\" width=\"900\" height=\"320\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2-1024x364.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2-300x107.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2-200x71.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2-367x131.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2.png 1172w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>What we see here is that every 60 seconds, the throughput drops from ~7.5K op/s to ~40 op/s, which is about 1800 times, basically halting transaction processing.</p>\n<p>What is the reason for this? It is WiredTiger checkpointing, which is configured to happen every 60 sec by default. We can confirm this with the Checkpoint Time dashboard.</p>\n<p><img class=\"aligncenter wp-image-69382 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1-1024x365.png\" alt=\"Checkpoint Time dashboard\" width=\"900\" height=\"321\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1-1024x365.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1-300x107.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1-200x71.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1-367x131.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1.png 1173w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>This is not the first time I have written about it. Actually, I brought up MongoDB checkpoint issues just about 5 years ago in my post <a target=\"_blank\" href=\"https://www.percona.com/blog/2015/08/03/checkpoint-strikes-back/\">Checkpoint strikes back</a>. With more detailed results here: <a target=\"_blank\" href=\"https://lab-docs.percona.com/en/latest/mongodb-sysbench-hppro2.html\">MongoDB sysbench-mongo benchmark</a></p>\n<p>To be fair, we brought these issues for MySQL too. Initially, Peter described it 14 years ago in his post, <a target=\"_blank\" href=\"https://www.percona.com/blog/2006/05/10/innodb-fuzzy-checkpointing-woes/\">Innodb Fuzzy checkpointing woes</a>. Eventually, it was fixed in Percona Server for MySQL: <a target=\"_blank\" href=\"https://www.percona.com/blog/2010/12/20/mysql-5-5-8-and-percona-server-being-adaptive/\">MySQL 5.5.8 and Percona Server: being adaptive</a> and <a target=\"_blank\" href=\"http://dimitrik.free.fr/blog/archives/2011/04/mysql-performance-56-notes-part-5-fixing-adaptive-flushing.html\" target=\"_blank\" rel=\"noopener\">brought to MySQL in later releases</a>.</p>\n<p>And recently Ivan Groenewold wrote about some tuning steps that may help to improve the situation in <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/05/05/tuning-mongodb-for-bulk-loads/\">Tuning MongoDB for Bulk Loads</a>.</p>\n<p>So I also tried the workload above with the following settings:</p>\n<p style=\"padding-left: 40px;\">db.adminCommand( { &#8220;setParameter&#8221;: 1, &#8220;wiredTigerEngineRuntimeConfig&#8221;: &#8220;<strong>eviction=(threads_min=20,threads_max=20</strong>),checkpoint=(wait=60),eviction_dirty_trigger=5,eviction_dirty_target=1,eviction_trigger=95,eviction_target=80&#8243;})</p>\n<p>The result is:</p>\n<p><img class=\"aligncenter wp-image-69383 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2-1024x362.png\" alt=\"Tuning MongoDB\" width=\"900\" height=\"318\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2-1024x362.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2-300x106.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2-200x71.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2-367x130.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2.png 1163w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>We can now see we are able to avoid complete stalls, by limiting the max throughput, and during checkpointing we go from 6.7K op/s to 1.25K op/s, but we still can’t fully eliminate the drops during checkpoint time. And I name this optimization “tuning by delaying clients queries”, and this is basically what is done here; we slow down clients&#8217; queries, allowing the server to deal with dirty pages in the meantime.</p>\n<p>The question which comes to mind: Should we try to fix this for Percona Server for MongoDB, as we did for InnoDB in MySQL before?</p>\n","descriptionType":"html","publishedDate":"Thu, 25 Jun 2020 15:32:16 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes.png","linkMd5":"2ce7fc9180213293a0555f637963f197","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_6/2020-07-25/1595696397595_a427f24e0012e393.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes.png","author":"Vadim Tkachenko","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/MongoDB-Checkpointing-Woes-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_5/2020-07-25/1595696399905_5be8eb19d28131ab.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image1-2-1024x364.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_5/2020-07-25/1595696400233_1958be8febca2a70.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1-1024x365.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_1/2020-07-25/1595696400309_7b8e7f5aaa652a4b.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image2-2-1024x362.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_2/2020-07-25/1595696402776_29ac5316cbba7fef.webp"}},{"createdTime":"2020-07-26 00:59:44","updatedTime":"2020-07-25 16:59:44","title":"MySQL 101: Linux Tuning for MySQL","link":"https://www.percona.com/blog/?p=69532","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Linux Tuning for MySQL\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright wp-image-69687 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-300x157.png\" alt=\"Linux Tuning for MySQL\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />When trying to do some Linux tuning for MySQL, there are a few options that will greatly influence the speed of MySQL.  Below are some of the most important of these settings to help you get started.</p>\n<h3>Swappiness</h3>\n<p>The first thing to look at is what swappiness is set to.  This will determine the tendency of the kernel to swap out memory pages.  In may cases, you will want to set this to &#8220;1&#8221; to keep the swapping to a minimum.  A value of &#8220;0&#8221; will disable it entirely.</p>\n<p>You can determine the current value with the following command:</p><pre class=\"crayon-plain-tag\">cat /proc/sys/vm/swappiness</pre><p>If this is not set to &#8220;1&#8221;, you should consider making the change by using one of the following options:</p><pre class=\"crayon-plain-tag\"># Make sure you are root and set swappiness to 1 \necho 1 &#62; /proc/sys/vm/swappiness\n\n# Or, you can use sysctl to do the same sysctl \nvm.swappiness vm.swappiness = 1</pre><p>If the change helps, you will want to make it permanent by making the change in /etc/sysctl.conf:</p><pre class=\"crayon-plain-tag\">vm.swappiness = 1</pre><p></p>\n<h3>I/O Scheduler</h3>\n<p>The default for many systems is either &#8220;noop&#8221; or &#8220;deadline&#8221;.  In almost all cases, &#8220;noop&#8221; is more efficient than &#8220;deadline&#8221;, &#8220;cfq&#8221;, or &#8220;anticipatory&#8221;.  To check the current value of the I/O Scheduler, issue the following command:</p><pre class=\"crayon-plain-tag\">cat /sys/block/sdb/queue/scheduler</pre><p>If you want to test performance with &#8220;noop&#8221;, try the following command:</p><pre class=\"crayon-plain-tag\">sudo echo noop &#62; /sys/block/sdb/queue/scheduler</pre><p>If you want to make the change permanent, you will need to do the following in the GRUB configuration file:</p><pre class=\"crayon-plain-tag\">GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash elevator=noop\"</pre><p></p>\n<h3>CPU Governor</h3>\n<p>Most modern processors are capable of operating with a number of different clock frequency and voltage configurations.  These are referred to as Operating Performance Points or P-states.  Generally, the higher the clock frequency and the higher the voltage, the more instructions can be executed by the CPU over time.  Likewise, the higher the clock frequency and the voltage, the more energy is consumed. As such, there is a trade-off between CPU capacity and power draw by the processor.</p>\n<p>You can check what driver and governor are in use with the following command:</p><pre class=\"crayon-plain-tag\">cpupower frequency-info\n...\ndriver: acpi-cpufreq\n...\navailable cpufreq governors: conservative, ondemand, userspace, powersave, performance\n...\nThe governor \"ondemand\" may decide which speed to use within this range.\n...</pre><p>In the above, you can see that the driver is “acpi-cpufreq” and the governor is set to “ondemand”.</p>\n<p>You can also check the governor setting by issuing the following:</p><pre class=\"crayon-plain-tag\">cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</pre><p>If &#8220;performance&#8221; is an option, you can make the change by issuing the command:</p><pre class=\"crayon-plain-tag\">echo \"performance\" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</pre><p>Of course, you will want to decide if this has helped performance, so testing is essential.</p>\n<h3>NUMA</h3>\n<p>NUMA stands for &#8220;Non-uniform memory access.&#8221; With NUMA, an SMP’s system processor can access its own local memory faster than non-local memory.  This may result in swapping and thus negatively impact database performance.  When the memory allocated to the InnoDB Buffer Pool is larger than the amount of the RAM available and the default memory allocation policy is selected, swapping will likely occur.  A server with NUMA enabled will report different node distances between CPU nodes.</p>\n<p>You can determine your setting with the following:</p><pre class=\"crayon-plain-tag\">numactl --hardware</pre><p>Whenever numactl shows different distances across nodes, the MySQL variable <a target=\"_blank\" href=\"https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_numa_interleave\" target=\"_blank\" rel=\"noopener\">innodb_numa_interleave</a> should be enabled to ensure memory interleaving. With Percona Server, you can improve NUMA support by using the <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/performance/innodb_numa_support.html#flush_caches\">flush_caches</a> variable.  This will help with allocation fairness across nodes.  To determine whether the allocation is equal across nodes, you can check numa_maps with the script: <a target=\"_blank\" href=\"https://github.com/sonots/bin/blob/master/numa-maps-summary.pl\">https://github.com/sonots/bin/blob/master/numa-maps-summary.pl</a></p>\n<h3>Conclusion</h3>\n<p>As is normally the case, you should always test any changes you make in a lower environment before making them in production.  Workloads are so different, and you must see how any changes impact your system.</p>\n","descriptionType":"html","publishedDate":"Mon, 06 Jul 2020 18:30:38 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL.png","linkMd5":"e46d8de7151a259b2f3eadd4185c8ba7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_1/2020-07-25/1595696397550_5c92bfefd72867cb.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL.png","author":"Michael Patrick","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_2/2020-07-25/1595696401893_69ce154db581bd63.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Linux-Tuning-MySQL-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_1/2020-07-25/1595696404783_425d0245b5c10f26.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"Percona Server for MySQL Highlights – Extended Slow Query Logging","link":"https://www.percona.com/blog/?p=66385","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"mysql extended slow query logging\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69496\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-300x168.png\" alt=\"mysql extended slow query logging\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Last year, I made the first post in a small series, which aimed to highlight unique features of Percona Server for MySQL, by discussing <a target=\"_blank\" href=\"https://www.percona.com/blog/2019/07/03/percona-server-for-mysql-highlights-binlog_space_limit/\"><code>binlog_space_limit</code> option</a>.</p>\n<p>Today, I am going to discuss another important type of log available in MySQL that is enhanced in Percona Server for MySQL &#8211; the <strong>slow query log</strong>. The reason why I am doing this is that although this extension has existed since the very early times of versions 5.1 (over 10 years ago!), many people are still unaware of it, which I see from time to time when working with Support customers.</p>\n<h2>Default Slow Log Inadequacy</h2>\n<p>How many times have you been wondering why, whilst reviewing slow query logs, the very same query occasionally runs way slower than usual? There may be many reasons for that, but the standard slow query log does not always provide any helpful information about that. Let&#8217;s take a look at these two real test examples:</p><pre class=\"crayon-plain-tag\"># Time: 2020-03-29T21:21:43.080863Z\n# User@Host: msandbox[msandbox] @ localhost [] Id: 8\n# Schema: employees Last_errno: 0 Killed: 0\n# Query_time: 18.234596 Lock_time: 0.000602 Rows_sent: 1 Rows_examined: 6317755 Rows_affected: 0\n# Bytes_sent: 253\nuse employees;\nSET timestamp=1585524084;\nselect hire_date,dept_name,salary from salaries join dept_emp using(emp_no) join departments using(dept_no) join employees using(emp_no) where salary=(select max(salary) from salaries);</pre><p>vs</p><pre class=\"crayon-plain-tag\"># Time: 2020-03-29T21:22:05.637495Z\n# User@Host: msandbox[msandbox] @ localhost [] Id: 8\n# Schema: employees Last_errno: 0 Killed: 0\n# Query_time: 2.681241 Lock_time: 0.000276 Rows_sent: 1 Rows_examined: 6317755 Rows_affected: 0\n# Bytes_sent: 253\nSET timestamp=1585524122;\nselect hire_date,dept_name,salary from salaries join dept_emp using(emp_no) join departments using(dept_no) join employees using(emp_no) where salary=(select max(salary) from salaries);</pre><p>The most important information that usually gives a good clue on why it is slow &#8211; Rows_examined &#8211; is the same in both cases. So where dies the big difference in execution time come from?</p>\n<h2>Extended Slow Query Log Virtue</h2>\n<p>And here comes the extended verbosity mode for slow log available in Percona Server for MySQL. By simply using <code><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/diagnostics/slow_extended.html#log_slow_verbosity\">log_slow_verbosity</a>=full</code> variable, here is how similar log entries now look:</p><pre class=\"crayon-plain-tag\"># Time: 2020-03-29T23:26:52.133388Z\n# User@Host: msandbox[msandbox] @ localhost [] Id: 8\n# Schema: employees Last_errno: 0 Killed: 0\n# Query_time: 18.272430 Lock_time: 0.000418 Rows_sent: 1 Rows_examined: 6317755 Rows_affected: 0\n# Bytes_sent: 253 Tmp_tables: 0 Tmp_disk_tables: 0 Tmp_table_sizes: 0\n# InnoDB_trx_id: 0\n# Full_scan: Yes Full_join: No Tmp_table: No Tmp_table_on_disk: No\n# Filesort: No Filesort_on_disk: No Merge_passes: 0\n# InnoDB_IO_r_ops: 4915 InnoDB_IO_r_bytes: 80527360 InnoDB_IO_r_wait: 14.253396\n# InnoDB_rec_lock_wait: 0.000000 InnoDB_queue_wait: 0.000000\n# InnoDB_pages_distinct: 6099\nSET timestamp=1585524393;\nselect hire_date,dept_name,salary from salaries join dept_emp using(emp_no) join departments using(dept_no) join employees using(emp_no) where salary=(select max(salary) from salaries);</pre><p>vs</p><pre class=\"crayon-plain-tag\"># Time: 2020-03-29T23:27:01.031350Z\n# User@Host: msandbox[msandbox] @ localhost [] Id: 8\n# Schema: employees Last_errno: 0 Killed: 0\n# Query_time: 2.726639 Lock_time: 0.000268 Rows_sent: 1 Rows_examined: 6317755 Rows_affected: 0\n# Bytes_sent: 253 Tmp_tables: 0 Tmp_disk_tables: 0 Tmp_table_sizes: 0\n# InnoDB_trx_id: 0\n# Full_scan: Yes Full_join: No Tmp_table: No Tmp_table_on_disk: No\n# Filesort: No Filesort_on_disk: No Merge_passes: 0\n# InnoDB_IO_r_ops: 0 InnoDB_IO_r_bytes: 0 InnoDB_IO_r_wait: 0.000000\n# InnoDB_rec_lock_wait: 0.000000 InnoDB_queue_wait: 0.000000\n# InnoDB_pages_distinct: 6098\nSET timestamp=1585524418;\nselect hire_date,dept_name,salary from salaries join dept_emp using(emp_no) join departments using(dept_no) join employees using(emp_no) where salary=(select max(salary) from salaries);</pre><p>So much more information! And we can now easily find the difference &#8211; InnoDB IO information. Basically the slow instance of the query had to read many data pages from disk, while the fast one clearly took advantage of the buffer pool.</p>\n<p>That&#8217;s not all, we can be even more verbose and make the slow log print full query profiling info as well:</p><pre class=\"crayon-plain-tag\">mysql &#62; \\! tail -11 /home/przemek/sandboxes/msb_ps8_0_18/data/przemek-dbg-slow.log|sed \"s/ Profile/\\nProfile/g\"\nTime Id Command Argument\n# Time: 2020-03-30T00:23:38.017357Z\n# User@Host: msandbox[msandbox] @ localhost [] Id: 8\n# Schema: employees Last_errno: 0 Killed: 0\n# Query_time: 18.017249 Lock_time: 0.000537 Rows_sent: 1 Rows_examined: 6317755 Rows_affected: 0\n# Bytes_sent: 253\n#\nProfile_starting: 0.000142\nProfile_starting_cpu: 0.000142\nProfile_Executing_hook_on_transaction_begin.: 0.000006\nProfile_Executing_hook_on_transaction_begin._cpu: 0.000006\nProfile_starting: 0.000011\nProfile_starting_cpu: 0.000011\nProfile_checking_permissions: 0.000006\nProfile_checking_permissions_cpu: 0.000006\nProfile_checking_permissions: 0.000003\nProfile_checking_permissions_cpu: 0.000003\nProfile_checking_permissions: 0.000003\nProfile_checking_permissions_cpu: 0.000003\nProfile_checking_permissions: 0.000003\nProfile_checking_permissions_cpu: 0.000003\nProfile_checking_permissions: 0.000108\nProfile_checking_permissions_cpu: 0.000109\nProfile_Opening_tables: 0.000247\nProfile_Opening_tables_cpu: 0.000247\nProfile_init: 0.000012\nProfile_init_cpu: 0.000011\nProfile_System_lock: 0.000015\nProfile_System_lock_cpu: 0.000015\nProfile_optimizing: 0.000022\nProfile_optimizing_cpu: 0.000022\nProfile_statistics: 0.000171\nProfile_statistics_cpu: 0.000171\nProfile_preparing: 0.000058\nProfile_preparing_cpu: 0.000065\nProfile_optimizing: 0.000017\nProfile_optimizing_cpu: 0.000009\nProfile_statistics: 0.000016\nProfile_statistics_cpu: 0.000016\nProfile_preparing: 0.000028\nProfile_preparing_cpu: 0.000028\nProfile_executing: 0.003004\nProfile_executing_cpu: 0.000687\nProfile_executing: 18.013309\nProfile_executing_cpu: 4.507004\nProfile_end: 0.000012\nProfile_end_cpu: 0.000011\nProfile_query_end: 0.000008\nProfile_query_end_cpu: 0.000007\nProfile_waiting_for_handler_commit: 0.000014\nProfile_waiting_for_handler_commit_cpu: 0.000014\nProfile_closing_tables: 0.000015\nProfile_closing_tables_cpu: 0.000014\nProfile_freeing_items: 0.000030\nProfile_freeing_items_cpu: 0.000030\nProfile_logging_slow_query: 0.000003\nProfile_logging_slow_query_cpu: 0.000003\n#\nProfile_total: 18.017262\nProfile_total_cpu: 4.508638\nuse employees;\nSET timestamp=1585527800;\nselect hire_date,dept_name,salary from salaries join dept_emp using(emp_no) join departments using(dept_no) join employees using(emp_no) where salary=(select max(salary) from salaries);</pre><p>And there are more perks in the extended slow query logging!</p>\n<h2>Safety For Busy Production Server and More</h2>\n<p>How many times were you concerned about using <code>long_query_time=0</code> (or just the very low value of), because logging hundreds or thousands of queries per second on a busy system could be too much overhead? Two very useful options come handy here: <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/diagnostics/slow_extended.html#log_slow_filter\"><code>log_slow_filter</code></a> and <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/diagnostics/slow_extended.html#log_slow_rate_limit\"><code>log_slow_rate_limit</code></a>. For example, you can order the server to log only queries that do joins without using indexes, but only 1% of such queries will be logged, with the following settings:</p><pre class=\"crayon-plain-tag\">log_slow_filter = full_join\nlog_slow_rate_type = query\nlog_slow_rate_limit = 100</pre><p>Of course, all those settings are dynamic and you can control whether they will apply to local sessions only or to all connections.<br />\nIt is important to mention here, that other Percona tools, like <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-toolkit/LATEST/pt-query-digest.html\">pt-query-digest</a> or <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management (PMM),</a> can take advantage of the extra information. Here is an example of Query Analytics view in PMM using the extra InnoDB details in its statistics:</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/?attachment_id=69431\"><img class=\"wp-image-69431 size-large aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483-1024x380.png\" alt=\"Query Analytics in Percona Monitoring and Management\" width=\"900\" height=\"334\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483-1024x380.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483-300x111.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483-200x74.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483-367x136.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483.png 1512w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<p>You will find more details about this rich, yet not very well known, feature set in <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/diagnostics/slow_extended.html\">our documentation pages</a>.</p>\n<h2>MySQL 8.0 Extra</h2>\n<p>Since MySQL 8.0.14 Community Edition (and Percona Server for MySQL as well), also in the upstream, another variant of extended slow query log info was implemented, which however only partially overlaps with Percona features. Basically, you can now make the status handlers printed as well, by using <a target=\"_blank\" href=\"https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_log_slow_extra\"><code>log_slow_extra</code></a>=1. An example entry for the same query looks like that:</p><pre class=\"crayon-plain-tag\"># Time: 2020-03-30T09:31:48.485389Z\n# User@Host: msandbox[msandbox] @ localhost [] Id: 8\n# Query_time: 2.722842 Lock_time: 0.000315 Rows_sent: 1 Rows_examined: 6317755 Thread_id: 8 Errno: 0 Killed: 0 Bytes_received: 0 Bytes_sent: 253 Read_first: 2 Read_last: 0 Read_key: 331615 Read_next: 3473707 Read_prev: 0 Read_rnd: 0 Read_rnd_next: 2844048 Sort_merge_passes: 0 Sort_range_count: 0 Sort_rows: 0 Sort_scan_count: 0 Created_tmp_disk_tables: 0 Created_tmp_tables: 0 Start: 2020-03-30T09:31:45.762547Z End: 2020-03-30T09:31:48.485389Z Schema: employees Rows_affected: 0\n\nSET timestamp=1585560705;\nselect hire_date,dept_name,salary from salaries join dept_emp using(emp_no) join departments using(dept_no) join employees using(emp_no) where salary=(select max(salary) from salaries);</pre><p>This can be super useful in cases where we want to check if the execution plan was changing. Interestingly, you can enable both extensions at the same time if needed.</p>\n<h2>Slow Log Automatic Rotation</h2>\n<p>Tired of maintaining logrotate or custom scripts that keep your disk space safe from too bloated logs? There are new options in Percona Server for MySQL 5.7 that can make DBA/Sysadmin life easier &#8211; <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/5.7/flexibility/slowlog_rotation.html#max_slowlog_size\"><code>max_slowlog_size</code></a> and <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/5.7/flexibility/slowlog_rotation.html#max_slowlog_files\"><code>max_slowlog_files</code></a>. You may now set up limits after which the log will be automatically rotated and removed.</p>\n<p>This auto-rotating functionality was not yet ported to Percona Server for MySQL 8.0 though. But if you think such should exist in MySQL, I&#8217;d suggest you vote for this 12-year-old feature request 🙂 <a target=\"_blank\" href=\"https://bugs.mysql.com/bug.php?id=38702\">https://bugs.mysql.com/bug.php?id=38702</a></p>\n<h3>Documentation reference:</h3>\n<p><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server/LATEST/diagnostics/slow_extended.html\">Slow Query Log</a></p>\n<p><a target=\"_blank\" class=\"reference external\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/manage/pmm.conf-mysql.settings.ps\">Percona Server specific settings</a></p>\n<hr />\n<p>Our solution brief &#8220;Get Up and Running with Percona Server for MySQL&#8221; outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/get-and-running-percona-server-mysql?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=slowquery&#38;utm_content=solutionbrief\" rel=\"noopener\">Download PDF</a></p>\n","descriptionType":"html","publishedDate":"Mon, 29 Jun 2020 18:59:31 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging.png","linkMd5":"8172a10974e4ed51cb2c347212eb4896","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_1/2020-07-25/1595696397484_c348af461faa98aa.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging.png","author":"Przemysław Malkowski","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_2/2020-07-25/1595696403706_95e12274621ce7f0.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/mysql-extended-slow-query-logging-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_2/2020-07-25/1595696402260_e82198ce6c483a3a.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Selection_483-1024x380.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_1/2020-07-25/1595696401011_330c668912e2833f.webp"}},{"createdTime":"2020-07-26 00:59:44","updatedTime":"2020-07-25 16:59:44","title":"MariaDB S3 Engine: Implementation and Benchmarking","link":"https://www.percona.com/blog/?p=69896","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MariaDB S3 Engine\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69974\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-300x168.png\" alt=\"MariaDB S3 Engine\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />MariaDB 10.5 has an excellent engine plugin called <a target=\"_blank\" href=\"https://mariadb.com/kb/en/s3-storage-engine/\">“S3”</a>. The S3 storage engine is based on the Aria code and the main feature is that you can directly move your table from a local device to S3 using ALTER. Still, your data is accessible from MariaDB client using the standard SQL commands. This is a great solution to those who are looking to archive data for future references at a low cost. The S3 engine is READ_ONLY so you can’t perform any write operations ( INSERT/UPDATE/DELETE ), but you can change the table structure.</p>\n<p>In this blog, I am going to explain the details about the S3 engine&#8217;s implementation and aspects. And in the end, I compare the performance results from both Local and S3 engine tables.</p>\n<h2>S3 Engine Implementation</h2>\n<p>The S3 engine is alpha-level maturity and it will not load by default during MariaDB startup. You have to enable the S3 engine as follows:</p><pre class=\"crayon-plain-tag\">[mysqld]\nplugin-maturity = alpha</pre><p>You also need to configure your S3 credentials in the MariaDB config file so that MariaDB can authenticate the connection and communicate with the S3 bucket. My config file looks like this:</p><pre class=\"crayon-plain-tag\">[mysqld]\nserver-id = 101\nplugin-maturity = alpha\nlog_error = /data/s3_testing_logs/mariadb.log\nport = 3310\n\n#s3\ns3=ON\ns3_access_key = xxxxxxxxxxxx\ns3_secret_key = yyyyyyyyyyyyyyyyyyyyyyy\ns3_bucket = mariabs3plugin\ns3_region = ap-south-1\ns3_debug = ON</pre><p><strong>Note:</strong> From a security perspective, your AWS credentials are plaintext. A new key pair should be created specifically for this plugin and only the necessary IAM grants be given.</p>\n<p>After configuring the parameters, you need to restart MariaDB to apply the settings. After the restart, you can install the plugin as follows.</p><pre class=\"crayon-plain-tag\">MariaDB [(none)]&gt; install soname 'ha_s3';\nQuery OK, 0 rows affected (0.000 sec)\n\nMariaDB [(none)]&gt; select * from information_schema.engines where engine = 's3'\\G\n*************************** 1. row ***************************\n      ENGINE: S3\n     SUPPORT: YES\n     COMMENT: Read only table stored in S3. Created by running ALTER TABLE table_name ENGINE=s3\nTRANSACTIONS: NO\n          XA: NO\n  SAVEPOINTS: NO\n1 row in set (0.000 sec)</pre><p>Now the S3 engine is ready to use.</p>\n<h2>How Do I Move The Table to The S3 Engine?</h2>\n<p>You can move the table to the S3 engine by using the ALTER. For testing, I have created the table “percona_s3” at my lab.</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; show create table percona_s3\\G\n*************************** 1. row ***************************\n       Table: percona_s3\nCreate Table: CREATE TABLE `percona_s3` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `name` varchar(16) DEFAULT NULL,\n  `c_date` datetime DEFAULT current_timestamp(),\n  `date_y` datetime DEFAULT current_timestamp(),\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n1 row in set (0.000 sec)\n\n[root@ip-172-31-19-172 ~]# ls -lrth /var/lib/mysql/s3_test/* | grep -i percona_s3\n-rw-rw----  1 mysql mysql 1019 Jul 14 01:50 /var/lib/mysql/s3_test/percona_s3.frm\n-rw-rw----  1 mysql mysql  96K Jul 14 01:50 /var/lib/mysql/s3_test/percona_s3.ibd</pre><p>Physically, you can see both .frm and .ibd files once the table is created (default InnoDB). I am going to convert the table “percona_s3” to the S3 engine.</p><pre class=\"crayon-plain-tag\">#MariaDB shell\n\nMariaDB [s3_test]&gt; alter table percona_s3 engine=s3;\nQuery OK, 0 rows affected (1.934 sec)              \nRecords: 0  Duplicates: 0  Warnings: 0\n\nMariaDB [s3_test]&gt; show create table percona_s3\\G\n*************************** 1. row ***************************\n       Table: percona_s3\nCreate Table: CREATE TABLE `percona_s3` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `name` varchar(16) DEFAULT NULL,\n  `c_date` datetime DEFAULT current_timestamp(),\n  `date_y` datetime DEFAULT current_timestamp(),\n  PRIMARY KEY (`id`)\n) ENGINE=S3 DEFAULT CHARSET=latin1 PAGE_CHECKSUM=1\n1 row in set (1.016 sec)\n\n#Linux shell\n\n[root@ip-172-31-19-172 ~]# ls -lrth /var/lib/mysql/s3_test/* | grep -i percona_s3\n-rw-rw----  1 mysql mysql 1015 Jul 14 01:54 /var/lib/mysql/s3_test/percona_s3.frm</pre><p><strong>Note:</strong> You will get the error “ERROR 3 (HY000):” if you enabled SELINUX, or if anything related to S3 access is misconfigured.</p>\n<p>After converting to the S3 engine, you can see only the .frm file. The data has been migrated out of InnoDB and into the S3 engine storage format.</p><pre class=\"crayon-plain-tag\">[root@ip-172-31-19-172 ~]# aws s3 ls s3://mariabs3plugin/s3_test/percona_s3/\n                           PRE data/\n                           PRE index/\n2020-07-14 01:59:28       8192 aria\n2020-07-14 01:59:28       1015 frm\n\n[root@ip-172-31-19-172 ~]# aws s3 ls s3://mariabs3plugin/s3_test/percona_s3/data/\n2020-07-14 01:59:29      16384 000001\n[root@ip-172-31-19-172 ~]# aws s3 ls s3://mariabs3plugin/s3_test/percona_s3/index/\n2020-07-14 01:59:28       8192 000001</pre><p><strong>Note:</strong> S3 will split the data and index pages and store them separately in respective folders.</p>\n<h2>S3 Engine Operation</h2>\n<p>For testing, I created the below table on S3. Let&#8217;s test the commands one by one.</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; select * from percona_s3;\n+----+-----------------+---------------------+---------------------+\n| id | name            | c_date              | date_y              |\n+----+-----------------+---------------------+---------------------+\n|  1 | hercules7sakthi | 2020-06-28 21:47:27 | 2020-07-01 14:37:13 |\n+----+-----------------+---------------------+---------------------+\n1 row in set (1.223 sec)\n\nMariaDB [s3_test]&gt; pager grep -i engine ; show create table percona_s3;\nPAGER set to 'grep -i engine'\n) ENGINE=S3 AUTO_INCREMENT=2 DEFAULT CHARSET=latin1 PAGE_CHECKSUM=1 |\n1 row in set (0.798 sec)</pre><p></p>\n<h4>S3 Engine with INSERT/UPDATE DELETE:</h4>\n<p>With all three statements, the query will return the “ERROR 1036: read only”.</p>\n<p>Sample output:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; insert into percona_s3 (name) values ('anti-hercules7sakthi');\nERROR 1036 (HY000): Table 'percona_s3' is read only</pre><p></p>\n<h4>S3 Engine with SELECT:</h4>\n<p></p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; select * from percona_s3;\n+----+-----------------+---------------------+---------------------+\n| id | name            | c_date              | date_y              |\n+----+-----------------+---------------------+---------------------+\n|  1 | hercules7sakthi | 2020-06-28 21:47:27 | 2020-07-01 14:37:13 |\n+----+-----------------+---------------------+---------------------+\n1 row in set (1.012 sec)</pre><p></p>\n<h4>Adding Index to S3 Engine Table:</h4>\n<p></p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; alter table percona_s3 add index idx_name (name);\nQuery OK, 1 row affected (8.351 sec)               \nRecords: 1  Duplicates: 0  Warnings: 0</pre><p></p>\n<h4>Modifying the Column on S3 Engine Table:</h4>\n<p></p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; alter table percona_s3 modify column date_y timestamp DEFAULT current_timestamp();\nQuery OK, 1 row affected (8.888 sec)               \nRecords: 1  Duplicates: 0  Warnings: 0</pre><p></p>\n<h4>S3 Engine with DROP:</h4>\n<p></p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; drop table percona_s3;\nQuery OK, 0 rows affected (2.084 sec)</pre><p><strong>Note:</strong> DROP TABLE will completely remove the data and index pages from S3 as well.</p>\n<p>In short, the S3 will allow the read commands and the structure modification commands. Changing or adding any data into the S3 is restricted. MariaDB community is planning to allow the BATCH UPDATE (single user) on S3. Right now, if you need to change any data on S3 tables, you need to follow the below procedure:</p>\n<ul>\n<li>Convert table from S3 to local (Engine = InnoDB)</li>\n<li>Modify the data</li>\n<li>Convert table from Local to S3 (Engine = S3)</li>\n</ul>\n<p>You can also query the metadata from INFORMATION_SCHEMA and retrieve the metadata using the SHOW commands.</p>\n<h2>Comparing the Query Results on Both S3 and Local</h2>\n<p>In this section, I am going to compare the query results on both the S3 engine and Local. We need to consider the below points before going to the test results.</p>\n<ul>\n<li>I have disabled the parameters “innodb_buffer_pool_dump_at_shutdown” and “innodb_buffer_pool_load_at_startup”.</li>\n<li>MariaDB server has restarted before and after executing every single SQL query shown below.</li>\n<li>MariaDB server and S3 are in the same zone.</li>\n<li>The ping time between the MySQL and s3 is 1.18 ms</li>\n</ul>\n<h3>S3 vs Local ( Count(*) )</h3>\n<p>At S3:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; select count(*) from percona_perf_compare;\n+----------+\n| count(*) |\n+----------+\n| 14392799 |\n+----------+\n1 row in set (0.16 sec)</pre><p>At local:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; select count(*) from percona_perf_compare;\n+----------+\n| count(*) |\n+----------+\n| 14392799 |\n+----------+\n1 row in set (18.718 sec)</pre><p><img class=\"size-full wp-image-69942 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Count.png\" alt=\"\" width=\"600\" height=\"371\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Count.png 600w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Count-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Count-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Count-367x227.png 367w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></p>\n<p>Count(*) is faster on S3engine. S3 tables are read_only, and it might display the stored value like MyISAM.</p>\n<h3>S3 vs Local (Entire Table Data)</h3>\n<p>At S3:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; pager md5sum; select * from percona_perf_compare;\nPAGER set to 'md5sum'\n1210998fc454d36ff55957bb70c9ffaf  -\n14392799 rows in set (16.10 sec)</pre><p>At Local:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; pager md5sum; select * from percona_perf_compare;\nPAGER set to 'md5sum'\n1210998fc454d36ff55957bb70c9ffaf  -\n14392799 rows in set (11.16 sec)</pre><p><img class=\"size-full wp-image-69943 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Entire-table-data-.png\" alt=\"\" width=\"600\" height=\"371\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Entire-table-data-.png 600w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Entire-table-data--300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Entire-table-data--200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Entire-table-data--367x227.png 367w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></p>\n<h3>S3 vs Local (PRIMARY KEY based lookup)</h3>\n<p>At S3:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; pager md5sum; select * from percona_perf_compare where id in (7196399);\nPAGER set to 'md5sum'\n13b359d17336bb7dcae344d998bbcbe0  -\n1 row in set (0.22 sec)</pre><p>At Local:</p><pre class=\"crayon-plain-tag\">MariaDB [s3_test]&gt; pager md5sum; select * from percona_perf_compare where id in (7196399);\nPAGER set to 'md5sum'\n13b359d17336bb7dcae344d998bbcbe0  -\n1 row in set (0.00 sec)</pre><p><img class=\"size-full wp-image-69944 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-PRIMARY-KEY-based-lookup-.png\" alt=\"\" width=\"600\" height=\"371\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-PRIMARY-KEY-based-lookup-.png 600w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-PRIMARY-KEY-based-lookup--300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-PRIMARY-KEY-based-lookup--200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-PRIMARY-KEY-based-lookup--367x227.png 367w\" sizes=\"(max-width: 600px) 100vw, 600px\" /></p>\n<p>S3 engine is pretty good with COUNT(*). And, if we retrieve the actual data from S3, we can see little delay compared to local.</p>\n<p>I have conducted the above tests with the default S3 settings. As per the MariaDB document, we can consider the below things to increase the performance on S3:</p>\n<ul>\n<li>Decreasing s3_block_size. This can be done both globally and per table.</li>\n<li>Use COMPRESSION_ALGORITHM=zlib when creating the table. This will decrease the amount of data transferred from S3 to the local cache.</li>\n<li>Increasing the size of the s3 page cache: s3_pagecache_buffer_size</li>\n</ul>\n<p>I would say the performance also depends on the disk access speed and network health between server and S3. Consider the below points:</p>\n<ul>\n<li>Having a low-performance disk and a good network between servers and S3 will favor S3.</li>\n<li>Having a good performance disk and poor network between servers and S3 will favor Local.</li>\n</ul>\n<h3>Conclusion</h3>\n<ul>\n<li>This is a very good solution for data archival from MariaDB community. You can query the historical data without restoring.</li>\n<li>The table is completely read-only.</li>\n<li>COUNT(*) is pretty fast like MyISAM.</li>\n<li>Pt-online-schema change will not work on both scenarios (S3 to Local &#38; Local to S3). It will fail because of the INSERT (when copying the data) and CREATE (the S3 table options will not support for InnoDB).</li>\n<li>CREATE TABLE, DROP TABLE, INFORMATION_SCHEMA tables are slower as those operations need to check the S3.</li>\n<li>For copying the Aria tables, you need to use the tool aria_s3_copy</li>\n</ul>\n<p>I am working with <a target=\"_blank\" href=\"https://www.linkedin.com/in/agust%C3%ADn-g/\">Agustin</a> on our next blog on this, covering the S3 engine compression. Stay tuned!</p>\n","descriptionType":"html","publishedDate":"Fri, 17 Jul 2020 16:31:24 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine.png","linkMd5":"5bc40e3c30485e1df6d2cd192e263f0d","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_2/2020-07-25/1595696397958_a097470d1476b55d.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine.png","author":"Sri Sakthivel","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_4/2020-07-25/1595696404799_c992b111106ec65c.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/MariaDB-S3-Engine-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_6/2020-07-25/1595696404356_6c8795580c2a8c28.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Count.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_5/2020-07-25/1595696399176_55114b8a18c954c1.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-Entire-table-data-.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_2/2020-07-25/1595696403866_ac082f3de64e37d3.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/S3-vs-Local-PRIMARY-KEY-based-lookup-.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_2/2020-07-25/1595696402124_c6923868b84219ab.webp"}},{"createdTime":"2020-07-26 00:59:43","updatedTime":"2020-07-25 16:59:43","title":"On-Demand Webinar: Securing MongoDB","link":"https://www.percona.com/blog/?p=69731","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Securing MongoDB\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69734\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-300x168.png\" alt=\"Securing MongoDB\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />MongoDB, like most any other commercial database you might care to name, has these five security subsystems:</p>\n<ul>\n<li style=\"list-style-type: none;\">\n<ul>\n<li>Authentication</li>\n<li>Authorization</li>\n<li>Network Encryption</li>\n<li>Data-at-rest Encryption</li>\n<li>Auditing</li>\n</ul>\n</li>\n</ul>\n<p>In this on-demand webinar, you&#8217;ll first learn which threat each of the subsystems addresses. Second, you&#8217;ll learn which parts probably differ, by convention, to other databases you know. And lastly, you&#8217;ll receive some practical &#8220;where&#8221; and &#8220;how&#8221; on enabling them, as well as the different options/levels in each.</p>\n<p>Join <a target=\"_blank\" href=\"https://www.percona.com/blog/author/akira-kurogane/\"><strong>Akira Kurogane</strong></a>, <strong>MongoDB Lead</strong>, <strong>Percona</strong>, for his on-demand webinar <strong>&#8220;Securing MongoDB&#8221;</strong>.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://register.gotowebinar.com/recording/6363604273759076865\" rel=\"noopener\">Watch Now</a></p>\n<p>&#160;</p>\n","descriptionType":"html","publishedDate":"Wed, 08 Jul 2020 16:00:02 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB.png","linkMd5":"3d94aa39332f1f75b337c59a41fe5e1a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_4/2020-07-25/1595696397955_00d2e755439758af.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_3/2020-07-25/1595696401871_cc27cc72c7cb19e8.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Securing-MongoDB-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_1/2020-07-25/1595696405942_45fabca4e1710e69.webp"}},{"createdTime":"2020-07-26 00:59:43","updatedTime":"2020-07-25 16:59:43","title":"Enable Email Sending in Percona Monitoring and Management 2","link":"https://www.percona.com/blog/?p=69632","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"email sending percona monitoring and management\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69706\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-300x168.png\" alt=\"email sending percona monitoring and management\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Many years ago I was banned from touching anything of critical importance as far as production infrastructure goes&#8230;something about “the leadership curse”, so the multiple layers of monitoring and notifications I once wrestled with were now overkill for my needs. What that means is that even though <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) now integrates with Prometheus AlertManager, it’s just too much for what I’m doing on a day-to-day basis&#8230;I will not be responding to a page at 3 am because my little test application server’s I/O is too high!  That doesn’t mean I don’t want to know when things happen, but instant notification isn’t critical; I can investigate later after a few cans of Diet Mtn Dew&#8230;so I figured I’d enable mail to be able to configure alerts natively in Grafana.</p>\n<h2>Setting up Email in Percona Monitoring and Management</h2>\n<p>Since this was more for personal use, I figured I’d start with my home setup&#8230;which is just a simple postfix server on a VM for use as a queue inside my house (I had a much less reliable ISP at the time). As I thought about the rest of the world, I figured I’d see what additional steps were necessary to integrate with the two most likely corporate options: Gmail and Office 365.  Just for fun, I’ll outline two ways to configure: by updating the grafana.ini inside the container, or by initializing your container with environment variables.</p>\n<p>We’ll start with the grafana.ini method.  Now before we get to the fun part, I need to call out that there are still several configuration scenarios that do not survive upgrades &#8211; and this is one of them.  To combat this, I have backups of my grafana.ini (and other changes) on my persistent storage volume (/srv/configs for me)&#8230;so make sure you keep a copy of the files you change either in a persistent container or on your local system.  If you do a docker container upgrade of PMM you <strong>WILL</strong> lose your work and for now, if we (the PMM team), make a significant change to grafana.ini, it will replace your changes even on an in-place upgrade.</p>\n<p><strong>Ok, on to the fun stuff!</strong>  I’m not even going to talk about the vanilla SMTP since it’s just a matter of plugging in the hostname and user/pass if needed.  Office 365 was a little more involved, though, and there were some unexpected gotchas I had to overcome.</p>\n<p>The basic flow for all systems</p>\n<p>→$sudo docker exec -it pmm-server bash<br />\n→##cd /etc/grafana<br />\n→## vi grafana.ini</p>\n<p>in the SMTP section</p><pre class=\"crayon-plain-tag\">[smtp]\nenabled = true\nhost = smtp.office365.com:587\nuser = &#60;o365 username&#62;\npassword = &#60;o365 Password&#62;\n;cert_file =\n;key_file =\nskip_verify = false\nfrom_address = &#60;o365 username&#62;\n;from_name = Grafana\n;EHLO identity in SMTP dialog (defaults to instance_name)\n;ehlo_identity = dashboard.example.com\n[emails]\n;welcome_email_on_sign_up = false</pre><p>→ ## supervisorctl restart grafana<br />\n→ ## exit</p>\n<p>Here are the gotchas though:</p>\n<ul>\n<li>Your password is in plaintext in grafana.ini&#8230;you can generate an “<a target=\"_blank\" href=\"https://support.office.com/en-us/article/create-an-app-password-for-microsoft-365-3e7c860f-bda4-4441-a618-b53953ee1183\" target=\"_blank\" rel=\"noopener\">application password</a>” but now we get into the nuance of all the configuration possibilities of each O365 tenant so you’re on your own there &#8211; and be warned this may involve bribery of your O365 admins!</li>\n<li>The from_address value <em>must</em> match the account used for the username or will fail.</li>\n<li>The from_name field is useless&#8230;this has no impact on o365&#8230;will send as the friendly name of whatever account is logged in&#8230;which means unless you want all the alerts coming as you, you probably need to beg for a full-blown service account.</li>\n</ul>\n<p>Gmail was a little more fun since I tested it on our corporate system with Multifactor enabled, which meant we’d need application passwords.  This time I’m showing how to do it by passing environment variables in your docker container creation.  There’s a gotcha here too&#8230;if you already have PMM installed with any custom parameters or environment variables, then you’ll need them in addition to what’s below, and you should also be using a persistent data container or you will lose whatever data you had.  This means you’ll need to reregister all your clients and have no history for them; do yourself a favor&#8230;<a target=\"_blank\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/install/docker-setting-up.html\">run pmm with the data container</a>!</p><pre class=\"crayon-plain-tag\"># Docker run -d -p 80:80 -p 443:443 --volumes-from pmm-data --name pmm-server \\\n--restart always \\\n-e GF_SMTP_ENABLED=true \\\n-e GF_SMTP_HOST=smtp.gmail.com:587 \\\n-e GF_SMTP_USER=&#60;gmail username&#62; \\\n-e GF_SMTP_PASSWORD=&#60;gmail password&#62; \\\n-e GF_SMTP_SKIP_VERIFY=false \\\n-e GF_SMTP_FROM_ADDRESS=&#60;gmail email address&#62; \\\n-e GF_SMTP_FROM_NAME=Grafana \\\npercona/pmm-server:2</pre><p>The Gmail gotchas</p>\n<ul>\n<li>I had to follow this <a target=\"_blank\" href=\"https://support.google.com/accounts/answer/185833?p=InvalidSecondFactor&#38;visit_id=637069985322183990-3850871888&#38;rd=1\">Google link</a> to generate an app password.</li>\n<li>Again, the from_address doesn’t look easily changed (I was really hoping in both cases to set it to “<a target=\"_blank\" href=\"mailto:no_reply@domain.com\">no_reply@domain.com</a>” but no dice.</li>\n<li>You CAN however change from_name when relaying through Gmail, so while the email would be mine, it will appear to be from “Grafana Alerts &#60;<a target=\"_blank\" href=\"mailto:emailaddress@gmail.com\">emailaddress@gmail.com</a>&#62;” which is better than nothing and doesn’t cost me however much for another account.  <img class=\"alignright wp-image-69633 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/notificaitons-300x274.jpg\" alt=\"Email Sending in Percona Monitoring and Management\" width=\"300\" height=\"274\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/notificaitons-300x274.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/notificaitons-164x150.jpg 164w, https://www.percona.com/blog/wp-content/uploads/2020/07/notificaitons-367x335.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/notificaitons.jpg 511w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></li>\n</ul>\n<p>So there you have it&#8230;once you’ve got the SMTP server configured you can log in to Grafana as an admin and create a “notification channel”.  From that point forward you can use that notification channel on any Grafana alerts you’d like to create.  If you’re monitoring hundreds or even thousands of systems this will probably become too tedious to maintain all the alerts and you’ll want to look at <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/02/21/percona-monitoring-and-management-meet-prometheus-alertmanger/\">setting up an external alert manager</a>.</p>\n<p>You may also have noticed there’s a “welcome on signup” in the SMTP section of the grafana.ini.  If you coupled that with an <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/01/08/enable-ldap-on-percona-monitoring-and-management-pmm/\">LDAP/AD integration</a> you could customize the welcome email to send new users to the FAQ and avoid the myriad of “How do I…” questions that are likely headed your way!  Try it out and let us know how you use email integration in Grafana!</p>\n","descriptionType":"html","publishedDate":"Tue, 07 Jul 2020 16:10:44 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management.png","linkMd5":"0c09c9e90c54860b0dd12102bf336309","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_5/2020-07-25/1595696397972_821194301b924bfd.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management.png","author":"Steve Hoffman","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_4/2020-07-25/1595696404403_e8d8ee2bc96a06cd.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/email-sending-percona-monitoring-and-management-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_2/2020-07-25/1595696405784_639d3fe8d1bb7218.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/notificaitons-300x274.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_5/2020-07-25/1595696400092_99ff3e65efb98c45.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"Analyzing MySQL with strace","link":"https://www.percona.com/blog/?p=69412","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Analyzing MySQL with strace\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69525\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-300x157.png\" alt=\"Analyzing MySQL with strace\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />In this blog post, we will briefly explore the OS tool <a target=\"_blank\" href=\"https://strace.io/\" target=\"_blank\" rel=\"noopener\"><code>strace</code></a>. It is not widely used due to its performance impacts, and we don’t recommend using it in production. Still, it is amazing at helping you understand some things that happen in MySQL, where the OS is involved, and as a last case resource for troubleshooting.</p>\n<p>The <code>strace</code> tool intercepts and records any system calls (a.k.a.  <code>syscalls</code>) performed and any signals received by a traced process. It is excellent for complex troubleshooting, but beware, as it has a high-performance impact for the traced process.</p>\n<p>We start our exploration with a simple question: what are the files opened in the OS when you issue <code>FLUSH LOGS</code> in MySQL? We could look at the documentation, but we decided to find out using <code>strace</code>.</p>\n<p>For that, we started a MySQL lab instance and attached <code>strace</code> to it using the command below:</p><pre class=\"crayon-plain-tag\">strace -s2048 -f -o /tmp/strace.out -p $(pgrep -x mysqld)</pre><p>The <code>-s</code> option is the size of the output print string. The default is 32, which most of the time is not enough, so we use 2k, which allows us to see more lines in the log.</p>\n<p>The <code>-f</code> is to follow forks. This is important because MySQL has a thread-based architecture, and the connection we want to analyze (the one running the <code>FLUSH LOGS</code>) is a forked thread from the main process.</p>\n<p>The option <code>-o</code> is used to specify an output file and <code>-p</code> specifies the PID to which we want to attach <code>strace</code> (the <code>mysqld</code> PID, in this case).</p>\n<p>First, we want to find out what is the OS Thread ID of our session. With that in hand, we can filter the <code>strace.out</code> file, so we don’t have unnecessary noise in our outputs.</p>\n<p>We can find our OS Thread ID with <code>SHOW PROCESSLIST</code> alongside the table <code>PERFORMANCE_SCHEMA.THREADS</code>:</p><pre class=\"crayon-plain-tag\">mysql&#62; show processlist;\n+----+------+-----------------+------+---------+------+----------+------------------+-----------+---------------+\n| Id | User | Host | db | Command | Time | State | Info | Rows_sent | Rows_examined |\n+----+------+-----------------+------+---------+------+----------+------------------+-----------+---------------+\n| 5 | root | localhost:47724 | test | Query | 0 | starting | show processlist | 0 | 0 |\n+----+------+-----------------+------+---------+------+----------+------------------+-----------+---------------+\n1 row in set (0.00 sec)\n\nmysql&#62; select THREAD_OS_ID from performance_schema.threads where PROCESSLIST_ID = 5;\n+--------------+\n| THREAD_OS_ID |\n+--------------+\n| 1851 |\n+--------------+\n1 row in set (0.00 sec)</pre><p>Now that we have our OS Thread ID, we can issue the <code>FLUSH LOGS</code> using that same session:</p><pre class=\"crayon-plain-tag\">mysql&#62; flush logs;\nQuery OK, 0 rows affected (0.16 sec)</pre><p>We <code>grep</code> the <code>strace.out</code> file using the Thread ID and look for open and close <code>syscalls</code> (which are the ones used to open and close files) as that is what we want to investigate:</p><pre class=\"crayon-plain-tag\">shell&#62; grep 1851 /tmp/strace.out | egrep \"open|close\"\n\n1851 open(\"/home/vinicius.grippa/sandboxes/rsandbox_5_7_22/master/data/msandbox.err\", O_WRONLY|O_CREAT|O_APPEND, 0666) = 32\n1851 close(32) = 0\n1851 open(\"/home/vinicius.grippa/sandboxes/rsandbox_5_7_22/master/data/msandbox.err\", O_WRONLY|O_CREAT|O_APPEND, 0666) = 32\n1851 close(32) = 0\n1851 openat(AT_FDCWD, \"./\", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 32\n1851 close(32) = 0\n1851 close(41) = 0\n1851 close(4) = 0\n1851 open(\"./mysql-bin.index\", O_RDWR|O_CREAT, 0640) = 4\n1851 open(\"./mysql-bin.~rec~\", O_RDWR|O_CREAT, 0640) = 32\n1851 close(32) = 0\n1851 open(\"./mysql-bin.~rec~\", O_RDWR|O_CREAT, 0640) = 32\n1851 open(\"./mysql-bin.000013\", O_WRONLY|O_CREAT, 0640) = 41\n1851 open(\"./mysql-bin.index_crash_safe\", O_RDWR|O_CREAT, 0640) = 42\n1851 close(42) = 0\n1851 close(4) = 0\n1851 open(\"./mysql-bin.index\", O_RDWR|O_CREAT, 0640) = 4\n1851 close(32) = 0</pre><p>Looking at the first line, we see the number 1851, which is the Thread ID that made the system calls (the one we used <code>grep</code> on) and then the system call (<code>open</code>) with the file <code>/home/vinicius.grippa/sandboxes/rsandbox_5_7_22/master/data/msandbox.err</code>, which is the configured error log for this particular lab instance.<br />\nWe also found the MySQL binlog index being opened (<code>mysql-bin.index</code>), a <code>mysql-bin.~rec~</code> file, the current MySQL binary log (<code>mysql-bin.000013</code>), the file <code>mysql-bin.index_crash_safe</code>, and lastly, <code>mysql-bin.index</code> again.</p>\n<p>The error log, the current binary log file, and the binary log index file were all expected to show up in the tests. The files <code>mysql-bin.~rec~</code> and <code>mysql-bin.index_crash_safe</code> were a surprise at first, but when you investigate a bit, it becomes clear what they are here for.</p>\n<p>Here is a quick look at the source code where both these are “created”:</p><pre class=\"crayon-plain-tag\">int MYSQL_BIN_LOG::set_purge_index_file_name(const char* base_file_name)\n{\n    int error = 0;\n    DBUG_TRACE;\n    if (fn_format(\n            purge_index_file_name, base_file_name, mysql_data_home, \".~rec~\",\n            MYF(MY_UNPACK_FILENAME | MY_SAFE_PATH | MY_REPLACE_EXT))\n        == nullptr) {\n        error = 1;\n        LogErr(ERROR_LEVEL, ER_BINLOG_FAILED_TO_SET_PURGE_INDEX_FILE_NAME);\n    }\n    return error;\n}</pre><p></p><pre class=\"crayon-plain-tag\">int MYSQL_BIN_LOG::set_crash_safe_index_file_name(const char* base_file_name)\n{\n    int error = 0;\n    DBUG_TRACE;\n    if (fn_format(crash_safe_index_file_name, base_file_name, mysql_data_home,\n            \".index_crash_safe\", MYF(MY_UNPACK_FILENAME | MY_SAFE_PATH | MY_REPLACE_EXT))\n        == nullptr) {\n        error = 1;\n        LogErr(ERROR_LEVEL, ER_BINLOG_CANT_SET_TMP_INDEX_NAME);\n    }\n    return error;\n}</pre><p>The function <code>fn_format</code> is used to format a file name. They are both auxiliary files that help in safely altering the binlog index file.</p>\n<h2>Conclusion</h2>\n<p>In conclusion, <code>strace</code> is a powerful tool that gives you unfiltered information on MySQL behavior (regarding system calls). It can be overwhelming, as it outputs a lot of information, but if you know what you are looking for, it is extremely helpful. As a final warning, it is a tool that severely degrades performance and should be avoided in production environments.</p>\n<h2>Useful Resources</h2>\n<p>Finally, you can reach us through the social networks, our forum, or access our material using the links presented below:</p>\n<ul>\n<li><a target=\"_blank\" href=\"https://www.percona.com/blog/\" target=\"_blank\" rel=\"noopener\"><b>Blog </b></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/resources/solution-brief\" target=\"_blank\" rel=\"noopener\"><b>Solution Briefs</b></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/resources/white-papers\" target=\"_blank\" rel=\"noopener\"><b>White Papers</b></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/resources/ebooks\" target=\"_blank\" rel=\"noopener\"><b>Ebooks</b></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/resources/technical-presentations\" target=\"_blank\" rel=\"noopener\"><b>Technical Presentations archive</b></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/resources/videos\" target=\"_blank\" rel=\"noopener\"><b>Videos/Recorded Webinars</b></a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/forums/\" target=\"_blank\" rel=\"noopener\"><strong>Forum</strong></a></li>\n<li><a target=\"_blank\" href=\"https://customers.percona.com/\" target=\"_blank\" rel=\"noopener\"><b>Knowledge Base (Percona Subscriber exclusive content)</b></a></li>\n</ul>\n","descriptionType":"html","publishedDate":"Tue, 30 Jun 2020 16:31:07 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace.png","linkMd5":"8b8cbd02c2f6e089e340466e55e46359","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace.png","author":"Leonardo Bacchi Fernandes","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_4/2020-07-25/1595696400666_0363d948fc165ebf.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Analyzing-MySQL-with-strace-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_3/2020-07-25/1595696402457_e62ab4a17c83e1cf.webp"}},{"createdTime":"2020-07-26 00:59:43","updatedTime":"2020-07-25 16:59:43","title":"MongoDB: Utilization of an Index on Subdocuments","link":"https://www.percona.com/blog/?p=69770","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MongoDB Index on Subdocument\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-70105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-300x168.png\" alt=\"MongoDB Index on Subdocument\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />MongoDB has a lot of possibilities for creating indexes. We have seen in previous articles some of the available index types and discussed explain() usage:</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/2018/12/19/using-partial-and-sparse-indexes-in-mongodb/\">Using Partial and Sparse Indexes in MongoDB</a></p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/2018/09/04/mongodb-index-usage-and-mongodb-explain-part-1/\">MongoDB Index Types and MongoDB explain() (part 1)</a></p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/2018/09/06/mongodb-investigate-queries-with-explain-index-usage-part-2/\">MongoDB: Investigate Queries with explain() and Index Usage (part 2)</a></p>\n<p>You can have a look at those if you need to review more details on the indexes and explain() investigation. In this article, we&#8217;ll see a particular case of an index created on an entire subdocument. We&#8217;ll see what you should be aware of when using it.</p>\n<h2>Create a Test Collection</h2>\n<p>Let&#8217;s create a collection with some random documents to run our tests. You can use the following javascript code to generate the sample <strong>test</strong> collection.</p><pre class=\"crayon-plain-tag\">for (var i = 1; i &#60;= 10000; i++) {\n\tdb.test.insert(\n\t\t{\n\t\t\tname: \"name_\"+i,\n\t\t\tsubdoc: {\n\t\t\t\ta: i,\n\t\t\t\tb: i*2,\n\t\t\t\tc: i*i\t\n\t\t\t}\n\t\t}\n\t)\n}</pre><p>Let&#8217;s take a look at the documents we&#8217;ve created:</p><pre class=\"crayon-plain-tag\">&#62; db.test.find().pretty()\n{\n\t\"_id\" : ObjectId(\"5f180f0fbdf0c5397723a6fe\"),\n\t\"name\" : \"name_1\",\n\t\"subdoc\" : {\n\t\t\"a\" : 1,\n\t\t\"b\" : 2,\n\t\t\"c\" : 1\n\t}\n}\n{\n\t\"_id\" : ObjectId(\"5f180f0fbdf0c5397723a6ff\"),\n\t\"name\" : \"name_2\",\n\t\"subdoc\" : {\n\t\t\"a\" : 2,\n\t\t\"b\" : 4,\n\t\t\"c\" : 4\n\t}\n}\n{\n\t\"_id\" : ObjectId(\"5f180f0fbdf0c5397723a700\"),\n\t\"name\" : \"name_3\",\n\t\"subdoc\" : {\n\t\t\"a\" : 3,\n\t\t\"b\" : 6,\n\t\t\"c\" : 9\n\t}\n}\n...\n...</pre><p>Now create an index on the subdocument <em><strong>subdoc</strong></em></p><pre class=\"crayon-plain-tag\">&#62; db.test.createIndex( { subdoc: 1  } )\n{\n\t\"createdCollectionAutomatically\" : false,\n\t\"numIndexesBefore\" : 1,\n\t\"numIndexesAfter\" : 2,\n\t\"ok\" : 1\n}</pre><p>&#160;</p>\n<h2>Querying the Subdocument</h2>\n<p>Now that we have the index on the subdocument, we can try to run some queries and see if the index is used or not.</p>\n<p>Run a query to find a subdocument:</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { subdoc: { a:220, b:440, c: 48400 } } ).pretty()\n{\n\t\"_id\" : ObjectId(\"5f180f0fbdf0c5397723a7d9\"),\n\t\"name\" : \"name_220\",\n\t\"subdoc\" : {\n\t\t\"a\" : 220,\n\t\t\"b\" : 440,\n\t\t\"c\" : 48400\n\t}\n}</pre><p>The query worked, let&#8217;s have a look at the execution plan using the explain():</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { subdoc: { a:220, b:440, c: 48400 } } ).explain()\n{\n\t\"queryPlanner\" : {\n\t\t\"plannerVersion\" : 1,\n\t\t\"namespace\" : \"corra.test\",\n\t\t\"indexFilterSet\" : false,\n\t\t\"parsedQuery\" : {\n\t\t\t\"subdoc\" : {\n\t\t\t\t\"$eq\" : {\n\t\t\t\t\t\"a\" : 220,\n\t\t\t\t\t\"b\" : 440,\n\t\t\t\t\t\"c\" : 48400\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"queryHash\" : \"07E4A30B\",\n\t\t\"planCacheKey\" : \"759877DE\",\n\t\t\"winningPlan\" : {\n\t\t\t\"stage\" : \"FETCH\",\n\t\t\t\"inputStage\" : {\n\t\t\t\t\"stage\" : \"IXSCAN\",\n\t\t\t\t\"keyPattern\" : {\n\t\t\t\t\t\"subdoc\" : 1\n\t\t\t\t},\n\t\t\t\t\"indexName\" : \"subdoc_1\",\n\t\t\t\t\"isMultiKey\" : false,\n\t\t\t\t\"multiKeyPaths\" : {\n\t\t\t\t\t\"subdoc\" : [ ]\n\t\t\t\t},\n\t\t\t\t\"isUnique\" : false,\n\t\t\t\t\"isSparse\" : false,\n\t\t\t\t\"isPartial\" : false,\n\t\t\t\t\"indexVersion\" : 2,\n\t\t\t\t\"direction\" : \"forward\",\n\t\t\t\t\"indexBounds\" : {\n\t\t\t\t\t\"subdoc\" : [\n\t\t\t\t\t\t\"[{ a: 220.0, b: 440.0, c: 48400.0 }, { a: 220.0, b: 440.0, c: 48400.0 }]\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"rejectedPlans\" : [ ]\n\t},\n\t\"serverInfo\" : {\n\t\t\"host\" : \"mdb1\",\n\t\t\"port\" : 27017,\n\t\t\"version\" : \"4.2.2-3\",\n\t\t\"gitVersion\" : \"2cdb6e50913583f627acc5de35dc4e04dbfe196f\"\n\t},\n\t\"ok\" : 1\n}</pre><p>Cool, the index we&#8217;ve created on the subdocument has been used for solving the query. You can see that the FETCH stage used <strong>IXSCAN</strong> on index <em><strong>subdoc_1</strong></em>.</p>\n<p>Let&#8217;s now try some different queries on the subdocument&#8217;s fields. We would like to execute a query to find the documents havign  <em>b=440 and c=48400.</em></p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { subdoc: { b:440, c:48400 }  }  ).pretty()\n&#62;</pre><p>No results. Indeed this is not the right syntax for the query.  When querying only a few of the fields in the subdocument we need to use the dot-notation. Only exact match filters are supported for querying a subdocument.</p>\n<p>Let&#8217;s try the query using the dot-notation:</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { \"subdoc.b\":440, \"subdoc.c\":48400 } ).pretty()\n{\n\t\"_id\" : ObjectId(\"5f180f0fbdf0c5397723a7d9\"),\n\t\"name\" : \"name_220\",\n\t\"subdoc\" : {\n\t\t\"a\" : 220,\n\t\t\"b\" : 440,\n\t\t\"c\" : 48400\n\t}\n}</pre><p>Ok, now we&#8217;ve got the result. Let&#8217;s take a look at the execution plan:</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { \"subdoc.b\":440, \"subdoc.c\":48400 } ).explain()\n{\n\t\"queryPlanner\" : {\n\t\t\"plannerVersion\" : 1,\n\t\t\"namespace\" : \"corra.test\",\n\t\t\"indexFilterSet\" : false,\n\t\t\"parsedQuery\" : {\n\t\t\t\"$and\" : [\n\t\t\t\t{\n\t\t\t\t\t\"subdoc.b\" : {\n\t\t\t\t\t\t\"$eq\" : 440\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"subdoc.c\" : {\n\t\t\t\t\t\t\"$eq\" : 48400\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t\"queryHash\" : \"D50037C0\",\n\t\t\"planCacheKey\" : \"D50037C0\",\n\t\t\"winningPlan\" : {\n\t\t\t\"stage\" : \"COLLSCAN\",\n\t\t\t\"filter\" : {\n\t\t\t\t\"$and\" : [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"subdoc.b\" : {\n\t\t\t\t\t\t\t\"$eq\" : 440\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"subdoc.c\" : {\n\t\t\t\t\t\t\t\"$eq\" : 48400\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"direction\" : \"forward\"\n\t\t},\n\t\t\"rejectedPlans\" : [ ]\n\t},\n\t\"serverInfo\" : {\n\t\t\"host\" : \"mdb1\",\n\t\t\"port\" : 27017,\n\t\t\"version\" : \"4.2.2-3\",\n\t\t\"gitVersion\" : \"2cdb6e50913583f627acc5de35dc4e04dbfe196f\"\n\t},\n\t\"ok\" : 1\n}</pre><p>The winning plan is a <strong>COLLSCAN</strong>. MongoDB is not able to use the index.</p>\n<p>Maybe it&#8217;s because the filter condition was not the left-prefix of the subdocument. We queried for <em>{ b:400, c:48400}</em>. Let&#8217;s try to query on the left-prefix involving <strong>a</strong> and <strong>b</strong> only and see what happens with the execution plan.</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { \"subdoc.a\":220, \"subdoc.b\":440 } ).pretty()\n{\n\t\"_id\" : ObjectId(\"5f180f0fbdf0c5397723a7d9\"),\n\t\"name\" : \"name_220\",\n\t\"subdoc\" : {\n\t\t\"a\" : 220,\n\t\t\"b\" : 440,\n\t\t\"c\" : 48400\n\t}\n}\n&#62; db.test.find( { \"subdoc.a\":220, \"subdoc.b\":440 } ).explain()\n{\n\t\"queryPlanner\" : {\n\t\t\"plannerVersion\" : 1,\n\t\t\"namespace\" : \"corra.test\",\n\t\t\"indexFilterSet\" : false,\n\t\t\"parsedQuery\" : {\n\t\t\t\"$and\" : [\n\t\t\t\t{\n\t\t\t\t\t\"subdoc.a\" : {\n\t\t\t\t\t\t\"$eq\" : 220\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"subdoc.b\" : {\n\t\t\t\t\t\t\"$eq\" : 440\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t\"queryHash\" : \"10B20F88\",\n\t\t\"planCacheKey\" : \"10B20F88\",\n\t\t\"winningPlan\" : {\n\t\t\t\"stage\" : \"COLLSCAN\",\n\t\t\t\"filter\" : {\n\t\t\t\t\"$and\" : [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"subdoc.a\" : {\n\t\t\t\t\t\t\t\"$eq\" : 220\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"subdoc.b\" : {\n\t\t\t\t\t\t\t\"$eq\" : 440\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"direction\" : \"forward\"\n\t\t},\n\t\t\"rejectedPlans\" : [ ]\n\t},\n\t\"serverInfo\" : {\n\t\t\"host\" : \"mdb1\",\n\t\t\"port\" : 27017,\n\t\t\"version\" : \"4.2.2-3\",\n\t\t\"gitVersion\" : \"2cdb6e50913583f627acc5de35dc4e04dbfe196f\"\n\t},\n\t\"ok\" : 1\n}</pre><p>Oh no, it&#8217;s still a <strong>COLLSCAN</strong>.</p>\n<p>The only query that can use the index on a subdocument is the exact-match. We need to provide the filter on all the fields of the subdocument. For any other different condition, MongoDB won&#8217;t be able to rely on the index.</p>\n<p>Also, the order of the fields in the filter must be exactly the same as in the document. Look at the following queries:</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { subdoc: { a:220, b:440, c:48400 } } )\n{ \"_id\" : ObjectId(\"5f180f0fbdf0c5397723a7d9\"), \"name\" : \"name_220\", \"subdoc\" : { \"a\" : 220, \"b\" : 440, \"c\" : 48400 } }\n&#62; db.test.find( { subdoc: { c: 48400, a:220, b:440 } } )\n&#62;</pre><p>Only the query with the right order returned the result. The second query didn&#8217;t return anything because the order doesn&#8217;t match the original order. Be aware of that.</p>\n<p>So, if you need to query internal fields on a subdocument and not the entire subdocument, you need to provide dot-notation filters only.</p>\n<h2>What About the Index?</h2>\n<p>We have seen that we need to run the query writing a different filter and that we cannot benefit from the index we&#8217;ve created. So, the index on the entire subdocument is not very useful. Indeed it&#8217;s not for the purpose of our queries.</p>\n<p>Now, if we would like to optimize the query we&#8217;re running using dot-notation we need to create additional indexes on the internal files only. For example, if the majority of our queries will use a filter on a and b only, we should create the following index:</p><pre class=\"crayon-plain-tag\">&#62; db.test.createIndex( { \"subdoc.a\":1, \"subdoc.b\":1 } )\n{\n\t\"createdCollectionAutomatically\" : false,\n\t\"numIndexesBefore\" : 2,\n\t\"numIndexesAfter\" : 3,\n\t\"ok\" : 1\n}</pre><p>Let&#8217;s now see if the query can use the new index.</p><pre class=\"crayon-plain-tag\">&#62; db.test.find( { \"subdoc.a\":220, \"subdoc.b\":440 } ).explain()\n{\n\t\"queryPlanner\" : {\n\t\t\"plannerVersion\" : 1,\n\t\t\"namespace\" : \"corra.test\",\n\t\t\"indexFilterSet\" : false,\n\t\t\"parsedQuery\" : {\n\t\t\t\"$and\" : [\n\t\t\t\t{\n\t\t\t\t\t\"subdoc.a\" : {\n\t\t\t\t\t\t\"$eq\" : 220\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"subdoc.b\" : {\n\t\t\t\t\t\t\"$eq\" : 440\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t},\n\t\t\"queryHash\" : \"10B20F88\",\n\t\t\"planCacheKey\" : \"B9C95AB7\",\n\t\t\"winningPlan\" : {\n\t\t\t\"stage\" : \"FETCH\",\n\t\t\t\"inputStage\" : {\n\t\t\t\t\"stage\" : \"IXSCAN\",\n\t\t\t\t\"keyPattern\" : {\n\t\t\t\t\t\"subdoc.a\" : 1,\n\t\t\t\t\t\"subdoc.b\" : 1\n\t\t\t\t},\n\t\t\t\t\"indexName\" : \"subdoc.a_1_subdoc.b_1\",\n\t\t\t\t\"isMultiKey\" : false,\n\t\t\t\t\"multiKeyPaths\" : {\n\t\t\t\t\t\"subdoc.a\" : [ ],\n\t\t\t\t\t\"subdoc.b\" : [ ]\n\t\t\t\t},\n\t\t\t\t\"isUnique\" : false,\n\t\t\t\t\"isSparse\" : false,\n\t\t\t\t\"isPartial\" : false,\n\t\t\t\t\"indexVersion\" : 2,\n\t\t\t\t\"direction\" : \"forward\",\n\t\t\t\t\"indexBounds\" : {\n\t\t\t\t\t\"subdoc.a\" : [\n\t\t\t\t\t\t\"[220.0, 220.0]\"\n\t\t\t\t\t],\n\t\t\t\t\t\"subdoc.b\" : [\n\t\t\t\t\t\t\"[440.0, 440.0]\"\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"rejectedPlans\" : [ ]\n\t},\n\t\"serverInfo\" : {\n\t\t\"host\" : \"mdb1\",\n\t\t\"port\" : 27017,\n\t\t\"version\" : \"4.2.2-3\",\n\t\t\"gitVersion\" : \"2cdb6e50913583f627acc5de35dc4e04dbfe196f\"\n\t},\n\t\"ok\" : 1\n}</pre><p>Great, it&#8217;s an <strong>IXSCAN</strong>! The new index is used and the query will run faster.</p>\n<h3>Conclusion</h3>\n<p>An index on a subdocument can be used only in the case of an exact match filter. For any other kind of queries against only a few fields in the subdocument, we must provide additional simple or compound indexes.</p>\n<p>So, the index on a subdocument is useless? <strong>No, it&#8217;s not</strong>. It depends on the queries you need to run. If you always have exact match filters then the index is useful. Also, you can create the index in combination with the <em><strong>unique</strong></em> clause if you need to assure the uniqueness of the subdocument. By the way, in this second case, you can achieve the same even with a compound unique index on all the fields.</p>\n","descriptionType":"html","publishedDate":"Fri, 24 Jul 2020 14:14:31 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument.png","linkMd5":"e02fac47256d99341171affaa0e70722","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_2/2020-07-25/1595696397902_9cdc4559776f33c4.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument.png","author":"Corrado Pandiani","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_1/2020-07-25/1595696401210_ef96de493cb77e14.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/MongoDB-Index-on-Subdocument-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_5/2020-07-25/1595696401634_0f320d3cb0f3e57b.webp"}},{"createdTime":"2020-07-26 00:59:42","updatedTime":"2020-07-25 16:59:42","title":"Updates to Percona Kubernetes Operator for Percona XtraDB Cluster","link":"https://www.percona.com/blog/?p=69892","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Updates to Percona Kubernetes Operator for Percona XtraDB Cluster\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69904\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-300x157.png\" alt=\"Updates to Percona Kubernetes Operator for Percona XtraDB Cluster\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />On July 21, 2020, Percona delivered an updated version of our <a target=\"_blank\" href=\"https://www.percona.com/software/percona-kubernetes-operators\">Percona Kubernetes Operator</a> for Percona XtraDB Cluster (PXC) focused on easing deployment and operations management of a clustered MySQL environment. Included in the Percona Distribution for MySQL, our Operator is based on the best practices for MySQL cluster configuration and setup in Kubernetes. This update adds a variety of important new features including:</p>\n<p><strong>Smart Update to Safely and Reliably Upgrade your PXC Environment Automatically</strong><br />\nWe implemented a new update strategy called <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/update.html#upgrading-percona-xtradb-cluster\">Smart Update</a>. Smart Update is aware of the context of your environment and minimizes the number of failover events that need to occur to fully upgrade a database cluster.</p>\n<p>For example, if you have a 3 member Percona XtraDB Cluster and Pod 1 is the writer, Smart Update ensures that Pod 0 and Pod 2 are upgraded and restarted first, ensuring that replication version constraints are honored and that only a single failover must occur when Pod 1 goes offline during its upgrade.</p>\n<p><strong>Smart Update Functionality Beyond Upgrade</strong><br />\nIn addition to managing failover during an upgrade, <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/update.html#upgrading-percona-xtradb-cluster\">Smart Update</a> can be used independently to reliably restart a cluster after an alteration, such as reallocation of resources or changing a password. In these cases, Smart Update again ensures that the entire cluster restart is handled with minimum failover.</p>\n<p><strong>Percona Version Service API for Kubernetes Operators</strong><br />\nThe Kubernetes Version Service API, which is an important component for automatic upgrades, can also be called directly. This API describes the <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/update.html#upgrading-percona-xtradb-cluster\">version information</a> for the current environment and can be used to assist with decisions to upgrade or when support help is needed.</p>\n<p><strong>Provide HAProxy as the Default Proxy Manager</strong><span><br />\n</span>HAProxy is a free, fast, and reliable solution offering high availability, load balancing, and proxying. It is now the <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/haproxy-conf.html\">default proxy manager</a> for Percona XtraDB Cluster because it is more directly compatible with MySQL, and its support of rules-based access control allows users to more easily control access to their database endpoints within a Kubernetes environment.</p>\n<p><strong>Automatic Syncing of MySQL Users into ProxySQL</strong><br />\nIf you choose to use ProxySQL with Percona XtraDB Cluster, the Operator now supports <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/users.html\">automatic synchronization</a> of MySQL users into the ProxySQL user manager.</p>\n<p><strong>Automated Password Rotation for Admin Users</strong><span><br />\n</span>This enables users to <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/users.html#system-users-rotation\">enforce password rotation</a> policies for system users in the Operator for long-running production databases. Using the standard Kubernetes API to change passwords and validate the age of the current password using their own audit systems is now provided.</p>\n<p><strong>Support for Compressed Backups</strong><br />\nWhen using the Operator to deploy your environment, it is possible to <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/backups.html#enabling-compression-for-backups\">implement LZ4 compression</a> for backups and Initial Node Synchronization (SST).</p>\n<p>To learn more about the latest release of the Percona Kubernetes Operator for Percona XtraDB Cluster, check out the <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/ReleaseNotes/Kubernetes-Operator-for-PXC-RN1.5.0.html\">release notes</a> and <a target=\"_blank\" href=\"https://github.com/percona/percona-xtradb-cluster-operator\">download the Operator</a>.</p>\n","descriptionType":"html","publishedDate":"Tue, 21 Jul 2020 17:20:31 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster.png","linkMd5":"45d9e9d2a52fcd61291ff236cc9f235b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_4/2020-07-25/1595696397958_9e9cefe3556c4f98.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster.png","author":"Rick Golba","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_3/2020-07-25/1595696399915_38a49eb0d3b48d47.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Updates-to-Percona-Kubernetes-Operator-for-Percona-XtraDB-Cluster-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_1/2020-07-25/1595696401270_81ebb14d6786ec22.webp"}},{"createdTime":"2020-07-26 00:59:47","updatedTime":"2020-07-25 16:59:47","title":"binlog2sql: Binlog to Raw SQL Conversion and Point In Time Recovery","link":"https://www.percona.com/blog/?p=69695","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"binlog2sql\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span><img class=\"alignright size-medium wp-image-69763\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-300x157.png\" alt=\"binlog2sql\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />In this blog, I will look at the tool </span><a target=\"_blank\" href=\"https://github.com/danfengcao/binlog2sql\" target=\"_blank\" rel=\"noopener\"><span>binlog2sql</span></a><span>, designed by Cao Danfeng. The tool will help to decode the MySQL binary logs and extract the raw SQL, which also helps to generate the ROLLBACK statements for point in time recovery (PITR) using the “flashback” feature. This feature is also implemented in MariaDB&#8217;s version of “mysqlbinlog” (Check out <a target=\"_blank\" href=\"https://www.percona.com/blog/2018/04/12/point-in-time-recovery-pitr-in-mysql-mariadb-percona-server/\">Flashback: Another Take on Point-In-Time Recovery in MySQL/MariaDB/Percona Server</a> by</span><span> Przemysław Malkowski, which describes this feature).</span></p>\n<p><span>With the native mysqlbinlog tool, we can convert the binary logs from binary to text format. We can also generate the exact SQL statements using the mysqlbinlog utility. It depends on the “binlog_format &#38; </span><span>binlog_rows_query_log_events</span><span>” parameters. Anyway, the tool really doesn&#8217;t have the feature to generate the ROLLBACK statements. </span></p>\n<ul>\n<li><span>With “binlog_format =  STATEMENT” format we do get SQL using mysqlbinlog. </span></li>\n<li><span>With the parameter </span><span>“</span><span>binlog_rows_query_log_events = ON</span><span>”</span><span> we do get both the SQL and binlog events using mysqlbinlog. </span></li>\n</ul>\n<h2>Mysqlbinlog vs Binlog2sql</h2>\n<p><span>Before jumping into the binlog2sql tool, I wanted to compare the outputs from Mysqlbinlog and bilog2sql. In this section, I am going to show the output format from each tool on various options. For reference,  I have used the below query to generate the binlog events.</span></p><pre class=\"crayon-plain-tag\">mysql&#62; insert into binlog2sqlVSmysqlbinlog (name) values ('hercules7sakthi');\nQuery OK, 1 row affected (0.00 sec)</pre><p>&#160;</p>\n<h3>Mysqlbinlog:</h3>\n<p><em><span>Mysqlbinlog with ( binlog_format=ROW, binlog_rows_query_log_events=OFF ),</span></em></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 ~]# mysqlbinlog --base64-output=decode-rows -vv --start-position 313 --stop-position 449 /var/lib/mysql/percona-bin.000003  \n... \n### INSERT INTO `percona`.`binlog2sqlVSmysqlbinlog` \n### SET \n###   @1=1 /* INT meta=0 nullable=0 is_null=0 */ \n###   @2='hercules7sakthi' /* VARSTRING(64) meta=64 nullable=1 is_null=0 */ \n...</pre><p></p>\n<ul>\n<li>Output: Text format, not real SQL.</li>\n<li>Rollback: The tool doesn&#8217;t have the feature to generate rollback statements.</li>\n</ul>\n<p><em><span>Mysqlbinlog with ( binlog_format = STATEMENT ),</span></em></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 ~]# mysqlbinlog --base64-output=decode-rows -vv --start-position 323 --stop-position 507 /var/lib/mysql/percona-bin.000004 \n...\n/*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/;\ninsert into binlog2sqlVSmysqlbinlog (name) values ('hercules7sakthi')\n/*!*/;\n...</pre><p></p>\n<ul>\n<li>Output: SQL format.</li>\n<li>Rollback: The tool doesn&#8217;t have the feature to generate rollback statements.</li>\n</ul>\n<p><em><span>Mysqlbinlog with ( binlog_format=ROW and </span><span>binlog_rows_query_log_events=ON </span><span>),</span></em></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 ~]# mysqlbinlog --base64-output=decode-rows -vv /var/lib/mysql/percona-bin.000006 --start-position 313\n...\n# insert into binlog2sqlVSmysqlbinlog (name) values ('hercules7sakthi')\n...\n### INSERT INTO `percona`.`binlog2sqlVSmysqlbinlog`\n### SET\n###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n###   @2='hercules7sakthi' /* VARSTRING(64) meta=64 nullable=1 is_null=0 */\n...</pre><p></p>\n<ul>\n<li>Output: SQL and Text format.</li>\n<li>Rollback: The tool doesn&#8217;t have the feature to generate rollback statements.</li>\n</ul>\n<h3>Binlog2sql:</h3>\n<p></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 ~]# ./binlog2sql/binlog2sql/binlog2sql.py --user percona -p --start-file percona-bin.000007\nPassword: \nINSERT INTO `percona`.`binlog2sqlVSmysqlbinlog`(`id`, `name`) VALUES (1, 'hercules7sakthi'); #start 4 end 542 time 2020-06-21 22:36:16</pre><p></p>\n<ul>\n<li>Output: SQL format</li>\n<li>Rollback: Possible with binlog2sql</li>\n</ul>\n<h2>What is binlog2sql?</h2>\n<ul>\n<li><span>It is an open source tool to parse the binary logs.</span></li>\n<li><span>It has the feature to extract the raw SQL statements from binary logs.</span></li>\n<li><span>It has the feature to generate the ROLLBACK SQL from binary logs for point-in-time recovery.</span></li>\n</ul>\n<h2>MySQL Requirements</h2>\n<p></p><pre class=\"crayon-plain-tag\">#vi my.cnf\n\n[mysqld]\n#binlog\nserver-id = #id\nlog-bin\nbinlog_format = row\nbinlog_row_image = full</pre><p><span>MySQL server must be active for two reasons:</span></p>\n<ul>\n<li><span>binlog2sql is based on BINLOG_DUMP protocol to obtain binlog content.</span></li>\n<li><span>binlog2sql is necessary to read the INFORMATION_SCHEMA.COLUMNS table from the server, to obtain metadata of the table. (</span><span>A server with lots of tables can have significant overhead to query INFORMATION_SCHEMA.COLUMNS table.</span><span>)</span></li>\n</ul>\n<h2>Installation</h2>\n<p></p><pre class=\"crayon-plain-tag\">git clone https://github.com/danfengcao/binlog2sql.git &#38;&#38; cd binlog2sql\npip install -r requirements.txt</pre><p></p>\n<h2>Tested Environment</h2>\n<ul>\n<li>Python 2.7, 3.4+</li>\n<li>MySQL 5.6,5.7,8+</li>\n</ul>\n<p><b>Note:</b><span> Make sure you need to have “pymysql &#8211; 0.9.3 ” for MySQL 8+ support. </span></p>\n<h2>Binlog2sql Options</h2>\n<p></p><pre class=\"crayon-plain-tag\">optional arguments:\n  --stop-never          Continuously parse binlog. default: stop at the latest\n                        event when you start.\n  --help                help information\n  -K, --no-primary-key  Generate insert sql without primary key if exists\n  -B, --flashback       Flashback data to start_position of start_file\n  --back-interval BACK_INTERVAL\n                        Sleep time between chunks of 1000 rollback sql. set it\n                        to 0 if do not need sleep\nconnect setting:\n  -h HOST, --host HOST  Host the MySQL database server located\n  -u USER, --user USER  MySQL Username to log in as\n  -p [PASSWORD [PASSWORD ...]], --password [PASSWORD [PASSWORD ...]]\n                        MySQL Password to use\n  -P PORT, --port PORT  MySQL port to use\ninterval filter:\n  --start-file START_FILE\n                        Start binlog file to be parsed\n  --start-position START_POS, --start-pos START_POS\n                        Start position of the --start-file\n  --stop-file END_FILE, --end-file END_FILE\n                        Stop binlog file to be parsed. default: '--start-file'\n  --stop-position END_POS, --end-pos END_POS\n                        Stop position. default: latest position of '--stop-\n                        file'\n  --start-datetime START_TIME\n                        Start time. format %Y-%m-%d %H:%M:%S\n  --stop-datetime STOP_TIME\n                        Stop Time. format %Y-%m-%d %H:%M:%S;\nschema filter:\n  -d [DATABASES [DATABASES ...]], --databases [DATABASES [DATABASES ...]]\n                        dbs you want to process\n  -t [TABLES [TABLES ...]], --tables [TABLES [TABLES ...]]\n                        tables you want to process\ntype filter:\n  --only-dml            only print dml, ignore ddl\n  --sql-type [SQL_TYPE [SQL_TYPE ...]]\n                        Sql type you want to process, support INSERT, UPDATE,\n                        DELETE.</pre><p></p>\n<h2>How to Extract the Raw SQL From Binary Logs</h2>\n<p><span>For testing purposes, I have installed a MySQL server and created the following data:</span></p><pre class=\"crayon-plain-tag\">mysql&#62; use percona\nDatabase changed\n\nmysql&#62; create table binlog2sql(id int primary key auto_increment, name varchar(16), status enum('A','NA'), up_date datetime default current_timestamp);\nQuery OK, 0 rows affected (0.16 sec)\n\nmysql&#62; insert into binlog2sql (name,status) values ('kani','A'),('ram','A'),('durai','A'),('asha','A'),('sakthi','A');\nQuery OK, 5 rows affected (0.00 sec)\nRecords: 5  Duplicates: 0  Warnings: 0</pre><p><span>Decoding binary log Using binlog2sql:</span></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -p --start-file percona-bin.000002 --start-position 321 --stop-position 510  | cut -f1 -d\"#\" \nPassword:  \nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 1, 'kani');  \nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 2, 'ram');\nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 3, 'durai');  \nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 4, 'asha');  \nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 5, 'sakthi');</pre><p></p>\n<h2>How Does binlog2sql Support PITR?</h2>\n<ul>\n<li><span>Binlog2sql tool has the option &#8220;</span><b>&#8211;flashback&#8221;</b><span>, which will help to generate the ROLLBACK statements.</span></li>\n<li><span>We can recover the data from DELETE and UPDATE statements. </span></li>\n<li><span>It will not support DDL ( DROP, TRUNCATE ) as the actual row events will not be available in the binary logs.</span></li>\n</ul>\n<h3>Case 1 &#8211; PITR from DELETE:</h3>\n<p><span>To test this case, I have deleted two records <strong>“id in (4,5)”</strong> from my test table “binlog2sql”.</span></p><pre class=\"crayon-plain-tag\">mysql&#62; delete from binlog2sql where id in (4,5);\nQuery OK, 2 rows affected (0.03 sec)\n\nmysql&#62; select * from binlog2sql;\n+----+-------+--------+---------------------+\n| id | name  | status | up_date             |\n+----+-------+--------+---------------------+\n|  1 | kani  | A      | 2020-06-20 14:54:51 |\n|  2 | ram   | A      | 2020-06-20 14:54:51 |\n|  3 | durai | A      | 2020-06-20 14:54:51 |\n+----+-------+--------+---------------------+\n3 rows in set (0.00 sec)</pre><p><span>For the reference, I am generating the raw SQL using binlog2sql.</span></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -pxxxxxxx --start-file percona-bin.000002 --start-position 1172 --stop-position 1344  | cut -f1 -d'#'\nDELETE FROM `percona`.`binlog2sql` WHERE `status`='A' AND `up_date`='2020-06-20 14:54:51' AND `id`=4 AND `name`='asha' LIMIT 1; \nDELETE FROM `percona`.`binlog2sql` WHERE `status`='A' AND `up_date`='2020-06-20 14:54:51' AND `id`=5 AND `name`='sakthi' LIMIT 1;</pre><p>The above DELETE&#8217;s are the exact statements, which executed to delete the records. Now, I am going to generate the ROLLBACK statements using the option &#8220;&#8211;flashback&#8221; as shown below,</p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -pxxxxxx --start-file percona-bin.000002 --start-position 1172 --stop-position 1344 --flashback  | cut -f1 -d'#'\nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 5, 'sakthi'); \nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 4, 'asha');</pre><p>You can see those DELETE statements are now converted to INSERT statements. Now, I can load<span> the rollback statements into the server,</span></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -pxxxxxx --start-file percona-bin.000002 --start-position 1172 --stop-position 1344 --flashback  | cut -f1 -d'#' | mysql -vvv \n--------------\nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 5, 'sakthi')\n--------------\nQuery OK, 1 row affected (0.03 sec)\n\n--------------\nINSERT INTO `percona`.`binlog2sql`(`status`, `up_date`, `id`, `name`) VALUES ('A', '2020-06-20 14:54:51', 4, 'asha')\n--------------\nQuery OK, 1 row affected (0.01 sec)\nBye\n\n[root@hercules7sakthi2 binlog2sql]# mysql -e \"select * from percona.binlog2sql\"\n+----+--------+--------+---------------------+\n| id | name   | status | up_date             |\n+----+--------+--------+---------------------+\n|  1 | kani   | A      | 2020-06-20 14:54:51 |\n|  2 | ram    | A      | 2020-06-20 14:54:51 |\n|  3 | durai  | A      | 2020-06-20 14:54:51 |\n|  4 | asha   | A      | 2020-06-20 14:54:51 |\n|  5 | sakthi | A      | 2020-06-20 14:54:51 |\n+----+--------+--------+---------------------+</pre><p>Recovered the data from DELETE.</p>\n<h3>Case 2 &#8211; PITR from UPDATE:</h3>\n<p><span>In this case, I am going to update two rows. I changed the status from “A” to “NA” for “id in (4,5)”.</span></p><pre class=\"crayon-plain-tag\">mysql&#62; update binlog2sql set status='NA' where id in (4,5);\nQuery OK, 2 rows affected (0.01 sec)\nRows matched: 2  Changed: 2  Warnings: 0\n\nmysql&#62; select * from binlog2sql;\n+----+--------+--------+---------------------+\n| id | name   | status | up_date             |\n+----+--------+--------+---------------------+\n|  1 | kani   | A      | 2020-06-20 14:54:51 |\n|  2 | ram    | A      | 2020-06-20 14:54:51 |\n|  3 | durai  | A      | 2020-06-20 14:54:51 |\n|  4 | asha   | NA     | 2020-06-20 14:54:51 |\n|  5 | sakthi | NA     | 2020-06-20 14:54:51 |\n+----+--------+--------+---------------------+\n5 rows in set (0.00 sec)</pre><p>For the reference, I am generating the executed UPDATEs using binlog2sql.</p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -pxxxxxxx --start-file percona-bin.000002 --start-position 2136 --stop-position 2343  | cut -f1 -d'#'\nUPDATE `percona`.`binlog2sql` SET `status`='NA', `up_date`='2020-06-20 14:54:51', `id`=4, `name`='asha' WHERE `status`='A' AND `up_date`='2020-06-20 14:54:51' AND `id`=4 AND `name`='asha' LIMIT 1; \nUPDATE `percona`.`binlog2sql` SET `status`='NA', `up_date`='2020-06-20 14:54:51', `id`=5, `name`='sakthi' WHERE `status`='A' AND `up_date`='2020-06-20 14:54:51' AND `id`=5 AND `name`='sakthi' LIMIT 1;</pre><p><span>Generating the ROLLBACK statements.</span></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -pxxxxxx --start-file percona-bin.000002 --start-position 2136 --stop-position 2343 --flashback  | cut -f1 -d'#'\nUPDATE `percona`.`binlog2sql` SET `status`='A', `up_date`='2020-06-20 14:54:51', `id`=5, `name`='sakthi' WHERE `status`='NA' AND `up_date`='2020-06-20 14:54:51' AND `id`=5 AND `name`='sakthi' LIMIT 1; \nUPDATE `percona`.`binlog2sql` SET `status`='A', `up_date`='2020-06-20 14:54:51', `id`=4, `name`='asha' WHERE `status`='NA' AND `up_date`='2020-06-20 14:54:51' AND `id`=4 AND `name`='asha' LIMIT 1;</pre><p><span>Loading the rollback statements into the server.</span></p><pre class=\"crayon-plain-tag\">[root@hercules7sakthi2 binlog2sql]# ./binlog2sql.py -upercona -pxxxxxxx --start-file percona-bin.000002 --start-position 2136 --stop-position 2343 --flashback  | cut -f1 -d'#' | mysql -vvv\n--------------\nUPDATE `percona`.`binlog2sql` SET `status`='A', `up_date`='2020-06-20 14:54:51', `id`=5, `name`='sakthi' WHERE `status`='NA' AND `up_date`='2020-06-20 14:54:51' AND `id`=5 AND `name`='sakthi' LIMIT 1\n--------------\nQuery OK, 1 row affected (0.02 sec)\nRows matched: 1  Changed: 1  Warnings: 0\n\n--------------\nUPDATE `percona`.`binlog2sql` SET `status`='A', `up_date`='2020-06-20 14:54:51', `id`=4, `name`='asha' WHERE `status`='NA' AND `up_date`='2020-06-20 14:54:51' AND `id`=4 AND `name`='asha' LIMIT 1\n--------------\nQuery OK, 1 row affected (0.00 sec)\nRows matched: 1  Changed: 1  Warnings: 0\nBye\n\n[root@hercules7sakthi2 binlog2sql]# mysql -e \"select * from percona.binlog2sql\"\n+----+--------+--------+---------------------+\n| id | name   | status | up_date             |\n+----+--------+--------+---------------------+\n|  1 | kani   | A      | 2020-06-20 14:54:51 |\n|  2 | ram    | A      | 2020-06-20 14:54:51 |\n|  3 | durai  | A      | 2020-06-20 14:54:51 |\n|  4 | asha   | A      | 2020-06-20 14:54:51 |\n|  5 | sakthi | A      | 2020-06-20 14:54:51 |\n+----+--------+--------+---------------------+</pre><p>The changes have been rolled back.</p>\n<h3>Conclusion</h3>\n<ul>\n<li><span>As I mentioned earlier, the tool will support rollback only for DELETE/UPDATE. </span></li>\n<li><span>The tool will not support DDL because the DDL statements do not log any actual data in the binary logs. </span></li>\n<li><span>The tool will not work with encrypted/compressed binary logs.</span></li>\n<li><span>The tool has been tested on MySQL 5.6 and MySQL 5.7 environments. It has the support for MySQL 8 with “pymysql &#8211; 0.9.3”.</span></li>\n</ul>\n","descriptionType":"html","publishedDate":"Thu, 09 Jul 2020 17:05:41 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql.png","linkMd5":"2b509ed403875a8dbe2453580e069f12","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_4/2020-07-25/1595696397963_d759ab975042cd51.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql.png","author":"Sri Sakthivel","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_1/2020-07-25/1595696401679_3f8a170ec20d53a0.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/binlog2sql-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_2/2020-07-25/1595696403171_3454f84c8cf57544.webp"}},{"createdTime":"2020-07-26 00:59:52","updatedTime":"2020-07-25 16:59:52","title":"MySQL Table Fragmentation: Beware of Bulk INSERT with FAILURE or ROLLBACK","link":"https://www.percona.com/blog/?p=69134","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL Table Fragmentation Insert\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69336\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-300x168.png\" alt=\"MySQL Table Fragmentation Insert\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Usually, database people are familiar with table fragmentation with DELETE statements. Whenever doing a huge delete, in most cases, they are always rebuilding the table to reclaim the disk space. <strong>But, are you thinking only DELETEs can cause table fragmentation?</strong> (<strong>Answer: NO</strong>).</p>\n<p>In this blog post, I am going to explain how table fragmentation is happening with the INSERT statement.</p>\n<p>Before going into the topic, we need to know that with MySQL, there are two kinds of fragmentation:</p>\n<ul>\n<li>Fragmentation where some of the InnoDB pages are completely free inside the table.</li>\n<li>Fragmentation where some of the InnoDB pages are not completely filled (the page has some free space).</li>\n</ul>\n<p>There are three major cases of table fragmentation with INSERTs :</p>\n<ul>\n<li>INSERT with ROLLBACK</li>\n<li>Failed INSERT statement</li>\n<li>Fragmentation with page-splits</li>\n</ul>\n<h2>Test Environment</h2>\n<p>I have created my own test environment to experiment with those cases.</p>\n<ul>\n<li>DB: percona</li>\n<li>Tables : frag, ins_frag, frag_page_spl</li>\n<li>Table Size: 2G</li>\n</ul>\n<h3>Case 1: INSERT with ROLLBACK</h3>\n<p><span>At first, I have created a new table “</span><b>ins_frag</b><span>”. Then I have created a transaction (with BEGIN) and started to copy the data from table “</span><b>frag</b><span>” to table “</span><b>ins_frag</b><span>” as shown below.</span></p><pre class=\"crayon-plain-tag\">mysql&#62; create table ins_frag like frag;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&#62; begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&#62; insert into ins_frag select * from frag;\nQuery OK, 47521280 rows affected (3 min 7.45 sec)\nRecords: 47521280  Duplicates: 0  Warnings: 0\n\n#Linux shell\nsakthi-3.2# ls -lrth\ntotal 8261632\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 02:43 frag.ibd\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 03:00 ins_frag.ibd</pre><p><span>From the above, you can see the INSERT was executed, but still, I did not commit/rollback the INSERT. You can note that both tables have occupied </span><b>2 GB</b><span> of disk space. </span></p>\n<p><strong>Now, I am going to ROLLBACK the INSERT.</strong></p><pre class=\"crayon-plain-tag\">mysql&#62; select count(*) from ins_frag;\n+----------+\n| count(*) |\n+----------+\n| 47521280 |\n+----------+\n1 row in set (1.87 sec)\n\nmysql&#62; rollback;\nQuery OK, 0 rows affected (5 min 45.21 sec)</pre><p></p><pre class=\"crayon-plain-tag\">mysql&#62; select count(*) from ins_frag;\n+----------+\n| count(*) |\n+----------+\n|        0 |\n+----------+\n1 row in set (0.00 sec)\n\n\n#Linux shell\nsakthi-3.2# ls -lrth\ntotal 8261632\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 02:43 frag.ibd\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 03:09 ins_frag.ibd</pre><p>&#160;</p>\n<p><span>Yes, after rollback the INSERT, the table “</span><b>ins_frag</b><span>” is still occupying the same <strong>2 GB</strong> of disk space. Let&#8217;s find the fragmented space through the MySQL client.</span></p>\n<p>&#160;</p><pre class=\"crayon-plain-tag\">mysql&#62; SELECT\n-&#62; table_schema as 'DATABASE',\n-&#62; table_name as 'TABLE',\n-&#62; CONCAT(ROUND(( data_length + index_length ) / ( 1024 * 1024 * 1024 ), 2), 'G') 'TOTAL',\n-&#62; CONCAT(ROUND(data_free / ( 1024 * 1024 * 1024 ), 2), 'G') 'DATAFREE'\n-&#62; FROM information_schema.TABLES\n-&#62; where table_schema='percona' and table_name='ins_frag';\n+----------+----------+-------+----------+\n| DATABASE | TABLE.   | TOTAL | DATAFREE |\n+----------+----------+-------+----------+\n| percona  | ins_frag | 0.00G | 1.96G    |\n+----------+----------+-------+----------+\n1 row in set (0.01 sec)</pre><p>&#160;</p>\n<p><span>So, this clears the rolling back the INSERT will create the fragmentation.  We need to rebuild the table to reclaim the disk space.</span></p>\n<p>&#160;</p><pre class=\"crayon-plain-tag\">mysql&#62; alter table ins_frag engine=innodb;\nQuery OK, 0 rows affected (2.63 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\n#Linux shell\n\nsakthi-3.2# ls -lrth\ntotal 4131040\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 02:43 frag.ibd\n-rw-r-----  1 _mysql  _mysql   112K Jun 17 03:11 ins_frag.ibd</pre><p>&#160;</p>\n<h3>Case 2: Failed INSERT Statement</h3>\n<p><span>To test this case, I have created two MySQL client sessions (session 1 and session 2). </span></p>\n<p><span>In session 1, I am running the same INSERT statement within the transaction.</span><span> But this time I have interrupted and killed the INSERT query at session 2.</span></p>\n<p><b>Session 1</b></p><pre class=\"crayon-plain-tag\">#Linux shell\n\nsakthi-3.2# ls -lrth\ntotal 4131040\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 02:43 frag.ibd\n-rw-r-----  1 _mysql  _mysql   112K Jun 17 04:02 ins_frag.ibd\n\n#MySQL shell\n\nmysql&#62; begin;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&#62; insert into ins_frag select * from frag;   #is running</pre><p><b>Session 2</b></p><pre class=\"crayon-plain-tag\">mysql&#62; pager grep -i insert ; show processlist;\nPAGER set to 'grep -i insert'\n| 33 | root            | localhost | percona | Query   |    14 | executing              | insert into ins_frag select * from frag |\n4 rows in set (0.00 sec)\n\nmysql&#62; kill 33;\nQuery OK, 0 rows affected (0.00 sec)</pre><p>The INSERT is interrupted and failed.</p>\n<p><b>Again, at Session 1:</b></p><pre class=\"crayon-plain-tag\">mysql&#62; insert into ins_frag select * from frag;\nERROR 2013 (HY000): Lost connection to MySQL server during query\n\n#Linux shell\n\nsakthi-3.2# ls -lrth\ntotal 4591616\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 02:43 frag.ibd\n-rw-r-----  1 _mysql  _mysql   212M Jun 17 04:21 ins_frag.ibd\n\n#MySQL shell\n\nmysql&#62; select count(*) from ins_frag;\n+----------+\n| count(*) |\n+----------+\n|        0 |\n+----------+\n1 row in set (0.10 sec)</pre><p>&#160;</p>\n<p><span>The INSERT is not completed and there is no data in the table. But still, the table .ibd file has grown up to 212M.  Let&#8217;s see the fragmented space through the MySQL client.</span></p>\n<p>&#160;</p><pre class=\"crayon-plain-tag\">mysql&#62; SELECT\n-&#62; table_schema as 'DATABASE',\n-&#62; table_name as 'TABLE',\n-&#62; CONCAT(ROUND(( data_length + index_length ) / ( 1024 * 1024 ), 2), 'M') 'TOTAL',\n-&#62; CONCAT(ROUND(data_free / ( 1024 * 1024 ), 2), 'M') 'DATAFREE'\n-&#62; FROM information_schema.TABLES\n-&#62; where table_schema='percona' and table_name='ins_frag';\n+----------+----------+---------+----------+\n| DATABASE | TABLE    | TOTAL   | DATAFREE |\n+----------+----------+---------+----------+\n| percona  | ins_frag | 0.03M   | 210.56M  |\n+----------+----------+---------+----------+\n1 row in set (0.01 sec)</pre><p>It shows the table has fragmented space and has to rebuild the table to reclaim the space.</p><pre class=\"crayon-plain-tag\">mysql&#62; alter table ins_frag engine='innodb';\nQuery OK, 0 rows affected (0.03 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\n#Linux shell\n\nsakthi-3.2# ls -lrth\ntotal 4131040\n-rw-r-----  1 _mysql  _mysql   2.0G Jun 17 02:43 frag.ibd\n-rw-r-----  1 _mysql  _mysql   112K Jun 17 04:32 ins_frag.ibd</pre><p></p>\n<h4></h4>\n<h4><strong>Case 3: Fragmentation with Page-Splits</strong></h4>\n<p>We know that internally, InnoDB records are stored in the InnoDB pages. Each page size is 16K by default, but you have the option to change the page size.</p>\n<p>If the InnoDB page doesn’t have enough space to accommodate the new record or index entry, it will be split in two pages, which will be about 50% full each. This means even for insert only workload, with no rollbacks or deletes, you may end up with only 75% avg page utilization &#8211; and so a 25% loss for this kind of internal page fragmentation.</p>\n<p>When the indexes are built by sort, they will have more congestion, and if the table has a lot of inserts that go to the random location in the index, it will cause the page-split.</p>\n<p>Check out this excellent blog written by Marco Tusa, <a target=\"_blank\" href=\"https://www.percona.com/blog/2017/04/10/innodb-page-merging-and-page-splitting/\">InnoDB Page Merging and Page Splitting,</a> that has the complete internals about the page-split and InnoDB page structure/operations.</p>\n<p>For an experiment, I have created a table with a sorted index (descending),</p>\n<p>&#160;</p><pre class=\"crayon-plain-tag\">mysql&#62; show create table frag_page_spl\\G\n*************************** 1. row ***************************\nTable: frag_page_spl\nCreate Table: CREATE TABLE `frag_page_spl` (\n`id` int NOT NULL AUTO_INCREMENT,\n`name` varchar(16) DEFAULT NULL,\n`messages` varchar(600) DEFAULT NULL,\nPRIMARY KEY (`id`),\nKEY `idx_spl` (`messages` DESC)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci\n1 row in set (0.07 sec)</pre><p>We can monitor the page split activity from the table INFORMATION_SCHEMA.INNODB_METRICS . For this, you need to enable the InnoDB monitor.</p><pre class=\"crayon-plain-tag\">mysql&#62; SET GLOBAL innodb_monitor_enable=all;\nQuery OK, 0 rows affected (0.09 sec)</pre><p>Then I have created the script to trigger the INSERTs randomly with 6 parallel threads. After the end of the scripts:</p><pre class=\"crayon-plain-tag\">mysql&#62; select name,count,type,status,comment from information_schema.innodb_metrics where name like '%index_page_spl%'\\G\n*************************** 1. row ***************************\nname: index_page_splits\ncount: 52186\ntype: counter\nstatus: enabled\ncomment: Number of index page splits\n1 row in set (0.05 sec)\n\nmysql&#62; SELECT\n-&#62; table_schema as 'DATABASE',\n-&#62; table_name as 'TABLE',\n-&#62; CONCAT(ROUND(( data_length + index_length ) / ( 1024 * 1024 ), 2), 'M') 'TOTAL',\n-&#62; CONCAT(ROUND(data_free / ( 1024 * 1024 ), 2), 'M') 'DATAFREE'\n-&#62; FROM information_schema.TABLES\n-&#62; where table_schema='percona' and table_name='frag_page_spl';\n+----------+---------------+----------+----------+\n| DATABASE | TABLE.        | TOTAL    | DATAFREE |\n+----------+---------------+----------+----------+\n| percona  | frag_page_spl | 2667.55M | 127.92M  |\n+----------+---------------+----------+----------+\n1 row in set (0.00 sec)</pre><p>Yes, from the metrics, we can see the page-split counter has increased. The output shows that there are <strong>52186 page-splits</strong> operations that occurred, which created <strong>127.92 MB</strong> of fragmentation.</p>\n<p>Once the split page is created, the only way to move back is to have the created page drop below the merge threshold. When that happens, InnoDB moves the data from the split page with a merge operation. MERGE_THRESHOLD is configurable for table and specific indexes.</p>\n<p>The other way is to reorganize the data is to OPTIMIZE the table. This can be a very heavy and long process, but often is the only way to recover from a situation where too many pages are located in sparse extents.</p>\n<h4><b>Conclusion</b></h4>\n<ul>\n<li><span>The first two cases are rare. Because most of the applications are not designed to write huge data in the table. </span></li>\n<li><span>You need to be aware of these issues, whenever doing bulk INSERTs (INSERT INTO SELECT * FROM, Loading data from Mysqldump, INSERT with huge data, etc.)</span></li>\n<li><span>Remember that your fragmented disk space is always re-usable.</span></li>\n</ul>\n","descriptionType":"html","publishedDate":"Wed, 24 Jun 2020 18:30:34 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert.png","linkMd5":"8534cbeda80b54a4a848932d45db6c76","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_1/2020-07-25/1595696398379_bc870d7ad1f21f01.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert.png","author":"Sri Sakthivel","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_1/2020-07-25/1595696403387_662d19d70da293e9.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Table-Fragmentation-Insert-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_3/2020-07-25/1595696399831_d82fb45b8e7fc715.webp"}},{"createdTime":"2020-07-26 00:59:48","updatedTime":"2020-07-25 16:59:48","title":"Webinar July 15: MySQL 8 Observability","link":"https://www.percona.com/blog/?p=69608","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL 8 Observability\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69609\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-300x168.png\" alt=\"MySQL 8 Observability\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Join Peter Zaitsev, Percona CEO, as he discusses MySQL 8 Observability.</p>\n<p>Broken MySQL means broken application, so maintaining insights in MySQL operational performance is critical. Thankfully, MySQL 8 offers a lot in terms of observability to resolve problems quickly and get great insights into opportunities for optimization. In this talk, we will cover the most important observability improvements in MySQL 8 ranging from Performance Schema and Information Schema to enhanced error logging and optimizer trace. If you are a Developer or DBA passionate about Observability, or just want to be empowered to resolve MySQL problems quickly and efficiently, you should attend.</p>\n<p>Please join <b>Peter Zaitsev</b> on <strong>Wednesday, July 15 at 1 pm EDT</strong> for his webinar &#8220;<strong>MySQL 8 Observability</strong>&#8220;.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" href=\"https://www.percona.com/resources/webinars/mysql-8-observability\">Watch the Recording</a></p>\n<p>If you can&#8217;t attend, <a target=\"_blank\" href=\"https://attendee.gotowebinar.com/register/6207774888517080334?source=Blog\">sign up anyway</a> and we&#8217;ll send you the slides and recording afterward.</p>\n","descriptionType":"html","publishedDate":"Tue, 07 Jul 2020 14:01:56 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability.png","linkMd5":"ff39ab399d341c5e5fa535bab5a0e9d1","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_2/2020-07-25/1595696398000_2ed21918aea090d5.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_5/2020-07-25/1595696401706_a9778798cf2fcc8b.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-8-Observability-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_1/2020-07-25/1595696401609_0fd9e29c5e40f995.webp"}},{"createdTime":"2020-07-26 00:59:46","updatedTime":"2020-07-25 16:59:46","title":"Backing Up Percona Kubernetes Operator for Percona XtraDB Cluster Databases to Google Cloud Storage","link":"https://www.percona.com/blog/?p=69807","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Backing up Percona XtraDB Google Cloud\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-70018\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-300x168.png\" alt=\"Backing up Percona XtraDB Google Cloud\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />The <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Percona Kubernetes Operator for Percona XtraDB Cluster</a> can send backups to Amazon S3 or S3-compatible storage. And every now and then at Support, we are asked how to send backups to Google Cloud Storage.</p>\n<p>Google Cloud Storage offers an “interoperability mode” which is S3-compatible. However, there are a few details to take care of when using it.</p>\n<h2>Google Cloud Storage Configuration</h2>\n<p>First, select “Settings” under “Storage” in the Navigation Menu. Under Settings, select the Interoperability tab. If Interoperability is not yet enabled, click Enable Interoperability Access. This turns on the S3-compatible interface to Google Cloud Storage.</p>\n<p>After enabling S3-compatible storage, an access key needs to be generated. There are two options: Access keys can be tied to Service accounts or User accounts. For production workloads, Google recommends Service account access keys, but for this example, a User account access key will be used for simplicity. The Interoperability page links to further documentation on the differences between the two, so this article does not go into those details.</p>\n<p>To create a User account HMAC (Hash-based Message Authentication Code) keys scroll down to “User account HMAC” and click “Create a key”. This generates an access key and accompanying secret. These keys will be used as an AWS access key and secret later on. The user account also needs access to the bucket that will be used for backups. This can be set up by selecting the bucket in Storage Browser, and going to the Permissions tab.</p>\n<h2>Operator Configuration</h2>\n<p>Once a key has been created and the account permissions are verified to be correct, the Percona XtraDB Cluster (PXC) Operator needs to be configured to use the new keys.</p>\n<p>First, the access key and secret need to be base64 encoded. For example:</p><pre class=\"crayon-plain-tag\">$ echo -n GOOGFJDEWQ3KJFAS | base64\nR09PR0ZKREVXUTNLSkZBUw==\n$ echo -n IFEWw99s0+ece3SXuf9q | base64\nSUZFV3c5OXMwK2VjZTNTWHVmOXE=</pre><p><span>The </span><i><span>-n</span></i><span> parameter to </span><i><span>echo</span></i><span> is important, without it a line break will also be encoded and the key won’t work.</span></p>\n<p><span>Next, the base64-encoded values need to be stored in the <em>deploy/backup-s3.yaml</em> file in the PXC Operator directory as the </span><i><span>AWS_ACCESS_KEY_ID</span></i><span> and </span><i><span>AWS_SECRET_ACCESS_KEY</span></i><span> like this:</span></p><pre class=\"crayon-plain-tag\">$ cat deploy/backup-s3.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-test-backup-s3\ntype: Opaque\ndata:\n  AWS_ACCESS_KEY_ID: R09PR0ZKREVXUTNLSkZBUw==\n  AWS_SECRET_ACCESS_KEY: SUZFV3c5OXMwK2VjZTNTWHVmOXE=</pre><p>After modifying the file, the secrets need to be stored in Kubernetes using:</p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/backup-s3.yaml</pre><p>In the cr.yaml of PXC Operator the backup destination is defined as follows:</p><pre class=\"crayon-plain-tag\">    storages:\n      s3-us-central1:\n        type: s3\n        s3:\n          bucket: my-test-bucket\n          credentialsSecret: my-test-backup-s3\n          region: us-central1\n          endpointUrl: https://storage.googleapis.com/</pre><p><i><span>bucket</span></i><span> is the name of the bucket as created in Google Cloud Storage, </span><i><span>credentialsSecret</span></i><span> must match the entry in backup-s3.yaml. </span><i><span>endpointUrl</span></i><span> is the “Storage URI” as shown in the Interoperability tab of Google Cloud Storage.</span></p>\n<p><span>Now that the backup destination has been defined, to take an on-demand backup the backup/backup.yaml file needs to be modified:</span></p><pre class=\"crayon-plain-tag\">apiVersion: pxc.percona.com/v1\nkind: PerconaXtraDBClusterBackup\nmetadata:\n  name: my-test-backup\nspec:\n  pxcCluster: cluster1\n  storageName: s3-us-central1</pre><p><span>Here </span><i><span>pxcCluster</span></i><span> needs to match the name of the cluster, and </span><i><span>storageName</span></i><span> needs to match the entry in cr.yaml. After modifying the file an on-demand backup can be started using:</span></p><pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/backup/backup.yml</pre><p>From here on the documentation for PXC Operator at <a target=\"_blank\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/backups.html\">https://www.percona.com/doc/kubernetes-operator-for-pxc/backups.html</a> can be followed, since after configuring the Google Cloud Storage destination taking and restoring backups works exactly as it does when using Amazon S3.</p>\n<h2>Conclusion</h2>\n<p><span>As you can see, using Google Cloud Storage together with Percona Kubernetes Operator for Percona XtraDB Cluster is not difficult at all, but few details are slightly different from Amazon S3.</span></p>\n<p>Be sure to get in touch with <a target=\"_blank\" href=\"https://percona.com/training/\">Percona’s Training Department</a> to schedule a hands-on tutorial session with our K8S Operator. Our instructors will guide you and your team through all the setup processes, learn how to take backups, handle recovery, scale the cluster, and manage high-availability with ProxySQL.</p>\n<hr />\n<p>Percona XtraDB Cluster is a cost-effective and robust clustering solution created to support your business-critical data. It gives you the benefits and features of MySQL along with the added enterprise features of Percona Server for MySQL.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/datasheets/percona-xtradb-cluster?utm_source=blog&#038;utm_medium=download&#038;utm_campaign=backingupoperator&#038;utm_content=datasheet\">Download Percona XtraDB Cluster Datasheet</a></p>\n","descriptionType":"html","publishedDate":"Mon, 20 Jul 2020 19:05:08 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud.png","linkMd5":"484aff5776bf040a821892b175c3c6f8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_1/2020-07-25/1595696397955_43f65a74049e8a48.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud.png","author":"Sami Ahlroos","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_6/2020-07-25/1595696404642_a98b1e2b344f6e14.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Backing-up-Percona-XtraDB-Google-Cloud-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_5/2020-07-25/1595696404013_7bcbcf4f40edf665.webp"}},{"createdTime":"2020-07-26 00:59:45","updatedTime":"2020-07-25 16:59:45","title":"Webinar July 23: Multi-Primary Replication Solutions for PostgreSQL","link":"https://www.percona.com/blog/?p=69844","description":"<p><img class=\"alignright size-medium wp-image-69915\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-300x168.png\" alt=\"Multi-Primary Replication Solutions for PostgreSQL\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Join <strong>Ibrar Ahmed</strong>, Sr. Software Engineer, Percona, as he discusses the concept of multi-primary replication and elaborates on the well-known multi-primary replication available.</p>\n<p>Scalability can be achieved horizontally or vertically. Vertical scalability means adding more resources/hardware to existing nodes to enhance the capability of the database to store and process more data, for example, adding a new process, memory, or disk to an existing node. Horizontal scalability means adding more nodes, therefore, horizontal scalability more challenging to implement. It requires more development effort and requires more work to set up.</p>\n<p>PostgreSQL provides quite a rich feature set for both vertical scalability and horizontal scalability. Replication is the crucial pillar of horizontal scalability, and PostgreSQL supports unidirectional primary-replica replication, which is enough for many use-cases, but there is still a need for Multi-Primary Replication. Multi-primary replications mean there is more than one node that acts as primary nodes. Data is replicated between nodes and updates and insertion can be possible on a group of primary nodes. In that case, there are multiple copies of the data. The system is also responsible for resolving any conflicts that occur between concurrent changes. There are two main reasons to have multiple primary replication; one is HA, and the second is performance.</p>\n<p>Please join <strong>Ibrar Ahmed</strong> on <strong>Thursday, July 23 at 1 pm EDT</strong> for his webinar &#8220;<strong>Multi-Primary Replication Solutions for PostgreSQL</strong>&#8220;.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" href=\"https://attendee.gotowebinar.com/register/3519588805372514573?source=Blog\">Register Now</a></p>\n<p>If you can&#8217;t attend, <a target=\"_blank\" href=\"https://attendee.gotowebinar.com/register/3519588805372514573?source=Blog\">sign up anyway</a> and we&#8217;ll send you the slides and recording afterward.</p>\n","descriptionType":"html","publishedDate":"Thu, 16 Jul 2020 13:46:15 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-300x168.png","linkMd5":"661011026adf3ac6f5ba125c49719f86","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_5/2020-07-25/1595696395574_360135b316ced8b1.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-300x168.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Multi-Primary-Replication-Solutions-for-PostgreSQL-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_5/2020-07-25/1595696395574_360135b316ced8b1.webp"}},{"createdTime":"2020-07-26 00:59:43","updatedTime":"2020-07-25 16:59:43","title":"A Simple MySQL Plugin to Retrieve System Metrics","link":"https://www.percona.com/blog/?p=69528","description":"<img width=\"200\" height=\"133\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-200x133.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL Plugin to Retrieve System Metrics\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-200x133.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-300x200.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-1024x683.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-1536x1024.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-2048x1365.png 2048w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-367x245.png 367w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><strong><img class=\"alignright size-medium wp-image-69721\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-300x200.png\" alt=\"MySQL Plugin to Retrieve System Metrics\" width=\"300\" height=\"200\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-300x200.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-1024x683.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-200x133.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-1536x1024.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-2048x1365.png 2048w, https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-367x245.png 367w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Ever wanted to extend MySQL and add some feature you think it is missing? </strong> With MySQL plugins, you can do exactly that.  One thing that has bothered me for several years is that you cannot easily retrieve system metrics from within MySQL.  Whether I am connecting via a remote connection or looking to add features to monitoring without the need for another interface with the server, I have wanted to retrieve system metrics without leaving the MySQL interface.</p>\n<p>So, I started a Proof of Concept for this.  My goal was to get metrics such as RAM (total, used, free), system load, CPU utilization, disk utilization for the file system containing the datadir, and more.  My objective was to do this as efficiently within MySQL as possible.  For this, I chose to utilize standard C libraries in as few lines of code as possible without having to scrape system files or run commands to get the data.  The data is pulled on demand so as not to increase the load on the system.</p>\n<p><strong>The MySQL plugin architecture is one of the most underutilized features in MySQL in my opinion</strong>.  It provides so much power, and I feel the MySQL ecosystem would be so much more powerful if more people took advantage of it.  Below is an example of a basic plugin I created to pull some system metrics.</p>\n<p>For this plugin, I chose to access the data via INFORMATION_SCHEMA.OS_METRICS.  This is defined in the plugin in the following:</p><pre class=\"crayon-plain-tag\">static struct st_mysql_information_schema simple_table_info = { MYSQL_INFORMATION_SCHEMA_INTERFACE_VERSION };\n\nstatic ST_FIELD_INFO simple_table_fields[]=\n{\n{\"NAME\", 15, MYSQL_TYPE_STRING, 0, 0, 0, 0},\n{\"VALUE\", 6, MYSQL_TYPE_FLOAT, 0, MY_I_S_UNSIGNED, 0, 0},\n{\"COMMENT\", 50, MYSQL_TYPE_STRING, 0, 0, 0, 0},\n{0, 0, MYSQL_TYPE_NULL, 0, 0, 0, 0}\n};\n\nstatic int simple_fill_table(THD *thd, TABLE_LIST *tables, Item *cond)\n{\nstruct sysinfo info;\nTABLE *table= tables-&#62;table;</pre><p>This defines the structure of the virtual table as having three columns: NAME, VALUE, and COMMENT.  NAME will be a string up to 15 characters long, followed by a float number for VALUE, and a text string for COMMENT up to 50 characters long.</p>\n<p>By invoking the sysinfo() function in C, I am able to pull various metrics.  These metrics are returned in a structure.  These can then be passed into the OS_METRICS &#8220;table&#8221; with the following commands:</p><pre class=\"crayon-plain-tag\">struct sysinfo info;\nsysinfo(&#38;info);\n\n// Total usable main memory size\ntable-&#62;field[0]-&#62;store(\"TOTAL_RAM\", 9, system_charset_info);\ntable-&#62;field[1]-&#62;store(info.totalram * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Total usable main memory size\", 29, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;</pre><p>In the above case, I reference the element &#8220;totalram&#8221; from the sysinfo structure and store it in the table.  You can see where there is a line for each column of the table and the values are stored one by one.</p>\n<p>Here is the most basic form of a plugin that only pulls RAM information and makes it available within INFORMATION_SCHEMA.OS_METRICS:</p><pre class=\"crayon-plain-tag\">#include &#60;sql_class.h&#62;\n#include &#60;table.h&#62;\n#include &#60;stdlib.h&#62;\n#include &#60;ctype.h&#62;\n#include &#60;mysql_version.h&#62;\n#include &#60;mysql/plugin.h&#62;\n#include &#60;my_global.h&#62;\n#include &#60;sys/time.h&#62;\n#include &#60;sys/resource.h&#62;\n#include &#60;sys/sysinfo.h&#62;\n\nstatic struct st_mysql_information_schema simple_table_info = { MYSQL_INFORMATION_SCHEMA_INTERFACE_VERSION };\n\nstatic ST_FIELD_INFO simple_table_fields[]=\n{\n{\"NAME\", 15, MYSQL_TYPE_STRING, 0, 0, 0, 0},\n{\"VALUE\", 6, MYSQL_TYPE_FLOAT, 0, MY_I_S_UNSIGNED, 0, 0},\n{\"COMMENT\", 50, MYSQL_TYPE_STRING, 0, 0, 0, 0},\n{0, 0, MYSQL_TYPE_NULL, 0, 0, 0, 0}\n};\n\nstatic int simple_fill_table(THD *thd, TABLE_LIST *tables, Item *cond)\n{\nstruct sysinfo info;\nTABLE *table= tables-&#62;table;\n\nsysinfo(&#38;info);\n\n// Total usable main memory size\ntable-&#62;field[0]-&#62;store(\"TOTAL_RAM\", 9, system_charset_info);\ntable-&#62;field[1]-&#62;store(info.totalram * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Total usable main memory size\", 29, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\n// Available memory size\ntable-&#62;field[0]-&#62;store(\"FREE_RAM\", 8, system_charset_info);\ntable-&#62;field[1]-&#62;store(info.freeram * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Available memory size\", 21, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\n// Used memory size\ntable-&#62;field[0]-&#62;store(\"USED_RAM\", 8, system_charset_info);\ntable-&#62;field[1]-&#62;store((info.totalram - info.freeram) * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Used memory size\", 16, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\n// Available memory (percentage)\ntable-&#62;field[0]-&#62;store(\"FREE_RAM_PCT\", 12, system_charset_info);\ntable-&#62;field[1]-&#62;store((float) info.freeram / info.totalram * 100 * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Available memory as a percentage\", 32, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\n// Used memory (percentage)\ntable-&#62;field[0]-&#62;store(\"USED_RAM_PCT\", 12, system_charset_info);\ntable-&#62;field[1]-&#62;store((float) (info.totalram - info.freeram) / info.totalram * 100 * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Free memory as a percentage\", 27, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\n// Amount of shared memory\ntable-&#62;field[0]-&#62;store(\"SHARED_RAM\", 10, system_charset_info);\ntable-&#62;field[1]-&#62;store(info.sharedram * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Amount of shared memory\", 23, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\n// Memory used by buffers\ntable-&#62;field[0]-&#62;store(\"BUFFER_RAM\", 10, system_charset_info);\ntable-&#62;field[1]-&#62;store(info.bufferram * info.mem_unit);\ntable-&#62;field[2]-&#62;store(\"Memory used by buffers\", 22, system_charset_info);\nif (schema_table_store_record(thd, table)) return 1;\n\nreturn 0;\n}\n\nstatic int simple_table_init(void *ptr)\n{\nST_SCHEMA_TABLE *schema_table= (ST_SCHEMA_TABLE*)ptr;\nschema_table-&#62;fields_info= simple_table_fields;\nschema_table-&#62;fill_table= simple_fill_table;\nreturn 0;\n}\n\nmysql_declare_plugin(os_metrics)\n{\nMYSQL_INFORMATION_SCHEMA_PLUGIN,\n&#38;simple_table_info, /* type-specific descriptor */\n\"OS_METRICS\", /* table name */\n\"Michael Patrick\", /* author */\n\"OS Metrics INFORMATION_SCHEMA table\", /* description */\nPLUGIN_LICENSE_GPL, /* license type */\nsimple_table_init, /* init function */\nNULL,\n0x0100, /* version = 1.0 */\nNULL, /* no status variables */\nNULL, /* no system variables */\nNULL, /* no reserved information */\n0 /* no flags */\n}\nmysql_declare_plugin_end;</pre><p>You will need to have the MySQL source code available on a server along with the libraries needed to compile C code.  For me, I went with the most basic approach of manually compiling the plugin, although I need to update it with cmake so it is easier to compile.</p>\n<p>I named my file, osmetricsplugin.cc.  Of course, in the example below, you will need to define the path for where your code lives where I have placed &#8220;{PATH_TO_YOUR_PLUGIN_CODE}&#8221;.</p>\n<p>You can compile the plugin with a command such as the following:</p><pre class=\"crayon-plain-tag\">SRCBASE=\"../percona-server-5.7.24-27\"\ng++ -DMYSQL_DYNAMIC_PLUGIN -Wall -fPIC -shared \\\n-I/usr/include/mysql -m64 \\\n-I${SRCBASE}/sql \\\n-I${SRCBASE}/include \\\n-I${SRCBASE}/libbinlogevents/export \\\n-I${SRCBASE}/libbinlogevents/include \\\n-I{PATH_TO_YOUR_PLUGIN_CODE} \\\n-o osmetricsplugin.so osmetricsplugin.cc</pre><p>If you are interested in seeing more of what can be done with the above, check out <a target=\"_blank\" href=\"https://github.com/toritejutsu/mysql_os_metrics\">the GitHub page</a> for the plugin I wrote.</p>\n<p>Once you compile it, you should get an osmetricsplugin.so file which can be copied to your MySQL plugin directory with a command such as:</p><pre class=\"crayon-plain-tag\">cp osmetricsplugin.so /usr/lib64/mysql/plugin/</pre><p>Once it is in place, you can tell MySQL to load the plugin with a command such as:</p><pre class=\"crayon-plain-tag\">mysql&#62; INSTALL PLUGIN OS_METRICS SONAME 'osmetricsplugin.so';</pre><p>You can verify that the plugin is loaded correctly:</p><pre class=\"crayon-plain-tag\">mysql&#62; SELECT * FROM information_schema.PLUGINS WHERE PLUGIN_NAME LIKE \"%OS%\";;\n+-------------+----------------+---------------+--------------------+---------------------+--------------------+------------------------+-----------------+-------------------------------------+----------------+-------------+\n| PLUGIN_NAME | PLUGIN_VERSION | PLUGIN_STATUS | PLUGIN_TYPE        | PLUGIN_TYPE_VERSION | PLUGIN_LIBRARY     | PLUGIN_LIBRARY_VERSION | PLUGIN_AUTHOR   | PLUGIN_DESCRIPTION                  | PLUGIN_LICENSE | LOAD_OPTION |\n+-------------+----------------+---------------+--------------------+---------------------+-------------------+------------------------+-----------------+-------------------------------------+----------------+-------------+\n| OS_METRICS  | 1.0            | ACTIVE        | INFORMATION SCHEMA | 50724.0             | osmetricsplugin.so | 1.7                    | Michael Patrick | OS Metrics INFORMATION_SCHEMA table | GPL            | ON          |\n+-------------+----------------+---------------+--------------------+---------------------+--------------------+------------------------+-----------------+-------------------------------------+----------------+-------------+\n1 row in set (0.00 sec)</pre><p>To query the data, execute a SQL command such as:</p><pre class=\"crayon-plain-tag\">mysql&#62; SELECT * FROM information_schema.OS_METRICS;\n+------------------------+-------------------+-------------------------------------------------+\n| NAME                   | VALUE             | COMMENT                                         |\n+------------------------+-------------------+-------------------------------------------------+\n| TOTAL_RAM              |        1039118336 | Total usable main memory size                   |\n| FREE_RAM               |         341049344 | Available memory size                           |\n| USED_RAM               |         698068992 | Used memory size                                |\n| FREE_RAM_PCT           | 32.82102966308594 | Available memory as a percentage                |\n| USED_RAM_PCT           | 67.17897033691406 | Free memory as a percentage                     |\n| SHARED_RAM             |                 0 | Amount of shared memory                         |\n| BUFFER_RAM             |           2158592 | Memory used by buffers                          |\n+------------------------+-------------------+-------------------------------------------------+\n7 rows in set (0.00 sec)</pre><p>There is much more work to be done with the plugin and there is more that can be done to improve it.  I believe it is a very useful feature to be able to access system metrics from within MySQL, but am very interested to hear what others think.</p>\n<p>If interested, please check out <a target=\"_blank\" href=\"https://github.com/toritejutsu/mysql_os_metrics\">a more advanced version of the plugin</a>, and here you can learn more about <a target=\"_blank\" href=\"https://dev.mysql.com/doc/refman/8.0/en/server-plugins.html\" target=\"_blank\" rel=\"noopener\">MySQL plugins</a>.</p>\n<hr />\n<p>Our solution brief &#8220;Get Up and Running with Percona Server for MySQL&#8221; outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/get-and-running-percona-server-mysql?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=simpleplugin&#38;utm_content=solutionbrief\" rel=\"noopener\">Download PDF</a></p>\n","descriptionType":"html","publishedDate":"Wed, 08 Jul 2020 13:45:36 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics.png","linkMd5":"fa77d309d43c48c7c92eb011b653b529","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_6/2020-07-25/1595696398474_26501de5bdd441f0.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics.png","author":"Michael Patrick","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-200x133.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_2/2020-07-25/1595696402852_7979ab5e88ab8e92.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/MySQL-Plugin-to-Retrieve-System-Metrics-300x200.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_6/2020-07-25/1595696402082_6c67741257b59a1a.webp"}},{"createdTime":"2020-07-26 00:59:53","updatedTime":"2020-07-25 16:59:53","title":"Achieving Consistent Read and High Availability with Percona XtraDB Cluster 8.0","link":"https://www.percona.com/blog/?p=69481","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"High Availability with Percona XtraDB Cluster 8.0\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span><img class=\"alignright size-medium wp-image-69578\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-300x168.png\" alt=\"High Availability with Percona XtraDB Cluster 8.0\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />In real life, there are frequent cases where getting a running application to work correctly is strongly dependent on consistent write/read operations. </span><span>This is no issue when using a single data node as a provider, but it becomes more concerning and challenging when adding additional nodes for high availability and/or read scaling. </span></p>\n<p><span>In the MySQL dimension, I have already described it here in my blog </span><a target=\"_blank\" title=\"Dirty reads in High Availability solution \" href=\"http://www.tusacentral.net/joomla/index.php/mysql-blogs/216-dirty-reads-in-high-availability-solution\">Dirty Reads in High Availability Solution.</a></p>\n<p><span>We go from the most loosely-coupled database clusters with primary-replica async replication, to the fully tightly-coupled database clusters with NDB Cluster (MySQL/Oracle).</span></p>\n<p><span>Adding components like ProxySQL to the architecture can, from one side, help in improving high availability, and from the other, it can amplify and randomize the negative effect of a stale read. </span><span>As such it is crucial to know how to correctly set up the environment to reduce the risk of stale reads, without reducing the high availability. </span></p>\n<p><span>This article covers a simple HOW-TO for <a target=\"_blank\" href=\"https://www.percona.com/software/mysql-database/percona-xtradb-cluster\">Percona XtraDB Cluster 8.0</a> (PXC) and ProxySQL, providing an easy to follow guide to obtain no stale reads, without the need to renounce at read, scaling or a high grade of HA thanks to PXC8.</span></p>\n<h2><span>The Architecture</span></h2>\n<p><span>The covered architecture is based on:</span></p>\n<ol>\n<li><span>PXC8 cluster compose by 3 nodes</span></li>\n<li><span>ProxySQL v2 node in a cluster to avoid a single point of failure</span></li>\n<li><span>Virtual IP with KeepAlived <a target=\"_blank\" href=\"http://www.tusacentral.net/joomla/index.php/mysql-blogs/185-making-proxysql-high-available-and-not-spof-1\">see here</a>. If you prefer to use your already-existing load balancer, feel free to do so.</span></li>\n<li><span>N number of application nodes, referring to VIP</span></li>\n</ol>\n<h2><span>Installation</span></h2>\n<p><span><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-xtradb-cluster/LATEST/install/index.html\">Install PXC8</a> </span></p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/2020/04/08/how-to-install-proxysql-from-the-percona-repository/\"><span>Install ProxySQL</span></a></p>\n<p><span>And finally, set the virtual IP as illustrated in the article mentioned above.  </span><span>It is now the time to do the first step towards the non-stale read solution. </span></p>\n<h2><span>Covering Stale Reads</span></h2>\n<p><span>With PXC, we can easily prevent stale reads by setting the parameter to one of the following values wsrep-sync-wait = 1 &#8211; 3 &#8211; 5 or 7 (default = 0). We will see what changes in more detail in part two of the blog to be published soon. For now, just set it to wsrep-sync-wait = 1 ;.</span></p>\n<p><span>The cluster will ensure consistent reads no matter from which node you will write and read.</span></p>\n<p><span>This is it. So simple!</span></p>\n<h2><span>ProxySQL Requirements</span></h2>\n<p><span>The second step is to be sure we set up our ProxySQL nodes to use:</span></p>\n<ul>\n<li><span>One writer a time to reduce the certification conflicts and Brutal Force Abort</span></li>\n<li><span>Avoid including the writer in the reader group</span></li>\n<li><span>Respect the order I am setting for failover in case of needs  </span></li>\n</ul>\n<p><span>Now here we have a problem; ProxySQL v2 comes with very interesting features like SSL Frontend/backend, support for AWS Aurora &#8230;and more. </span><span>But it also comes with a very poor native PXC support. I have already raised this <a target=\"_blank\" href=\"http://www.tusacentral.net/joomla/index.php/mysql-blogs/209-proxysql-native-support-for-percona-xtradb-cluster-pxc\">in my old article</a> on February 19, 2019, and raised other issues with discussions and bug reports. </span></p>\n<p><span>In short, we cannot trust ProxySQL for a few factors:</span></p>\n<ul>\n<li><span>The way it deals with the nodes failover/failback is not customizable</span></li>\n<li><span>The order of the nodes is not customizable</span></li>\n<li><span>As of this writing, the support to have the writer NOT working as a reader is broken</span></li>\n</ul>\n<p><span>In the end, the reality is that in order to support PXC/Galera, the use of an external script using the scheduler is more flexible, solid, and trustworthy. </span><span>As such, the decision is to ignore the native Galera support, and instead focus on the implementation of a more robust script. </span></p>\n<p><span>For the scope of this article, I have reviewed, updated, and extended <a target=\"_blank\" href=\"https://github.com/Tusamarco/proxy_sql_tools\" target=\"_blank\" rel=\"noopener\">my old script</a>.</span></p>\n<p><span>Percona had also developed a Galera checker script that was part of the ProxySQL-Admin-Tools suite, but that now has been externalized and available in the <a target=\"_blank\" href=\"https://github.com/Percona-Lab/proxysql-scheduler/tree/v2.0\" target=\"_blank\" rel=\"noopener\">PerconaLab GitHub</a>.</span></p>\n<h2><span>Setting All Blocks </span></h2>\n<p>The setup for this specific case will be based on:</p>\n<ul>\n<li><span>Rules to perform read-write split.</span></li>\n<li><span>One host group to define the writer HG 200</span></li>\n<li><span>One host group to define the reader HG 201</span></li>\n<li><span>One host group to define candidate writers HG 8200</span></li>\n<li><span>One host group to define candidate readers HG 8201</span></li>\n</ul>\n<p>The final architecture will look like this:</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/wp-content/uploads/2020/06/architecture.png\"><img class=\"aligncenter wp-image-69483 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/architecture.png\" alt=\"High Availability with Percona XtraDB Cluster\" width=\"691\" height=\"501\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/architecture.png 691w, https://www.percona.com/blog/wp-content/uploads/2020/06/architecture-300x218.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/architecture-200x145.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/architecture-367x266.png 367w\" sizes=\"(max-width: 691px) 100vw, 691px\" /></a></p>\n<p><strong>ProxySQL Nodes:</strong></p>\n<p><span>Node1 192.168.4.191 public ip<br />\n</span><span>      10.0.0.191    internal ip<br />\n</span><span>Node1 192.168.4.192 public ip<br />\n</span><span>     </span> <span> 10.0.0.192    internal ip<br />\n</span><span>Node1 192.168.4.193 public ip<br />\n</span><span>      </span> <span>10.0.0.193    internal ip  </span></p>\n<p><span>VIP   192.168.4.194 public ip</span></p>\n<p><span><strong>PXC8 Nodes:<br />\n</strong><br />\n</span><span>pxc1  10.0.0.22<br />\n</span><span>pxc2  10.0.0.23<br />\n</span><span>pxc3  10.0.0.33</span></p>\n<p><span>Let us configure PXC8 first. </span><span>Operation one is to create the users for ProxySQL and the script to access the PXC cluster for monitoring.</span></p><pre class=\"crayon-plain-tag\">CREATE USER monitor@'10.0.%' IDENTIFIED BY '&#60;secret&#62;';\nGRANT USAGE ON *.* TO monitor@'10.0.%';\nGRANT SELECT ON performance_schema.* TO monitor@'10.0.%';</pre><p><span>The second step is to configure ProxySQL as a cluster:</span></p>\n<p><span>Add a user able to connect from remote. This is will require ProxySQL nodes to be restarted.</span></p><pre class=\"crayon-plain-tag\">update global_variables set Variable_Value='admin:admin;cluster1:clusterpass'  where Variable_name='admin-admin_credentials';\nSAVE ADMIN VARIABLES TO DISK;\n\nsystemctl restart proxysql.</pre><p><span>On rotation, do all ProxySQL nodes. </span></p>\n<p><span>The third part is to set the variables below.</span></p>\n<p><span>Please note that the value for </span><i><span>admin-cluster_mysql_servers_diffs_before_sync </span></i><span>is not standard and is set to </span><b>1</b><span>. </span></p><pre class=\"crayon-plain-tag\">update global_variables set variable_value='cluster1' where variable_name='admin-cluster_username';\nupdate global_variables set variable_value='clusterpass' where variable_name='admin-cluster_password';\nupdate global_variables set variable_value=200 where variable_name='admin-cluster_check_interval_ms';\nupdate global_variables set variable_value=100 where variable_name='admin-cluster_check_status_frequency';\n\nupdate global_variables set variable_value='true' where variable_name='admin-cluster_mysql_query_rules_save_to_disk';\nupdate global_variables set variable_value='true' where variable_name='admin-cluster_mysql_servers_save_to_disk';\nupdate global_variables set variable_value='true' where variable_name='admin-cluster_mysql_users_save_to_disk';\nupdate global_variables set variable_value='true' where variable_name='admin-cluster_proxysql_servers_save_to_disk';\n\nupdate global_variables set variable_value=3 where variable_name='admin-cluster_mysql_query_rules_diffs_before_sync';\nupdate global_variables set variable_value=1 where variable_name='admin-cluster_mysql_servers_diffs_before_sync';\nupdate global_variables set variable_value=3 where variable_name='admin-cluster_mysql_users_diffs_before_sync';\n\n\nupdate global_variables set Variable_Value=500  where Variable_name='mysql-max_stmts_per_connection';\nupdate global_variables set variable_value=\"33554432\" where variable_name='mysql-max_allowed_packet';\nupdate global_variables set Variable_Value=0  where Variable_name='mysql-hostgroup_manager_verbose';\nupdate global_variables set Variable_Value='true'  where Variable_name='mysql-query_digests_normalize_digest_text';\nupdate global_variables set Variable_Value='8.0.19'  where Variable_name='mysql-server_version';\nupdate global_variables set Variable_Value='utf8'  where Variable_name='mysql-default_charset';\n\n\nupdate global_variables set Variable_Value=500  where Variable_name='mysql-tcp_keepalive_time';\nupdate global_variables set Variable_Value='true'  where Variable_name='mysql-use_tcp_keepalive';\nupdate global_variables set Variable_Value='true'  where Variable_name='mysql-verbose_query_error';\nupdate global_variables set Variable_Value=50000  where Variable_name='mysql-max_stmts_cache';\nupdate global_variables set Variable_Value=1  where Variable_name='mysql-show_processlist_extended';\n\nLOAD ADMIN VARIABLES TO RUN;SAVE ADMIN VARIABLES TO DISK;\nLOAD MYSQL VARIABLES TO RUN;SAVE MYSQL VARIABLES TO DISK;</pre><p><span>It is now time to define the ProxySQL cluster nodes:</span></p><pre class=\"crayon-plain-tag\">INSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES('192.168.4.191',6032,100,'PRIMARY');\nINSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES('192.168.4.192',6032,100,'SECONDARY');\nINSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES('192.168.4.193',6032,100,'SECONDARY');\nLOAD PROXYSQL SERVERS TO RUN;SAVE PROXYSQL SERVERS TO DISK;</pre><p><span>Check the ProxySQL logs and you should see that the nodes are now linked:</span></p><pre class=\"crayon-plain-tag\">2020-05-25 09:24:30 [INFO] Cluster: clustering with peer 192.168.4.192:6032 . Remote version: 2.1.0-159-g0bdaa0b . Self version: 2.1.0-159-g0bdaa0b\n2020-05-25 09:24:30 [INFO] Cluster: clustering with peer 192.168.4.193:6032 . Remote version: 2.1.0-159-g0bdaa0b . Self version: 2.1.0-159-g0bdaa0b</pre><p><span>Once this is done let us continue the setup, adding the PXC nodes and all the different host groups to manage the architecture:</span></p><pre class=\"crayon-plain-tag\">delete from mysql_servers where hostgroup_id in (200,201);\nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.22',200,3306,10000,2000,'default writer');\nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.23',201,3306,10000,2000,'reader');    \nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.33',201,3306,10000,2000,'reader');        \nLOAD MYSQL SERVERS TO RUNTIME; SAVE MYSQL SERVERS TO DISK;    \n    \ndelete from mysql_servers where hostgroup_id in (8200,8201);\nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.22',8200,3306,1000,2000,'Writer preferred');\nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.23',8200,3306,999,2000,'Second preferred');    \nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.33',8200,3306,998,2000,'Thirdh and last in the list');      \nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.22',8201,3306,1000,2000,'reader setting');\nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.23',8201,3306,1000,2000,'reader setting');    \nINSERT INTO mysql_servers (hostname,hostgroup_id,port,weight,max_connections,comment) VALUES ('10.0.0.33',8201,3306,1000,2000,'reader setting');       \nLOAD MYSQL SERVERS TO RUNTIME; SAVE MYSQL SERVERS TO DISK;</pre><p><span>You can see that as mentioned we have two host groups to manage the cluster 8200 and 8201. </span><span>Those two host groups work as templates and they will change only by us manually.</span></p>\n<p><span>The 8200 host group weight defines the order of the writers from higher to lower. Given that node 10.0.0.22 with weight 1000 is the preferred writer. At the moment of writing, I chose to NOT implement automatic fail-back. I will illustrate later how to trigger that manually. </span></p>\n<p><span>Once we have all the servers up, lets&#8217; move on and create the users:</span></p><pre class=\"crayon-plain-tag\">insert into mysql_users (username,password,active,default_hostgroup,default_schema,transaction_persistent,comment) values ('app_test2','test',1,200,'mysql',1,'application test user');\ninsert into mysql_users (username,password,active,default_hostgroup,default_schema,transaction_persistent,comment) values ('dba','dbapw',1,200,'mysql',1,'generic dba for application');\nLOAD MYSQL USERS TO RUNTIME;SAVE MYSQL USERS TO DISK;</pre><p><span>And the query rules to have Read/Write split:</span></p><pre class=\"crayon-plain-tag\">insert into mysql_query_rules (rule_id,proxy_port,destination_hostgroup,active,retries,match_digest,apply) values(1040,6033,200,1,3,'^SELECT.*FOR UPDATE',1);\ninsert into mysql_query_rules (rule_id,proxy_port,destination_hostgroup,active,retries,match_digest,apply) values(1042,6033,201,1,3,'^SELECT.*$',1);\n\nLOAD MYSQL QUERY RULES TO RUN;SAVE MYSQL QUERY RULES TO DISK;</pre><p><span>The final step is to set the scheduler:</span></p><pre class=\"crayon-plain-tag\">INSERT  INTO scheduler (id,active,interval_ms,filename,arg1) values (10,0,2000,\"/var/lib/proxysql/galera_check.pl\",\"-u=cluster1 -p=clusterpass -h=192.168.4.191 -H=200:W,201:R -P=6032  --main_segment=2 --debug=0  --log=/var/lib/proxysql/galeraLog --active_failover=1 --single_writer=1 --writer_is_also_reader=0\");\n\nLOAD SCHEDULER TO RUNTIME;SAVE SCHEDULER TO DISK;</pre><p><span>Let analyze the script parameters:</span></p>\n<ul>\n<li><span>The schedule ID.</span><span>    id: 10</span></li>\n<li><span>As a best practice, always keep the scheduler script not active by default and enable it only when in the need.</span><span> active: 0</span><span>  </span></li>\n<li><span>Interval is how often the scheduler should execute the script; it needs to be often enough to reduce the time the service is in a degraded state, but not so often to be noisy. An interval of two seconds is normally a good start.  </span><span> interval_ms: 2000</span></li>\n<li><span>The location of the script that must be set as executable filename: </span><span>/var/lib/proxysql/galera_check.pl</span></li>\n</ul>\n<p><span>Given the scheduler limitation to five arguments, we collapse all the parameters in one and let the script then parse them.   arg1:  -u=cluster1 -p=clusterpass -h=192.168.4.191 -H=200:W,201:R -P=6032 &#8211;retry_down=2 &#8211;retry_up=1 &#8211;main_segment=2 &#8211;debug=0  &#8211;log=/var/lib/proxysql/galeraLog &#8211;active_failover=1 &#8211;single_writer=1 &#8211;writer_is_also_reader=0</span></p>\n<p><span> The parameters we pass here are:</span></p>\n<ul>\n<li><span>The credential to connect to ProxySQL: -u=cluster1 -p=clusterpass -h=192.168.4.191 -P=6032</span></li>\n<li><span>The host group definition: -H=200:W,201:R This setting is necessary because you can have multiple script running serving multiple clusters.</span></li>\n<li><span>The retry settings are to reduce the risk of false positive, say a network hiccup or other momentary events against which you do not want to take action: &#8211;retry_down=2 &#8211;retry_up=1 </span></li>\n<li><span>Given the script is segment-aware, you need to declare the main segment that is serving the applications: &#8211;main_segment=2 </span></li>\n<li><span> Log location/name the final name will be the combination of this plus the host groups (ie galeraLog_200_W_201_R.log ) : &#8211;log=/var/lib/proxysql/galeraLog </span></li>\n<li><span> If script should deal with failover or not and what type (read documentation/help for details): &#8211;active_failover=1 </span></li>\n<li><span> If the script should support SINGLE writer (default recommended), or multiple writer nodes: &#8211;single_writer=1 </span></li>\n<li><span> Is (are) the writers also working as readers or fully write dedicated: &#8211;writer_is_also_reader=0</span></li>\n</ul>\n<p><span>Once we are confident our settings are right, let us put the script in production:</span></p><pre class=\"crayon-plain-tag\">update scheduler set active=1 where id=10;\nLOAD SCHEDULER TO RUNTIME;</pre><p>&#160;</p>\n<p><b><a target=\"_blank\" href=\"https://www.percona.com/blog/wp-content/uploads/2020/06/redflag.jpg\"><img class=\"alignnone wp-image-69485\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/redflag.jpg\" alt=\"\" width=\"63\" height=\"75\" /></a>Warning </b></p>\n<p><span>One important thing to keep in mind is that ProxySQL scheduler IS NOT part of the cluster synchronization, as such we must manually configure that part on each node. </span><span>Once the script runs, any change done inside ProxySQL to the mysql_server table will be kept in sync by the ProxySQL cluster. </span><span>It is strongly recommended to not mix ProxySQL nodes in the cluster and sparse one, as this may cause unexpected behavior. </span></p>\n<p><span>At this point, your PXC8 cluster architecture is fully running and will provide you with a very high level of HA and write isolation while preserving the read scaling capabilities.</span></p>\n<p><span>In part two of this post, we will see the cluster in action and how it behaves in case of standard operations like backup or emergency cases like node crashes.</span></p>\n<hr />\n<p>Percona XtraDB Cluster is a cost-effective and robust clustering solution created to support your business-critical data. It gives you the benefits and features of MySQL along with the added enterprise features of Percona Server for MySQL.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/datasheets/percona-xtradb-cluster?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=ha&#38;utm_content=datasheet\" rel=\"noopener\">Download Percona XtraDB Cluster Datasheet</a></p>\n","descriptionType":"html","publishedDate":"Wed, 01 Jul 2020 13:35:00 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0.png","linkMd5":"0ecdc3342b5c4f7daea056fbaa879566","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_4/2020-07-25/1595696397644_82236fe18297ab7e.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0.png","author":"Marco Tusa","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_4/2020-07-25/1595696399652_420a7288860c0a1e.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/High-Availability-with-Percona-XtraDB-Cluster-8.0-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_5/2020-07-25/1595696403493_ef41eb509c903dfe.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/architecture.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_4/2020-07-25/1595696404319_b42d841280267d85.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/redflag.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_3/2020-07-25/1595696405237_9fc8e21218009c76.webp"}},{"createdTime":"2020-07-26 00:59:53","updatedTime":"2020-07-25 16:59:53","title":"Webinar July 14: How Percona Monitoring and Management Improves Database Security","link":"https://www.percona.com/blog/?p=69460","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Monitoring and Management Improves Database Security\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><div>\n<p><img class=\"alignright size-medium wp-image-69462\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-300x168.png\" alt=\"Percona Monitoring and Management Improves Database Security\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Data breaches seem to be a daily occurrence, so much so that seeing an article about the latest company to fall victim draws little attention outside of those immediately impacted. That is of course unless that company is yours and data security is your responsibility!</p>\n<p>While companies continue to invest heavily in fortifying networks with firewalls and event monitors, spend heavily on user education, and add layer after layer of complex protection, incidents continue to rise. What we’re seeing is an alarming trend of protecting against the most complex attack vectors and ignoring the simplest ones: an improperly configured database.</p>\n<p>Join Steve Hoffman, VP of Engineering and Daniel Guzman Burgos, Technical Lead as they discuss the Percona Monitoring and Management Security Threat Tool which was designed to keep a constant watch for some of the most common database security mistakes and draw immediate attention to the issues that can&#8217;t wait to be addressed</p>\n</div>\n<p>Please join <strong>Steve Hoffman</strong> and <strong>Daniel Guzman Burgos</strong> on <strong>Tuesday, July 14 at 2 pm EDT</strong> for their webinar &#8220;<strong>How Percona Monitoring and Management Improves Database Security</strong>&#8220;.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/webinars/how-percona-monitoring-and-management-pmm-improves-database-security\" rel=\"noopener\">Watch the Recording</a></p>\n<p>If you can&#8217;t attend, <a target=\"_blank\" href=\"https://attendee.gotowebinar.com/register/6779539626657554448?source=Blog\">sign up anyway</a> and we&#8217;ll send you the slides and recording afterward.</p>\n","descriptionType":"html","publishedDate":"Wed, 01 Jul 2020 15:56:23 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security.png","linkMd5":"3334263b25b02eb0278c3f1942e38c08","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_4/2020-07-25/1595696398086_7cb5fa26ec4e66a9.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_1/2020-07-25/1595696403771_a20ffb2d2bd97cf3.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-Monitoring-and-Management-Improves-Database-Security-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_6/2020-07-25/1595696404286_cc6597912bcb09b7.webp"}},{"createdTime":"2020-07-26 00:59:44","updatedTime":"2020-07-25 16:59:44","title":"Improvements to Query Analytics (QAN) Component of Percona Monitoring and Management","link":"https://www.percona.com/blog/?p=69780","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"improvements to qan percona monitoring and management\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69923\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-300x157.png\" alt=\"improvements to qan percona monitoring and management\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />We&#8217;ve been improving the Query Analytics component of <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM). Here&#8217;s a summary of what we&#8217;ve done, beginning with a reminder of what PMM is and what it&#8217;s for.</p>\n<h2>Database Troubleshooting and PMM</h2>\n<p>Troubleshooting performance issues can be tricky, especially when all you&#8217;ve been told is that &#8220;the database seems slow&#8221;.</p>\n<p>In such cases, your first thought should be to use PMM, the free, open-source database troubleshooting and performance optimization platform.</p>\n<p>PMM lets you actively monitor open-source databases, providing detailed, time-based analysis of MySQL, PostgreSQL, MongoDB, or MariaDB databases. PMM helps you analyze your data to make sure that everything is working as efficiently as possible. And it helps you quickly identify problematic queries.</p>\n<p>PMM works on-premise or in the cloud with major cloud providers, and it can scale up to manage large environments.</p>\n<h2>Query Analytics (QAN)</h2>\n<p>QAN is the application for the analysis of specific requests. Where PMM lets you home in on the problem in general, QAN helps you to focus on the core problem and understand exactly where your database is performing poorly.</p>\n<p>Up until now, QAN was a little overcomplicated. So, we reworked a lot of functionality. Along the way, we added some interesting features, fixed a lot of old bugs, and made some general technical improvements. (I&#8217;ll explain some of them in this post.)</p>\n<p>Also, in PMM version 2.9, we&#8217;ve completed a fairly long piece of work in migrating PMM2 from angular1 and angular8 to React. The migration of QAN completes this transition.</p>\n<p>So, what&#8217;s new?</p>\n<h2>Functionality Improvements</h2>\n<h4>MongoDB Explains</h4>\n<p>We&#8217;ve added support for MongoDB &#8216;explain&#8217;.</p>\n<p><img class=\"wp-image-69798 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb.png\" alt=\"\" width=\"726\" height=\"408\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb.png 2226w, https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb-1024x576.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb-1536x864.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb-2048x1152.png 2048w, https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb-367x206.png 367w\" sizes=\"(max-width: 726px) 100vw, 726px\" /></p>\n<h4>More Graphs</h4>\n<p>There are two new graphs:</p>\n<ul>\n<li>Percentage &#8211; Shows the percentage ratio between the value of the metric and its value in total.</li>\n<li>Time distribution &#8211; Shows which parts of the request spend the most time on execution:</li>\n</ul>\n<p><img class=\"wp-image-69797 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution.png\" alt=\"\" width=\"930\" height=\"74\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution.png 2626w, https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution-300x24.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution-1024x82.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution-200x16.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution-1536x123.png 1536w, https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution-2048x164.png 2048w, https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution-367x29.png 367w\" sizes=\"(max-width: 930px) 100vw, 930px\" /></p>\n<h4>Improved Metrics Selection</h4>\n<p>We&#8217;ve added the ability to swap any metric with main, without additional intermediate actions. A convenient indication has been added showing which types of databases the metric belongs to; you no longer need to guess which metrics are available.</p>\n<p><img class=\"size-medium wp-image-69793 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/metrics-selection-300x219.png\" alt=\"\" width=\"300\" height=\"219\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/metrics-selection-300x219.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/metrics-selection-200x146.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/metrics-selection-367x267.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/metrics-selection.png 884w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></p>\n<h4>Overview Table Simplification</h4>\n<p>One of the problems with QAN is that most of its users are experts, who tend to want to see an interface full of information and data. However, even experts were finding it hard to see what was happening in the main table, which was heavily overloaded with information and quite difficult to understand.</p>\n<p>So, we&#8217;ve made the interface easier to understand without depriving experts of the details they need.</p>\n<p><strong>Cleaner Metrics</strong></p>\n<p>In each column, you can now see only the main numerical value and a graph showing it as a percentage of the total value:<strong><br />\n</strong></p>\n<p><img class=\"size-medium wp-image-69789 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/metric-columns-258x300.png\" alt=\"\" width=\"258\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/metric-columns-258x300.png 258w, https://www.percona.com/blog/wp-content/uploads/2020/07/metric-columns-129x150.png 129w, https://www.percona.com/blog/wp-content/uploads/2020/07/metric-columns.png 310w\" sizes=\"(max-width: 258px) 100vw, 258px\" /></p>\n<p>More information is revealed by mouse-over tooltips:</p>\n<p><img class=\"size-medium wp-image-69790 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/metric-tooltip-300x244.png\" alt=\"\" width=\"300\" height=\"244\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/metric-tooltip-300x244.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/metric-tooltip-184x150.png 184w, https://www.percona.com/blog/wp-content/uploads/2020/07/metric-tooltip-367x298.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/metric-tooltip.png 846w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></p>\n<p><strong>Standardized Column Widths</strong></p>\n<p>The additional information in tooltips lets us equalize and reduce the widths of all columns. This means we can show significantly more information in the same screen area without scrolling.</p>\n<h4>Consistent Formatting</h4>\n<p>Now, all SQL queries in the output are formatted, not only those in the example sections.</p>\n<p><img class=\"size-medium wp-image-69791 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted-267x300.png\" alt=\"\" width=\"267\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted-267x300.png 267w, https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted-912x1024.png 912w, https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted-134x150.png 134w, https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted-367x412.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted.png 1226w\" sizes=\"(max-width: 267px) 100vw, 267px\" /></p>\n<h4>Removed Duplicated Filters Search and Selection functionality</h4>\n<p>The filter search string and filter selection blocks have been combined into one section.</p>\n<p><img class=\"wp-image-69792 aligncenter\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new-464x1024.png\" alt=\"\" width=\"270\" height=\"596\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new-464x1024.png 464w, https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new-136x300.png 136w, https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new-68x150.png 68w, https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new-367x810.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new.png 520w\" sizes=\"(max-width: 270px) 100vw, 270px\" /></p>\n<h4>Improved UI Styling and Consistency</h4>\n<p>We&#8217;ve reduced the number of different fonts, colors, and styles used in the UI and removed a lot of unnecessary clutter from the interface.</p>\n<h2>Future Plans</h2>\n<p>We&#8217;re not done yet! Here are some PMM improvements we&#8217;ll be working on in the coming weeks and months.</p>\n<ol>\n<li>Simplify the onboarding process for users;</li>\n<li>Improve user documentation;</li>\n<li>Improve the search and filter performance so that it performs better with large amounts of data;</li>\n<li>Add more information content without compromising the simplicity of the interface, to make the system even more appealing to experts.</li>\n</ol>\n<p>With our recent changes, we don’t need to worry about maintaining compatibility between many weakly-compatible elements of the system. Instead, we can focus all of our attention on the front-end, further improving the functionality and polishing the interface.</p>\n<h2>Why Did We Start a Migration?</h2>\n<p>A few years ago, QAN was created as a small, separate application. <span class=\"tlid-translation translation\" lang=\"en\">PMM1 development began with Angular1, because Grafana was written with Angular1. </span><span class=\"tlid-translation translation\" lang=\"en\">Then, when Angular2 was released, </span><span class=\"tlid-translation translation\" lang=\"en\">we noticed in Grafana&#8217;s source code hints of a possible future migration to Angular2.</span></p>\n<p><span class=\"tlid-translation translation\" lang=\"en\">Originally, QAN was a proof of concept. As we needed to add functionality and rethink the architecture, and given the hints we&#8217;d seen in Grafana development, we decided to rewrite QAN in Angular2.</span></p>\n<p>However, the Grafana team later announced their decision to migrate to React. We were left with a bunch of panels in which we&#8217;d invested a lot of time and which needed to be supported. There was no hope of adding native support from the Grafana side. We were being faced with these difficulties:</p>\n<ol>\n<li>Grafana is implemented with both Angular1 and React;</li>\n<li>We integrate tightly with Grafana, and our developers need to work with its code; our developers now need to be experts in three different frameworks;</li>\n<li>When Grafana chose React, our differences meant that it was just a matter of time before another rewrite would be necessary.</li>\n</ol>\n<h2>How It Began</h2>\n<p>As we considered these difficulties, we concluded that rewriting should start as soon as possible. Fortunately, Grafana&#8217;s approach to organizing panels as separate applications meant we could do this with minimal pain.</p>\n<p>The goals we had when starting the migration were:</p>\n<ul>\n<li>To simplify and unify the technical stack. This would also allow us to hire developers specializing in React.</li>\n<li>To synchronize with Grafana&#8217;s development strategy.</li>\n<li>To switch to a simpler and more comfortable framework, one that allows us to speed up the adding of new functionality, and streamline development as a whole.</li>\n</ul>\n<p>As an added advantage, we eliminated many legacy bugs, improved project documentation, and also simplified user interaction. And yes, we ejected a lot of redundant code that had been accumulating for a long time.</p>\n<h2>Technical Changes</h2>\n<p>This section is for those who may be interested in participating in the development.</p>\n<p>What does migration mean for us, as front-end developers?</p>\n<ol>\n<li>Problems with tooling are now gone, and we don’t need to assemble different parts of the interface in different ways. All assembly, testing, and checks happen as they should for Grafana plugins.</li>\n<li>Almost all the code related to the frontend has moved to one repository. (This has already greatly simplified our work with assembly pipelines.)</li>\n<li>We now have a homogeneous code base. Our requirements for hiring only specify an in-depth knowledge of React (in addition to the usual stack development skills).</li>\n<li>There is improved integration with e2e tests that now form an integral part of the solution. Previously, developers and QA lived their own lives.</li>\n</ol>\n<h3>Your Opinion Matters!</h3>\n<div>\n<div>\n<p dir=\"ltr\">We are constantly working to improve <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> –your feedback is invaluable. Please tell us about your user experiences by completing <a target=\"_blank\" href=\"https://forms.gle/H1qWXLhukD1m5UNm7\">this one-minute survey</a>.</p>\n</div>\n<p dir=\"ltr\">You&#8217;ll be helping to decide how PMM develops and continues to grow in the future. Don&#8217;t miss this chance to have your voice heard!</p>\n</div>\n","descriptionType":"html","publishedDate":"Thu, 16 Jul 2020 17:31:33 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management.png","linkMd5":"633884842b6510ad4e90124b7919beb8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_2/2020-07-25/1595696398029_6108cc2ad91ef843.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management.png","author":"Roman Misyurin","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_5/2020-07-25/1595696399696_abc485c94388674a.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/improvements-to-qan-percona-monitoring-and-management-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_4/2020-07-25/1595696399634_5e5f9b15d05fc0ff.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/mongodb.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_2/2020-07-25/1595696404084_3d163f6bdf6fc91b.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/time-distribution.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_5/2020-07-25/1595696400012_11ea921d4c628b99.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/metrics-selection-300x219.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_5/2020-07-25/1595696399534_709aa78fbf41f7db.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/metric-columns-258x300.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_4/2020-07-25/1595696404599_20e2c07a2b86ea47.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/metric-tooltip-300x244.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_4/2020-07-25/1595696401333_6277e92171297b6a.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/SQL-formatted-267x300.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_3/2020-07-25/1595696401842_67b46dabe5b1970e.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/filters-new-464x1024.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_2/2020-07-25/1595696400216_a92054481cf0c8e6.webp"}},{"createdTime":"2020-07-26 00:59:41","updatedTime":"2020-07-25 16:59:41","title":"Rework of QAN for Percona Monitoring and Management, Update to Percona Distribution for MongoDB: Release Roundup July 20, 2020","link":"https://www.percona.com/blog/?p=69741","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Software Updates\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates.png 712w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2><img class=\"alignright size-medium wp-image-69745\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-300x169.png\" alt=\"Percona Software Updates\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates.png 712w\" sizes=\"(max-width: 300px) 100vw, 300px\" />It&#8217;s release roundup time here at Percona!</h2>\n<p>Our Release Roundups <span class=\"s1\">showcase the latest software updates, tools, and features to help you manage and deploy our software, with</span> highlights and critical information, as well as links to the full release notes and direct links to the software or service itself.</p>\n<p>Today&#8217;s post includes those releases and updates that have come out since July 6, including a new version of QAN for Percona Monitoring and Management, and version updates to Percona Server for MongoDB and Percona Distribution for MongoDB.</p>\n<h2></h2>\n<h2>Percona Server for MongoDB 4.2.8-8</h2>\n<p>On July 7, 2020, <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.2/release_notes/4.2.8-8.html\">Percona Server for MongoDB 4.2.8-8</a> was released. It is an enhanced, open source, and highly-scalable database that is a fully-compatible, drop-in replacement for MongoDB 4.2.8 Community Edition, supporting MongoDB 4.2.8 protocols and drivers. There are several bug fixes in this release, including ensuring a user’s permissions remain intact after a user is removed from LDAP and a bug where some LDAP servers drop idle connections upon timeout.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/downloads/percona-server-mongodb-LATEST/\">Download Percona Server for MongoDB 4.2.8-8</a></p>\n<p>&#160;</p>\n<h2>Percona Distribution for MongoDB 4.2.8</h2>\n<p>July 7, 2020, saw the release of <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-distribution-for-mongodb/4.2/release-notes-v4.2.8.html\">Percona Distribution for MongoDB 4.2.8</a>. It is a collection of solutions to run and operate your MongoDB efficiently with the data being consistently backed up. This release is based on Percona Server for MongoDB 4.2.8-8 and Percona Backup for MongoDB 1.2.0.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/downloads/percona-distribution-mongodb/LATEST/\">Download Percona Distribution for MongoDB 4.2.8</a></p>\n<p>&#160;</p>\n<h2>Percona Monitoring and Management 1.17.4</h2>\n<p><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/release-notes/1.17.4.html\">Percona Monitoring and Management 1.17.4</a> was released on July 8, 2020. It is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. In this release, we added missing dashboards to the list in the README.md file, and fixed several bugs, including a fix for Grafana vulnerability, update to ESLints dependencies, update minimist to 1.2.3, and a fix to the Wizard page where it was not showing for AMI or OVF images.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/downloads/pmm/LATEST/\">Download Percona Monitoring and Management 1.17.4</a></p>\n<p>&#160;</p>\n<h2>Percona Monitoring and Management 2.9.0</h2>\n<p>On July 14, 2020, <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/release-notes/2.9.0.html\">Percona Monitoring and Management 2.9.0</a> was released. This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. Also, new dashboards for MongoDB Replica Set Summary, MongoDB Cluster Summary, MySQL User Details, and user interface for MongoDB EXPLAIN. Several bugs were fixed, as well, and a full list is available in the release notes.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/downloads/pmm2/\">Download Percona Monitoring and Management 2.9.0</a></p>\n<p>&#160;</p>\n<p>That&#8217;s it for this roundup, and be sure to <a target=\"_blank\" href=\"https://twitter.com/Percona\" target=\"_blank\" rel=\"&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;noopener&#34; noopener noreferrer\">follow us on Twitter</a> to stay up-to-date on the most recent releases! Percona is a leader in providing best-of-breed enterprise-class support, consulting, managed services, training, and software for MySQL, MariaDB, MongoDB, PostgreSQL, and other open source databases in on-premises and cloud environments.</p>\n<hr />\n<p>We understand that choosing open source software for your business can be a potential minefield. You need to select the best available options, which fully support and adapt to your changing needs. Choosing the right open source software can allow you access to enterprise-level features, without the associated costs.</p>\n<p>In our white paper, we discuss the key features that make open source software attractive, and why Percona&#8217;s software might be the best option for your business.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/white-papers/when-percona-software-right-choice?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=roundup&#38;utm_content=whitepaper\" rel=\"noopener\">Download: When is Percona Software the Right Choice?</a></p>\n","descriptionType":"html","publishedDate":"Mon, 20 Jul 2020 13:27:44 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates.png","linkMd5":"d4f8b7dbece47be0c286521f43f87e7e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_1/2020-07-25/1595696398047_391b1dedeb595c98.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_6/2020-07-25/1595696399893_27fb4b39e53430fe.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Percona-Software-Updates-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_4/2020-07-25/1595696400347_b6d30046b2574454.webp"}},{"createdTime":"2020-07-26 00:59:50","updatedTime":"2020-07-25 16:59:50","title":"MySQL 101: Parameters to Tune for MySQL Performance","link":"https://www.percona.com/blog/?p=69345","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Parameters to Tune for MySQL Performance\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69522\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-300x157.png\" alt=\"Parameters to Tune for MySQL Performance\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />While there is no magic bullet for MySQL tuning, there are a few areas that can be focused on upfront that can dramatically improve the performance of your MySQL installation. While much information has been published on this topic over the years, I wanted to break down some of the more critical settings that anyone can implement with no guesswork required.</p>\n<p>Depending on the version of MySQL you are running, some of the default values used in this post may differ from your install, but the premise is still largely the same.</p>\n<p>Initial MySQL performance tuning can be broken down to the following categories:</p>\n<ul>\n<li><strong>Tuning for your hardware</strong></li>\n<li><strong>Tuning for best performance / best practices</strong></li>\n<li><strong>Tuning for your workload</strong></li>\n</ul>\n<h2><strong>Tuning MySQL for Your Hardware</strong></h2>\n<p>Depending on the hardware you have installed MySQL on, some variables need to be set based on the machine (or VM) specifications. The following variables are largely dependent on your hardware:</p>\n<p class=\"crayon-selected\"><strong>innodb_buffer_pool_size</strong></p>\n<ul>\n<li>Generally, set to 50% &#8211; 70% of your total RAM as a starting point.</li>\n<li>It does not need to be set any larger than the total database size.</li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management (PMM)</a> can offer additional insight, showing your buffer pool usage and allowing you to tune accordingly.\n<ul>\n<li><img class=\"alignnone size-medium wp-image-69360\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_2-300x54.png\" alt=\"\" width=\"300\" height=\"54\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_2-300x54.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_2-200x36.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_2-367x66.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_2.png 447w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></li>\n<li><img class=\"alignnone size-medium wp-image-69361\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_1-300x54.png\" alt=\"\" width=\"300\" height=\"54\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_1-300x54.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_1-200x36.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_1-367x66.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_1.png 447w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></li>\n<li><img class=\"alignnone size-medium wp-image-69359\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_3-300x54.png\" alt=\"\" width=\"300\" height=\"54\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_3-300x54.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_3-200x36.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_3-367x66.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_3.png 447w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></li>\n</ul>\n</li>\n</ul>\n<p class=\"crayon-selected\"><strong>innodb_log_file_size</strong></p>\n<ul>\n<li>This is generally set between 128M – 2G.</li>\n<li>Should be large enough to hold at most an hour or so of logs.\n<ul>\n<li>This is more than enough so that MySQL can reorder writes to use sequential I/O during the flushing and checkpointing processes.</li>\n</ul>\n</li>\n<li>PMM can offer additional insight, as if you are using more than 50% of your log space, you may benefit from a log file size increase.\n<ul>\n<li><img class=\"alignnone size-medium wp-image-69357\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/innodb_log_file_size_PMM-300x53.png\" alt=\"\" width=\"300\" height=\"53\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/innodb_log_file_size_PMM-300x53.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/innodb_log_file_size_PMM-200x35.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/innodb_log_file_size_PMM-367x65.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/innodb_log_file_size_PMM.png 448w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></li>\n</ul>\n</li>\n</ul>\n<p class=\"crayon-selected\"><strong>innodb_flush_log_at_trx_commit</strong></p>\n<ul>\n<li>Setting to “1” (default in 5.7) gives the most durability.</li>\n<li>Setting to “0” or “2” will give more performance, but less durability.</li>\n</ul>\n<p class=\"crayon-selected\"><strong>innodb_flush_method</strong></p>\n<ul>\n<li>Setting this to O_DIRECT will avoid a performance penalty from double buffering.</li>\n</ul>\n<h2><strong>MySQL Tuning for Best Performance/Best Practices</strong></h2>\n<p class=\"crayon-selected\"><strong>innodb_file_per_table</strong></p>\n<ul>\n<li>Setting this to “ON” will generate an independent InnoDB table space for every table in the database.</li>\n</ul>\n<p class=\"crayon-selected\"><strong>innodb_stats_on_metadata</strong></p>\n<ul>\n<li>Setting this to “OFF” avoids unnecessary updating of InnoDB statistics and can greatly improve read speeds.</li>\n</ul>\n<p class=\"crayon-selected\"><strong>innodb_buffer_pool_instances</strong></p>\n<ul>\n<li>A best practice is to set this to “8” unless the buffer pool size is &#60; 1G, in which case set to “1”.</li>\n</ul>\n<p class=\"crayon-selected\"><strong>query_cache_type</strong> &#38; <strong>query_cache_size</strong></p>\n<ul>\n<li>Setting both of these to “0” will entirely disable the query cache.</li>\n</ul>\n<h2><strong>Tuning for Your Workload</strong></h2>\n<p>To tune further, more information will be required. The best way to gather this information is to install a MySQL monitoring / graphing tool like <a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> platform. Once you have a tool installed, we can dive into the individual metrics and start customizing based on the data.</p>\n<p>I would recommend starting with one of the most impactful variables – the <strong>innodb_buffer_pool_size</strong>.  Compare the RAM and number of free pages on your instance to the total buffer pool size. Based on these metrics, you can determine if you need to increase or decrease your overall buffer pool size setting.</p>\n<p>Next, take a look at your metrics for the InnoDB Log File usage. The rule of thumb is that your log files should hold approximately one hour of data. If you see that your data written to the log files hourly exceeds the total size of the log files, you would want to increase the <strong>innodb_log_file_size</strong> variable and restart MySQL. You could also verify with “SHOW ENGINE INNODB STATUS;” via the MySQL CLI to assist in calculating a good InnoDB log file size.</p>\n<h2><strong>Other Settings</strong></h2>\n<p>Other InnoDB settings that can be further tuned for better performance are:</p>\n<p class=\"crayon-selected\"><strong>innodb_autoinc_lock_mode</strong></p>\n<ul>\n<li>Setting this to “2” (interleaved mode) can remove the need for an auto-inc lock (at the table level) and can increase performance when using multi-row insert statements to insert values into a table with an auto increment primary key. Note that this requires either ROW or MIXED binlog format.</li>\n</ul>\n<p class=\"crayon-selected\"><strong>innodb_io_capacity</strong> /<strong> innodb_io_capacity_max</strong></p>\n<ul>\n<li>These settings will impact your database if you are utilizing a write-heavy workflow. This does not apply to read (SELECT) traffic. To tune these values, it is best to know how many iops your system can perform. It is a good idea to run sysbench or another benchmark tool to determine your storage throughput.</li>\n<li>PMM can offer additional insight, showing your IO usage and allowing you to tune accordingly.\n<ul>\n<li><img class=\"alignnone size-medium wp-image-69362\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/inndob_disk_io-300x129.png\" alt=\"\" width=\"300\" height=\"129\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/inndob_disk_io-300x129.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/inndob_disk_io-200x86.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/inndob_disk_io-367x158.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/inndob_disk_io.png 895w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></li>\n</ul>\n</li>\n</ul>\n<h3><strong>In Summary</strong></h3>\n<p>While this is by no means a comprehensive article on MySQL tuning, the suggestions above should clear some of the low hanging fruit and get your system closer to an ideal setup. As with all database tuning, your process should be an ongoing one based on current information.</p>\n<ul>\n<li>Examine the settings proposed above, and implement if they make sense for your environment/workload.</li>\n<li>Install a good monitoring tool to give insight into the database (<a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> is our suggestion).</li>\n<li>Stay current on your monitoring graphs to determine other areas where you may need to tune.</li>\n</ul>\n<hr />\n<p>Our solution brief &#8220;Get Up and Running with Percona Server for MySQL&#8221; outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/get-and-running-percona-server-mysql?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=parameters&#38;utm_content=solutionbrief\" rel=\"noopener\">Download PDF</a></p>\n","descriptionType":"html","publishedDate":"Tue, 30 Jun 2020 13:38:55 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance.png","linkMd5":"8a2d4c2b59ef115987ccd1b604f85b9e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn91@2020_3/2020-07-25/1595696397688_01ce1180cd2c3e0f.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance.png","author":"Brian Sumpter","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_5/2020-07-25/1595696403875_71e075027889db09.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_3/2020-07-25/1595696400116_0f35d339ec375f55.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_2-300x54.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_2/2020-07-25/1595696402019_6930b20cabead18c.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_1-300x54.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_2/2020-07-25/1595696399810_c06f494c2dbbaace.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/buffer_pool_3-300x54.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn71@2020_1/2020-07-25/1595696403580_3732fc66df4cdc57.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/innodb_log_file_size_PMM-300x53.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_4/2020-07-25/1595696399780_0076620fa036b625.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/inndob_disk_io-300x129.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_5/2020-07-25/1595696402328_aec0b4e16e44c439.webp"}},{"createdTime":"2020-07-26 00:59:52","updatedTime":"2020-07-25 16:59:52","title":"Top 7 Reasons Why Security Goes Wrong In A Database System","link":"https://www.percona.com/blog/?p=69202","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Why Security Goes Wrong In A Database System\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69401\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-300x157.png\" alt=\"Why Security Goes Wrong In A Database System\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Today I&#8217;m going to talk about my favorite trope, &#8220;database security&#8221;.</p>\n<p>When done right, a good security policy not only protects your data but improves performance, system stability, and enhances the development life-cycle. When done wrong it not only risks the confidentiality and integrity of your data but it leaves your organization open to significant financial risks.</p>\n<h3>Top 7</h3>\n<ol>\n<li>User account policies</li>\n<li>User account access control</li>\n<li>Session connections</li>\n<li>Logging/history</li>\n<li>Dynamically created queries (prepared statements)</li>\n<li>Host-based authentication rules</li>\n<li>Sensitive data (encryption)</li>\n</ol>\n<h2>1. USER ACCOUNT: POLICIES</h2>\n<p><img class=\"alignleft size-medium wp-image-69404\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/account-access-300x200.png\" alt=\"account access security\" width=\"300\" height=\"200\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/account-access-300x200.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/account-access-200x133.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/account-access-367x245.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/account-access.png 900w\" sizes=\"(max-width: 300px) 100vw, 300px\" />If it&#8217;s one thing that drives me crazy, it&#8217;s presiding over a database system populated with active user accounts of employees and contractors who have long since departed. This is such an easy fix, just delete the user account! However, removing accounts isn&#8217;t always straight forward because they can own relations critical to the operation of the database system.</p>\n<p>One can set an expiration date when the user account is first created. Of course, you&#8217;ll want to consider your options when managing key application processes.</p>\n<p>&#160;</p>\n<h2>2. USER ACCOUNT: ACCESS CONTROL</h2>\n<p>Another security consideration is the GRANTING and REVOKING of user account privileges. This is a subtle issue because of the temptation of using a single account with super-user privileges in order to facilitate rapid product development. One must always remember to review and harden all user privileges before leaving development and moving onto QA and, eventually, production. Consider implementing a standard sanity test for all your projects in order to catch the unexpected oopsie.</p>\n<p>Security failures typically come in two flavors:</p>\n<ol>\n<li>User accounts with too much privilege:\n<ul>\n<li>A user account that can log in to unauthorized databases</li>\n<li>A user account possessing unnecessary, redundant, escalation privileges</li>\n</ul>\n</li>\n<li>User accounts used for the wrong task:\n<ul>\n<li>A superuser account used by a monitoring process</li>\n<li>An account with superuser privileges managing routine application processes. <em>Just try logging into a system when you&#8217;re out of connections and see how that works out.</em></li>\n</ul>\n</li>\n</ol>\n<h2>3. SESSION CONNECTIONS</h2>\n<p>Security versus performance: it&#8217;s a simple enough equation. Either one enforces encrypted sessions as mitigation against network sniffing or instead opt for as high a level of performance as possible using un-encrypted sessions.</p>\n<p>Consider using session encryption for:</p>\n<ul>\n<li>Administrative activities</li>\n<li>Handling sensitive information i.e. social security numbers, passwords, etc.</li>\n</ul>\n<p>A truly secure environment includes encrypting connections between the DBMS and its monitoring system. Remember, meta-data is just as important as the data itself and its leakage can provide critical information to those desirous of destabilizing your production environment.</p>\n<h2>4. LOGGING/HISTORY</h2>\n<p><img class=\"alignleft size-medium wp-image-69406\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/logging-database-history-300x200.jpeg\" alt=\"database log history\" width=\"300\" height=\"200\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/logging-database-history-300x200.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/logging-database-history-200x133.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/logging-database-history-367x245.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/logging-database-history.jpeg 900w\" sizes=\"(max-width: 300px) 100vw, 300px\" />One of the most basic operations of a DBMS is the logging of its events. Although no longer as referenced to as in the past, they still provide the best line of inquiry determining the root causes.</p>\n<p>Seasoned DBAs and SREs shine during production incidents because they understand the power of this most basic system. Good logging not only empowers one the ability to mitigate an issue but, as part of a larger root cause analysis exercise, forms the basis of an improved system. Always make certain to configuring logging as many of the available variables as reasonable, you never know when you&#8217;ll need them.</p>\n<p>&#160;</p>\n<h2>5. DYNAMICALLY CREATED QUERIES</h2>\n<p>Dynamically created queries, i.e. prepared statements, are strings constructed and used as the basis of a SQL instruction on the DBMS.</p>\n<p>Fortunately, this is one issue hammered deep into the heart of most developers.</p>\n<p>The exploitation of this extremely useful facility by hostile agents is often facilitated by appending well-crafted strings to the original argument which performs additional instructions.</p>\n<p>Server-side programming languages, such as python, php, perl, etc, provide functions calls rendering the string &#8220;safe&#8221; by removing all control characters and undesired strings. The DBMS also have these kinds of functions/sprocs thereby providing another layer of defense.</p>\n<p><strong>TIP</strong>: <em>Perform a little pen-testing by using junk strings as arguments in your prepared statements and monitor the results.</em></p>\n<h2>6. HOST BASED AUTHENTICATION RULES</h2>\n<p>Leveraging host-based authentication rules mitigates unauthorized access by knowing ahead of time where application processes are connecting from and which addresses can connect to what databases for a particular purpose such as monitoring and administration.</p>\n<p>Mature database management systems have the ability to accept or deny a client connection request based upon one or more parameters and includes:</p>\n<ul>\n<li>Source IP address</li>\n<li>User account</li>\n<li>Database</li>\n<li>Encrypted vs un-encrypted sessions</li>\n</ul>\n<h2>7. SENSITIVE DATA</h2>\n<p><img class=\"size-medium wp-image-69405 alignleft\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-300x168.jpeg\" alt=\"sensitive data security\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-300x168.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-1024x575.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-200x112.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-1536x863.jpeg 1536w, https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-2048x1150.jpeg 2048w, https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-367x206.jpeg 367w\" sizes=\"(max-width: 300px) 100vw, 300px\" /></p>\n<p>There&#8217;s sensitive data and then there&#8217;s SENSITIVE data. You&#8217;ll know the difference by the amount of money that can be lost due to its unauthorized release. For example, on the dark web, a stolen health card number is <a target=\"_blank\" href=\"https://www.cyberpolicy.com/cybersecurity-education/why-medical-records-are-10-times-more-valuable-than-credit-card-info\" target=\"_blank\" rel=\"noopener\">worth substantially more</a> than a credit card number.</p>\n<p>Sensitive data can be protected by the use of encryption and includes the following methods:</p>\n<ul>\n<li style=\"list-style-type: none;\">\n<ul>\n<li><strong>Hashing</strong> the data as a checksum, i.e. <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Transparent_data_encryption\" target=\"_blank\" rel=\"noopener\">translucent</a> data: This method is commonly used when one wants to validate a decision, or process such as identifying oneself with a social security number or providing a credit card number in order to make a purchase, when the user submits the sensitive data across an encrypted session and is compared against a hashed version of the same data. This is by far the safest method available as technically speaking one is not storing any sensitive data but merely its hash.</li>\n<li><strong>Encrypting</strong> the data in the database: There are two variations encrypting data i.e. <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Symmetric-key_algorithm\" target=\"_blank\" rel=\"noopener\">symmetrical</a> and <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Public-key_cryptography\" target=\"_blank\" rel=\"noopener\">asymmetrical</a> encryption. Then there&#8217;s the encryption strength where the stronger the encryption the more CPU, and time, it takes to encrypt it. Even if the data is stolen decryption is only possible with the password, private key, or a brute force attack (good luck with that!).</li>\n<li>Encrypting the entire database through the use of <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Data_at_rest#Encryption\" target=\"_blank\" rel=\"noopener\">encryption-at-rest</a> technologies.</li>\n</ul>\n</li>\n</ul>\n<p><strong>ATTENTION</strong>: <em>I&#8217;ve seen in a couple of places where people decided to get smart and chose to create their own encryption algorithm. Don&#8217;t! Use the standard stuff that&#8217;s out there and avoid reinventing the wheel.</em></p>\n","descriptionType":"html","publishedDate":"Thu, 25 Jun 2020 17:28:49 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System.png","linkMd5":"85825cc047e3454451faf3fae4f9ad26","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_6/2020-07-25/1595696397973_04637942f4233761.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System.png","author":"Robert Bernier","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn11@2020_1/2020-07-25/1595696403070_591f579bdc56fdf9.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Why-Security-Goes-Wrong-In-A-Database-System-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_1/2020-07-25/1595696402382_08290040e20e1634.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/account-access-300x200.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_1/2020-07-25/1595696405697_ee43ded71677731c.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/logging-database-history-300x200.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_4/2020-07-25/1595696404857_1210f156306b878a.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/sensitive-data-security-300x168.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_5/2020-07-25/1595696402816_e6b67cd6c490b1da.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"Evaluating MongoDB Under Python TPCC 1000W Workload","link":"https://www.percona.com/blog/?p=69301","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"evaluting mongodb python tpcc\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69319\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-300x168.png\" alt=\"evaluting mongodb python tpcc\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Following my blog post <a target=\"_blank\" href=\"https://www.percona.com/blog/2020/06/15/evaluating-the-python-tpcc-mongodb-benchmark/\">Evaluating the Python TPCC MongoDB Benchmark</a>, I wanted to evaluate how MongoDB performs under workload with a bigger dataset. This time I will load a 1000 Warehouses dataset, which in raw format should equal to 100GB of data.</p>\n<p>For the comparison, I will use the same hardware and the same MongoDB versions as in the blog post mentioned above. To reiterate:</p>\n<h2>Hardware Specs</h2>\n<p>For the client and server, I will use identical bare metal servers, connected via a 10Gb network.</p>\n<p>The node specification:</p><pre class=\"crayon-plain-tag\"># Percona Toolkit System Summary Report ######################\n    Hostname | beast-node4-ubuntu\n      System | Supermicro; SYS-F619P2-RTN; v0123456789 (Other)\n    Platform | Linux\n     Release | Ubuntu 18.04.4 LTS (bionic)\n      Kernel | 5.3.0-42-generic\nArchitecture | CPU = 64-bit, OS = 64-bit\n# Processor ##################################################\n  Processors | physical = 2, cores = 40, virtual = 80, hyperthreading = yes\n      Models | 80xIntel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz\n      Caches | 80x28160 KB\n# Memory #####################################################\n       Total | 187.6G\n  Swappiness | 0</pre><p></p>\n<h2>MongoDB Topology</h2>\n<p>For MongoDB I used:</p>\n<ul>\n<li><span>Single node instance without limiting cache size. As the bare metal server has 180GB of RAM, MongoDB should allocate 90GB of memory for WiredTiger cache and the rest will be used for OS cache. This should produce more CPU bound workload.</span></li>\n<li><span>Single node instance with limited cache size. For WiredTiger cache I will set limit 25GB, and to limit OS cache I will limit the memory available to mongodb instance to 50GB, as described in <a target=\"_blank\" href=\"https://www.percona.com/blog/2015/07/01/using-cgroups-to-limit-mysql-and-mongodb-memory-usage/\">Using Cgroups to Limit MySQL and MongoDB memory usage</a>.</span></li>\n<li><span>Replicate set setup with 3 nodes and limited cache as described above.</span></li>\n</ul>\n<h3>MongoDB Versions:</h3>\n<ul>\n<li><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.0/release_notes/4.0.18-11.html\" target=\"_blank\" rel=\"noopener\">Percona Server for MongoDB 4.0.18-11</a></li>\n<li><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.2/release_notes/4.2.7-7.html\" target=\"_blank\" rel=\"noopener\">Percona Server for MongoDB 4.2.7-7</a></li>\n<li>MongoDB Community 4.4-rc8 (the latest 4.4 version available as for the time of testing)</li>\n</ul>\n<h3>Loading Data</h3>\n<p><span>I will load data using PyPy python version and using 100 clients and timing it:</span></p><pre class=\"crayon-plain-tag\">time /mnt/data/vadim/bench/pypy2.7-v7.3.1-linux64/bin/pypy tpcc.py --config mconfig --warehouses 1000 --clients=100 --no-execute mongodb</pre><p>The results:</p>\n<h3><strong>4.0</strong></h3>\n<p></p><pre class=\"crayon-plain-tag\">time /mnt/data/vadim/bench/pypy2.7-v7.3.1-linux64/bin/pypy  tpcc.py --config mconfig --warehouses 1000 --clients 100 --no-execute mongodb\n2020-06-17 13:21:35,159 [&#60;module&#62;:245] INFO : Initializing TPC-C benchmark using MongodbDriver\n2020-06-17 13:21:35,159 [&#60;module&#62;:255] INFO : Loading TPC-C benchmark data using MongodbDriver\n\nreal    19m43.605s\nuser    100m19.637s\nsys     26m27.597s</pre><p></p>\n<h3>4.2</h3>\n<p></p><pre class=\"crayon-plain-tag\">time /mnt/data/vadim/bench/pypy2.7-v7.3.1-linux64/bin/pypy tpcc.py --config mconfig --warehouses 1000 --clients=100 --no-execute mongodb\n2020-06-17 13:28:48,325 [&#60;module&#62;:245] INFO : Initializing TPC-C benchmark using MongodbDriver\n2020-06-17 13:28:48,325 [&#60;module&#62;:255] INFO : Loading TPC-C benchmark data using MongodbDriver\n\nreal    13m34.238s\nuser    87m30.806s\nsys     34m20.460s</pre><p></p>\n<h3>4.4</h3>\n<p></p><pre class=\"crayon-plain-tag\">time /mnt/data/vadim/servers/pypy2.7-v7.3.1-linux64/bin/pypy  tpcc.py\n--config mconfig --warehouses 1000 --no-execute --clients=100 mongodb\n2020-06-17 14:02:26,426 [&#60;module&#62;:245] INFO : Initializing TPC-C\nbenchmark using MongodbDriver\n2020-06-17 14:02:26,426 [&#60;module&#62;:255] INFO : Loading TPC-C benchmark\ndata using MongodbDriver\n\nreal    259m40.658s\nuser    83m36.256s\nsys     14m11.330s</pre><p><strong>To Highlight:</strong></p>\n<p>4.2 loaded data a little faster than 4.0, and 4.4 performed extremely bad, being about 20 times slower than 4.2. I hope this is some Release Candidate bug which will be fixed for the release.</p>\n<p><span>The size of MongoDB datadir is 165GB, it seems there is an overhead compared to the raw 100GB datasize.</span></p>\n<h2>Benchmark Results</h2>\n<h3>Results With an Unlimited Cache</h3>\n<p>The results are in NEW ORDER transactions per minute, AKA more is better.</p>\n<p><img class=\"aligncenter wp-image-69305 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.44.20-PM.png\" alt=\"MongoDB Python Benchmarks\" width=\"726\" height=\"434\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.44.20-PM.png 726w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.44.20-PM-300x179.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.44.20-PM-200x120.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.44.20-PM-367x219.png 367w\" sizes=\"(max-width: 726px) 100vw, 726px\" /></p>\n<p>&#160;</p>\n<p><img class=\"aligncenter wp-image-69306 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image5-1024x633.png\" alt=\"MongoDB Version Benchmarks\" width=\"900\" height=\"556\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image5-1024x633.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image5-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image5-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image5-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image5.png 1200w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h3>Results With a Limited Cache</h3>\n<p>In this cache, I allocate only 25GB for WiredTiger and 50GB for the mongodb process in total.</p>\n<p>The results are in NEW ORDER transactions per minute; more is better.</p>\n<p><img class=\"aligncenter wp-image-69315 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-24-at-8.35.02-AM.png\" alt=\"Results With a Limited Cache MongoDB\" width=\"860\" height=\"382\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-24-at-8.35.02-AM.png 860w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-24-at-8.35.02-AM-300x133.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-24-at-8.35.02-AM-200x89.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-24-at-8.35.02-AM-367x163.png 367w\" sizes=\"(max-width: 860px) 100vw, 860px\" /></p>\n<p>&#160;</p>\n<p><img class=\"aligncenter size-large wp-image-69307\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1-1024x633.png\" alt=\"\" width=\"900\" height=\"556\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1-1024x633.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1.png 1200w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h3>Results with 3 Nodes ReplicaSet and Limited Cache</h3>\n<p>In this case, I only compare 4.0 and 4.2, as from the previous results, there is something going on with 4.4 and I want to wait until GA release to measure it in ReplicaSet setup.</p>\n<p>&#8216;write_concern&#8217;: 1 for this benchmark.</p>\n<p>The results are in NEW ORDER transactions per minute, and more is better.</p>\n<p><img class=\"aligncenter wp-image-69308 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.52.53-PM.png\" alt=\"Results with 3 Nodes ReplicaSet and Limited Cache\" width=\"544\" height=\"434\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.52.53-PM.png 544w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.52.53-PM-300x239.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.52.53-PM-188x150.png 188w, https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.52.53-PM-367x293.png 367w\" sizes=\"(max-width: 544px) 100vw, 544px\" /></p>\n<p>&#160;</p>\n<p><img class=\"aligncenter size-large wp-image-69309\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1-1024x633.png\" alt=\"\" width=\"900\" height=\"556\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1-1024x633.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1.png 1200w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>Now we can compare how much overhead there is from ReplicaSets:</p>\n<p><img class=\"aligncenter size-large wp-image-69310\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image4-1024x633.png\" alt=\"\" width=\"900\" height=\"556\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image4-1024x633.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image4-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image4-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image4-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image4.png 1200w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><img class=\"aligncenter size-large wp-image-69311\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1024x633.png\" alt=\"\" width=\"900\" height=\"556\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1024x633.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-300x186.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-200x124.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3-367x227.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/image3.png 1200w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>With &#8216;write_concern&#8217;: 1 there really should not be much overhead from replicaset, which is confirmed for version 4.0. However, 4.2 shows a noticeable difference, which is a point for further investigation.</p>\n<h3>Conclusion</h3>\n<p>What is obvious from the collective results is that the 4.2 version took a noticeable performance hit, sometimes showing as much as a 2x throughput decline compared to 4.0.</p>\n<p>Version 4.4, as of the current RC status, showed long load times and variation in the performance results under high concurrent load. I want to wait for the GA release for the final evaluation.</p>\n","descriptionType":"html","publishedDate":"Wed, 24 Jun 2020 14:42:17 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc.png","linkMd5":"99a10abf414eb451bff1bfc7b1285a17","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_1/2020-07-25/1595696397730_4c531c1c4fd05d34.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc.png","author":"Vadim Tkachenko","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_4/2020-07-25/1595696403293_8f553215b51c0e84.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/evaluting-mongodb-python-tpcc-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_6/2020-07-25/1595696403766_e9ff277b8d89f5e2.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.44.20-PM.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_3/2020-07-25/1595696401766_a7fd3d543e8d145e.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image5-1024x633.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_6/2020-07-25/1595696402833_cb2bda2a7fb3d5c1.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-24-at-8.35.02-AM.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn95@2020_2/2020-07-25/1595696402996_9415c7891986758b.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image2-1-1024x633.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_6/2020-07-25/1595696404967_cbe505756075887e.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Screen-Shot-2020-06-23-at-1.52.53-PM.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_3/2020-07-25/1595696400096_23542916cd6df39a.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image1-1-1024x633.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_3/2020-07-25/1595696404376_992264a239534e2d.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image4-1024x633.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_3/2020-07-25/1595696403903_18bbacf50b6bb3f5.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/image3-1024x633.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_3/2020-07-25/1595696402368_d165fdb72f5a253c.webp"}},{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","title":"Webinar July 9 – Modern Solutions for Modern Database Load: MySQL 8.0 and Percona","link":"https://www.percona.com/blog/?p=69368","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Modern Solutions for Modern Database Load_ MySQL 8.0 and Percona\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69445\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-300x168.png\" alt=\"Modern Solutions for Modern Database Load_ MySQL 8.0 and Percona\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Join <strong>Sveta Smirnova</strong>, <strong>MySQL Engineer at Percona</strong>, as she discusses modern solutions for modern database loads.</p>\n<p>MySQL is famous for working well in high performing environments. This is the reason why it is the most popular backend for web applications. But our view of what to call high-performance changes over the cycles. Every year we get faster data transfer speed; more devices, connected to the Internet; more users and, as a result, more data.</p>\n<p>The challenges MySQL developers have to solve are getting harder over time.</p>\n<p>In this session, Sveta will show how use-case scenarios are changing over 25 years of MySQL history. She will show what did MySQL engineers do to keep the product up to date and cover topics such as handling a large number of active connections and high volumes of data as well as how the latest MySQL versions handle increased load better.</p>\n<p>After attending this session users who are currently on the older versions will know why an upgrade makes sense and those who already upgraded will learn how to use modern MySQL at its full speed.</p>\n<p>Please join <strong>Sveta Smirnova</strong> on <strong>Thursday, July 9 at 12 pm EDT</strong> for her webinar &#8220;<strong>Modern Solutions for Modern Database Load: MySQL 8.0 and Percona</strong>&#8220;.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/webinars/modern-solutions-modern-database-load-mysql-80-and-percona\" rel=\"noopener\">Watch the Recording</a></p>\n<p>If you can&#8217;t attend, <a target=\"_blank\" href=\"https://attendee.gotowebinar.com/register/2081679384401545227?source=Blog\">sign up anyway</a> and we&#8217;ll send you the slides and recording afterward.</p>\n","descriptionType":"html","publishedDate":"Fri, 26 Jun 2020 14:37:45 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1.png","linkMd5":"e280a31896a091f6b09b7323400e28bb","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_6/2020-07-25/1595696398078_33d3565f6c995611.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn31@2020_3/2020-07-25/1595696400861_d174dc8847602f00.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Modern-Solutions-for-Modern-Database-Load_-MySQL-8.0-and-Percona-1-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn35@2020_3/2020-07-25/1595696399950_ba3d5d0970c1e62f.webp"}},{"createdTime":"2020-07-26 00:59:41","updatedTime":"2020-07-25 16:59:41","title":"Using Percona Kubernetes Operators with Percona Monitoring and Management","link":"https://www.percona.com/blog/?p=69941","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Using Percona Kubernetes Operators with Percona Monitoring and Management\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management.png 1024w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><a target=\"_blank\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\"><img class=\"alignright size-medium wp-image-70109\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-300x168.png\" alt=\"Using Percona Kubernetes Operators with Percona Monitoring and Management\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management.png 1024w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Percona Monitoring and Management</a> (PMM) provides an excellent solution to monitor both <a target=\"_blank\" href=\"https://www.percona.com/software/mysql-database/percona-xtradb-cluster\">Percona XtraDB Cluster</a> and <a target=\"_blank\" href=\"https://www.percona.com/software/mongodb/percona-server-for-mongodb\">Percona Server for MongoDB</a>. Currently, there is no Percona-supported deployment model to deploy PMM by <a target=\"_blank\" href=\"https://www.percona.com/software/percona-kubernetes-operators\">Percona Kubernetes Operators</a>. Still, you can try the following experimental approach to use PMM in Kubernetes or OpenShift environment together with the operators for Percona XtraDB Cluster or Percona Server for MongoDB.</p>\n<h2>Installing the PMM Server</h2>\n<p>This first thing to do is to install the PMM Server to monitor Percona Server for MongoDB or Percona XtraDB Cluster on Kubernetes or OpenShift. The following three steps are optional if you already have installed the PMM Server. The PMM Server available on your network does not require another installation in Kubernetes!</p>\n<p>1. The recommended installation approach is based on using <a target=\"_blank\" class=\"reference external\" href=\"https://github.com/helm/helm\">helm</a> &#8211; the package manager for Kubernetes, which will substantially simplify further steps. Install helm following its <a target=\"_blank\" class=\"reference external\" href=\"https://docs.helm.sh/using_helm/#installing-helm\">official installation instructions</a>.</p>\n<p>2. Using helm, add the Percona chart repository and update the information for the available charts as follows:</p><pre class=\"crayon-plain-tag\">$ helm repo add percona https://percona-charts.storage.googleapis.com\n$ helm repo update</pre><p>3. Use helm to install PMM Server:</p>\n<p>OpenShift command:</p>\n<div class=\"highlight-default\">\n<div class=\"highlight\">\n<pre class=\"crayon-plain-tag\">$ helm install monitoring percona/pmm-server --set platform=openshift --version 1.17.3 --set \"credentials.password=supa|^|pazz\"</pre>\n</div>\n</div>\n<p>Kubernetes command:</p>\n<div class=\"highlight-default\">\n<div class=\"highlight\">\n<pre class=\"crayon-plain-tag\">$ helm install monitoring percona/pmm-server --set platform=kubernetes --version 2.7.0 --set \"credentials.password=supa|^|pazz\"</pre>\n</div>\n<h2>Installing the PMM Client</h2>\n<p>The following steps are needed for the PMM client installation:</p>\n<p>1. The PMM client installation is initiated by updating the <code class=\"docutils literal\"><span class=\"pre\">pmm</span></code> section in the <a target=\"_blank\" class=\"reference external\" href=\"https://github.com/percona/percona-server-mongodb-operator/blob/master/deploy/cr.yaml\">deploy/cr.yaml</a> file.</p>\n<ul>\n<li>Set <code class=\"docutils literal\"><span class=\"pre\">pmm.enabled=true</span></code>.</li>\n<li>Ensure the <code class=\"docutils literal\"><span class=\"pre\">serverHost</span></code> (the PMM service name is <code class=\"docutils literal\"><span class=\"pre\">monitoring-service</span></code> by default) is the same as the value specified for the <code class=\"docutils literal\"><span class=\"pre\">name</span></code> parameter on the previous step, but with an additional <code class=\"docutils literal\"><span class=\"pre\">-service</span></code> suffix.</li>\n<li>Make sure that PMM user name and password are correct.</li>\n</ul>\n<p>In case of the Operator for <strong>Percona Server for MongoDB</strong>:</p>\n<ul>\n<li><code class=\"docutils literal\"><span class=\"pre\">PMM_SERVER_USER</span></code> key in the <a target=\"_blank\" class=\"reference external\" href=\"https://github.com/percona/percona-server-mongodb-operator/blob/master/deploy/secrets.yaml\">deploy/secrets.yaml</a> secrets file should be a based64 encoded equivalent of the PMM Server user name (<code class=\"docutils literal\"><span class=\"pre\">pmm</span></code> by default for PMM 1.x and <code class=\"docutils literal\"><span class=\"pre\">admin</span></code> for PMM 2.x).</li>\n<li><code class=\"docutils literal\"><span class=\"pre\">PMM_SERVER_PASSWORD</span></code> key in the <a target=\"_blank\" class=\"reference external\" href=\"https://github.com/percona/percona-server-mongodb-operator/blob/master/deploy/secrets.yaml\">deploy/secrets.yaml</a> secrets file should be a base64 encoded equivalent of the value specified for the <code class=\"docutils literal\"><span class=\"pre\">credentials.password</span></code> parameter on the last PMM Server installation step.</li>\n</ul>\n<p>In case of the Operator for <strong>Percona XtraDB Cluster</strong>:</p>\n<ul>\n<li style=\"list-style-type: none;\">\n<ul>\n<li><code class=\"docutils literal\"><span class=\"pre\">serverUser</span></code> key in the <a target=\"_blank\" class=\"reference external\" href=\"https://github.com/percona/percona-server-mongodb-operator/blob/master/deploy/secrets.yaml\">deploy/cr.yaml</a> configuration file should match the PMM Server user name (<code class=\"docutils literal\"><span class=\"pre\">pmm</span></code> by default for PMM 1.x and <code class=\"docutils literal\"><span class=\"pre\">admin</span></code> for PMM 2.x).</li>\n<li><code class=\"docutils literal\"><span class=\"pre\">pmmserver</span></code> key in the <a target=\"_blank\" class=\"reference external\" href=\"https://github.com/percona/percona-server-mongodb-operator/blob/master/deploy/secrets.yaml\">deploy/secrets.yaml</a> secrets file is a base64 encoded equivalent of the value specified for the <code class=\"docutils literal\"><span class=\"pre\">credentials.password</span></code> parameter on the last PMM Server installation step.</li>\n</ul>\n</li>\n</ul>\n<p>Apply changes with the <code class=\"docutils literal\"><span class=\"pre\">kubectl</span> <span class=\"pre\">apply</span> <span class=\"pre\">-f</span> <span class=\"pre\">deploy/secrets.yaml</span></code> command, if needed.</p>\n<p>When done, apply the edited <code class=\"docutils literal\"><span class=\"pre\">deploy/cr.yaml</span></code> file:</p>\n<div class=\"highlight-default\">\n<div class=\"highlight\">\n<pre class=\"crayon-plain-tag\">$ kubectl apply -f deploy/cr.yaml</pre>\n</div>\n</div>\n<p>2. Check that correspondent Pods are not in a cycle of stopping and restarting. This cycle occurs if there are errors in the previous steps:</p>\n<div class=\"highlight-default\">\n<div class=\"highlight\">\n<pre class=\"crayon-plain-tag\">$ kubectl get pods\n$ kubectl logs my-cluster-name-rs0-0 -c pmm-client</pre>\n</div>\n</div>\n<p>3. Run the following command:</p><pre class=\"crayon-plain-tag\">$ kubectl get service/monitoring-service -o wide</pre><p>In the results, locate the the <code class=\"docutils literal\"><span class=\"pre\">EXTERNAL-IP</span></code> field. This external IP address can be used to access PMM via <em>https</em> in a web browser, with the login/password authentication, and the PMM Server is already configured to show either <a target=\"_blank\" class=\"reference external\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/index.metrics-monitor.dashboard.html#pmm-dashboard-mongodb-list\">Percona Server for MongoDB metrics</a> or <a target=\"_blank\" class=\"reference external\" href=\"https://www.percona.com/doc/percona-xtradb-cluster/LATEST/manual/monitoring.html#using-pmm\">Percona XtraDB Cluster metrics</a>, depending on the Operator with which these steps were applied.</p>\n</div>\n","descriptionType":"html","publishedDate":"Thu, 23 Jul 2020 17:00:47 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management.png","linkMd5":"cbb4d8d4080fc4948dad160632ee6ef0","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn23@2020_3/2020-07-25/1595696397611_2cd10c911a85cff9.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management.png","author":"Dmitriy Kostiuk","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn83@2020_4/2020-07-25/1595696399337_5ab1f1d659592a9f.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/Using-Percona-Kubernetes-Operators-with-Percona-Monitoring-and-Management-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_4/2020-07-25/1595696402755_4d2213b37044ab95.webp"}},{"createdTime":"2020-07-26 00:59:47","updatedTime":"2020-07-25 16:59:47","title":"MySQL Deadlocks Are Our Friends","link":"https://www.percona.com/blog/?p=69469","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL Deadlocks\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h4><img class=\"alignright size-medium wp-image-69716\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-300x157.png\" alt=\"MySQL Deadlocks\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Why another article on this, Marco?</h4>\n<p>MySQL deadlocks is a topic covered many times, including here at Percona. I suggest you review the reference section at the end of this post for articles on how to identify deadlocks and from where they are generated.</p>\n<p><em>So why another article?</em></p>\n<p>The answer is that messages we receive like the following are still very common:</p>\n<p style=\"padding-left: 40px;\">User (John): “Marco, our MySQL is having problems”<br />\nMarco: “Ok John what problems? Can you be a bit more specific?”<br />\nJohn: “Our log scraper is collecting that MySQL has a lot of errors”<br />\nMarco: “Ok can you share the MySQL log so I can review it?”<br />\nJohn: “Errors are in the application log, will share one application log”</p>\n<p style=\"padding-left: 40px;\">Marco reviews the log and in it he finds:</p>\n<p></p><pre class=\"crayon-plain-tag\">“ERROR 1213 (40001): Deadlock found when trying to get lock;\ntry restarting transaction”</pre><p>Marco&#8217;s reaction is: &#8220;Oh my &#8230;&#8221;</p>\n<p>Why? Because deadlocks are not what is expressed in the message, and of course, we have a problem of mindset and terminology. In this very short article, I will try to change your point of view around deadlocks.</p>\n<h2>What is a MySQL Deadlock?</h2>\n<p>A deadlock is a situation wherein two or more competing actions are waiting for the other to finish. As a consequence, neither ever does. In computer science, deadlock refers to a specific condition when two or more processes are each waiting for each other to release a resource.</p>\n<p>In order for a deadlock to happen, four conditions (Coffman conditions) should exist:</p>\n<p><strong>1. Mutual exclusion:</strong> At least one resource must be held in a non-shareable mode. Otherwise, the processes would not be prevented from using the resource when necessary. Only one process can use the resource at any given instant of time.</p>\n<p><strong>2. Hold and wait or resource holding:</strong> A process is currently holding at least one resource and requesting additional resources that are being held by other processes.</p>\n<p><strong>3. No preemption: </strong>A resource can be released only voluntarily by the process holding it.</p>\n<p><strong>4. Circular wait:</strong> Each process must be waiting for a resource that is being held by another process, which in turn is waiting for the first process to release the resource.</p>\n<p>All the above illustrates conditions that are not bound only to RDBMS but to any system dealing with data transaction processing. In any case, it is a fact that today, in most cases, deadlocks are not avoidable unless to prevent one of the above conditions from happening without compromising the system execution integrity. Breaking or ignoring one of the above rules, especially for RDBMS, could affect data integrity, which goes against the reason for an RDBMS to exist.</p>\n<p><strong>Just to help us to better contextualize, let us review a simple case of deadlock.</strong></p>\n<p>Say I have MySQL with the World schema loaded, and I have the TWO transactions running, both looking for the same two cities in Tuscany (Firenze and Prato) but in a different order.</p><pre class=\"crayon-plain-tag\">mysql&#62; select * from City where CountryCode = 'ITA' and District='Toscana';\n+------+---------+-------------+----------+------------+\n| ID   | Name    | CountryCode | District | Population |\n+------+---------+-------------+----------+------------+\n| 1471 | Firenze | ITA    | Toscana       | 376662     | &#60;---\n| 1483 | Prato   | ITA    | Toscana       |  172473    | &#60;--- ...\n+------+---------+-------------+----------+------------+\n\nAnd both transactions are updating the population: \n\nConnection 1 will have: connection1 &#62; start transaction;\nQuery OK, 0 rows affected (0.01 sec)\n\nconnection1 &#62; select * from City where ID=1471;\n+------+---------+-------------+----------+------------+\n| ID   | Name    | CountryCode | District | Population |\n+------+---------+-------------+----------+------------+\n| 1471 | Firenze | ITA         | Toscana  | 376662     |\n+------+---------+-------------+----------+------------+\n1 row in set (0.00 sec)\n\nconnection1 &#62; update City set Population=Population + 1 where ID = 1471;\nQuery OK, 1 row affected (0.05 sec)\nRows matched: 1 Changed: 1 Warnings: 0\n\nconnection1 &#62; update City set Population=Population + 1 where ID = 1483;\nQuery OK, 1 row affected (2.09 sec)\nRows matched: 1 Changed: 1 Warnings: 0\n\nConnection 2 will have:\nconnection 2 &#62;start transaction;\nQuery OK, 0 rows affected (0.01 sec)\n\nconnection 2 &#62;select * from City where ID=1483;\n+------+-------+-------------+----------+------------+\n| ID   | Name  | CountryCode | District | Population |\n+------+-------+-------------+----------+------------+\n| 1483 | Prato | ITA         | Toscana  | 172473     |\n+------+-------+-------------+----------+------------+\n1 row in set (0.01 sec)\n\nconnection 2 &#62;update City set Population=Population + 1 where ID = 1483;\nQuery OK, 1 row affected (0.00 sec)\nRows matched: 1 Changed: 1 Warnings: 0\n\nconnection 2 &#62;update City set Population=Population + 1 where ID = 1471;\nERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction</pre><p>This is a very simple example of deadlock detection. An image may help:</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1.png\"><img class=\"aligncenter wp-image-69474 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1.png\" alt=\"\" width=\"451\" height=\"231\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1.png 451w, https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1-300x154.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1-200x102.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1-367x188.png 367w\" sizes=\"(max-width: 451px) 100vw, 451px\" /></a></p>\n<p>If we stop for a second and ignore the word “<strong>ERROR</strong>” in the message, what is really happening is that MySQL is preventing us from modifying the data in the wrong way. If the locks would not be in place, one of the two transactions would modify the population, incrementing a number that is not valid anymore.</p>\n<h2>So, What&#8217;s the Right Thing to Do?</h2>\n<p>The right thing to do is to abort one of the two transactions and NOTIFY the application that, if you really need to perform the action, and in this case, increase the population, better to redo the execution and be sure it is still the case. Just think, it could happen that the application re-runs transaction two and identifies there is no need to increase the value because it is already what it is supposed to be.</p>\n<p>Think about this: you are calculating the financial situation of your company and you and your colleague are processing the same data but for different tasks. Without locks and deadlocks, you may end up corrupting each other&#8217;s interpretation of the data, and perform the wrong operations. As a result, you may end up paying the wrong salaries &#8211; or worse.</p>\n<p>Given that, and more, deadlocks (and locks) need to be seen as friends helping us in keeping our data consistent. The problems arise when we have applications poorly designed and developed, and unfortunately, the wrong terminology (in my opinion) in MySQL.</p>\n<h2>Deadlock Detection</h2>\n<p>Let us start with MySQL. &#8220;Deadlock detection&#8221; is detecting an intrinsic inevitable condition in the RDBMS/ACID world. As such, defining it as an ERROR is totally misleading. A deadlock is a CONDITION, and its natural conclusion is the abortion of one of the transactions by reason of the deadlock. The message should be a NOTIFICATION, not an ERROR.</p>\n<p>The problem in the apps, instead, is that normally the isolation and validation of the data are demanded to RDBMS, which is fine. But then only seldom can we see applications able to deal with messages like lock-timeout or deadlock. This is, of course, a huge pitfall, because while it is natural to have the RDBMS dealing with the data consistency, it is not, and should not, be responsible for the retry that is bound to the application logic.</p>\n<p>Nowadays we have a lot of applications that require very fast execution, and locks and deadlocks are seen as enemies because they have a time cost.</p>\n<p>But this is a mistake &#8211; a design mistake. Because if you are more willing to have speed instead of data consistency, then you should not use an RDBMS that must respect specific rules, at any (time) cost. Other systems to store data (eventually consistent) will be more appropriate in your case.</p>\n<p>If you care about your data, then you need to listen to your RDBMS and write the code in a way that you will get all the benefits out of it, including when it comes to deadlocks.</p>\n<h3>Conclusion</h3>\n<p><strong>MySQL deadlocks (and locks) should be seen as friends.</strong> They are mechanisms that exist to keep our data consistent. We should not bypass them unless willing to compromise our data. As previously indicated, if you want to understand, in detail, how to diagnose a deadlock, please review the links in the references below.</p>\n<p><strong>References</strong></p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/2012/09/19/logging-deadlocks-errors/\">Logging Deadlock Errors</a></p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/blog/2014/10/28/how-to-deal-with-mysql-deadlocks/\">How to Deal With MySQL Deadlocks</a></p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/community-blog/2018/09/24/minimize-mysql-deadlocks-3-steps/\">Minimize MySQL Deadlocks with 3 Steps</a></p>\n<hr />\n<p>Our solution brief &#8220;Get Up and Running with Percona Server for MySQL&#8221; outlines setting up a MySQL® database on-premises using Percona Server for MySQL. It includes failover and basic business continuity components.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/solution-brief/get-and-running-percona-server-mysql?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=mysqldeadlocks&#38;utm_content=solutionbrief\" rel=\"noopener\">Download PDF</a></p>\n","descriptionType":"html","publishedDate":"Tue, 07 Jul 2020 18:09:25 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks.png","linkMd5":"bdc4cd69aebbd9f710860a38bfba217b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_4/2020-07-25/1595696397996_155ddc812665b727.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks.png","author":"Marco Tusa","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_1/2020-07-25/1595696399985_8caac0f1b7506999.webp","https://www.percona.com/blog/wp-content/uploads/2020/07/MySQL-Deadlocks-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_2/2020-07-25/1595696400360_a0c15d6eac03db81.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/dl_ff_1.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn3@2020_2/2020-07-25/1595696402968_06f168b161302eb0.webp"}},{"createdTime":"2020-07-26 00:59:51","updatedTime":"2020-07-25 16:59:51","title":"Are You The Next Great Percona TAM?","link":"https://www.percona.com/blog/?p=69348","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona TAM\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img class=\"alignright size-medium wp-image-69501\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-300x157.png\" alt=\"Percona TAM\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />In 2006, a small company formed, specializing in MySQL database consulting at a time when very few companies had fully embraced open source. This small company quickly grew, riding the wave of open source adoption within the greater enterprise market and became the Percona we know and love today.</p>\n<p>My first awareness of Percona came at a time when I was part of a large enterprise MySQL DBA team. It seemed that suddenly all of my web searches for information were returning blog posts written by Percona employees, and without fail these posts always had the correct information I needed to resolve my current issue. This pattern repeated over the years until finally, I decided it was time for me to apply for a position at Percona, as it was clearly apparent to me that the best minds in the MySQL world worked here.</p>\n<p>After an extremely thorough interview process, I joined Percona as a member of the TAM (Technical Account Manager) team in 2016. As Percona is almost always hiring for various positions, I wanted to detail my experience in working here as a TAM, my thoughts on the company as a whole, and why <a target=\"_blank\" href=\"https://www.percona.com/about-percona/careers/technical-advisor-open-source-database-solutions\"><strong>YOU should submit an application</strong></a> to work for Percona.</p>\n<h2>The TAM Role</h2>\n<p>As a member of the TAM team, I am responsible for a subset of clients, advocating for them and meeting regularly to discuss both technical and business issues. As with all worthwhile work, some days are tougher than others, but overall there is a great satisfaction to be gained from putting all the pieces together and helping a client achieve something that they initially thought was impossible, or resolving that critical outage quickly and hearing the sighs of relief on the other end of the conference line.</p>\n<h2>The Company</h2>\n<p>Percona is unique in that it is one of the few companies with the majority of the workforce working remotely, and has done so since the very beginning. As such, Percona has a vast amount of experience with remote collaboration and has arranged the entire company culture around this.</p>\n<p>Video meetings keep everyone connected internally and help put faces to those names you work with on a routine basis. Occasional onsite meetings and conference meetups allow some real-world interaction as well, so we’re not always trapped behind our webcams.</p>\n<p>The best part of Percona, however, is the employees themselves. I have never worked for a company with so many incredibly bright minds gathered in a single place. If you have an interest in open source technologies and want to rub shoulders with some of the best talents in the industry, you’ll likely find those people virtually strolling the halls of Percona.</p>\n<h2>We Want You!</h2>\n<p>As mentioned, Percona is almost always looking for great talent, and always looking for TAM&#8217;s to increase our team capacity, in both the EMEA and Americas regions. While in many companies the TAM role is not very technical, this is not true of the TAM role at Percona. A Percona TAM generally comes from a DBA or consulting background, and this allows a depth of knowledge not typically seen in this role. A great Percona TAM will exhibit the following qualities:</p>\n<h3>Expertise</h3>\n<p>Percona TAMs are often SMEs (Subject Matter Experts) in many areas of database administration. Percona TAMs come from a professional DBA (or consulting) background, with experience managing large and complex database installations.</p>\n<h3>Efficiency</h3>\n<p>As a TAM, you will be involved with a customer’s planning/testing both before and after any major changes, such as database upgrades, architecture and design, and project planning. This work often includes documenting the procedures that are followed during planned events, reviewing active support tickets, and analyzing customer environments.</p>\n<h3>Continuous Learning</h3>\n<p>TAMs should review the latest white papers, blogs, and newsletters to stay up to date on various database technologies, including MySQL, MongoDB, and PostgreSQL. Our clients need the latest information to stay current, compliant, and secure.</p>\n<p>TAMs are also exposed to all of Percona’s products and services with a unique internal insight. This enables us to offer expert advice to our customers to take full advantage of our products and services to optimize their database infrastructure.</p>\n<h3>What Else?</h3>\n<p>There is, of course, much more to the TAM role than just these areas. Some of the other regular duties a Percona TAM might perform are:</p>\n<ul>\n<li>Hosting regular meetings with Percona clients to better understand the business/technical environment, infrastructure, goals, and challenges.</li>\n<li>Taking ownership of support tickets and retaining that ownership through to a successful resolution.</li>\n<li>Providing an internal escalation point for client support and consulting projects.</li>\n<li>Perform implementation reviews, discuss new product features, and ensure prompt and proper resolution of technical challenges.</li>\n<li>Staying current with any ongoing and upcoming client projects and initiatives.</li>\n</ul>\n<p>TAMs are equipped to use all this information to assist customers with proactive solutions and best practices for recommended changes to production. As a Percona TAM, our commitment is to be honest, empathetic, and exceed expectations, whether a client is using a Percona product or not.</p>\n<h3>Up For The Challenge?</h3>\n<p>So do YOU have what it takes to become the next great Percona TAM? <a target=\"_blank\" href=\"https://www.percona.com/about-percona/careers/technical-advisor-open-source-database-solutions\">Apply today, and find out!</a></p>\n","descriptionType":"html","publishedDate":"Mon, 29 Jun 2020 14:52:41 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM.png","linkMd5":"8bb5ca18b9c60c4bcd4443269244befd","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_1/2020-07-25/1595696397957_81afbddc8d2145cf.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM.png","author":"Brian Sumpter","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_2/2020-07-25/1595696403189_9e760305b6261e90.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Percona-TAM-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_5/2020-07-25/1595696405135_7a618b8d876be26f.webp"}},{"createdTime":"2020-07-26 00:59:44","updatedTime":"2020-07-25 16:59:44","title":"New Percona Distribution for MySQL, Improvements to Percona Monitoring and Management: Release Roundup July 6, 2020","link":"https://www.percona.com/blog/?p=69245","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Release Roundup July 6, 2020\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020.png 712w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2><img class=\"alignright size-medium wp-image-69325\" src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-300x169.png\" alt=\"Release Roundup July 6, 2020\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020.png 712w\" sizes=\"(max-width: 300px) 100vw, 300px\" />It&#8217;s release roundup time here at Percona!</h2>\n<p>Our Release Roundups <span class=\"s1\">showcase the latest software updates, tools, and features to help you manage and deploy our software, with</span> highlights and critical information, as well as links to the full release notes and direct links to the software or service itself.</p>\n<p>Today&#8217;s post includes those releases and updates that have come out since June 22, 2020, including the new Percona Distribution for MySQL, new features for Percona Server for MongoDB 4.0.19-12, and improvements and bug fixes for Percona Monitoring and Management.</p>\n<p>&#160;</p>\n<h2>Percona Distribution for MySQL 8.0.19</h2>\n<p>On June 22, 2020, we released <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-distribution-mysql/8.0/release-notes-v8.0.19.html\">Percona Distribution for MySQL 8.0.19</a>, a single solution with the best and most critical enterprise components from the MySQL open source community, designed and tested to work together. This distribution delivers the key components you need to implement any MySQL environment, from a single server through to primary/secondary, group replication enabled, and full high availability.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/software/mysql-database\">Download Percona Distribution for MySQL 8.0.19</a></p>\n<p>&#160;</p>\n<h2>Percona Server for MongoDB 4.0.19-12</h2>\n<p><a target=\"_blank\" href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.0/release_notes/4.0.19-12.html\">Percona Server for MongoDB 4.0.19-12</a> was released on June 24, 2020. It is an enhanced, open source, and highly-scalable database that is a fully-compatible, drop-in replacement for MongoDB 4.0.19 Community Edition, supporting MongoDB 4.0.19 protocols and drivers. New features in this release include options to control the size of several WiredTiger hash arrays concerned with data handles and one for session cursor cache, as well as the addition of Kerberos authentication to Percona Server for MongoDB.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/software/mongodb/percona-server-for-mongodb\">Download Percona Server for MongoDB 4.0.19-12</a></p>\n<p>&#160;</p>\n<h2>Percona XtraDB Cluster 5.7.30-31.43</h2>\n<p>On June 25, 2020,  <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-xtradb-cluster/5.7/release-notes/Percona-XtraDB-Cluster-5.7.30-31.43.html\">Percona XtraDB Cluster 5.7.30-31.43</a> was released. It is a free, open source, enterprise-grade solution includes the high availability and security features your business needs to meet customer expectations and business goals. In this release, several bugs were fixed, including when thread pooling could hang on shutdown as well as allowing COM_FIELD_LIST to be executed when WSREP was not ready.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/software/mysql-database/percona-xtradb-cluster\">Download Percona XtraDB Cluster 5.7.30-31.43</a></p>\n<p>&#160;</p>\n<h2>Percona Monitoring and Management 2.8.0</h2>\n<p>June 25, 2020, saw the release of <a target=\"_blank\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/release-notes/2.8.0.html\">Percona Monitoring and Management 2.8.0</a>, a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. This release features several improvements, such as Agents, Services and Nodes can now be removed via the ‘PMM Inventory’ page, and user-installed Grafana plugins are unaffected by PMM upgrade. In addition, there are some bug fixes, including when PMM 2.7.0 was inoperable when no Internet connectivity, missing data in MongoDB Cluster summary, RocksDB, and MMAPv1 details, and an incorrect metric used in formula for “Top Users by Rows Fetched/Read” graph.</p>\n<p><a target=\"_blank\" href=\"https://www.percona.com/downloads/pmm2/\">Download Percona Monitoring and Management 2.8.0</a></p>\n<p>&#160;</p>\n<p>That&#8217;s it for this roundup, and be sure to <a target=\"_blank\" href=\"https://twitter.com/Percona\" target=\"_blank\" rel=\"&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;&#34;noopener&#34; noopener noreferrer\">follow us on Twitter</a> to stay up-to-date on the most recent releases! Percona is a leader in providing best-of-breed enterprise-class support, consulting, managed services, training, and software for MySQL, MariaDB, MongoDB, PostgreSQL, and other open source databases in on-premises and cloud environments.</p>\n<hr />\n<p>We understand that choosing open source software for your business can be a potential minefield. You need to select the best available options, which fully support and adapt to your changing needs. Choosing the right open source software can allow you access to enterprise-level features, without the associated costs.</p>\n<p>In our white paper, we discuss the key features that make open source software attractive, and why Percona&#8217;s software might be the best option for your business.</p>\n<p style=\"text-align: center;\"><a target=\"_blank\" class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/resources/white-papers/when-percona-software-right-choice?utm_source=blog&#38;utm_medium=download&#38;utm_campaign=roundup&#38;utm_content=whitepaper\" rel=\"noopener\">Download: When is Percona Software the Right Choice?</a></p>\n","descriptionType":"html","publishedDate":"Mon, 06 Jul 2020 14:03:31 +0000","feedId":11,"bgimg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020.png","linkMd5":"794e8e602ac8bc8aa3afe73b3b72fa29","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_1/2020-07-25/1595696397870_9fd3a5ced02fc7b4.webp","metaImg":"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020.png","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn47@2020_2/2020-07-25/1595696403561_0e4d38b88c55f418.webp","https://www.percona.com/blog/wp-content/uploads/2020/06/Release-Roundup-July-6-2020-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn59@2020_5/2020-07-25/1595696403671_62dcac767fb34faa.webp"}}],"record":{"createdTime":"2020-07-26 00:59:54","updatedTime":"2020-07-25 16:59:54","feedId":11,"fetchDate":"Sat, 25 Jul 2020 16:59:54 +0000","fetchMs":786,"handleMs":16054,"totalMs":31414,"newArticles":0,"totalArticles":40,"status":1,"type":0,"ip":"52.214.214.9","hostName":"europe-020.herokuapp.com","requestId":"60d430c9185c48c0b864cfc10bb080eb_11"},"extra4":{"start":1595696377997,"total":0,"statList":[{"spend":786,"msg":"获取xml内容"},{"spend":16054,"msg":"解释文章"},{"spend":3536,"msg":"上传封面图到cdn"},{"spend":10613,"msg":"正文链接上传到cdn"},{"spend":9373,"msg":"修正封面图上传失败重新上传"}]},"extra5":135,"extra6":135}