{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"State-of-the-art algorithm accelerates path for quantum computers to address climate change","link":"https://www.microsoft.com/en-us/research/?p=678456","description":"\n<p>While there has been a focus in the quantum computing industry on growing the number of qubits in a quantum computer, the reality is there are many important factors when building an overall system to bring quantum solutions to fruition. Hardware scaling, temperature control, software optimizations, and many other considerations must be reimagined in ways that allow large-scale quantum computers to do the necessary, meaningful work to solve some of today’s and tomorrow’s biggest problems. A question emerges that is both scientific and philosophical in nature: once a quantum computer scales to handle problems that classical computers cannot, what problems should we solve on it? Quantum researchers at Microsoft are not only thinking about this question—we are producing tangible results that will shape how large-scale quantum computer applications will accomplish these tasks.</p>\n\n\n\n<p>We have begun creating quantum computer applications in chemistry, and they could help to address one of the world’s biggest challenges to date: climate change. In January, Microsoft launched a bold new environmental sustainability initiative focusing on carbon, water, waste, and biodiversity, announcing one of the most ambitious carbon commitments put forward by any company: Microsoft will be carbon negative by 2030 and remove from the environment more carbon than we have emitted since our founding by 2050. Last week, we announced seven important new steps on our path to be carbon negative by 2030. Learn more on the <a href=\"https://blogs.microsoft.com/on-the-issues/2020/07/21/carbon-negative-transform-to-net-zero/\">Microsoft on the Issues blog</a>. </p>\n\n\n\n<p class=\"has-text-align-left\">Microsoft has prioritized making an impact on this global issue, and Microsoft Quantum researchers have teamed up with researchers at <a href=\"https://ethz.ch/en.html\">ETH Zurich</a> to develop a new quantum algorithm to simulate catalytic processes. In the context of climate change, one goal will be to find an efficient catalyst for carbon fixation—a process that reduces carbon dioxide by turning it into valuable chemicals. One of our key findings is that the resource requirements to implement our algorithm on a fault-tolerant quantum computer are more than 10 times lower than recent state-of-the-art algorithms. These improvements significantly decrease the time it will take a quantum computer to do extremely challenging computations in this area of chemistry. In our research, we have not only improved upon quantum algorithms and have shown how they can help effectively find new catalysts, we have also learned more about other quantum resources that are necessary to perform these calculations at an exponentially faster rate than classical computers. These learnings include the size of quantum computers and their runtime—and more generally how to better co-design a hybrid quantum-classical computing system to handle this type of problem. Our research is detailed in a paper called “<a href=\"https://www.microsoft.com/en-us/research/publication/quantum-computing-enhanced-computational-catalysis/\">Quantum computing enhanced computational catalysis</a>.” </p>\n\n\n\n<h3>Carbon fixation: An opportunity in chemistry opens the door for a new application in quantum computing</h3>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-1024x564.jpg\" alt=\"\" class=\"wp-image-678549\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-1024x564.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-300x165.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-768x423.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1.jpg 1158w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption><em>Figure 1: In the catalytic cycle studied in our paper, a Ruthenium-based catalyst reacts with carbon dioxide and hydrogen molecules to produce water and methanol, leaving the catalyst unchanged to react with another carbon dioxide molecule.</em></figcaption></figure></div>\n\n\n\n<p>Synthetic carbon fixation is a process that has potential to help greatly reduce carbon dioxide in the atmosphere by converting CO<sub>2</sub> into other useful chemical compounds. Carbon fixation is not a new process. In fact, it is a very old one. Plants use a form of carbon fixation to convert carbon dioxide into energy-rich molecules such as glucose. But glucose isn’t the only possible biproduct of carbon fixation. When using different <a href=\"https://ahdictionary.com/word/search.html?q=catalyst\">catalysts</a>, natural or synthetic, carbon dioxide can be converted into other compounds.</p>\n\n\n\n<p>Currently, synthetic catalytic processes are found through lengthy trial-and-error lab experiments. In a process that requires testing thousands of molecular combinations, computer simulations that very accurately model quantum correlations could replace complex synthesis of new candidate catalysts. Whereas computers today can have a difficult time accurately calculating properties of complex molecules, quantum computers are especially suited for this task and will give more reliable and predictive simulation results. We hope that quantum computers will complement traditional methods and, together, could reveal a process that both removes carbon dioxide from the atmosphere and provides valuable chemicals in return.</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Why begin with a known catalytic reaction if the goal is to find new ones?</strong></td></tr></tbody></table><figcaption>It’s important to first look at how these catalytic processes work in order to find ways they can be improved upon, especially exploring where quantum computers can make computational catalysis, a method to simulate catalysts already being performed on classical computers, more effective, more accurate, and less time-consuming.</figcaption></figure>\n\n\n\n<p>In order to better understand how quantum computer algorithms can assist in discovering new, more efficient catalysts, we decided to focus our analysis on a previously published catalytic process based on the transition metal Ruthenium to convert carbon dioxide into methanol. It is also—like all known catalysts resulting in methanol to date—extremely inefficient. This inefficiency offers an opportunity for finding catalytic reactions that are more scalable. Using this reaction as a foundation for testing our algorithm, we were able to gain knowledge about how to best optimize algorithms for simulating these types of reactions on a quantum computer (see Figure 1 above).</p>\n\n\n\n<h3>Our algorithmic advancement: Boosting computational efficiency through compression</h3>\n\n\n\n<p>We need to develop more efficient algorithms for quantum computers because problems that involve calculating molecular energies with high precision, such as for catalytic processes, will be resource intensive—even on quantum computers. </p>\n\n\n\n<div class=\"annotations\" data-bi-area=\"margin-callout\">\n\t<ul class=\"annotations__list annotations__list--right\">\n\t\t<li class=\"annotations__list-item\">\n\t\t\t\t\t\t<span class=\"annotations__type\">ENGAGE</span>\n\t\t\t<a href=\"https://www.microsoft.com/en-us/quantum/development-kit\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Quantum Development Kit \">\n\t\t\t\tQuantum Development Kit \t\t\t</a>\n\t\t\t<span class=\"svg-icon icon-external-link\"></span>\n\t\t\t\t\t\t\t<span class=\"annotations__caption\">Are you a researcher or developer who wants to help in discovering new algorithms for quantum computers? Check out the Quantum Development Kit and Q#, a toolkit and high-level programming language for developing quantum algorithms, which allow you to try out a small chemical algorithm for yourself. </span>\n\t\t\t\t\t</li>\n\t</ul>\n</div>\n\n\n\n<p>Obtaining high-precision energy estimates requires simulating the molecule’s quantum state for a long period of time, which is split into multiple smaller time steps. All the interaction terms in the problem description, the so-called Hamiltonian, need to be loaded over and over again at every single time step since quantum information cannot be copied. The natural approach to reduce overall runtime is then to reduce both the information that needs to be loaded as well as the number of time steps required for the simulation. One promising approach is to use a so-called “double-factorized” representation of the Hamiltonian. In this representation, the information describing the interaction between electrons is compressed into fewer terms.</p>\n\n\n\n<p>In our research, we precisely achieve this runtime reduction by developing a new, efficient quantum algorithm. Our algorithm exploits the improved compression properties of the double-factorized form, and it also manages to perform the simulation with significantly larger step sizes compared to prior state of the art that exploits the <a href=\"https://docs.microsoft.com/en-us/quantum/user-guide/libraries/chemistry/concepts/second-quantization#second-quantized-fermionic-hamiltonian\">unfactorized </a>or single-factorized forms of the Hamiltonian. The extent of our improvement for molecules such as Ruthenium catalysts is driven primarily by the larger time step size, as illustrated in the table below. Moreover, aggressive compression can further reduce the number of terms at the cost of accuracy in the simulation. Importantly, our use of a so-called “qubitization” simulation algorithm allows for good control over the target accuracy. Combined, these factors reduce runtime by orders of magnitude for obtaining reliable results.</p>\n\n\n\n<figure class=\"wp-block-table\"><table><thead><tr><th class=\"has-text-align-center\" data-align=\"center\"></th><th class=\"has-text-align-center\" data-align=\"center\"><strong>Ruthenium catalyst configuration</strong><br><strong>VIII with 130 spin-orbitals</strong></th><th class=\"has-text-align-center\" data-align=\"center\"></th></tr></thead><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Approach</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Number of steps per unit <br>of time evolution</strong></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Overall algorithmic speedup</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Unfactorized</td><td class=\"has-text-align-center\" data-align=\"center\">10,600</td><td class=\"has-text-align-center\" data-align=\"center\">1.0x</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Single-factorized</td><td class=\"has-text-align-center\" data-align=\"center\">42,200</td><td class=\"has-text-align-center\" data-align=\"center\">0.4x</td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">Our results</td><td class=\"has-text-align-center\" data-align=\"center\">570</td><td class=\"has-text-align-center\" data-align=\"center\">18.9x</td></tr></tbody></table></figure>\n\n\n\n<h3>Designing quantum computers for the hunt for new catalysts</h3>\n\n\n\n<p>The computational design of catalysts relies on very accurate energy calculations. Quantum computers can avoid uncontrolled approximations of classical simulations. They scale much better and open opportunities to assess the energetics of chemical species with sufficient accuracy. By using the Hamiltonian parameters generated on classical computers, a quantum computer could solve the exact energies of the chemical systems and help profile a quantitatively accurate landscape of reaction pathways. Catalyst structures could then be further refined or modified through the insights generated by reaction kinetics analysis. Such a process could iterate until a desired catalyst is found.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2-1024x518.jpg\" alt=\"\" class=\"wp-image-678552\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2-1024x518.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2-300x152.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2-768x388.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2.jpg 1329w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption><em>Figure 2: Protocol of quantum computing enhanced computational catalysis workflow. The energies of all species in the catalytic reaction cycle can be evaluated through the quantum computer using the output parameters of classical computers (upper right). The kinetics analysis (bottom left) can then be performed on the whole reaction pathways and new insights on the catalyst structures can be generated. This process repeats until an ideal catalyst structure is found.</em></figcaption></figure></div>\n\n\n\n<p>Our <a href=\"https://www.microsoft.com/en-us/research/publication/quantum-computing-enhanced-computational-catalysis/\">paper </a>is the first to show analysis of a quantum algorithm being done on a specific chemical reaction along the entire reaction pathway. Instead of just a single configuration, we analyzed relevant configurations of the reactants along this pathway. In addition, we performed state-of-the-art classical calculations, but our results with these confirmed that they lack reliability for truly predictive computational catalysis. Thus, one of the first roles for quantum computers will be not only to provide accurate results for novel catalysts, but also to benchmark validity of various classical approximations and develop better classical simulation methods.</p>\n\n\n\n<p>Beyond this, we want to further optimize quantum algorithms to enable the simulation of larger numbers of electrons. Current algorithms limit the accurate quantum computation to so-called active spaces of the most correlated electrons. While that may often be sufficiently accurate, we will not know unless we can simulate larger active spaces or ideally all electrons in a molecule.</p>\n\n\n\n<p>Finally, with the estimates for gate counts calculated, we were able to translate this information into potential runtime estimates for quantum computation on this problem. Depending on the assumptions made about future quantum computers, we estimate that it may take anywhere from a little over a day to several years to perform such calculations. This clearly shows the need not only for fast algorithms but also fast and scalable quantum hardware.</p>\n\n\n\n<p>Our newer, faster quantum algorithm for calculating molecular energy levels is itself an exciting development and a crucial step in the computational catalysis workflow (see Figure 2 above), but it will take more than that to find an efficient catalyst. In fact, knowing more about the quantum algorithms needed to undertake improved computational catalysis opens the door to even more questions about the scale of quantum computers. What is the amount of memory we need to run these algorithms at a meaningful speed? What does this imply for the needed hybrid workflow and quantum architecture it runs on to successfully find these catalysts? Our results after testing this algorithm reveal some important discoveries going forward.</p>\n\n\n\n<h3>Where do quantum computers and chemistry applications go from here?</h3>\n\n\n\n<p>The research presented in this post is evidence that rapid advances in quantum computing are happening now—our algorithm is 10,000 times faster than <a href=\"https://dx.doi.org/10.1073/pnas.1619152114\">the one we created just three years ago</a>. By gaining more insight into how quantum computers can improve computational catalysis, including ways that will help to address climate change while creating other benefits, we hope to spur new ideas and developments on the road to creating some of the first applications for large-scale quantum computers of the future. The advancements in algorithms and knowledge gained from our research are a springboard for future work, including exploring additional ways algorithms can be made even more effective. Given the promise and potential that quantum computing represents for tackling the toughest challenges in chemistry, we hope to work alongside the chemistry community to better understand how quantum computers can be best utilized to further develop new chemical processes, molecules, and, eventually, materials.</p>\n\n\n\n<p>We are encouraging those who are interested in exploring how chemistry can be impacted by quantum computing to explore <a href=\"https://azure.microsoft.com/en-us/services/quantum/#features\">Azure Quantum</a>, which comprises a full set of tools, ranging from the <a href=\"https://www.microsoft.com/en-us/quantum/development-kit\">Quantum Development Kit</a> (QDK) and the Q# programming language for quantum to simulators and resource estimators. The QDK allows researchers to develop and test new quantum algorithms for chemistry, run small examples on a simulator, use Azure Quantum on quantum hardware, and estimate resource requirements to run simulations at scale on future quantum computers.</p>\n\n\n\n<p>As part of the QDK, we developed a <a href=\"https://cloudblogs.microsoft.com/quantum/2018/12/04/simulating-nature-with-the-new-microsoft-quantum-development-kit-chemistry-library/\">Q# chemistry library</a>, with our partner Pacific Northwest National Laboratories (PNNL), that provides several fundamental data structures and tools to explore quantum algorithms for chemistry. If you are looking to get started with the QDK and Q#, check out our <a href=\"https://docs.microsoft.com/en-us/users/buildcollections2020-6557/collections/1o2iogrmn8x4r\">Microsoft Learn modules</a> released at Microsoft Build 2020.</p>\n\n\n\n<p></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/state-of-the-art-algorithm-accelerates-path-for-quantum-computers-to-address-climate-change/\">State-of-the-art algorithm accelerates path for quantum computers to address climate change</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Thu, 30 Jul 2020 16:01:47 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-1024x564.jpg","linkMd5":"dd61ab70c47710f9a8ebc4b9d75ca1c8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn14@2020_4/2020/08/24/21-39-24-004_8dbed831dff23127.webp","destWidth":1024,"destHeight":564,"sourceBytes":41620,"destBytes":25204,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-1024x564.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn14@2020_4/2020/08/24/21-39-24-004_8dbed831dff23127.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2-1024x518.jpg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn71@2020_6/2020/08/24/21-40-53-038_a8a20defada77229.webp"},"publishedOrCreatedDate":1598305163712},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"Researchers use a strand-displacing DNA polymerase to do biocomputing","link":"https://www.microsoft.com/en-us/research/?p=676782","description":"\n<figure class=\"wp-block-image alignwide\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_No_Logo_DNA.gif\" alt=\"\" class=\"wp-image-676824\"/></figure>\n\n\n\n<p>All around us, biochemical systems are regulating a variety of natural processes, from our body’s ability to protect our skin through the timely production of melanin to plants’ ability to convert carbon dioxide into carbohydrates and oxygen using sunlight. Replicating and programming such complex systems—essentially creating computing networks capable of operating in biological environments—offers a unique opportunity to go where traditional silicon-based computers can’t. For example, with synthetic biocompatible controllers, we’re looking at the potential for targeted medical therapies. Think cancer treatments that attack only dangerous cells, sparing healthy ones, or capsules that delivery drugs or antibodies at opportune times.</p>\n\n\n\n<p>Building these systems requires programming chemicals, and synthetic DNA is an ideal raw material to work with. Highly programmable, like transistors in silicon technology, and biocompatible, DNA can be used to implement chemical reaction networks (CRNs), a programming language for representing chemistry and biological processes, or the algorithmic and logical functions in traditional computing terms. Existing architectures implementing CRNs via DNA come in two varieties: DNA-only systems and multienzyme DNA systems. Researchers have been using DNA-only systems to build interesting digital logic circuits, such as <a href=\"https://science.sciencemag.org/content/332/6034/1196\">circuits that can compute the square root of four-bit numbers</a> and others that can <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\">classify handwritten digits</a> by implementing pretrained neural networks. But both DNA-only and multienzyme DNA architectures have drawbacks, mainly slow rates and leaky reactions for DNA-only systems and increased biological complexity that can restrict environmental conditions for multienzyme systems. These challenges can limit the size of the systems, as well as their introduction into biological environments.</p>\n\n\n\n<p>We propose a promising solution with a novel method for implementing chemical reaction networks that incorporates one enzyme. The idea is to replace the most fundamental unit of DNA computing, namely, toehold mediated strand displacement (TMSD), a DNA-only architecture, with polymerase-based strand displacement (PSD). Using DNA polymerase-based systems has several benefits: The polymerase enzyme gives an external energy source to the system, which is usually required; can synthesize new DNA strands, unlike enzyme-free systems; and can be potentially really fast. We present polymerase-based strand displacement in the paper “<a href=\"https://www.microsoft.com/en-us/research/publication/using-strand-displacing-polymerase-to-program-chemical-reaction-networks/\">Using Strand Displacing Polymerase to Program Chemical Reaction Networks</a>,” which was published in the Journal of the American Chemical Society.</p>\n\n\n\n<h3>Toehold mediated strand displacement vs. polymerase-based strand displacement</h3>\n\n\n<p>In 2006, researchers from the California Institute of Technology introduced <a href=\"https://science.sciencemag.org/content/314/5805/1585\">enzyme-free logic computing</a>, which has since become the go-to approach for programming chemical reaction networks among computer scientists. Because of its simplicity and tunability, TMSD in particular has become one of the most fundamental DNA computing architectures. TMSD and PSD are similar in design, though. But first, a few things about DNA.</p>\n<p>DNA strands are composed of sequences of the chemical bases adenine (A), guanine (G), cytosine (C), and thymine (T); DNA strands combine to form complexes, with A joining to its complementary base T and C to its complementary base G. Usually, an abstract representation of DNA strands is used to make it easier to design complicated architectures, which can have multiple DNA strands, each with sequences numbering in the tens or hundreds. So, for example, a portion of a single-stranded DNA with the sequence TACGTATGAATCAG might be referred to as <em>domain</em> <em>o </em>(Figure 1a). The reverse complement of that sequence would be ATGCATACTTAGTC, or more simply <em>domain o*</em>. This way, we don’t need to worry about actual sequences but can operate abstractly at the domain level. Sequences on a DNA strand can also be split into different parts, each with a separate domain name. In the <em>domain o</em> example, the sequence could be split after the first G: TACG TATGAATCAG. The first portion of the sequence could be <em>domain t</em> and the second <em>domain o</em>; the overall strand is <em>t</em> o.</p>\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-1_Polymerase-based-stand-displacement.png\" alt=\"A multipart figure showing (a) three representations of DNA; (b) the process of toehold mediated strand displacement (TMSD); and (c) the process of polymerase-based strand displacement (PSD). The strands of DNA for each process are represented by arrows and labeled with domain names t*, o*, t, and o. Input DNA strands—t* o* in the case of TMSD and t* in the case of PSD—bind with an exposed single-stranded portion of a double-stranded DNA complex t o. In TMSD, the output is displaced in a tug of war; in PSD, the output is displaced when the polymerase enzyme elongates the input strand. \" class=\"wp-image-677013\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-1_Polymerase-based-stand-displacement.png 979w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-1_Polymerase-based-stand-displacement-300x208.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-1_Polymerase-based-stand-displacement-768x533.png 768w\" sizes=\"(max-width: 979px) 100vw, 979px\" /><figcaption>Figure 1: Highly programmable and biocompatible, synthetic DNA is an ideal raw material for implementing chemical reaction networks. In Section (a) in the above figure, DNA is represented in several ways: as a double helix; as a sequence of its chemical bases joined with a complementary sequence; and as a more abstract representation in which the letters <em>o </em>and <em>o* </em>stand in for the specific sequences. To make it easier to design complicated architectures, the researchers use the latter representation. Sections (b) and (c) illustrate toehold mediated strand displacement (TMSD) and polymerase-based strand displacement (PSD), respectively. Polymerase-based strand displacement incorporates one enzyme, polymerase, represented by the red oval.</figcaption></figure></div>\n\n\n\n<p>In TMSD, an input DNA strand consisting of complementary domains—let’s call the strand <em>t*</em> <em>o*</em>—binds with an exposed single-stranded, or <em>toehold</em>, portion of a double-stranded DNA complex <em>t</em> <em>o </em>(Figure 1b). This binding displaces the output strand in a “tug of war” between the duplicate portion of the input strand—the <em>o*</em> domain—and the <em>o*</em> domain that already exists in the complex. In PSD, a shorter input DNA strand without the <em>o*</em> domain attaches to the exposed portion of the complex; once there—and in our design, it’s a permanent bind—the polymerase enzyme elongates the input strand by printing new DNA, a <em>t* o*</em> copy, displacing the output in the double-stranded complex (Figure 1c).</p>\n\n\n\n<p>Though similar in design, the addition of just one enzyme, polymerase, offers promising benefits. TMSD challenges include leaky reactions, which require workarounds that result in slower computation and/or greater system complexity. One reason for leaks is the presence of the output domain in the input, creating an overlap between the signals that increases the potential for error. In PSD, there is no overlap. The fact that there’s no longer a tug of war between strands, a time-consuming process, also contributes to faster computation times. Additionally, polymerase provides an external energy source, which allows for more complex computation within a reasonable time frame. For example, in 24 hours, DNA-only architectures may only be able to complete three layers of circuits; the faster computation offered by PSD can potentially complete 20 layers in the same period. This external energy source comes from the hydrolysis of the A, G, C, and T bases and the sugar and phosphate molecules that accompany them, collectively known as nucleoside triphosphates (NTPs). For example, it takes DNA-only architectures 10 hours to compute the square root function of a four-bit input; <a href=\"https://www.nature.com/articles/s41565-019-0544-5?proof=trueIn\">the faster computation offered by PSD can complete the same function within 40 minutes</a>. With PSD, since external NTPs and polymerase are used, several hundreds of bases can be displaced even if the toehold is short.<br></p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><a href=\"https://people.duke.edu/~sns37/assets/dna25_polymeraseCRN.pdf\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA-1024x524.jpg\" alt=\"\" class=\"wp-image-677835\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA-1024x524.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA-300x154.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA-768x393.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA.jpg 1367w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a><figcaption>Figure 2: In chemistry, unimolecular and bimolecular CRNs are considered the two most fundamental gates. On the left are a typical unimolecular reaction and a typical bimolecular reaction. A unimolecular reaction has one input (A) and a bimolecular reaction has two inputs (A and B). Their domain-level DNA implementation using the PSD architecture is shown on the right. For both reactions, it’s a two-step process. Note that for the bimolecular reaction, input B is a double-stranded gate(Gbi), and because it is required to be in a double-stranded form or a single-stranded form, depending on the system, there is a Linker reaction to facilitate the conversion (for more information, see theoretical paper).</figcaption></figure></div>\n\n\n\n<h3>Designing unimolecular and bimolecular reactions</h3>\n\n\n\n<p>Much like logic gate AND and logic gate OR are considered universal gates in traditional computation, unimolecular and bimolecular CRNs are considered the two most fundamental gates in chemistry as far as design and implementation. In unimolecular reactions, only one reactant is used; in a bimolecular reaction, two reactants are used. Depending on the design, both CRNs are capable of producing either a single output or multiple outputs. Complex CRNs with more than two inputs and/or outputs can be built using these basic CRNs, enabling more advanced processes.</p>\n\n\n\n<p>In theory, as described in the earlier paper <a href=\"https://people.duke.edu/~sns37/assets/dna25_polymeraseCRN.pdf\">“Implementing Arbitrary CRNs Using Strand Displacing Polymerase,”</a> we demonstrate that we can implement unimolecular and bimolecular CRNs using PSD and do any arbitrary computations. We also apply these CRNs in an autocatalytic amplifier, a molecular-scale consensus network, and a dynamic rock-paper-scissor oscillatory system. In our architecture, unimolecular reactions and bimolecular reactions are both implemented in a two-step process. In the unimolecular reaction, A combines with an auxiliary gate to release intermediate strand I, which then combines with the next gate to produce output strand B. In the bimolecular reaction, input A combines with input B to produce intermediate strand I, which combines with the next gate to produce output strand C.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA-1024x418.jpg\" alt=\"\" class=\"wp-image-677934\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA-1024x418.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA-300x122.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA-768x313.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA.jpg 1456w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 3: The main experimental result of our work is a catalytic amplifier. The amplifier workflow is cyclic: Input strand A performs a two-step reaction and gets recycled back to the solution along with the output release. The recycled input continues the reaction with other gates while the output attaches with a fluorescent complex for recording its activity. The graph on the right shows the experimentally observed catalytic activity. It demonstrates that when input quantity is less than 100 percent, the output still gets fully triggered. We tried inputs from 0 percent to 100 percent (in the graph, 0.00x = 0 percent, 4.00x = 40 percent, 6.00x = 60 percent, etc.), as compared to all the supporting gates, which are at 100 percent.</figcaption></figure>\n\n\n\n<h3>Implementing PSD in lab</h3>\n\n\n\n<p>In our <em>Journal of the American Chemical Society</em> <a href=\"https://www.microsoft.com/en-us/research/publication/using-strand-displacing-polymerase-to-program-chemical-reaction-networks/\">paper</a>, we take the first step toward demonstrating in lab that our architecture can implement CRNs, showing the fundamentals of PSD systems and designing an<em> in vitro</em> catalytic amplifier. A catalytic amplifier is a unimolecular reaction with two outputs, but one of the outputs is also input. A catalytic amplifier is a strong proof of concept. Not only is it a show of complex DNA design, but it’s also sustainable.</p>\n\n\n\n<p>Unlike in noncatalytic systems, where you need the same amount of input to trigger a certain amount of output, in a catalytic system like a catalytic amplifier, you can trigger 100 percent output with, let’s say, only 10 percent input. That’s because of the presence of a catalyst, or a fuel, which helps recycle the input so it can be used again to release more output. Even if there is a signal loss, you can still get experimentally full output activation from lower levels of input. For example, a catalytic amplifier could be used for fast signal restoration in deep circuits and neural nets.</p>\n\n\n\n<p>In executing a catalytic amplifier, as well as other fundamental reactions, we show we’re able to tune two defining properties of a chemical reaction: stoichiometry and reaction rate. Our PSD architecture will only activate the same amount of output as input, allowing us to control the amount of output via the amount of input (stoichiometry), and the speed at which output is produced can be controlled by varying the lengths of the input strands.</p>\n\n\n\n<h3>Infinite possibilities</h3>\n\n\n\n<p>This was the first study on implementing CRNs using PSD, so for us, the goal was to explore the fundamentals. Our work mainly focuses on the design and demonstration of the basic properties of such systems, such as tuning the reaction speed and controlling the stoichiometry, and with the catalytic amplifier, we put the fundamentals to work. Next steps include in vitro demonstrations of larger-scale autocatalytic systems such as oscillators, linear controllers, and pulses, which are the accepted gold standards in the field. Such complex biochemical controllers can have computing applications in the biological context, where traditional silicon can’t reach, offering new possibilities in medical care, as mentioned above, agriculture, energy, molecular biology, computing, and sensor networks, among other areas.</p>\n\n\n\n<p>DNA computing is decades behind its silicon counterpart, but we see fast growth as a real possibility, with the field learning from current technology. And thanks to design software like <a href=\"https://www.microsoft.com/en-us/research/publication/visual-dsd-a-design-and-analysis-tool-for-dna-strand-displacement-systems/\">Visual DSD</a> from Microsoft Research, DNA computing will be more streamlined, and designing and testing a new architecture—without leaks—should be easier, opening the way for some very cool applications.</p>\n\n\n\n<p><em>Acknowledgment: This work was conducted by <a href=\"http://people.duke.edu/~sns37/\">Shalin Shah</a>, <a href=\"https://www.linkedin.com/in/jasminewee7\">Jasmine Wee</a>, <a href=\"https://scholar.google.com/citations?user=LYNUgUcAAAAJ&hl=en\">Tianqi Song</a>, <a href=\"https://www.cs.washington.edu/people/faculty/luisceze\">Luis Ceze</a>, and<a href=\"https://www.microsoft.com/en-us/research/people/kstrauss/\"> Karin Strauss</a>, jointly led by <a href=\"https://users.cs.duke.edu/~reif/\">John Reif</a> at Duke University and <a href=\"https://www.microsoft.com/en-us/research/people/yuanjc/\">Yuan-Jyue Chen</a> at Microsoft Research. Shah was a Microsoft Research intern at the time of the work.</em></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/researchers-use-a-strand-displacing-dna-polymerase-to-do-biocomputing/\">Researchers use a strand-displacing DNA polymerase to do biocomputing</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Thu, 23 Jul 2020 19:37:41 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_No_Logo_DNA.gif","linkMd5":"6bf554702573476505a26f06881bee2b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn93@2020_2/2020/08/24/21-40-46-787_841a52cbf2ac8d9f.webp","destWidth":1400,"destHeight":788,"sourceBytes":394551,"destBytes":900332,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_No_Logo_DNA.gif":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn93@2020_2/2020/08/24/21-40-46-787_841a52cbf2ac8d9f.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-1_Polymerase-based-stand-displacement.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn35@2020_2/2020/08/24/21-40-53-026_004554824c419ee0.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA-1024x524.jpg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn25@2020_1/2020/08/24/21-40-52-994_6a7bf8cd312ae883.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA-1024x418.jpg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn33@2020_4/2020/08/24/21-40-53-122_dd4bee411a2bd1e8.webp"},"publishedOrCreatedDate":1598305163711},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"Defending DRAM for data safety and security in the cloud with Dr. Stefan Saroiu","link":"https://www.microsoft.com/en-us/research/?p=670293","description":"<h3><img class=\"alignnone size-large wp-image-670308\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1024x577.png\" alt=\"head shot of Dr. Stefan Saroiu for the Microsoft Research Podcast\" width=\"1024\" height=\"577\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1024x577.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1536x865.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-2048x1153.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1920x1080.png 1920w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></h3>\n<h3>Episode 120 | July 8, 2020</h3>\n<p>Dynamic random-access memory – or DRAM – is the most popular form of volatile computer memory in the world but it’s particularly susceptible to Rowhammer, an adversarial attack that can cause data loss and security exploits in everything from smart phones to the cloud.</p>\n<p>Today, <a href=\"https://www.microsoft.com/en-us/research/people/ssaroiu/\">Dr. Stefan Saroiu</a>, a Senior Principal Researcher in MSR’s <a href=\"https://www.microsoft.com/en-us/research/group/mobility-and-networking-research/\">Mobility and Networking group</a>, explains why DRAM remains vulnerable to Rowhammer attacks today, even after several years of mitigation efforts, and then tells us how a new approach involving bespoke extensibility mechanisms for DRAM might finally hammer Rowhammer in the fight to keep data safe and secure.</p>\n<h3>Related:</h3>\n<ul type=\"disc\">\n<li><a href=\"https://www.microsoft.com/en-us/research/podcast\">Microsoft Research Podcast</a>: View more podcasts on Microsoft.com</li>\n<li><a href=\"https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2\">iTunes</a>: Subscribe and listen to new podcasts each week on iTunes</li>\n<li><a href=\"https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml\">Email</a>: Subscribe and listen by email</li>\n<li><a href=\"https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml\">Android</a>: Subscribe and listen on Android</li>\n<li><a href=\"https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU\">Spotify</a>: Listen on Spotify</li>\n<li><a href=\"https://www.blubrry.com/feeds/microsoftresearch.xml\">RSS feed</a></li>\n<li><a href=\"https://note.microsoft.com/ww-registration-microsoft-research-newsletter-s.html?wt.mc_id=S-webpage_podcast\">Microsoft Research Newsletter</a>: Sign up to receive the latest news from Microsoft Research</li>\n</ul>\n<hr />\n<h3>Transcript</h3>\n<p>Stefan Saroiu: So our philosophy is that what we would like to see in the standard, rather than describing <i>the</i> solution for Rowhammer, what we would like to see is describing extensibility mechanisms that companies, hardware vendors, can implement their favorite form of mitigations, the one that works best for their particular type of memory by leveraging these extensions.</p>\n<p><b>Host: </b><b>You’re listening to the Microsoft Research Podcast, a show that brings you closer to the cutting-edge of technology research and the scientists behind it. I’m your host, Gretchen Huizinga.</b></p>\n<p><b>Host: </b><b>Dynamic </b><b>r</b><b>andom</b><b>-a</b><b>ccess </b><b>m</b><b>emory – or DRAM – is the most popular form of volatile computer memory in the world but it’s particularly susceptible to Rowhammer, an adversarial attack that can cause data loss and security exploits in everything from smart phones to the cloud.</b></p>\n<p><b>Today, Dr. Stefan Saroiu, a Senior Principal Researcher in MSR’s Mobility and Networking group, explains why DRAM remains vulnerable to Rowhammer attacks, even after several years of mitigation efforts, and then tells us how a new approach involving bespoke extensibility mechanisms for DRAM might finally hammer Rowhammer in the fight to keep data safe and secure.</b><b> </b><b>That and much more on this episode of the Microsoft Research Podcast.</b></p>\n<p><b>Host: Stefan Saroiu, welcome to the podcast.</b></p>\n<p>Stefan Saroiu: Thank you, Gretchen. It’s great to be here.</p>\n<p><b>Host: Some of my favorite people on the planet are working on making things work for us and you’re one of those people. So, first, thanks. </b><b>As we begin though, </b><b>let’s talk about your people for a minute. You’re a Senior Principle Researcher in the Mobility and Networking group, which isn’t totally separate from Systems and Networking, but they’re not totally the same either. So, give us a verbal Venn diagram of the two groups, why they exist, where they’re different and where they overlap, and how, in broad strokes, each of them is working to make our lives better.</b></p>\n<p>Stefan Saroiu: Yes, thank you for the kind words, Gretchen. So back in the day, Microsoft Research had a single Systems and Networking group and as the group got larger, the group split into several smaller groups like the Systems group, the Security group, the Distributed Systems group and the Mobility and Networking group. But we’re all systems researchers at the end of the day, whether we work on operating systems or networks or mobile systems or on distributed systems. So, I’m part of the Mobility and Networking group. But over my research career, my work has focused on systems, both in terms of mobile systems and networking systems. And for the past couple of years, these systems that I’ve been working on aim to improve the security of users and the security of infrastructure.</p>\n<p><b>Host: Let’s get specific and talk about the work you do within the </b><b>M</b><b>obility and </b><b>N</b><b>etworking group now. So, sort of in general, what big problems are you trying to solve as a researcher and, and maybe more importantly, why does the world need you to solve them? What gets you up in the morning?</b></p>\n<p>Stefan Saroiu: So, I do two kinds of work. The first kind is creative work because I really value creativity very highly and I believe it’s very difficult to come up with a truly creative idea. The second kind of work that I do is driven by intellectual curiosity and by revisiting assumptions or turning them on their head. And I strongly believe that the role of an expert is to break preconceived assumptions and rules. Unfortunately, you have to be an expert first. In fact, trying to break assumptions before understanding deeply an area and a problem is a very bad idea. So, I’ve been working on secure systems research for almost a decade now. We built a secure network tracing system that offers very strong privacy. So, for example, network operators can monitor their networks in a way that all the sensitive data is locked down without anybody being able to subvert it or use it in any ways other than originally intended. We built sensors that can attest that information is correct and has not been manipulated or changed. So, as a simple example, consider a photo where one can check whether the photo has been photoshopped or is indeed captured by a proper camera. Then I worked on a secure payment system called Zero-Effort Payments, that was a little like the precursor of Amazon Go Store. So, our system was a little different in that, you’d pick up the food and you’d go through a cashier who’d ring you up, but you’d not have to do any explicit thing to actually pay. The system would know who you are, and since you’d have to pre-register with the system and the payment would be processed. So, I’ve worked on all these things. I also worked on a firmware TPM, which brings trusted computing to mobile devices and it works in millions of smart phones and tablets today.</p>\n<p><b>Host: Mmm.</b></p>\n<p>Stefan Saroiu: But for the past couple of years, I’ve worked on Azure security in particular, and we started a project called Project STEMA. STEMA stands for Secure Trustworthy and Enhanced Memory for Azure and we’ve been focusing a lot on Rowhammer attacks.</p>\n<p><b>Host: Well, let’s talk about memory and computer memory specifically…</b></p>\n<p>Stefan Saroiu: Yeah.</p>\n<p><b>Host: …</b><b> </b><b>since</b><b> </b><b>it’s a foundational storage unit for digital data, but there are many kinds of containers,</b><b> </b><b>as you well know, </b><b>so let’s do a quick primer for the flavor that we’re </b><b>really most</b><b> interested in today, which is DRAM. </b><b>So, how does it work physically? What are its vulnerabilities, both internally and externally? And you don’t need to get ridiculously granular here because I saw your </b><b>hundred and fourteen</b><b>-page deck and </b><b>a hundred</b><b> pages of </b><b>it is explaining DRAM! No, I’m kidding</b><b>…</b><b> </b><b>D</b><b>on’t be afraid to get as technical as you need to set the problem up.</b></p>\n<p>Stefan Saroiu: Okay. So, DRAM is the world’s most popular form of volatile memory. Pretty much every form of computing out there has DRAM. You can find DRAM in smart phones, in tablets, in PCs… You can find DRAM in cars. You can find DRAM in washing machines. And a DRAM cell stores a zero or a one, and it does that by using a very simple circuit with one capacitor. And a capacitor can be charged or discharged, and that can mean a one or a zero. So, for example, if you want to store the value one/zero/one/zero, you just sort of have four cells and you have one charge capacitor and one discharge capacitor, one charge and one discharge and you encode one/zero/one/zero that way. Now, capacitors leak over time. They sort of lose their charge over time. So, DRAM has to continuously refresh these capacitors. And the cells are built to maintain their charge for a small period of time, say something like sixty four milliseconds, and the contract is that the hardware has to make sure that every single cell in its DRAM is refreshed once within sixty four milliseconds and in that way the cell maintains its data, its charge. Now, DRAM cells are organized in rows and columns and when you read a value from DRAM, you read by row, and the way you read this is by switching some transistors in such a way that the capacitors are then coupled with some sensors. So, the sensors sense whether these capacitors are charged or discharged and then they can translate that into data. Now, unfortunately what’s happening is that when you actually sense the data on the capacitors, it turns out that rows located in the vicinity, in the adjacency, of this row you’re trying to read, those capacitors also get affected and they get affected by having them discharge faster than normal. And this phenomenon is called a DRAM Disturbance Error because, by causing them to discharge faster within a sixty-four-millisecond period, you lose the content of that cell and in some sense the bit flips that way. And the bit that flips is one that you never actually meant to read or access before. Maybe you don’t even have control over it. Maybe it’s some other software component that controls it.</p>\n<p><b>Host: Okay.</b></p>\n<p>Stefan Saroiu: So that’s where sort of the concern lies.</p>\n<p><b>Host: Right.</b></p>\n<p>Stefan Saroiu: In the DRAM space, there is this Rowhammer attack and the contract, from day one when you build any system, any software system, anything you want, any computer, the contract is that if you give me a piece of memory, when I write something to it, I want to be able to read what I wrote. And with Rowhammer, you violate this very simple contract. You read a different value than the one you wrote. And, doing that you can basically exploit systems in ways that are unimaginable before.</p>\n<p><b>Host: Well, since we’re talking about </b><b>Rowhammer </b><b>right </b><b>now, let’s move into i</b><b>t. </b><b>A</b><b>s you put it once, it’s one of the hottest research topics in the security research community. So</b><b>,</b><b> give </b><b>us a level</b><b>&#8211;</b><b>set. What is </b><b>Rowhammer</b><b>, specifically, and why, particularly, does it make </b><b>cloud</b><b> providers and server farmers nervous?</b></p>\n<p>Stefan Saroiu: So, I described this DRAM Disturbance Errors effect and this effect gets worse as DRAM gets denser and denser and we want DRAM to get denser and denser because that’s how we store more capacity, that’s how we build better DRAM. But the phenomenon gets worse, this DRAM Disturbance Error. A Rowhammer attack, it’s an attack in which an adversary generates a workload that exploits disturbance errors to flip the value of bits that have critical importance to the security of the system. Like for example, bits that form a secret key. And cloud vendors are very nervous because the entire business model is one where you have multiple parties share your hardware. In particular, in this case, they share your memory. They share your DRAM. Well, what if one of these customers becomes rogue? They themselves get exploited through some other attack. Can they attack other customers by flipping bits in their memory? And yes, they can attack it in very devastating ways.</p>\n<p><b>Host: How did Rowhammer get its name? Is it because of the rows in the DRAM?</b></p>\n<p>Stefan Saroiu: I was explaining how, when you access a row, an adjacent row gets affected by that, and the attack, in order to create this disturbance error, what you have to do is you have to keep accessing that row over and over and over again and that’s the term hammering the row. And the attack got this name Rowhammer.</p>\n<p><b>Host: So, if I’m an attacker, am I trying to do something specific or am I just trying to mess you up?</b></p>\n<p>Stefan Saroiu: Oh, that’s a great question. Depends, right? So, as a cloud provider, cloud providers are nervous about both scenarios. A Rowhammer attack in general refers to flipping a security-critical bit, so by flipping that bit, I’m trying to target something specifically. I’m trying to exploit something. However, the simpler, and in fact, the likelier form of attack is one where I’m just messing up the bits. And systems, actually, today in the cloud, they’re pretty good at detecting when these bits are messed up, but if the bits are messed up, there is just very little we can do about that. I see these bits and I’ve encoded enough redundancy in the data to know that they’re messed up, but I can’t recover to where I was before.</p>\n<p><b>Host: Right.</b></p>\n<p>Stefan Saroiu: And there is really not a good way to solve that problem. Once the bits have flipped, it’s like I can’t go back, and the best thing I can hope for is maybe reboot the server and let’s start all over again. And that’s also very, very bad for a cloud customer because there are a lot of workloads in the cloud that have a lot of data in memory, they do a lot of computation, maybe they train a machine learning model and then, you know, for many, many days, and at some point you say, oh, sorry, guys, you have to start all over again because we’ve messed it up.</p>\n<p><b><i>(music plays)</i></b></p>\n<p><b>Host: Before we dive into the technical aspects of your research on the Rowhammer threat, I find this whole drama really fascinating and I think it would be good to set the stage and the cast of characters. We’ve talked about cloud providers. Who are the other players? Who provides to the providers and what’s their motivation? Who sets and guards the standards, and finally, who’s got an eye on everybody?</b></p>\n<p>Stefan Saroiu: It’s a fascinating landscape. Microsoft is a cloud provider and I started with cloud providers because part of our role at Microsoft Research is also to make the cloud better. There are several other players, and one big players are the companies that sell DRAM. And when the attack was first described or published, which was in 2014, the hardware vendors jumped to quickly dismiss these concerns. It’s, we knew about Rowhammer, but that’s a problem that that older type of memory has. The newer type of memory, it doesn’t have this problem anymore. And in fact, there are quotes online where vendors claim that DDR4, which is the memory that we always use today, is Rowhammer-free. And of course, researchers have shown over and over again that DDR4 is not Rowhammer-free and then they said, oh, yes, but then you should buy this newer DDR4 that has a form of defense called TRR and just this year, earlier, there was a wonderful paper from an academic group in the Netherlands that showed a huge number of DDR4 DRAM with this form of defense TRR being vulnerable by slightly changing the form of the attack, okay? So basically what the vendors have done is they’ve patched the old way of mounting the attack, but they haven’t told anyone how they patched it and you just have to sort of try different things until one of the new things clicks and then you can bypass those defenses. So, now we have the cloud providers, the DRAM vendors and then the security and research community. And there is this feeding loop where that the DRAM vendors say, oh, yeah, we knew about it. The new memory is safe. Give it a year or two. The security and research community says, oh, it’s not safe… and sort of the cloud providers and the smart phone manufacturer as well, are caught in the middle.</p>\n<p><b>Host: So Rowhammer is a problem. It’s a big one, and it hasn’t been ignored, but it hasn’t been solved either. So, when we talked before, you said that more than forty papers…</b></p>\n<p>Stefan Saroiu: Oh, yeah.</p>\n<p><b>Host: …have been published on this subject and DRAM still remains as vulnerable as ever. So, what has the academic community done to date to try to solve the Rowhammer problem, and what, to date, have they got right </b><b>and</b><b> got wrong?</b></p>\n<p>Stefan Saroiu: Right, so there’s sort of two bodies of work in academia. One is the security research community. And then there is the computer architecture community, and to give them credit, actually, the computer architecture community were the first ones to show this problem to sort of raise the flag saying, hey, we have a DRAM Disturbance Error. And the architecture community has been very good at putting forward the whole bunch of Rowhammer mitigation proposals. However, all these proposals, they come with trade-offs and implementing one of these mitigations inside of DRAM will make that DRAM ultimately more expensive in some way. Maybe it will decrease the density, maybe the DRAM vendors will have to add extra memory or extra sort of counters to keep track of who’s accessing what. And the market forces in the DRAM world are in such a way that they need to use every single piece of real estate they have to just cram more and more cells. And that’s where the security research community comes in where they keep sort of reverse engineering and trying different things and they’ve found ways to go around those mitigations and show new forms of attack. So, to be fair to them, it’s sort of also a business thing. Like I said, people knew about Rowhammer and there were discussions in their sort of standardization body – that is an organization called JEDEC where a lot of hardware vendors and software companies actually participate – there’s been a lot of discussion over the years on implementing a solution and in fact, that’s what they’re doing now. There won’t be a single solution for Rowhammer that will work for every single type of memory out there and for which every single hardware vendor will be willing to actually implement. So our philosophy is that what we would like to see in the standard, rather than describing <i>the</i> solution for Rowhammer, what we would like to see is describing extensibility mechanisms that companies, hardware vendors, can implement their favorite form of mitigations, the one that works best for their particular type of memory by leveraging these extensions. So that’s what we’re trying to sort of change and shift.</p>\n<p><b>Host: In light of all that, Stefan, tell us about your most recent work that involves what you called an end-to-end methodology to help cloud providers determine if they’re susceptible to Rowhammer because that’s that upstream approach that you’re talking about instead of the patch afterwards that’s impossible. So in the context of our cast of characters</b><b>,</b><b> and against the backdrop of computer memory solutions that have trust issues, tell us how you are attacking this, what your methodology is, how successful it is, and what are the key challenges </b><b>that </b><b>you face?</b></p>\n<p>Stefan Saroiu: So, what we tried to do was we tried to help the software company by building a systematic and scalable testing methodology to test whether your DRAM is susceptible to Rowhammer attack. And to build such a methodology, you have to overcome two practical challenges. You have to devise a sequence of instructions that your processor executes, that hammers the memory at the fastest possible rate. You want to create what we call the “worst-case testing conditions” for memory. The second thing you want to do, you want to know where you’re hammering. Remember I was telling you how DRAM disturbance occurs to rows that are adjacent to the row you’re hammering. Well, the rows that are adjacent are the worst affected, but even rows that are nearby are affected. So, a row that’s sort of two rows away, like the next neighbor or something like that, can be affected, but it’s very difficult to affect a row that’s very far somewhere inside of your array. So, you have to actually know, what is the row-by-row layout of your DRAM chip? And this is, in fact, a trade secret. What we did was we built a hardware fault injector that allows us to… you can think of it like short-circuiting the memory in such a way that we can actually <i>always</i> create these Rowhammer attacks by not letting the memory refresh itself. So, if you hammer a row and the memory never refreshes, you’re going to flip bits eventually because the capacitors will lose their charge. Then you go and study the patterns of how these bits have flipped and that tells you about the layouts of the cells inside of the DRAM. Because guess what? The row you hammered, most of the bits that flipped are going to be in its adjacent rows. And there will be some bits flipped in the next to the adjacent rows, and then fewer bits and so on, so you create these kind of heat maps so you can reverse, really, row-by-row adjacency by this form of short-circuiting the memory or sort of suppressing refresh commands.</p>\n<p><b>Host: Okay, so you’re reverse engineering to find out what’s going on?</b></p>\n<p>Stefan Saroiu: Yes, we have a methodology… our methodology can reverse engineer every single DDR4 DIMM in the world. And what you actually end up discovering when you’re reverse engineering is that these maps change. They change from one vendor to another and they can also change from one DIMM to another depending on the DIMM’s revision. It’s called Post Packaging Repair. So, we can actually also measure how many fixes that DRAM has had before it was shipped to you.</p>\n<p><b>Host: Okay</b><b>. </b><b>S</b><b>o</b><b> this methodology has to attack, for lack of a better word…</b></p>\n<p>Stefan Saroiu: Yes, yes.</p>\n<p><b>Host: …every vendor’s particular proprietary chip…</b></p>\n<p>Stefan Saroiu: Yes.</p>\n<p><b>Host: …and within the vendors, there’s different chips as well. So, you’ve got a lot of things you have to be looking at. How’s it working so far?</b></p>\n<p>Stefan Saroiu: Now, that’s a great question. It’s very difficult to test every single chip that a cloud provider has. So, instead what we’re doing is we are mapping the DRAM fabrication process for different DRAM devices and for different vendors. And then within those packets, we sample and we test. And we actually, what we do, we look at the trends. And we want to make sure that the workloads that we see in the cloud will not generate activations that will actually start flipping bits.</p>\n<p><b>Host: Right.</b></p>\n<p>Stefan Saroiu: Because I was telling you, the DRAM gets worse over time, not better. So in fact, we can even, by waving our hands a little bit, we can predict how many years in the future it’s going to be until we’re going to see workloads reach a point where, just by using the memory, they’re going to start flipping bits. And it’s our job to influence sort of a more principled approach to fixing the problem rather than a band-aid approach and also to keep an eye, not just for Microsoft, but the entire cloud industry, as to at which point we’ll have to do something to make sure that the workloads are not going to actually start causing bit flips. So for example, one of the things you might want to do when you actually detect that a virtual machine starts accessing the memory in a way that might actually induce bit flips, you could try to slow it down or migrate it to a new DRAM or do something like that.</p>\n<p><b>Host: Yeah, so there’s more solutions than just fixing the chip?</b></p>\n<p>Stefan Saroiu: Right.</p>\n<p><b>Host: There’s other mitigations.</b></p>\n<p>Stefan Saroiu: In fact, I think the solutions will have to span the entire stack. There’ll be some fixing the chip things, but those fixing the chips have to sort of be programmed or used by things higher up, both by the CPU and by the software.</p>\n<p><b>Host: Well, that leads </b><b>well </b><b>into the next question. There’s a lot of people that need to be involved in this so, tell us a little bit about who you’re working with as partners and what kinds of cooperative expertise do you need to solve for X in the systems security equation?</b></p>\n<p>Stefan Saroiu: None of these works I’ve been describing is mine alone and in Microsoft Research, I’ve worked with a small team of very talented people who have expertise that is very, very different than mine. You know, I come from a computer science background and I am not sort of equipped to short circuit memory or anything like that. And in fact, until a couple of years ago, I really didn’t understand how DRAM works very well. So, we have that sort of expertise in Microsoft Research to basically build hardware prototypes that actually can inject these sort of failures into hardware, into the memory.</p>\n<p><b>Host: Okay.</b></p>\n<p>Stefan Saroiu: And then I also collaborate very strongly with a group of wonderful engineers in Azure. There is a group called the Next Cloud System Architecture, or NCSA, and these folks have decades of expertise of understanding how DRAM works. And working with JEDEC and working with the memory vendors and they’ve been very good in two ways. One was in describing to us how memory works in ways that go beyond what the manual can teach you, and sort of what the concerns are and the limitations, and the forces that act when you actually build these circuits in practice. And the second way that they’ve been very helpful was that, when we interact with JEDEC, and we try to sort of make the shift, they’ve been very good at coaching us on how to put forward that proposal in a way that’s more amenable.</p>\n<p><b>Host: Who are your other kind of big partnership associations? Are you working with other academics? Are you working with other industry? Are you working with other cloud providers?</b></p>\n<p>Stefan Saroiu: We’re lucky that we have very strong collaborations with two top academic places. One is ETH Zurich and the other one is at Max Planck in Germany. And we are working closely with companies that can be massively affected by Rowhammer. When security researchers actually go and sort of find a new way to actually attack the memory, they go through a process that’s called Responsible Disclosure. So, what that means is that they will not make their findings publicly available, but instead they’re going to reach out to all the industry involved and describe their findings and give some time to the industry to form a response. And once this period has ended, then the research becomes public. So when these new forms of attacks came about, there was a group of companies formed that started looking at these problems again and the first thing that they had on their mind is, like look, you know, so we have these research results, but can anybody go and independently validate them on their hardware? So, we at Project STEMA were the first to actually validate these findings on server-grade hardware… hardware that is run in the data centers.</p>\n<p><b>Host: Right. Well, it sounds like you all have the same goals. You don’t want things to break. You want things to work well. Ultimately, you want customers to have safe data and things not to break…</b></p>\n<p>Stefan Saroiu: We do, and in fact, I was mentioning how, for Rowhammer and for testing memory, understanding row by row adjacency is very, very important. And I also said how DRAM vendors do not want to reveal this information, okay? In fact, the extensibility mechanisms we proposed for people to build their own forms of Rowhammer mitigations, from the beginning, we designed them in a way where DRAM vendors do not have to tell anyone these adjacency maps. In fact, there is a large swath of Rowhammer mitigations that people have proposed over the past five or six years that all rest on the assumption that the software company will have complete access to this information and these companies are very reluctant, so rather than mandating that, or forcing them to do something they don’t want to do, instead, we designed this with the assumption that hey, you guys don’t have to tell us anything. And we think that this has a much better likelihood of being adopted in practice and then again, not mandating the solution, letting people build their favorite solution for the kind of hardware they want to use. You know, the Rowhammer solution that you build for the DRAM in a server running in Azure is very, very different than the Rowhammer solution that you should build for an IoT device that has a little bit of DRAM and runs a little bit of code.</p>\n<p><b><i>(music plays)</i></b></p>\n<p><b>Host: We’ve reached the “what could possibly go wrong?” segment of the podcast where I ask all my guests what keeps them up at night</b><b>, and s</b><b>o, despite the fact that the majority of your work could actually be classified as a research response to what keeps us all up at night, sometimes the so-called solutions actually present new problems. So, do you have any concerns about the work you’re doing and if so, how are you addressing them?</b></p>\n<p>Stefan Saroiu: My concern is, I was telling you how we have this hardware out there, whether it’s DRAM, whether it’s CPUs, whether it’s, you know, chip sets, whether GPUs, so on and so forth, and this hardware is very, very complex and one of the things that we’ve learned is that in the quest of higher performance, we have designed this hardware in ways that can be exploitable, and these exploits are done in such a way that we never thought possible before and what keeps me up at night is, I don’t really understand everything that’s going on in a DRAM device. What if there’s another way out there that you can actually mount these attacks in very, very simple ways? And unfortunately, with a cloud and with the consolidation of the entire computing power into data centers, I’m concerned that we might have an event that wipes out an entire cloud, that wipes out, you know, a big part of our infrastructure, that shuts down the entire internet. And you know, we’re going to have exploits and little things here and there. We’ve always had those. We’re going to continue to have them. But we’ve really never had a single wipe out event. So, having a wipe out event would be quite, quite concerning.</p>\n<p><b>Host: So how are you thinking about that as a researcher? I mean, it’s one of those giant problems </b><b>which</b><b> you think I can’t possibly solve this, but are there any strategies that come into your head, given the kind of work you do, that say, hey, this is how we might try to mitigate such an event?</b></p>\n<p>Stefan Saroiu: It’s a very hard problem. You’re absolutely right. My part comes when it’s about DRAM, because I understand DRAM better than many people, and my part is to make sure that there won’t be a wipe out event happening because of DRAM… From the point of view of DRAM, I hope that my work plays a role in that.</p>\n<p><b>Host: That’s a beautiful way to frame it, Stefan, because as you point out, there’s many fronts in this war against, you know, people that are working to keep things safe and secure and people that are working to tear things down. So, you say, hey, at least on my watch, the DRAM part is going to be good</b><b>!</b></p>\n<p>Stefan Saroiu: That’s right.</p>\n<p><b>Host: I love that.</b></p>\n<p>Stefan Saroiu: That’s right.</p>\n<p><b>Host: Well, I don’t want to let you go before we talk a little bit about industry standards and the tension between gatekeepers and practitioners </b><b>in</b><b> a time when technical innovation is moving so fast that</b><b> </b><b>regulatory bodies have a hard time keeping up. So, what are the key challenges to organizations like JEDEC in your field and how would you frame the role of these kinds of gatekeepers in the future?</b></p>\n<p>Stefan Saroiu: So, the industry is changing at a fantastic pace and the role of JEDEC was actually to standardize how memory is used. At least, let’s talk about DRAM. They actually standardize things other than DRAM, but DRAM is a big part of it. And in the 80s and in the 90s, we needed that because we were building PCs and we had a massive number of stakeholders, of people who were actually building all sorts of hardware components, and you wanted these hardware components, when you put them in a box, to all work together. Now, there is a little less of that. There’s more of data centers and there’s no need for the computers that Google puts in their data centers to make sure they work with the computers that Microsoft puts in their data centers. All they have to do is to offer a software platform that is common enough that people can actually use it. But because we see this consolidation, I believe there is less of a need for standardization happening because if a cloud provider buys memory from three different vendors, all four parties can agree on how they build their hardware in using their data center. So, I believe we’re going to see an increasing amount of fragmentation that way.</p>\n<p><b>Host: Well, so, what does an organization like JEDEC have to do to stay alive?</b></p>\n<p>Stefan Saroiu: I think JEDEC has to shift the way they view themselves as a specification of what the functionality of the hardware is to one that specifies mechanisms that are flexible enough to allow increasing amounts of innovation from the different stakeholders in place.</p>\n<p><b>Host: Well, every researcher has a unique life story and it’s time to hear yours, but I want to preface this by noting that you’ve gone to the trouble of including what I would call a scholarly genealogy on your personal website, </b><b>s</b><b>o we can trace your academic ancestors back to</b><b>,</b><b> on one side of the family</b><b>,</b><b> the 17</b><b>th</b><b> century! So, tell us your story, Stefan. Upon whose shoulders do you stand, both personally and academically, and how did you get where you are today from where you started back in your early years?</b></p>\n<p>Stefan Saroiu: You know, when I look at it, it feels very humbling. Every once in a while, I go back and take a pause to reflect because you look at those names and you go, oh, gosh, those are some big shoes to fill! I had the opportunity to work with two different advisors in my graduate school and therefore I have two genealogies. And like you said, one goes back to the 17th century and included names like, Karl Jacobi and John McCarthy. And Jacobi was mostly known for math and things like elliptic functions and number theory, and John McCarthy is known for being one of the founders of AI. John’s student was Barbara Liskov, who just won the Turing Award a couple of years ago and she’s sort of a role model for me. So yeah, I feel very, very humbled and I look at that, periodically to sort of remind myself as to where the bar is!</p>\n<p><b>Host: Right</b><b>?</b><b> So, tell, tell us a little bit about you</b><b>,</b><b> then. You’ve come into this in the 20</b><b>th</b><b> century and have started to make your own mark. </b><b>H</b><b>ow did you get from A to B?</b></p>\n<p>Stefan Saroiu: I was born and grew up behind the iron curtain in Bucharest, Romania, and I was very lucky to attend a high school that was very strong academically, especially in mathematics and computer science. So, in the late 80s and early 90s, I was sort of studying things like algorithms and graph theory and programming languages and so forth and we also did a lot of math. And in some sense, I was very lucky to have this training in math and computer science, but it came at a cost that I am terrible at anything else! And when I finished high school, my family decided to immigrate to Canada and they went to Calgary, Alberta, and when I got there, Canada had a program where immigrants who lacked English skills, they would be enrolled in free English as a Second Language kind of form of schooling and I spent three months going to English school with my mom and dad. And my mom and I were in the same class, actually. We were in Level 3 and my dad was in Level 1. And I remember… I remember very clearly, sort of, meeting my dad in the hallways during lunch and having lunch together. It’s – yeah, this was…</p>\n<p><b>Host: I’m dying!</b></p>\n<p>Stefan Saroiu: …I was… I was about nineteen, yeah, I was nineteen at the time. A couple of years later, I went to college at the University of Waterloo, which is a great school in computer science in Canada, and Waterloo has this wonderful co-op program where, as part of graduating college, you have to actually go do internships in industry. And I was lucky to do three internships with Microsoft back in the 90s. And I came to Seattle, and I saw Seattle, and when time came to go to graduate school, one of the places I applied to was University of Washington in Seattle. So, I came to U Dub, and once I graduated, I wanted to join academia. So, I went and took a job as a professor at the University of Toronto. And at Toronto I worked with some fabulous students. And then after a couple of years at Toronto, I kind of started missing the intensity of the west coast and the tech scene here. One of the things I didn’t realize before leaving Seattle is that, west coast is really one of the best place on earth to be a computer scientist because you meet a lot of people who understand what you’re doing and speak your language. And I talked to some of my friends and they offered me to come interview at MSR. I knew Seattle. And I came, and I never left, and I had a wonderful time!</p>\n<p><b>Host: What is one interesting thing we might not know about you, maybe it’s a personality trait, a defining life moment, hobby, side quest</b><b>…</b><b> that has impacted your life or career?</b></p>\n<p>Stefan Saroiu: A lot of computer science researchers in the US who are not born here, they came here to do their PhDs. But I came much, much earlier, and I did not come with a plan to actually continue my education or pursue any advanced education. So, I have a lot of immigrant stories and I think a lot of those sort of have marked the way I think. People had a hard time working with me because I would insist that we work as hard as possible, and we never have any moment of relaxation or anything like that and I really, truly believe that that’s actually very detrimental to a researcher. A researcher has to be a little bit more balanced and I remember very clearly, Steve Gribble, one of my advisors, telling me over and over again, Stefan, it’s not just about working harder. It’s also about working smarter. And it took me a long time to understand what he meant. You have to let time allow you to have the flow of creativity and see things in ways that maybe others haven’t seen them before.</p>\n<p><b>Host: As we close, I’d like you to take a shot at painting a picture of a future world in which you’ve been wildly successful. At the end of your career, what do you hope to have accomplished as a scientist and how will your research have made a difference in our lives?</b></p>\n<p>Stefan Saroiu: Thank you for asking this question. I actually thought about this and I’m thinking quite a bit about it. If you had asked me this question ten years ago, my answer would have been, I want to make sure that my research work is being used by millions of people. And I was very fortunate to be able to accomplish that at MSR, not just once, but maybe a couple of times. So, going forward, again, I see my role as keeping an eye on making sure that we avoid a form of a wipe out event, at the very least, that exploits some form of DRAM. And if, you know, a decade or two from now, we manage to say, hey, yeah, you know, we’ve had compromises here and there, but for the most part, the internet worked really well, cloud computing worked really well, you know, AI worked really well… then I hope that at least a little part of that was due to my work as well.</p>\n<p><b>Host: Stefan Saroiu, I for one am glad you’re doing the job you’re doing. Thank you for that and thank you for joining us today on the podcast.</b></p>\n<p>Stefan Saroiu: Thank you and thank you for your insightful questions, Gretchen.</p>\n<p><b><i>(music plays)</i></b></p>\n<p><b><i>To learn more about Dr. Stefan Saroiu, and the ongoing fight against Rowhammer attacks, visit Microsoft.com/research</i></b></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/podcast/defending-dram-for-data-safety-and-security-in-the-cloud-with-dr-stefan-saroiu/\">Defending DRAM for data safety and security in the cloud with Dr. Stefan Saroiu</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Wed, 08 Jul 2020 10:00:58 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1024x577.png","linkMd5":"7bd6175a3659baf642494020f7c1efb1","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn7@2020_4/2020/08/24/21-39-23-955_da7f7e726872235f.webp","destWidth":1024,"destHeight":577,"sourceBytes":471839,"destBytes":39324,"author":"Alyssa Hughes","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1024x577.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn7@2020_4/2020/08/24/21-39-23-955_da7f7e726872235f.webp"},"publishedOrCreatedDate":1598305163714},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"MineRL sample-efficient reinforcement learning challenge—back for a second year—benefits organizers, as well as larger research community","link":"https://www.microsoft.com/en-us/research/?p=685122","description":"\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1024x576.png\" alt=\"A trained RL agent searched for a diamond in Minecraft game. \" class=\"wp-image-686049\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1536x864.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image.png 1658w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n\n\n\n<p>To unearth a diamond in the block-based open world of Minecraft requires the acquisition of materials and the construction of tools before any diamond mining can even begin. Players need to gather wood, which they’ll use to make a wood pickaxe for mining stone underground. They’ll use the stone to fashion a stone pickaxe and, with the tool upgrade, mine iron ore. They’ll build a furnace for smelting the iron and use that to make the iron pickaxe they need to start their search for the precious gem. Each task results in a stronger tool and brings players closer to retrieving that coveted diamond. The efforts to organize competitions designed to advance the state of the art feel quite similar. In fact, the research process in general feels quite similar. As you gain more knowledge and experience, forge new and deeper collaborations, and leverage and improve resources—collectively, <em>stronger tools</em>—uncovering a gem or many of them in the form of breakthroughs and promising new directions to explore becomes more reachable.</p>\n\n\n\n<p>After none of the submitted agents were able to obtain a diamond in Minecraft during <a href=\"https://www.microsoft.com/en-us/research/publication/retrospective-analysis-of-the-2019-minerl-competition-on-sample-efficient-reinforcement-learning/\">last year’s MineRL competition</a>, the sample-efficient reinforcement learning challenge is back and even better thanks to an additional dataset, structural changes that we see contributing to a more robust and wider-appealing contest, and the addition of DeepMind and OpenAI to the organizing team. <a href=\"https://www.aicrowd.com/challenges/neurips-2020-minerl-competition\">MineRL 2020</a> is the fourth competition based on <a href=\"https://www.microsoft.com/en-us/research/project/project-malmo/\">Project Malmo</a>, an experimentation platform using Minecraft to advance AI. MineRL, the brainchild of a team of researchers from Carnegie Mellon University, tackles an ambitious problem facing the machine learning community: an increasing demand for large amounts of computational resources to replicate state-of-the-art research. Solving this challenge is key to making AI more accessible. To encourage the kind of efficiency in the RL space that will help make that possible, MineRL participants are limited in the amount of data they can use and time they can spend training an agent to complete the competition task of mining a diamond—no more than 8 million samples over four days or less using a single GPU machine.</p>\n\n\n\n<p>MineRL 2020, hosted and supported again by the competition platform AIcrowd, is part of the competition track at this year’s Conference on Neural Information Processing Systems (<a href=\"https://nips.cc/Conferences/2020/\">NeurIPS 2020</a>). Last year, the competition was also included in the conference lineup. Over 1,000 participants registered, and more than 50 people attended the affiliated NeurIPS workshop, during which the top teams presented their creative approaches. Microsoft is delighted to once again be among the organizers of a competition that is truly a result of great teamwork.</p>\n\n\n\n<figure class=\"wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\"><div class=\"wp-block-embed__wrapper\">\n<iframe title=\"MineRL Competition 2020\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/fsbskvwvEBc?feature=oembed&rel=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div></figure>\n\n\n\n<p></p>\n\n\n\n<h3>Leveling the playing field</h3>\n\n\n\n<p><p>At its core, the MineRL competition is about lowering the barrier to entry, encouraging the research community to devise solutions that don’t require the increasing amounts of samples and resources currently needed, which is what drew CMU PhD student <a href=\"https://stephmilani.github.io/\">Stephanie Milani</a> to the organizing committee last year. Despite having played Minecraft and used Project Malmo as a research tool, Milani didn’t participate in either of the first two Malmo competitions. For someone relatively new to ML research like herself, the challenges felt too “daunting,” she said. \n\n\nNot so with MineRL, which brings together reinforcement and imitation learning with a large-scale dataset of human demonstrations. She had actually been involved in developing the dataset, helping with revisions to the dataset paper and contributing samples. When she heard there might be a competition around the dataset, she knew she wanted to be involved with organizing it. She was intrigued by its potential to promote sample efficiency, to allow people with limited resources to break in to machine learning, and to help democratize AI. “Groups with access to massive computational resources can train their learning algorithms for thousands of years on the desired task; the average person cannot do that,” she said. “Constraining the computational resources available to train the submitted algorithms is one step toward leveling the playing field: Everyone’s algorithm is evaluated using the same number of environment interactions and computational resources.”</p>\n\n<table style=\"float: right; width: 50%; margin: 15px; text-align: center; border: 1px solid #000000; border-collapse: collapse; border-spacing: inherit;\">\n<tbody>\n<tr style=\"height: 24px;\">\n<td style=\"background-color: #000000; padding: 5px 30px; border: inherit; height: 24px;\"><span style=\"color: #ffffff;\"><strong>MineRL 2020: Sign up and submit an agent</strong></span></td>\n</tr>\n<tr style=\"height: 23px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 23px;\">The submission period for the MineRL 2020 competition is open. Using a single GPU machine and no more than 8 million samples, train an agent to mine a diamond in Minecraft in four days or less.</td>\n</tr>\n<tr style=\"height: 46px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 46px;\"><strong> Getting started: </strong> Visit the <a href=\"https://www.aicrowd.com/challenges/neurips-2020-minerl-competition\">MineRL competition page on AIcrowd</a> to download the starter kit and participate.</td>\n</tr>\n<tr style=\"height: 46px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 46px;\"><strong> Important dates: </strong> Round 1 submissions are being accepted until Sept. 30, when organizers will evaluate the agents. Round 2 submissions from Round 1 finalists will be accepted from September through November. Results will be posted in November-December. Winning teams will present their agents at <a href=\"https://nips.cc/Conferences/2020/\">NeurIPS 2020</a> on Dec. 6.</td>\n</tr>\n<tr style=\"height: 46px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 46px;\"><strong> Community support: </strong> Exchange ideas and ask questions on the <a href=\"https://www.aicrowd.com/challenges/neurips-2020-minerl-competition/discussion\">competition forum</a> or on the <a href=\"https://discord.com/invite/BT9uegr\">MineRL Discord server</a> to contribute.</td>\n</tr>\n</tbody>\n</table></p>\n\n\n\n<p>Lead organizer <a href=\"http://wguss.ml/\">William Guss</a>, a CMU PhD student and research scientist at OpenAI, describes the benefits of lowering the barrier to entry as twofold: From a social justice perspective, it helps ensure the engineering and benefits of AI aren’t concentrated among only those with access to large amounts of resources. From a science perspective, it means more diverse solutions. “We often see in science that the best innovations come from left field; those who can see the field from a higher purview than just recombining old ideas,” said Guss.</p>\n\n\n\n<p>Last year, the competition comprised a “demonstrations and environment” track in which participants were able to train their agent using the human demonstrations and 8 million Minecraft interactions. This year, the addition of a second track—human demonstrations only—not only addresses broader research interests but also effectively makes machine learning more accessible. As Guss explains, making a widely available dataset allows anyone who has access to an internet connection to leverage just the dataset without having to re-simulate an environment, which can be expensive. (Returning participants will also notice another change—the introduction of action and observation obfuscation, a technique through which the semantic mechanisms of the game are hidden using an autoencoder. The change—motivated by the use of hierarchal RL in last year’s submissions—is designed to encourage domain-agnostic solutions.)</p>\n\n\n\n<h3>Robust baselines and plenty of quality data</h3>\n\n\n\n<p>A key challenge in making competitions more accessible to people with different levels of interest, expertise, and resource access is the preparation of a good set of baselines they can use to ramp up the task and environment and leverage in their solutions. That’s actually what we learned in the previous <a href=\"https://www.microsoft.com/en-us/research/project/project-malmo/#!competitions\">Project Malmo–based competitions</a>. Preferred Networks (PFN), a tech startup Microsoft has been <a href=\"https://docs.microsoft.com/en-us/archive/blogs/stevengu/decode-2017-in-tokyo-japan\">collaborating with for years to make deep learning technologies more easily available</a>, joined the organizing committee last year to help on that front.</p>\n\n\n\n<p>The company’s intensive work resulted in an <a href=\"https://github.com/minerllabs/baselines/tree/master/general/chainerrl\">extensive set of excellent baselines</a>, which utilized its deep learning framework Chainer and included behavioral cloning, Deep Q-learning from Demonstrations (DQfD), Rainbow, and proximal policy optimization (PPO). These baselines were well received by participants; about 40% of the entries used Chainer code in their submissions. Some of the algorithms PFN made baselines for hadn’t been replicated before. This increased the visibility of those algorithms, as well as the number of algorithms included in PFN’s ChainerRL deep reinforcement learning library, resulting in a common resource for the research community. Developing the baselines required PFN to become an early tester of the MineRL competition platform, and the company worked closely with the CMU team and AIcrowd to validate and improve the platform and dataset. Milani and Guss both describe the company’s involvement and contributions as crucial to the success and accessibility of the competition. We’re lucky to have PFN return as part of the organizing committee in 2020; the company will again be preparing baselines, making adjustments to accommodate the observation obfuscation element of the competition.</p>\n\n\n\n<p><a href=\"https://www.microsoft.com/en-us/research/wp-admin/edit.php?post_type=post\"></a></p>\n\n\n<table style=\"float: right; width: 50%; margin: 15px; text-align: center; border: 1px solid #000000; border-collapse: collapse; border-spacing: inherit;\">\n<tbody>\n<tr style=\"height: 24px;\">\n<td style=\"background-color: #000000; padding: 5px 30px; border: inherit; height: 24px;\"><span style=\"color: #ffffff;\"><strong>Play Minecraft for science!</strong></span></td>\n</tr>\n</tbody>\n</table>\n<table style=\"float: right; width: 50%; margin: 15px; text-align: center; border: 1px solid #000000; border-collapse: collapse; border-spacing: inherit;\">\n<tbody>\n<tr style=\"height: 46px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 46px;\">The open 3D world of Minecraft is ideal for training agents on specific tasks, as well as general problem-solving. Thanks to the Minecraft community, the MineRL-v0 dataset has more than 60 million human demonstrations of key tasks for agents to learn from. Visit the free public <a href=\"https://minerl.io/play/\">MineRL Server for data collection</a> to contribute.</td>\n</tr>\n</tbody>\n</table>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>The resource making the contest possible—the data—has undergone a change of its own for the 2020 competition. Last year, the CMU team released <a href=\"https://minerl.io/dataset\">MineRL-v0</a>. Built using a novel end-to-end platform for recording and automatically labeling samples, the dataset consists of more than 60 million frames of human demonstrations isolating four classes of structured, goal-based Minecraft tasks, most of which are required to mine a diamond. This year, the competition is also providing a “survival” dataset. It comprises millions of frames of human players freely exploring and interacting with Minecraft to accomplish whatever unique goals they’ve set, an indicator of the CMU team’s vision of moving the competition toward more general problem-solving.</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:heading {\"level\":3} --></p>\n<p><!-- /wp:heading --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:core-embed/twitter --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:heading {\"level\":3} --></p>\n<p><!-- /wp:heading --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:heading {\"level\":3} --></p>\n<h3>Competition diamonds: Compelling research contributions and organizer growth</h3>\n<p><!-- /wp:heading --></p>\n<p><!-- wp:paragraph --></p>\n<p>Last year’s competition was delivered quite successfully. We received great coverage and had a strong turnout in participation and workshop attendance, and the submissions were impressive, including the use of a discriminator soft actor critic and a hierarchical Deep Q-Network. The response reinforces how effective competitions can be in bringing together the expertise of academia, industry, and the larger research community to move research forward. In our case, it also attracted the insights of those outside of tech—Minecraft fans with no ML background. And the value extends beyond the sample-efficient RL solutions submitted.</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:core-embed/twitter {\"url\":\"https://twitter.com/wgussml/status/1205545508254748672?s=20\",\"type\":\"rich\",\"providerNameSlug\":\"twitter\",\"className\":\"\"} --></p>\n<figure class=\"wp-block-embed-twitter wp-block-embed is-type-rich is-provider-twitter\">\n<div class=\"wp-block-embed__wrapper\">\n<blockquote class=\"twitter-tweet\" data-width=\"500\" data-dnt=\"true\"><p lang=\"en\" dir=\"ltr\">Wow, I am so excited! The <a href=\"https://twitter.com/hashtag/MineRL?src=hash&ref_src=twsrc%5Etfw\">#MineRL</a> competition was featured in The Verge! <br><br>We&#39;ve come a long way towards Minecraft AI in the past 2 years, but this competition is a great reminder that there are so many more challenges to solve!<br><br>Read More: <a href=\"https://t.co/C504t4pRt8\">https://t.co/C504t4pRt8</a> <a href=\"https://t.co/7LJ6pfPhKv\">https://t.co/7LJ6pfPhKv</a></p>&#8212; william (@wgussml) <a href=\"https://twitter.com/wgussml/status/1205545508254748672?ref_src=twsrc%5Etfw\">December 13, 2019</a></blockquote><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n</div>\n</figure>\n<p><!-- /wp:core-embed/twitter --></p>\n<p><!-- wp:paragraph --></p>\n<p>The research community benefits from a growing and comprehensive dataset of quality human priors, a library of deep learning baselines, and a framework that can be used to explore new challenges, like multi-agent coordination, even after the submission period closes. As the competition and research evolves, I also see growth on the parts of members of the organizing committee. PFN credits making last year’s baselines for accelerating the development of ChainerRL and has since <a href=\"https://preferred.jp/en/news/pr20191205/\">announced it is migrating its deep learning research platform</a> from Chainer to the widely used PyTorch framework, a move the company believes will take it in a more exciting direction in serving the research community. PFN will be <a href=\"https://preferred.jp/en/news/pr20200730/\">using this newly released deep RL library for PyTorch users, PFRL</a>, to implement the competition baselines. And since the 2019 contest, Guss and organizer Brandon Houghton have both moved on to opportunities to extend their research agenda in industry. I’m very glad to know the competition and the platform helped these committee members further develop their impact and their careers, respectively.</p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>The Minecraft diamond may still be up for grabs, but as far as I’m concerned, the MineRL competition has already unearthed some gems, and my Microsoft Research colleagues and I feel privileged to be a part of this fantastic competition.</p>\n<p><!-- /wp:paragraph --></p>\n<p><strong>The MineRL competition organizing team</strong></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>William H. Guss, OpenAI and Carnegie Mellon University<br>Brandon Houghton, OpenAI and Carnegie Mellon University<br>Stephanie Milani, Carnegie Mellon University<br>Nicholay Topin, Carnegie Mellon University<br>Ruslan Salakhutdinov, Carnegie Mellon University<br>John Schulman, OpenAI<br>Mario Ynocente Castro, Preferred Networks<br>Crissman Loomis, Preferred Networks<br>Keisuke Nakata, Preferred Networks<br>Shinya Shiroshita, Preferred Networks<br>Avinash Ummadisingu, Preferred Networks<br>Sharada Mohanty, AIcrowd<br>Sam Devlin, Microsoft Research<br>Noboru Sean Kuno, Microsoft Research<br>Oriol Vinyals, DeepMind</p>\n<p><strong>The MineRL competition advisory committee</strong></p>\n<p><!-- /wp:paragraph --></p>\n<p><!-- wp:paragraph --></p>\n<p>Fei Fang, Carnegie Mellon University<br>Zachary Chase Lipton, Carnegie Mellon University<br>Manuela Veloso, Carnegie Mellon University and JPMorgan Chase<br>David Ha, Google Brain<br>Chelsea Finn, Google Brain and UC Berkeley<br>Anca Dragan, UC Berkeley<br>Sergey Levine, UC Berkeley</p>\n<p></p><p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/minerl-sample-efficient-reinforcement-learning-challenge-back-for-a-second-year-benefits-organizers-as-well-as-larger-research-community/\">MineRL sample-efficient reinforcement learning challenge—back for a second year—benefits organizers, as well as larger research community</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Thu, 20 Aug 2020 21:48:48 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1024x576.png","linkMd5":"472191d91b1c75b6e03cfa1008781f42","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn84@2020_3/2020/08/24/21-39-24-082_87008072687d8d5a.webp","destWidth":1024,"destHeight":576,"sourceBytes":370379,"destBytes":20122,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1024x576.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn84@2020_3/2020/08/24/21-39-24-082_87008072687d8d5a.webp"},"publishedOrCreatedDate":1598305163712},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"Toward trusted sensing for the cloud: Introducing Project Freta","link":"https://www.microsoft.com/en-us/research/?p=670932","description":"<p><img class=\"aligncenter wp-image-671769 size-full\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2.png\" alt=\"Representative image shows a light source directing light at a square which reveals lines of binary code some of which are green and some of which are red. \" width=\"5834\" height=\"3284\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2.png 5834w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-1536x865.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-2048x1153.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2-1920x1080.png 1920w\" sizes=\"(max-width: 5834px) 100vw, 5834px\" /></p> \n<h4></h4> \n<blockquote> \n <h4><em>“Sunlight is said to be the best of disinfectants.”</em></h4> \n <h4>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;―Louis D. Brandeis, 1914</h4> \n</blockquote> \n<p>We often think about the field of computer security as a field of walls and barriers that keep intruders out. With Project Freta, we invite readers to think not of walls but of sunlight. When attackers build malware that cannot be detected, they gain enormous economic value. Undetected malware can be continuously re-used: it is never part of attack reporting, never summons incident responders, and never alerts the victim to a data theft event. The economics of reuse can justify enormous attacker investment in malware non-discoverability. Conversely, once a malware strain is discovered, its value plummets in tandem with its reusability. In this stealth economy, that which hides in darkness is removed with sunlight. The question for defenders, then, is how can we raise the cost of non-discovery? Is there a point beyond which a class of malware is no longer economically viable?</p> \n<table style=\"float: right; width:50%; margin: 15px; text-align: center; border: 1px solid #000000; border-collapse: collapse; border-spacing: inherit;\"> \n <tbody> \n  <tr style=\"height: 24px;\"> \n   <td style=\"background-color: #000000; padding: 5px 30px; border: inherit; height: 24px;\"><span style=\"color: #ffffff;\"><strong>Quick Start</strong></span></td> \n  </tr> \n  <tr style=\"height: 23px;\"> \n   <td style=\"padding: 5px 30px; border: inherit; height: 23px;\"><strong>Project Freta:</strong> free service from Microsoft Research for detecting evidence of OS and sensor sabotage, such as rootkits and advanced malware, in memory snapshots of live Linux systems</td> \n  </tr> \n  <tr style=\"height: 46px;\"> \n   <td style=\"padding: 5px 30px; border: inherit; height: 46px;\"><a href=\"https://freta.azurewebsites.net\">Access Project Freta Portal</a><br /> (connect with any AAD or Microsoft Account)</td> \n  </tr> \n  <tr style=\"height: 23px;\"> \n   <td style=\"padding: 5px 30px; border: inherit; height: 23px;\"><a href=\"https://docs.microsoft.com/security/research/project-freta\">Documentation</a> | <a href=\"mailto:project-freta@microsoft.com\">Questions or Feedback?</a></td> \n  </tr> \n </tbody> \n</table> \n<p>Incubated at Microsoft Research, Project Freta is a roadmap toward trusted sensing for the cloud that can allow enterprises to engage in regular, complete discovery sweeps for undetected malware. The project’s namesake, Warsaw’s Freta Street, was the <a href=\"https://www.bing.com/maps?osid=207f4a10-cacf-477c-9a3b-f070a4008478&amp;cp=52.251633~21.007387&amp;lvl=18&amp;imgid=8878a1c7-8a60-4ab7-adf3-394be45a9708&amp;v=2&amp;sV=2&amp;form=S00027\">birthplace</a> of Marie Curie, a pioneer of <a href=\"https://www.smithsonianmag.com/history/how-marie-curie-brought-x-ray-machines-to-battlefield-180965240/\">battlefield imaging</a>. While snapshot-based memory forensics is a field now in its second decade, no commercial cloud has yet provided customers the ability to perform full memory audits of thousands of virtual machines (VMs) without intrusive capture mechanisms and <em>a priori</em> forensic readiness. Just as yesteryear’s film cameras and today’s smartphones have similar megapixels but vastly different ease of use and availability, Project Freta intends to <em>automate</em> and <em>democratize</em> VM forensics to a point where every user and every enterprise can sweep volatile memory for unknown malware with the push of a button—no setup required.</p> \n<p>The goal of this democratization effort is to increase the development cost of undiscoverable cloud malware toward its theoretical maximum. What would happen if a commercial cloud could <em>guarantee</em> the capture of malware, no matter how expensive or exotic, in volatile memory? Producers of stealthy malware would then be locked into an expensive cycle of complete re-invention, rendering such a cloud an unsuitable place for cyberattacks. This is the future we wish to realize.</p> \n<p>To this end, we propose four properties of trusted sensing to maximize malware discovery and present our technical work along this roadmap to date. As a technology demonstration, Project Freta is opening public access to an analysis portal capable of automatically fingerprinting and auditing a memory snapshot of most cloud-based Linux VMs; over 4,000 kernel versions are supported automatically. Hyper-V checkpoint files captured from a modern enterprise can be searched for everything from cryptominers to advanced kernel rootkits. This prototype previews an exciting future option for cloud consumers: transitioning from boutique forensic consulting services to automated malware discovery built into the bedrock of a commercial cloud.</p> \n<h3>Unbiased data from armored sensors</h3> \n<p>In computer security we strive to be evidence-driven. Unfortunately, using reports of cyberattacks to guide defensive approaches is subject to a well-known yet powerful <a href=\"https://en.wikipedia.org/wiki/Survivorship_bias\">survivor bias</a> that can distort the importance of data and lead us astray.</p> \n<p>Abraham Wald’s work during WWII provided a <a href=\"http://www.ams.org/publicoutreach/feature-column/fc-2016-06\">famously repeated</a> example of survivor bias in a dataset: reports of bullet holes in airplanes. Rather than recommend armor where bullet holes were reported, Wald recommended armor only in areas where there were no reports of bullet holes. This was due to a critical insight about the nature of bullet hole reporting: <em>bullet holes were only counted when an airplane arrived home.</em> Successful attacks removed the attack from the dataset, hence successful attacks could be described only by their absence from the dataset. No reports of bullet attacks on the engines? <em>Armor the engines.</em></p> \n<p><img class=\" wp-image-671109 alignleft\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta-fig-1-_plane-300x229.jpg\" alt=\"\" width=\"437\" height=\"333\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta-fig-1-_plane-300x229.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta-fig-1-_plane-80x60.jpg 80w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta-fig-1-_plane.jpg 603w\" sizes=\"(max-width: 437px) 100vw, 437px\" /></p> \n<p>Today’s computer security sensors suffer from this same bias. When attackers obtain a model of our sensors and design a way to evade these sensors, we receive no reports of cyberattacks. It is tempting to look at areas in which we receive few or no attack reports and think: <em>“I’ve received no reports of successful attacks on my attack reporting capability!”</em> This statement is not as reassuring as it sounds. While we are flooded with billions of reports of malware a year, it is important to understand that synthesizing or mutating existing malware is a <a href=\"https://www.usenix.org/system/files/conference/woot12/woot12-final19.pdf\">nearly cost-free exercise</a>.</p> \n<p>Meanwhile, attackers that manage to strike directly at a security sensor, security agent, or privileged operating system component can disable one step in the attack reporting pipeline and vanish without a trace, leaving only an absence in our attack reporting. The lowest-resourced attackers dominate our datasets; the highly resourced live outside our datasets forever. <em>We must invert this data.</em></p> \n<p>Project Freta was designed and built with survivor bias at its core. It is a security project designed from first principles to drive the cost of sensor evasion as high as possible and in many cases render evasion technically infeasible. To achieve this, in July of 2018 we started with a clean slate.</p> \n<h3>A Greenfield Mandate</h3> \n<p>The fact that sensor evasion is possible at all is surprising to many outsiders to the field of computer security. Anyone who’s visited a retail store understands that it’s a good idea to put the security cameras out of reach—why haven’t we done something similar in the cloud? The answer is a familiar one:<em> backwards compatibility</em>. Our first networked computers didn’t have the silicon real estate to devote to an isolated security sensor. Opportunities for technology companies to break backwards compatibility and “greenfield” redesign with security in mind have appeared only periodically, seen primarily in the mobile and console industries. These redesigns have allowed for increasing hardware separation between the compute plane and the security plane, a detailed topic for platform security journals beyond the scope of this blog post. In today’s cloud, this separation between compute and security planes usually occurs at the hypervisor; tenant workloads are separated from provider workloads via the hypervisor barrier. Is the hypervisor barrier strong enough to prevent sensor evasion? <em>Maybe not.</em></p> \n<p>Recently, microarchitectural flaws and forensics research have called some of the properties of the hypervisor barrier into question. A recent forensics research paper, <a href=\"https://dfrws.org/sites/default/files/session-files/paper_who_watches_the_watcher_detecting_hypervisor_introspection_from_unprivileged_guests.pdf\">“Who Watches The Watcher? Detecting Hypervisor Introspection from Unprivileged Guests”</a> presented at DFRWS in 2018, articulated a method that attackers resident in the compute plane could employ to detect when they are being observed from the security plane—piercing the hypervisor barrier and allowing for pre-emptive self-destruct in order to avoid discovery. This paper came complete with an open-source implementation of the technique. This meant that even <a href=\"https://suif.stanford.edu/papers/vmi-ndss03.pdf\">Virtual Machine Introspection</a> (VMI) endpoint sensors could be evaded with existing open-source software.</p> \n<p>Implementing and democratizing trusted sensing for the cloud meant first articulating the properties of a system that would be immune to these types of attacks:</p> \n<table style=\"border-collapse: collapse; padding: 10px; width: 100%; border-spacing: inherit;\" border=\"1\"> \n <tbody> \n  <tr> \n   <td style=\"width: 100%; padding: 10px; border: 1px solid; text-align: center;\"><strong>Project Freta’s four properties of trusted sensing</strong></td> \n  </tr> \n  <tr> \n   <td style=\"width: 100%; padding: 10px; border: 1px solid;\"> <p><strong>1. Detect. </strong>No program can:</p> <p><em>Detect the presence of a sensor prior to installing itself</em></p></td> \n  </tr> \n  <tr> \n   <td style=\"width: 100%; padding: 10px; border: 1px solid;\"> <p><strong>2. Hide. </strong>No program can:</p> <p><em>Reside in an area out of view of the sensor</em></p></td> \n  </tr> \n  <tr> \n   <td style=\"width: 100%; padding: 10px; border: 1px solid;\"> <p><strong>3. Burn. </strong>No program can:</p> <p><em>Detect operation of the sensor and erase or modify itself prior to acquisition</em></p></td> \n  </tr> \n  <tr> \n   <td style=\"width: 100%; padding: 10px; border: 1px solid;\"> <p><strong>4. Sabotage. </strong>No program can:</p> <p><em>Modify the sensor in a way that can prevent the program’s acquisition</em></p></td> \n  </tr> \n </tbody> \n</table> \n<p>&nbsp;</p> \n<p>To achieve these properties and end the stealthy-malware arms race, incrementally improving existing endpoint technology is insufficient. When attackers and defenders share a microarchitecture, every detection move a defender makes disturbs the environment in a way that is eventually discoverable by an attacker invested in secrecy. The only way to discover such attackers is to remove their insight into defense. This left the question: how much engineering was required to fully automate memory forensics, operate at efficiencies that enabled cloud-scale processing, and still retain the element of surprise?</p> \n<h3>Building Project Freta</h3> \n<p>A brief technology evaluation taught the team that starting from scratch was the only viable approach. To leave no place to <strong>hide</strong>, we needed to accept the huge data footprint imposed by whole-system memory analysis. To address <strong>detection</strong> and <strong>burn</strong>, such as “<a href=\"https://www.usenix.org/system/files/conference/woot14/woot14-ho.pdf\">red pill attacks</a>,” we needed two components: 1) an offline analysis system that could operate in batch mode and 2) a sensor that could provide whole-system memory captures without executing a single clarifying instruction on the guest. Finally, to mitigate <strong>sabotage</strong> we needed to ensure our system was built from the ground up to be memory safe.</p> \n<p>For Project Freta, many of these challenges compounded to make “instant forensics for everyone” a daunting task:</p> \n<ul> \n <li><strong>Untouchable Images:</strong> Many existing forensic approaches execute clarifying instructions on the guest, such as copying KASLR keys. Unfortunately, these instructions can tip off malware to a capture event. The requirement not to interact with the target OS, needed to ensure the element of surprise, mandated a forensic imaging technology that was completely “blind.” As a consequence, memory scrambled by security mechanisms such as ASLR needed to be decoded without keys or context. This task is complex enough for one operating system, and it’s a templating nightmare to support <em>any</em> operating system. Project Freta now supports over 4,000 Linux kernels.</li> \n <li><strong>Universal OS Support: </strong>The long-standing forensics requirement that information about the operating system be arrived at <em>a priori</em> needed to be removed. This meant quickly fingerprinting any operating system in the cloud given only a scrambled memory image. We knew from the beginning that, given private symbols, this could be achieved for Windows in a believable way. So, we chose Linux instead, knowing that the large number of publicly available kernels for Linux would make this problem significantly more difficult. It also meant that a functional result would pay down the technology debt required to build faith in the approach. With Linux behind us, Windows support is on our roadmap.</li> \n <li><strong>Cloud Scale: </strong>Automated capture and analysis won’t matter to customers if a day of cloud compute is needed to perform a single audit. To operate on modern cloud enterprises, we knew that the ability to programmatically audit 100,000 machines in a short, cost-bounded timeframe was a minimum requirement. This meant architecting from the beginning for batch processing in the cloud, including OS fingerprinting in the performance requirements, and thinking ahead about edge cases such as high-performance-compute VMs with 100+ gigabytes of RAM.</li> \n <li><strong>Memory Safety:</strong> We knew that any system designed to hunt for tools fielded by the most well-resourced attackers would itself become a target. Given the history and preponderance of memory-corruption exploits, we made the choice as a team to embrace Rust at the beginning, architecting the entire capability from scratch in Rust from line one and building upon no existing software. This has yielded a high-performance analysis engine for memory images of arbitrary size that <em>also</em> has memory safety properties built in.</li> \n</ul> \n<h3>Project Freta: This release</h3> \n<p>The Project Freta analysis engine consumes snapshots of whole-system Linux volatile memory and extracts an enumeration of system objects. Some kernel hooking identification is performed automatically; this can be used by analysts to detect novel rootkits. The analysis portal is available in prototype form for public use: <a href=\"https://freta.azurewebsites.net\">https://freta.azurewebsites.net.</a></p> \n<p>The prototype portal supports many types of memory snapshots as inputs. Currently, only a Hyper-V checkpoint has been evaluated to provide a reasonable approximation of the “element of surprise” necessary to achieve trusted sensing:</p> \n<ul> \n <li>Use the Hyper-V checkpoint feature to produce a VMRS file</li> \n <li>Convert a VMWare snapshot to produce a CORE file</li> \n <li>Extract memory from within a running system using AVML</li> \n <li>Extract memory from within a running system using LiME</li> \n</ul> \n<p>Once the snapshot is uploaded to the portal and analyzed, the report data is made available via the portal and both REST and Python APIs. Project Freta’s initial release supports API-driven automated use.</p> \n<p><img class=\"aligncenter wp-image-671844 size-full\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta_Rootkits_Figure_UpdatedV.jpg\" alt=\"\" width=\"927\" height=\"531\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta_Rootkits_Figure_UpdatedV.jpg 927w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta_Rootkits_Figure_UpdatedV-300x172.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta_Rootkits_Figure_UpdatedV-768x440.jpg 768w\" sizes=\"(max-width: 927px) 100vw, 927px\" /></p> \n<p>The report contains an enumeration of system objects over the interval during which the sample was taken:</p> \n<ul> \n <li>Global values and addresses</li> \n <li>Debugged processes</li> \n <li>In-memory files</li> \n <li>Kernel interrupt table</li> \n <li>Kernel modules</li> \n <li>Kernel syscall table</li> \n <li>Networks</li> \n <li>Open files</li> \n <li>ARP table (arp)</li> \n <li>Open sockets</li> \n <li>Processes</li> \n <li>Unix sockets (lsof)</li> \n</ul> \n<p>Of interest to defenders: debugging relationships are provided to allow for investigation of counter-debugging techniques; library imports are listed to allow for investigation of LD_PRELOAD based attacks; and simple hooking of systems calls is detected and mapped.</p> \n<p>For further info, please visit our <a href=\"https://docs.microsoft.com/security/research/project-freta\">documentation</a>.</p> \n<h3>Project Freta: Futures</h3> \n<p>Project Freta’s second component for achieving trusted sensing is a sensor built for Azure that allows operators to migrate the volatile memory of live virtual machines to an offline analysis environment without disrupting execution. Completed in the winter of 2019, this sensor capability is currently only available to Microsoft researchers and is not fielded to any of our commercial clouds—executive briefings and demos are available. This sensor, coupled with the Freta analysis environment, demonstrates a path to cheap, automated memory forensic audits of large enterprises (10,000+ VMs).</p> \n<p>A great deal of development lies ahead for Project Freta: adding support for Windows, extending our automated program analysis capabilities, and experimenting with AI-based decision-making for novel threat detection. For now, we are opening access to the analysis portal for customer use and experimentation. We hope that Project Freta empowers administrators and responders and is used globally as it has been used at Microsoft: to hunt advanced intruders and their toolkits. We welcome your feedback at <a href=\"mailto:project-freta@microsoft.com\">project-freta@microsoft.com</a>.</p> \n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/toward-trusted-sensing-for-the-cloud-introducing-project-freta/\">Toward trusted sensing for the cloud: Introducing Project Freta</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>","descriptionType":"html","publishedDate":"Mon, 06 Jul 2020 18:15:30 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2.png","linkMd5":"c269c53b18a032812af6dc89fa7ea483","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn88@2020_1/2020/08/24/21-39-24-773_73b2efeb2df2b752.webp","destWidth":5834,"destHeight":3284,"sourceBytes":2456048,"destBytes":492794,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn88@2020_1/2020/08/24/21-39-24-773_73b2efeb2df2b752.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta-fig-1-_plane-300x229.jpg":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn40@2020_6/2020/08/24/21-40-53-126_c45861889622432b.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta_Rootkits_Figure_UpdatedV.jpg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_3/2020/08/24/21-40-52-964_d6fbdd8500e6a51c.webp"},"publishedOrCreatedDate":1598305163714},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"Three new reinforcement learning methods aim to improve AI in gaming and beyond","link":"https://www.microsoft.com/en-us/research/?p=677052","description":"\n<p>Reinforcement learning (RL) provides exciting opportunities for game development, as highlighted in our recently announced <a href=\"https://aka.ms/ProjectPaidia\">Project Paidia</a>—a research collaboration between our <a href=\"https://www.microsoft.com/en-us/research/theme/game-intelligence/\">Game Intelligence</a> group at Microsoft Research Cambridge and game developer Ninja Theory. In Project Paidia, we push the state of the art in reinforcement learning to enable new game experiences. In particular, we focus on developing game agents that learn to genuinely collaborate in teams with human players. In this blog post we showcase three of our recent research results that are motivated by these research goals. We give an overview of key insights and explain how they could lead to <a href=\"https://aka.ms/InnProjectPaidia\">AI innovations</a> in modern video game development and other real-world applications.</p>\n\n\n\n<p>Reinforcement learning can give game developers the ability to craft much more nuanced game characters than traditional approaches, by providing a reward signal that specifies high-level goals while letting the game character work out optimal strategies for achieving high rewards in a data-driven behavior that organically emerges from interactions with the game. To learn how you can use RL to develop your own agents for gaming and begin writing training scripts, <a href=\"https://developer.microsoft.com/en-us/games/blog/supercharge-games-with-azure-ai-and-reinforcement-learning/\">check out this Game Stack Live blog post</a>. Getting started with reinforcement learning is easier than you think—Microsoft Azure also offers tools and resources, including <a href=\"https://azure.microsoft.com/\">Azure Machine Learning</a>, which provides RL training environments, libraries, virtual machines, and more.</p>\n\n\n\n<p>The key challenges our research addresses are how to make reinforcement learning efficient and reliable for game developers (for example, by combining it with uncertainty estimation and imitation), how to construct deep learning architectures that give agents the right abilities (such as long-term memory), and how to enable agents that can rapidly adapt to new game situations. Below, we highlight our latest research progress in these three areas.</p>\n\n\n\n<h3>Highlight 1: More accurate uncertainty estimates in deep learning decision-making systems</h3>\n\n\n\n<p>From computer vision to reinforcement learning and machine translation, deep learning is everywhere and achieves state-of-the-art results on many problems. We give it a dataset, and it gives us a prediction based on a deep learning model’s best guess. The success of deep learning means that it is increasingly being applied in settings where the predictions have far-reaching consequences and mistakes can be costly. &#160;</p>\n\n\n\n<div class=\"annotations\" data-bi-area=\"margin-callout\">\n\t<ul class=\"annotations__list annotations__list--right\">\n\t\t<li class=\"annotations__list-item\">\n\t\t\t\t\t\t<span class=\"annotations__type\">Publication</span>\n\t\t\t<a href=\"https://www.microsoft.com/en-us/research/publication/conservative-uncertainty-estimation-by-fitting-prior-networks/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Conservative Uncertainty Estimation By Fitting Prior Networks\">\n\t\t\t\tConservative Uncertainty Estimation By Fitting Prior Networks\t\t\t</a>\n\t\t\t<span class=\"svg-icon icon-chevron-right-med-blue\"></span>\n\t\t\t\t\t</li>\n\t</ul>\n</div>\n\n\n\n<p>The problem is that the best-guess approach taken by most deep learning models isn’t enough in these cases. Instead, we want a technique that provides us not just with a prediction but also the associated degree of certainty. Our <a href=\"https://iclr.cc/Conferences/2020\">ICLR 2020</a> paper, “<a href=\"https://www.microsoft.com/en-us/research/publication/conservative-uncertainty-estimation-by-fitting-prior-networks/\">Conservative Uncertainty Estimation By Fitting Prior Networks</a>,” explores exactly that—we describe a way of knowing what we don’t know about predictions of a given deep learning model. This work was conducted by <a href=\"https://www.microsoft.com/en-us/research/people/kaciosek/\">Kamil Ciosek</a>, <a href=\"https://bmi.inf.ethz.ch/people/person/vincent-fortuin/\">Vincent Fortuin</a>, <a href=\"https://www.microsoft.com/en-us/research/people/ryoto/\">Ryota Tomioka</a>, <a href=\"https://www.microsoft.com/en-us/research/people/kahofman/\">Katja Hofmann</a>, and <a href=\"http://www.eng.cam.ac.uk/profiles/ret26\">Richard Turner</a>. </p>\n\n\n\n<p>In more technical terms, we provide an analysis of Random Network Distillation (RND), a successful technique for estimating the confidence of a deep learning model.</p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-1024x693.png\" alt=\"\" class=\"wp-image-678237\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-1024x693.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-300x203.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-768x520.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1.png 1037w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 1: Predictor (green) and prior (red) agree on seen data (left), disagree on unseen data (right). Our uncertainty estimate at a point is defined as the gap between prior and predictor.</figcaption></figure></div>\n\n\n\n<p>The version of RND we analyze maintains an uncertainty model separate from the model making predictions. To provide a bit more intuition about how the uncertainty model works, let’s have a look at the Figure 1 above. We have two types of neural networks: the predictor (green) and the prior (red). The prior network is fixed and does not change during training. When we see a new data point, we train the predictor to match the prior on that point. In the figure, the data points we have observed are represented with red dots. We can see that close to the points, the predictor and the prior overlap. On the other hand, we see a huge gap between the predictor and prior if we look at the values to the right, far from the observed points.</p>\n\n\n\n<p>Roughly speaking, theoretical results in the paper show that the gap between prior and predictor is a good indication of how certain the model should be about its outputs. Indeed, we compare the obtained uncertainty estimates to the gold standard in uncertainty quantification—the posterior obtained by Bayesian inference—and show they have two attractive theoretical properties. First, the variance returned by RND always overestimates the Bayesian posterior variance. This means that while RND can return uncertainties larger than necessary, it won’t become overconfident. Second, we show that the uncertainties concentrate, that is they eventually become small after the model has been trained on multiple observations. &#160;In other words, the model becomes more certain about its predictions as we see more and more data.</p>\n\n\n\n<h3>Highlight 2: Utilizing order-invariant aggregators to enhance agent recall</h3>\n\n\n\n<p>In many games, players have partial observability of the world around them. To act in these games requires players to recall items, locations, and other players that are currently out of sight but have been seen earlier in the game. Typically, deep reinforcement learning agents have handled this by <a href=\"https://arxiv.org/abs/1507.06527\">incorporating recurrent layers</a> (such as LSTMs or GRUs) or the ability to read and write to external memory as in the case of <a href=\"https://www.nature.com/articles/nature20101\">differential neural computers</a> (DNCs).</p>\n\n\n\n<div class=\"annotations\" data-bi-area=\"margin-callout\">\n\t<ul class=\"annotations__list annotations__list--left\">\n\t\t<li class=\"annotations__list-item\">\n\t\t\t\t\t\t<span class=\"annotations__type\">Publication </span>\n\t\t\t<a href=\"https://www.microsoft.com/en-us/research/publication/amrl-aggregated-memory-for-reinforcement-learning/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"AMRL: Aggregated Memory For Reinforcement Learning\">\n\t\t\t\tAMRL: Aggregated Memory For Reinforcement Learning\t\t\t</a>\n\t\t\t<span class=\"svg-icon icon-chevron-right-med-blue\"></span>\n\t\t\t\t\t</li>\n\t</ul>\n</div>\n\n\n\n<p>Using recurrent layers to recall earlier observations was common in natural language processing, where the sequence of words is often important to their interpretation. However, when agents interact with a gaming environment, they can influence the order in which they observe their surroundings, which may be irrelevant to how they <em>should</em> act. To give a human-equivalent example, if I see a fire exit when moving through a new building, I may need to later recall where it was regardless of what I have seen or done since. In our ICLR 2020 paper “<a href=\"https://www.microsoft.com/en-us/research/publication/amrl-aggregated-memory-for-reinforcement-learning/\">AMRL: Aggregated Memory For Reinforcement Learning</a>,” we propose the use of order-invariant aggregators (the sum or max of values seen so far) in the agent’s policy network to overcome this issue.</p>\n\n\n\n<figure class=\"wp-block-image alignwide size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687-1024x363.png\" alt=\"\" class=\"wp-image-678252\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687-1024x363.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687-300x106.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687-768x272.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687.png 1100w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 2: Model architectures. From left to right, LSTM, DNC, SET, and AMRL. AMRL extends LSTMs with SET-based aggregators (for example average or max value observed).</figcaption></figure>\n\n\n\n<p>While approaches that enable the ability to read and write to external memory (such as DNCs) can also learn to directly recall earlier observations, the complexity of their architecture is shown to require significantly more samples of interactions with the environment, which can prevent them from learning a high-performing policy within a fixed compute budget.</p>\n\n\n\n<p>In our experiments, our Minecraft-playing agents were shown either a red or green cube at the start of an episode that told them how they must act at the end of the episode. In the time between seeing the green or red cube, the agents could move freely through the environment, which could create variable-length sequences of irrelevant observations that could distract the agent and make them forget the color of the cube at the beginning.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-1024x599.png\" alt=\"\" class=\"wp-image-680994\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-1024x599.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-300x175.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-768x449.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-1536x898.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-2048x1197.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-480x280.png 480w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 3: A top-down view of the Minecraft maze that tests an agent’s memory (bottom) and a sample of observations an agent may see whilst moving through this environment (top).</figcaption></figure>\n\n\n\n<p>By combining recurrent layers with order-invariant aggregators, AMRL can both infer hidden features of the state from the sequence of recent observations and recall past observations regardless of when they were seen. Enabling our agents, to efficiently recall the color of the cube and make the right decision at the end of the episode. Now empowered with this new ability, our agents can play more complex games or even be deployed in non-gaming applications where agents must recall distant memories in partially observable environments.</p>\n\n\n\n<p>Researchers who contributed to this work include <a href=\"https://www.jakebeck.com/\">Jacob Beck</a>, <a href=\"https://www.microsoft.com/en-us/research/people/kaciosek/\">Kamil Ciosek</a>, <a href=\"https://www.microsoft.com/en-us/research/people/sadevlin/\">Sam Devlin</a>, <a href=\"https://www.tschiatschek.net/\">Sebastian Tschiatschek</a>, <a href=\"https://www.microsoft.com/en-us/research/people/chezha/\">Cheng Zhang</a>, and <a href=\"https://www.microsoft.com/en-us/research/people/kahofman/\">Katja Hofmann</a>.</p>\n\n\n\n<h3>Highlight 3: VariBAD—exploring unknown environments with Bayes-Adaptive Deep RL and meta-learning</h3>\n\n\n\n<p>Most current reinforcement learning work, and the majority of RL agents trained for video game applications, are optimized for a single game scenario. However, a key aspect of human-like gameplay is the ability to continuously learn and adapt to new challenges. In our joint work with Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, and Shimon Whiteson from the University of Oxford, we developed a flexible new approach that enables agents to learn to explore and rapidly adapt to a given task or scenario.</p>\n\n\n\n<div class=\"annotations\" data-bi-area=\"margin-callout\">\n\t<ul class=\"annotations__list annotations__list--right\">\n\t\t<li class=\"annotations__list-item\">\n\t\t\t\t\t\t<span class=\"annotations__type\">Publication</span>\n\t\t\t<a href=\"https://www.microsoft.com/en-us/research/publication/varibad-a-very-good-method-for-bayes-adaptive-deep-rl-via-meta-learning/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning\">\n\t\t\t\tVariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning\t\t\t</a>\n\t\t\t<span class=\"svg-icon icon-chevron-right-med-blue\"></span>\n\t\t\t\t\t</li>\n\t</ul>\n</div>\n\n\n\n<p>In “<a href=\"https://www.microsoft.com/en-us/research/publication/varibad-a-very-good-method-for-bayes-adaptive-deep-rl-via-meta-learning/\">VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning</a>,” we focus on problems that can be formalized as so-called Bayes-Adaptive Markov Decision Processes. Briefly, in this setting an agent learns to interact with a wide range of tasks and learns how to infer the current task at hand as quickly as possible. Our goal is to train Bayes-optimal agents—agents that behave optimally given their current belief over tasks. For example, imagine an agent trained to reach a variety of goal positions. At the beginning of each new episode, the agent is uncertain about the goal position it should aim to reach. A Bayes-optimal agent takes the optimal number of steps to reduce its uncertainty and reach the correct goal position, given its initial belief over possible goals.</p>\n\n\n\n<p>Our new approach introduces a flexible encoder-decoder architecture to model the agent’s belief distribution and learns to act optimally by conditioning its policy on the current belief. We demonstrate that this leads to a powerful and flexible solution that achieves Bayes-optimal behavior on several research tasks. In our ongoing research we investigate how approaches like these can enable game agents that rapidly adapt to new game situations.</p>\n\n\n\n<figure class=\"wp-block-image alignwide size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-1024x430.png\" alt=\"\" class=\"wp-image-678261\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-1024x430.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-300x126.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-768x323.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-665x280.png 665w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4.png 1381w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 4: Illustration of different exploration strategies. (a) Environment: The agent starts at the bottom left. There is a goal somewhere in the grey area, unknown to the agent. (b) A Bayes-optimal exploration strategy that systematically searches possible grid cells to find the goal, shown in solid (interactions so far) and dashed (future interactions) blue lines. A simplified posterior is shown in the background in grey (p = 1/(number of possible goal positions left) of containing the goal) and white (p = 0). (c) Posterior sampling repeatedly samples a possible goal position (red squares) and takes the shortest route there, which is suboptimal. Once the goal is found, every sample matches the true goal position and the agent acts optimally. (d) Exploration strategy learned by variBAD. The grey background represents the approximate posterior the agent has learned.<br></figcaption></figure>\n\n\n\n<h3>Continuing work in game intelligence</h3>\n\n\n\n<p>In this post we have shown just a few of the exciting research directions that we explore within the Game Intelligence theme at Microsoft Research Cambridge and in collaboration with our colleagues at Ninja Theory. A key direction of our research is to create artificial agents that learn to genuinely collaborate with human players, be it in team-based games like Bleeding Edge, or, eventually, in real world applications that go beyond gaming, such as virtual assistants. We view the research results discussed above as key steps towards that goal: by giving agents better ability to detect unfamiliar situations and leverage demonstrations for faster learning, by creating agents that learn to remember longer-term dependencies and consequences from less data, and by allowing agents to very rapidly adapt to new situations or human collaborators. </p>\n\n\n\n<p>To learn more about our work with gaming partners, visit the AI Innovation page. To learn more about our research, and about opportunities for working with us, visit <a href=\"https://www.microsoft.com/en-us/research/theme/game-intelligence/\">aka.ms/gameintelligence</a>.<br></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/three-new-reinforcement-learning-methods-aim-to-improve-ai-in-gaming-and-beyond/\">Three new reinforcement learning methods aim to improve AI in gaming and beyond</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Mon, 03 Aug 2020 14:01:15 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-1024x693.png","linkMd5":"08c6b949e99b41201da1351546bb68f2","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn97@2020_3/2020/08/24/21-39-24-046_f39cd952816cc6e9.webp","destWidth":1024,"destHeight":693,"sourceBytes":70166,"destBytes":22168,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-1024x693.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn97@2020_3/2020/08/24/21-39-24-046_f39cd952816cc6e9.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687-1024x363.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn39@2020_5/2020/08/24/21-40-53-180_c0f86eb838e846b3.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-1024x599.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn8@2020_6/2020/08/24/21-40-52-971_8c2b0d8f0423cc7f.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-1024x430.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn84@2020_5/2020/08/24/21-40-53-194_4a6ed8450116c0ab.webp"},"publishedOrCreatedDate":1598305163714},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"Adversarial robustness as a prior for better transfer learning","link":"https://www.microsoft.com/en-us/research/?p=683157","description":"\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/1400x788_AirSim_noLogo5secs-1.gif\" alt=\"\"/></figure>\n\n\n\n<p><em>Editor’s note: This post and its research are the collaborative efforts of our team, which includes </em><a href=\"http://andrewilyas.com/\"><em>Andrew Ilyas</em></a><em> (PhD Student, MIT), </em><a href=\"http://loganengstrom.com/\"><em>Logan Engstrom</em></a><em> (PhD Student, MIT), </em><a href=\"https://people.csail.mit.edu/madry/\"><em>Aleksander Mądry</em></a><em> (Professor at MIT), </em><a href=\"https://www.microsoft.com/en-us/research/people/akapoor/\"><em>Ashish Kapoor</em></a><em> (Partner Research Manager).</em></p>\n\n\n\n<p>In practical machine learning, it is desirable to be able to transfer learned knowledge from some “source” task to downstream “target” tasks. This is known as transfer learning—a simple and efficient way to obtain performant machine learning models, especially when there is little training data or compute available for solving the target task. Transfer learning is very useful in practice. For example, transfer learning allows <a href=\"https://www.microsoft.com/en-us/research/blog/training-deep-control-policies-for-the-real-world/\">perception models on a robot</a> or other <a href=\"https://www.microsoft.com/en-us/ai/autonomous-systems\">autonomous system</a> to be trained on a synthetic dataset generated via a high-fidelity simulator, such as <a href=\"https://github.com/microsoft/AirSim\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">AirSim</a>, and then refined on a small dataset collected in the real world.</p>\n\n\n\n<p>Transfer learning is also common in many computer vision tasks, including image classification and object detection, in which a model uses some pretrained representation as an “initialization” to learn a more useful representation for the specific task in hand. In a recent collaboration with MIT, we explore adversarial robustness as a prior for improving transfer learning in computer vision. We find that adversarially robust models outperform their standard counterparts on a variety of downstream computer vision tasks.</p>\n\n\n\n<p class=\"has-text-align-center\"><strong><a href=\"https://www.microsoft.com/en-us/research/publication/do-adversarially-robust-imagenet-models-transfer-better/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">Read Paper</a>   &#160;  &#160;  &#160;    &#160;   &#160;       &#160;     &#160;  &#160;            &#160;  &#160;                             <a href=\"https://github.com/microsoft/robust-models-transfer\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">Code & Models</a> </strong></p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1-1024x211.jpg\" alt=\"Workflow for transfer learning: Train a model on large-scale source datasets (showing a collection of images from ImageNet). An arrow labeled \"Transfer the learned representation\" points to Target datasets (a number of image sets are shown: flowers, X-rays, airplanes).\" class=\"wp-image-683973\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1-1024x211.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1-300x62.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1-768x159.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1.jpg 1366w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 1: A depiction of transfer learning.</figcaption></figure></div>\n\n\n\n<p>In our work we focus on computer vision and consider a standard transfer learning pipeline: &#8220;ImageNet pretraining.&#8221; This pipeline trains a deep neural network on <a href=\"http://image-net.org\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">ImageNet</a>, then tweaks this pretrained model for another target task, ranging from image classification of smaller datasets to more complex tasks like object detection and image segmentation.</p>\n\n\n\n<p>Refining the ImageNet pretrained model can be done in several ways. In our work we focus on two common methods:</p>\n\n\n\n<ul><li><strong>Fixed-feature transfer: </strong>we replace the last layer of the neural network with a new layer that fits the target task. Then we train the last layer on the target dataset while keeping the rest of the layers fixed.</li><li><strong>Full-network transfer:</strong> we do the same as in fixed-feature, but instead of fine-tuning the last layer only, we fine-tune the full model.</li></ul>\n\n\n\n<p>The full-network transfer setting typically outperforms the fixed-feature strategy in practice.</p>\n\n\n\n<h3>How can we improve transfer learning?</h3>\n\n\n\n<p>The performance of the pretrained model on the source tasks plays a major role in determining how well it transfers to the source tasks. In fact, a recent study by <a href=\"https://arxiv.org/abs/1805.08974\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">Kornblith, Shlens, and Le</a> finds that a higher accuracy of pretrained ImageNet models leads to better performance on a wide range of downstream classification tasks. The question that we would like to answer here is whether improving the ImageNet accuracy of the pretrained model is the only way to improve its transfer learning.</p>\n\n\n\n<p><img width=\"14\" height=\"14\" src=\"\">After all, our goal is to learn broadly applicable features on the source dataset that can transfer to target datasets. ImageNet accuracy likely correlates with the quality of features that a model learns, but it may not fully capture the downstream utility of those features. Ultimately, the quality of learned features stems from the priors we impose on them during training. For example, there have been several studies of the priors imposed by architectures (such as <a href=\"https://dmitryulyanov.github.io/deep_image_prior\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">convolutional layers</a>), <a href=\"https://arxiv.org/abs/1811.00401\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">loss functions</a>, and <a href=\"https://www.tandfonline.com/doi/abs/10.1198/10618600152418584\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">data</a> <a href=\"https://arxiv.org/abs/1911.09071\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">augmentation</a> on network training.</p>\n\n\n\n<p>In our paper <a aria-label=\"undefined (opens in a new tab)\" href=\"https://www.microsoft.com/en-us/research/publication/do-adversarially-robust-imagenet-models-transfer-better/\" target=\"_blank\" rel=\"noreferrer noopener\">“Do Adversarially Robust ImageNet Models Transfer Better?”</a> we study another prior: <strong>adversarial robustness</strong>, which refers to a model&#8217;s invariance to small imperceptible perturbations of its inputs, namely <a href=\"https://gradientscience.org/intro_adversarial/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">adversarial examples</a>. It is well known by now that standard neural networks are extremely vulnerable to such adversarial examples. For instance, Figure 2 shows that a tiny perturbation (or change) of the pig image, a pretrained ImageNet classifier will mistakenly predict it as an &#8220;airliner&#8221; with very high confidence:</p>\n\n\n\n<figure class=\"wp-block-image size-large is-resized\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-blog-_-figure-2.png\" alt=\"An image of a pig labeled \"pig,\" +0.005 times perturbation (represented by multicolored square) equals the image of pig, unmodified to the human eye but labeled as \"airplane.\" \" class=\"wp-image-683631\" width=\"900\" height=\"269\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-blog-_-figure-2.png 468w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-blog-_-figure-2-300x90.png 300w\" sizes=\"(max-width: 900px) 100vw, 900px\" /><figcaption>Figure 2: An adversarial example. A pig on the left which is imperceptibly perturbed to be classified as an airliner on the right.</figcaption></figure>\n\n\n\n<p>Adversarial robustness is therefore typically enforced by replacing the standard loss objective with a <a href=\"https://gradientscience.org/robust_opt_pt1/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">robust optimization</a> objective:</p>\n\n\n\n\\[\\min_{\\theta} \\mathbb{E}_{(x,y)\\sim D}\\left[\\mathcal{L}(x,y;\\theta)\\right]  \\rightarrow \\min_{\\theta} \\mathbb{E}_{(x,y)\\sim D} \\left[\\max_{\\|\\delta\\|_2 \\leq \\varepsilon} \\mathcal{L}(x+\\delta,y;\\theta) \\right].\\]\n\n\n\n<p>This objective trains models to be robust to worse-case image perturbations within an \\(\\ell_2\\) ball around the input. The hyperparameter \\(\\varepsilon\\) governs the intended degree of invariance to the corresponding perturbations. Note that setting \\(\\varepsilon=0\\) corresponds to standard training, while increasing ε induces robustness to increasingly large perturbations.</p>\n\n\n\n<p>Adversarial robustness has been initially studied solely through the lens of machine learning security, but recently a <a href=\"https://gradientscience.org/robust_apps/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">line of</a> <a href=\"https://gradientscience.org/adv/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">work</a> studied the effect of imposing adversarial robustness as a prior on learned feature representations. These works have found that although these adversarially robust models tend to attain lower accuracies than their standardly trained counterparts, their learned feature representations carry several advantages over those of standard models. These advantages include <a href=\"https://arxiv.org/abs/1805.12152\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">better-behaved</a> <a href=\"https://arxiv.org/abs/1905.09797\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">gradients</a> (see Figure 3), <a href=\"https://arxiv.org/abs/1910.08640\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">representation invertibility</a>, and more <a href=\"https://arxiv.org/abs/2005.10190\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">specialized features</a>. These desirable properties might suggest that robust neural networks are learning better feature representations than standard networks, which could improve the transferability of those features.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3-1024x340.jpg\" alt=\"Two rows showing representations learning by standard models (on bottom row) and robust models (on top row). Visually, unlike standard models where you see abstractions and colors only, robust models show vibrant, partially blurred or morphed images: in one of the robust images, you can vaguely see various animal prints, whereas in the standard image it is only multicolored dots. \" class=\"wp-image-683640\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3-1024x340.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3-300x100.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3-768x255.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3.jpg 1430w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 3: Representations learning by adversarially robust (top) and standard (bottom) models: robust models tend to learn more perceptually aligned representations which seem to transfer better to downstream tasks.</figcaption></figure>\n\n\n\n<h3>Adversarial robustness and transfer learning</h3>\n\n\n\n<p>To sum up, we have two options of pretrained models to use for transfer learning. We can either use standard models that have high accuracy but little robustness on the source task; or we can use adversarially robust models, which are worse in terms of ImageNet accuracy but are robust and have the &#8220;nice&#8221; representational properties (see Figure 3). Which models are better for transfer learning?</p>\n\n\n\n<p>To answer this question, we trained a large number of standard and robust ImageNet models. (All models are available for download via our <a href=\"https://github.com/microsoft/robust-models-transfer\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">code/model release</a>, and more details on our training procedure can be found there and in <a href=\"https://arxiv.org/abs/2007.08489\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">our paper</a>.) We then transferred each model (using both the fixed-feature and full-network settings) to 12 downstream classification tasks and evaluated the performance.</p>\n\n\n\n<p>We find that adversarially robust source models almost always outperform their standard counterparts in terms of accuracy on the target task. This is reflected in the table below, in which we compare the accuracies of the best standard model and the best robust model (searching over the same set of hyperparameters and architectures):</p>\n\n\n\n<figure class=\"wp-block-table alignwide\"><table><tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td><strong>Dataset </strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td><strong>Mode</strong></td><td><strong>Model</strong></td><td>Aircraft</td><td>Birdsnap</td><td>CIFAR-10</td><td>CIFAR-100</td><td>Caltech-101</td><td>Caltech-256</td><td>Cars</td><td>DTD</td><td>Flowers</td><td>Food</td><td>Pets</td><td>SUN397</td></tr><tr><td>Fixed-feature</td><td>Robust Standard </td><td><strong>44.14 </strong><br>38.69</td><td><strong>50.72</strong><br>48.35</td><td><strong>95.53</strong><br>81.31</td><td><strong>81.08</strong><br>60.14</td><td><strong>92.76</strong><br>90.12</td><td><strong>85.08</strong><br>82.78</td><td><strong>50.67</strong><br>44.63</td><td><strong>70.37</strong><br><strong>70.09</strong></td><td><strong>91.84</strong><br><strong>91.90</strong></td><td><strong>69.26</strong><br>65.79</td><td><strong>92.05</strong><br><strong>91.83</strong></td><td>58.75<br><strong>55.92</strong></td></tr><tr><td>Full-network </td><td>Robust Standard </td><td><strong>86.24</strong><br><strong>86.57</strong></td><td><strong>76.55</strong><br>75.71</td><td><strong>98.68</strong><br>97.63</td><td><strong>89.04</strong><br>85.99</td><td><strong>95.62</strong><br>94.75</td><td><strong>87.62</strong><br>86.55</td><td><strong>91.48</strong><br><strong>91.52</strong></td><td><strong>76.93<br></strong>75.80</td><td><strong>97.21</strong><br><strong>97.04</strong></td><td><strong>89.12</strong><br>88.64</td><td><strong>94.53</strong><br><strong>94.20</strong></td><td><strong>64.89</strong><br>63.72</td></tr></tbody></table><figcaption>Table 1: <strong>The main result</strong>—adversarially robust models outperform their standard counterparts when transferred to 12 downstream classification tasks.</figcaption></figure>\n\n\n\n<p>The following graph shows, for each architecture and downstream classification task, the performance of the best standard model compared to that of the best robust model. As we can see, adversarially robust models improve on the performance of their standard counterparts per architecture too, and the gap tends to increase as the network’s width increases:</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Figure-4-_AirSim-blog.jpg\" alt=\"Bar charts showing generally better transfer accuracy (%) using adversarially robust models versus standard models on Aircraft, Birdsnap, CIFAR-10, CIFAR-100, Caltect-101, Caltech-256, Cars, DTD, Flowers, Food, Pets, and  SUN397.\" class=\"wp-image-683652\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Figure-4-_AirSim-blog.jpg 988w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Figure-4-_AirSim-blog-300x107.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Figure-4-_AirSim-blog-768x274.jpg 768w\" sizes=\"(max-width: 988px) 100vw, 988px\" /><figcaption>Figure 4: Adversarially robust models tend to improve over standard networks for individual architectures too.</figcaption></figure>\n\n\n\n<p>We also evaluate transfer learning on other downstream tasks including object detection and instance segmentation, both for which using robustness backbone models outperforms using standard models as shown in the table below:</p>\n\n\n\n<figure class=\"wp-block-table aligncenter\"><table><tbody><tr><td class=\"has-text-align-center\" data-align=\"center\"><strong>Task </strong></td><td></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Box AP</strong></td><td></td><td class=\"has-text-align-center\" data-align=\"center\"><strong>Mask AP</strong></td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\"></td><td></td><td class=\"has-text-align-center\" data-align=\"center\">Standard &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Robust </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">Standard &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Robust </td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">VOC Object Detection </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">52.80 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;53.87 </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">&#8212; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#8212;   </td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">COCO Object Detection </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">39.61 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;40.13 </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">&#8212; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#8212; </td></tr><tr><td class=\"has-text-align-center\" data-align=\"center\">COCO Instance Segmentation </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">40.74 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;41.04 </td><td></td><td class=\"has-text-align-center\" data-align=\"center\">36.98 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;37.23</td></tr></tbody></table></figure>\n\n\n\n<h3>Empirical mysteries and future work</h3>\n\n\n\n<p>Overall, we have seen that adversarially robust models, although being less accurate on the source task than standard-trained models, can improve transfer learning on a wide range of downstream tasks. In <a href=\"https://arxiv.org/abs/2007.08489\">our paper</a>, we study this phenomenon in more detail. There, we analyze the effects of model width and robustness levels on the transfer performance, and we compare adversarial robustness to other notions of robustness. We also uncover a few somewhat mysterious properties: for example, resizing images seems to have a non-trivial effect on the relationship between robustness and downstream accuracy.</p>\n\n\n\n<p>Finally, our work provides evidence that adversarially robust perception models transfer better, yet understanding precisely what causes this remains an open question. More broadly, the results we observe indicate that we still do not yet fully understand (even empirically) the ingredients that make transfer learning successful. We hope that our work paves the way for more research initiatives to explore and understand what makes transfer learning work well.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/adversarial-robustness-as-a-prior-for-better-transfer-learning/\">Adversarial robustness as a prior for better transfer learning</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Tue, 11 Aug 2020 17:00:18 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/1400x788_AirSim_noLogo5secs-1.gif","linkMd5":"abfa94c375d1b9bbf2e239da4c6b7139","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_2/2020/08/24/21-39-41-148_eae27493c21b80f7.webp","destWidth":1400,"destHeight":788,"sourceBytes":1610991,"destBytes":1809008,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/1400x788_AirSim_noLogo5secs-1.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_2/2020/08/24/21-39-41-148_eae27493c21b80f7.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1-1024x211.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn32@2020_2/2020/08/24/21-40-52-991_dcd5e081da91fdd3.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-blog-_-figure-2.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn99@2020_4/2020/08/24/21-40-53-187_d7bc199bc40dcf72.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3-1024x340.jpg":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn43@2020_4/2020/08/24/21-40-53-155_62d0a37937f2d4db.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Figure-4-_AirSim-blog.jpg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn37@2020_5/2020/08/24/21-40-53-027_2a1a1b24b9277200.webp"},"publishedOrCreatedDate":1598305163716},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"ICML 2020 highlights: A Transformer-based RL agent, causal ML for increased privacy, and more","link":"https://www.microsoft.com/en-us/research/?p=680043","description":"<figure class=\"wp-block-image alignwide size-large\">\n <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1024x577.png\" alt=\"\" class=\"wp-image-682260\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1024x577.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1536x865.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero.png 1643w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n</figure> \n<p>With over 50 papers from Microsoft accepted at this year’s <a href=\"https://icml.cc/Conferences/2020\">International Conference on Machine Learning (ICML 2020)</a>, a number of which were presented in virtual workshops, Microsoft researchers are in full summer swing when it comes to advancing machine learning in accessibility, privacy, healthcare, and other areas. As Microsoft Partner Research Manager and ICML President <a href=\"https://www.microsoft.com/en-us/research/people/jcl/\">John Langford</a> puts it, “ICML is a very broad conference, so its specialty is in some sense ‘all of the above.’” But Langford goes on to add that one of the topics that ICML has a long track record on is currently trending: reinforcement learning. A brief glance through the <a href=\"https://www.microsoft.com/en-us/research/event/icml-2020/#!sessions\">sessions </a>and <a href=\"https://www.microsoft.com/en-us/research/event/icml-2020/#!workshops\">workshops </a>presented by Microsoft researchers shows the wide influence reinforcement learning has in our world today, from natural language to robotics to infrastructure considerations like transportation.</p> \n<p>Beyond the research contributions, Microsoft was also a sponsor of and recruiter at the conference. Additionally, the company sponsored two events co-located with the conference, the <a href=\"https://wimlworkshop.org/icml2020/\">first Women in Machine Learning Un-Workshop</a> and <a href=\"https://sites.google.com/view/queer-in-ai/icml-2020\">the fourth Queer in AI Workshop</a>. The impact of the conference—now and in the future—is multifaceted, according to Langford. “ICML is ‘the’ summer machine learning conference. As such, it’s critically important to the academic discovery, review, and dissemination process, a great way to meet fellow researchers, and a natural recruiting point for the field,” he says.</p> \n<p>Below&nbsp;are&nbsp;five&nbsp;selections&nbsp;of research presented by&nbsp;Microsoft.&nbsp;These projects highlight how broadly&nbsp;researchers are thinking about&nbsp;ML&nbsp;and its implications for society.&nbsp;But this diverse group of papers&nbsp;represents&nbsp;only a small slice of the&nbsp;advancements presented by&nbsp;Microsoft&nbsp;researchers.&nbsp;Explore the&nbsp;<a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/event/icml-2020/#!accepted-papers\" target=\"_blank\">Microsoft at&nbsp;ICML&nbsp;2020&nbsp;accepted papers list</a>&nbsp;to learn about&nbsp;further&nbsp;research contributions.&nbsp;</p> \n<p>See sections on: <u>ICML 2020 Overview</u> | <a href=\"https://www.microsoft.com/en-us/research/?p=680043#AIModels\">How AI models reason </a> | <a href=\"https://www.microsoft.com/en-us/research/?p=680043#Utility\">Utility and privacy with causal machine learning</a> | <a href=\"https://www.microsoft.com/en-us/research/?p=680043#Transformers\">Using Transformers to create RL agents</a> | <a href=\"https://www.microsoft.com/en-us/research/?p=680043#Pretraining\">Pretraining for bidirectional language models</a> | <a href=\"https://www.microsoft.com/en-us/research/?p=680043#Normalization\">Identifying layer normalization location</a></p> \n<figure class=\"wp-block-image alignwide size-large\">\n <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Neural-network-3_-shorter.png\" alt=\"\" class=\"wp-image-681039\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Neural-network-3_-shorter.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Neural-network-3_-shorter-300x106.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Neural-network-3_-shorter-768x272.png 768w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n</figure> \n<div id=\"AIModels\" style=\"height: 30px;\"></div> \n<h3>Understanding how AI models reason about what they&nbsp;see&nbsp;</h3> \n<p><strong>Bottom line:</strong>&nbsp;“We propose a principled approach to isolate, analyze, and interpret how visual&nbsp;question-answering&nbsp;models reason about what they see.”&nbsp;<br> —Machine Learning Scientist&nbsp;<a href=\"https://www.microsoft.com/applied-sciences/people/saeed-amizadeh\">Saeed&nbsp;Amizadeh</a></br></p> \n<div class=\"annotations\" data-bi-area=\"margin-callout\"> \n <ul class=\"annotations__list annotations__list--left\"> \n  <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication </span> <a href=\"https://www.microsoft.com/en-us/research/publication/neuro-symbolic-visual-reasoning-disentangling-visual-from-reasoning/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Neuro-Symbolic Visual Reasoning: Disentangling “Visual” from “Reasoning”\"> Neuro-Symbolic Visual Reasoning: Disentangling “Visual” from “Reasoning” </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n </ul> \n</div> \n<p><strong>Quick glance:</strong>&nbsp;In&nbsp;<a aria-label=\"undefined (opens in a new tab)\" href=\"https://www.microsoft.com/en-us/research/publication/neuro-symbolic-visual-reasoning-disentangling-visual-from-reasoning/\" target=\"_blank\" rel=\"noreferrer noopener\">“Neuro-Symbolic Visual Reasoning: Disentangling ‘Visual’ from&nbsp;‘Reasoning,’”</a>&nbsp;researchers from&nbsp;the&nbsp;Microsoft Applied Sciences Lab&nbsp;and&nbsp;MSR AI&nbsp;collaborated to combine&nbsp;visual understanding&nbsp;and&nbsp;neuro-symbolic&nbsp;reasoning&nbsp;with&nbsp;natural language processing and program synthesis.&nbsp;“We develop a&nbsp;novel way to perform differentiable logical inference over visual scenes, which allows us to disentangle the processes of reasoning and perception in visual question answering (VQA) models,” explains&nbsp;Amizadeh.&nbsp;The work also led to creating a methodology for evaluating state-of-the-art VQA models, and the researchers propose expanding beyond pure probabilistic logical reasoning to incorporate other contextual signals and improve visual perception of the models. </p> \n<p><strong>Areas of impact:&nbsp;</strong>This research lies at the intersection of natural language and visual perception, which makes it a good candidate for systems using AI for accessibility.&nbsp;Key is&nbsp;the work’s focus&nbsp;on&nbsp;interpretability.&nbsp;The&nbsp;user should understand throughout the whole process how the neural reasoning is&nbsp;connecting&nbsp;what it “sees”&nbsp;to&nbsp;language, building trust and reliability in AI.&nbsp;</p> \n<p><strong>Fun Fact:</strong>&nbsp;This project initially began as a project&nbsp;out of&nbsp;the&nbsp;Microsoft AI Residency Program.&nbsp;</p> \n<p><strong>The research team: </strong><a href=\"https://www.microsoft.com/applied-sciences/people/saeed-amizadeh\">Saeed&nbsp;Amizadeh</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/hpalangi/\">Hamid Palangi</a>, <a href=\"https://www.microsoft.com/en-us/research/people/polozov/\">Alex Polozov</a>, Yichen Huang, <a href=\"https://www.microsoft.com/en-us/research/people/kazukoi/\">Kazuhito Koishida</a></p> \n<p><strong>Additional Resources:&nbsp;</strong>&nbsp;<br>&nbsp;<br><a href=\"https://www.microsoft.com/applied-sciences/\">Applied Sciences homepage&nbsp;</a><br>&nbsp;<br><a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-ai/\" target=\"_blank\">MSR AI homepage</a>&nbsp;&nbsp;<br>&nbsp;<br><a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/\" target=\"_blank\">Microsoft AI Residency Program</a></br></br></br></br></br></br></p> \n<div id=\"Utility\" style=\"height: 30px;\"></div> \n<h3>Improving utility and privacy&nbsp;with causal machine&nbsp;learning</h3> \n<p><strong>Bottom line:&nbsp;</strong>“<em>What if you can build&nbsp;machine learning&nbsp;models&nbsp;that are both accurate and preserve privacy of individuals?</em> Try causal predictive models:&nbsp;We&nbsp;show that they are more robust to privacy attacks like membership inference and have higher accuracy&nbsp;on new domains&nbsp;than typical ML models.”&nbsp;<br> —Microsoft&nbsp;Senior Researchers&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/amshar/\">Amit Sharma&nbsp;</a>and&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/shtople/\">Shruti&nbsp;Tople</a></br></p> \n<div class=\"annotations\" data-bi-area=\"margin-callout\"> \n <ul class=\"annotations__list annotations__list--right\"> \n  <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication </span> <a href=\"https://www.microsoft.com/en-us/research/publication/alleviating-privacy-attacks-via-causal-learning/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Alleviating Privacy Attacks via Causal Learning\"> Alleviating Privacy Attacks via Causal Learning </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n </ul> \n</div> \n<p><strong>Quick glance:&nbsp;</strong>Privacy is paramount for&nbsp;institutions&nbsp;like hospitals&nbsp;and&nbsp;governments,&nbsp;which&nbsp;handle sensitive datasets and use ML models. Standard&nbsp;ML&nbsp;privacy approaches&nbsp;add noise to a model or data to protect information, but this can have the undesired effect of reducing accuracy or utility of the model.&nbsp;This work shows that causal&nbsp;learning,&nbsp;by which&nbsp;ML models are trained based on domain knowledge about causal relationships between features and outcomes, can increase both privacy and utility when compared to associational ML models&nbsp;with the same&nbsp;amount of noise.&nbsp;Researchers from&nbsp;Microsoft Research India<strong>&nbsp;</strong>provided knowledge of causal ML for this project, while researchers from&nbsp;Microsoft Research Cambridge&nbsp;brought expertise on privacy and security. Their paper is called <a href=\"https://www.microsoft.com/en-us/research/publication/alleviating-privacy-attacks-via-causal-learning/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">“Alleviating Privacy Attacks via Causal Learning.”</a></p> \n<p><strong>Areas of impact:&nbsp;</strong>This work aims to improve privacy protections for institutions&nbsp;using&nbsp;sensitive data with causal ML. In addition, this direction allows for improved model sharing across institutions and allows individuals to voluntarily share their own data without risk of information being leaked by an ML model.&nbsp;&nbsp;</p> \n<p><strong>New tools:&nbsp;</strong>The researchers have released an <a href=\"https://github.com/microsoft/robustdg\">open-source toolkit</a>, RobustDG,&nbsp;for evaluating causal ML models on privacy, robustness, and out-of-distribution accuracy.</p> \n<p><strong>The research team: </strong><a href=\"https://www.microsoft.com/en-us/research/people/amshar/\" target=\"_blank\" rel=\"noreferrer noopener\">Amit Sharma</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/shtople/\" target=\"_blank\" rel=\"noreferrer noopener\">Shruti&nbsp;Tople</a>, and&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/adityan/\" target=\"_blank\" rel=\"noreferrer noopener\">Aditya Nori</a></p> \n<p><strong>Additional Resources:</strong>&nbsp;<br>&nbsp;<br><a aria-label=\"undefined (opens in a new tab)\" href=\"https://github.com/microsoft/robustdg\" target=\"_blank\" rel=\"noreferrer noopener\">GitHub repository including open-source toolkit</a><br>&nbsp;<br><a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-india/\" target=\"_blank\">Microsoft Research India homepage&nbsp;<br><br /></br></a><a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/\" target=\"_blank\">Microsoft Research Cambridge homepage</a>&nbsp;&nbsp;</br></br></br></br></p> \n<div id=\"Transformers\" style=\"height: 30px;\"></div> \n<h3>Using Transformers to create&nbsp;RL&nbsp;agents&nbsp;suited&nbsp;for real-world&nbsp;tasks</h3> \n<p><strong>Bottom line:&nbsp;</strong>“Transformers for RL!”&nbsp;<br> —Senior Research Software Engineer&nbsp;<a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/people/riloynd/\" target=\"_blank\">Ricky Loynd</a></br></p> \n<div class=\"annotations\" data-bi-area=\"margin-callout\"> \n <ul class=\"annotations__list annotations__list--left\"> \n  <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/working-memory-graphs/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Working Memory Graphs\"> Working Memory Graphs </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n </ul> \n</div> \n<p><strong>Quick glance:&nbsp;</strong>“<a rel=\"noreferrer noopener\" href=\"https://www.microsoft.com/en-us/research/publication/working-memory-graphs/\" target=\"_blank\">Working Memory Graphs</a>”&nbsp;presents a new reinforcement learning agent “that accelerates learning on challenging tasks by leveraging the power of Transformers in three ways,”&nbsp;explains&nbsp;Loynd.&nbsp;These three approaches apply Transformer attention to past observations, recurrent state vectors, and factored observations,&nbsp;respectively. “By leveraging the power of Transformers in these ways, our Working Memory Graph (WMG) agent accelerates learning on several challenging tasks: BabyAI, Pathfinding, and Sokoban. In&nbsp;BabyAI, WMG achieves drastic improvements in sample efficiency when observations are factored into more succinct representations,” says Loynd.&nbsp;The team includes&nbsp;members from the reinforcement learning and&nbsp;deep learning&nbsp;groups&nbsp;within&nbsp;MSR AI.</p> \n<p><strong>Areas of impact:&nbsp;</strong>This work shows that WMG is effective in handling&nbsp;the&nbsp;structured, factored&nbsp;observations used in today’s real-world applications of RL and accelerates RL so that AI agents will eventually be able to accomplish&nbsp;previously unattainable&nbsp;real-world tasks.</p> \n<p><strong>Performance and novel features:&nbsp;</strong>WMG outperforms a&nbsp;GRU&nbsp;(Gated Recurrent Unit)&nbsp;baseline agent at complex reasoning over past observations, and WMG has a new form of “shortcut recurrence” that proves to be more effective than standard gated recurrence. Sokoban results demonstrate that WMG performs better on this complex domain than the state-of-the-art&nbsp;Deep Repeated ConvLSTM (DRC) agent (by Google DeepMind)&nbsp;throughout 20 million steps of training.</p> \n<p><strong>The research team: </strong><a href=\"https://www.microsoft.com/en-us/research/people/riloynd/\">Ricky Loynd</a>, <a href=\"https://www.microsoft.com/en-us/research/people/rfernand/\">Roland Fernandez</a>, <a href=\"https://www.microsoft.com/en-us/research/people/aslicel/\">Asli Celikyilmaz</a>, <a href=\"https://www.microsoft.com/en-us/research/people/adswamin/\">Adith Swaminathan</a>, and <a href=\"https://www.microsoft.com/en-us/research/people/mahauskn/\">Matthew Hausknecht</a>. </p> \n<p><strong>Additional Resources:</strong>&nbsp;<br>&nbsp;<br><a rel=\"noreferrer noopener\" href=\"https://github.com/microsoft/wmg_agent\" target=\"_blank\">Working Memory Graph GitHub repository&nbsp;<br><br /></br></a><a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-ai/\" target=\"_blank\" rel=\"noreferrer noopener\">MSR AI homepage</a></br></br></p> \n<div id=\"Pretraining\" style=\"height: 30px;\"></div> \n<h3>Efficient pretraining&nbsp;for&nbsp;bidirectional language models in one forward&nbsp;pass</h3> \n<p><strong>Bottom line:&nbsp;</strong>“Our work efficiently realizes unified pretraining of bidirectional language models&nbsp;(via autoencoding) and sequence-to-sequence language models&nbsp;(via&nbsp;partially autoregressive) with&nbsp;a&nbsp;pseudo-masked&nbsp;language&nbsp;model for language understanding and&nbsp;generation.”&nbsp;&nbsp;<br> —Senior Principal Research Manager <a href=\"https://www.microsoft.com/en-us/research/people/fuwei/\">Furu&nbsp;Wei</a></br></p> \n<div class=\"annotations\" data-bi-area=\"margin-callout\"> \n <ul class=\"annotations__list annotations__list--right\"> \n  <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/unilmv2-pseudo-masked-language-models-for-unified-language-model-pre-training/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training\"> UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n </ul> \n</div> \n<p><strong>Quick glance:&nbsp;</strong>This research&nbsp;introduces pseudo-masked language models, allowing for efficient pretraining of bidirectional language models in natural language&nbsp;understanding and sequence-to-sequence language models in natural language generation in one forward pass.&nbsp;This work is a collaboration between Microsoft Research Asia, Microsoft Research Redmond, and both the DeepSpeed and Project Turing teams, who&nbsp;help&nbsp;scale up the&nbsp;pretraining to larger&nbsp;models and are working to implement those models&nbsp;in Microsoft products&nbsp;in an initiative called AI at Scale.&nbsp;The paper is&nbsp;titled&nbsp;“<a href=\"https://www.microsoft.com/en-us/research/publication/unilmv2-pseudo-masked-language-models-for-unified-language-model-pre-training/\" target=\"_blank\" rel=\"noreferrer noopener\">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training</a>.”</p> \n<p><strong>Areas of impact:&nbsp;</strong>This novel language model improves techniques for natural language generation, including document summarization and dialog generation. It also builds on techniques for&nbsp;natural language understanding, which includes text classification, question answering, and information extraction.&nbsp;&nbsp;</p> \n<p><strong>New state of the art:&nbsp;</strong>Results show this model achieves state of the art on various natural language generation and understanding tasks&nbsp;across&nbsp;numerous benchmarks.</p> \n<p><strong>The research&nbsp;team:&nbsp;</strong></p> \n<p><strong>MSR Asia:&nbsp;</strong><a href=\"https://www.microsoft.com/en-us/research/people/lidong1/\">Li Dong</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/fuwei/\">Furu&nbsp;Wei</a>,&nbsp;Wenhui&nbsp;Wang,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/nanya/\">Nan Yang</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/mingzhou/\">Ming Zhou</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/hon/\">Hsiao-Wuen&nbsp;Hon</a>, and collaborators Hangbo&nbsp;Bao&nbsp;and&nbsp;Songhao&nbsp;Piao</p> \n<p><strong>MSR Redmond:&nbsp;</strong><a href=\"https://www.microsoft.com/en-us/research/people/xiaodl/\">Xiaodong&nbsp;Liu,</a> <a href=\"https://www.microsoft.com/en-us/research/people/yuwwan/\">Yu Wang</a>,&nbsp;and&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/jfgao/\">Jianfeng Gao&nbsp;</a></p> \n<p><strong>Additional resources: </strong></p> \n<p><a href=\"https://github.com/microsoft/unilm\">GitHub repository of UniLM&nbsp;</a></p> \n<p><a href=\"https://www.microsoft.com/en-us/research/project/ai-at-scale/\" target=\"_blank\" rel=\"noreferrer noopener\">AI at Scale&nbsp;homepage</a>&nbsp;</p> \n<p><a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft Research Asia homepage</a><strong></strong>&nbsp;</p> \n<p><a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/\">Microsoft Research Redmond homepage&nbsp;</a></p> \n<p><a href=\"https://www.deepspeed.ai/\" target=\"_blank\" rel=\"noreferrer noopener\">DeepSpeed homepage</a>&nbsp;</p> \n<p><a href=\"https://msturing.org/\">Project Turing homepage</a></p> \n<div id=\"Normalization\" style=\"height: 30px;\"></div> \n<h3>Correctly identifying layer normalization location for better Transformer optimization</h3> \n<p><strong><strong>Bottom line:</strong></strong>&nbsp;“Use Pre-LN Transformer to remove the annoying warm-up stage and save greatly on converge time.”<strong><br /></strong> — Microsoft Researchers <a href=\"https://www.microsoft.com/en-us/research/people/dihe/#:~:text=Di%20He%20%28%E8%B4%BA%E7%AC%9B%29%20is%20currently%20a%20Senior%20Researcher,degrees%20from%20Peking%20University%2C%20advised%20by%20Liwei%20Wang.\">Di He</a> and&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/shuz/\">Shuxin&nbsp;Zheng</a></p> \n<div class=\"annotations\" data-bi-area=\"margin-callout\"> \n <ul class=\"annotations__list annotations__list--left\"> \n  <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/on-layer-normalization-in-the-transformer-architecture/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"On Layer Normalization in the Transformer Architecture\"> On Layer Normalization in the Transformer Architecture </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n </ul> \n</div> \n<p><strong>Quick glance:&nbsp;</strong>This research explores a&nbsp;known&nbsp;optimization issue with&nbsp;the original&nbsp;Transformer (BERT)&nbsp;that causes slowed&nbsp;down training, requiring hyperparameter tunings. The researchers offer theoretical proof the issue emerges from the location of layer normalization. They propose a&nbsp;variant of&nbsp;Pre-LN&nbsp;Transformer&nbsp;that correctly locates the layer normalization with easy optimization and the ability to quickly converge.&nbsp;This work was done by researchers affiliated with Microsoft Research Asia, the Chinese Academy of Sciences, and Peking University.&nbsp;The research is detailed in the paper&nbsp;“<a href=\"https://www.microsoft.com/en-us/research/publication/on-layer-normalization-in-the-transformer-architecture/\" target=\"_blank\" rel=\"noreferrer noopener\">On Layer Normalization in the Transformer Architecture</a>.”&nbsp;&nbsp;</p> \n<p><strong>Areas of impact:&nbsp;</strong>There are many projects already using&nbsp;Pre-LN Transformer&nbsp;to train large-scale BERT models&nbsp;because of&nbsp;its exceptional optimization stability, including training on&nbsp;NVIDIA’s&nbsp;Megatron, Open AI’s&nbsp;GPT-2, and Open AI’s&nbsp;GPT-3 models.</p> \n<p><strong>Added benefits:&nbsp;</strong>Because of&nbsp;the way this variant operates, it requires no additional hyperparameter tuning.&nbsp;This&nbsp;fact,&nbsp;combined with faster convergence,&nbsp;results in boosted energy efficiency.&nbsp;&nbsp;</p> \n<p><strong>The research&nbsp;team:</strong>&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/dihe/\" target=\"_blank\" rel=\"noreferrer noopener\">Di He</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/shuz/\" target=\"_blank\" rel=\"noreferrer noopener\">Shuxin&nbsp;Zheng</a>,&nbsp;<a href=\"https://www.microsoft.com/en-us/research/people/huzhang/\" target=\"_blank\" rel=\"noreferrer noopener\">Huishuai&nbsp;Zhang</a>,&nbsp;and&nbsp;<a href=\"http://research.microsoft.com/en-us/people/tyliu/\" target=\"_blank\" rel=\"noreferrer noopener\">Tie-Yan Liu</a>&nbsp;&nbsp;</p> \n<p><strong>Additional Resources:</strong><strong>&nbsp;</strong>&nbsp;<br>&nbsp;<br><a href=\"https://www.microsoft.com/en-us/research/project/ai-at-scale/\" target=\"_blank\" rel=\"noreferrer noopener\">AI at Scale homepage</a>&nbsp;</br></br></p> \n<p><a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/\" target=\"_blank\" rel=\"noreferrer noopener\">Microsoft Research Asia homepage</a>&nbsp;</p> \n<p><a href=\"https://www.microsoft.com/en-us/research/project/machine-translation-2/\">Neural Machine Translation&nbsp;</a></p> \n<p></p> \n<div class=\"annotations\" data-bi-area=\"citation\"> \n <ul class=\"annotations__list \"> \n  <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Explore more </span> <a href=\"https://www.microsoft.com/en-us/research/event/icml-2020/#!accepted-papers\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"ICML 2020 accepted papers \"> ICML 2020 accepted papers </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">Check out the complete list of accepted papers from MSR at ICML 2020 </span> </li> \n </ul> \n</div> \n<p><br>&nbsp;<br>&nbsp;</br></br></p> \n<p></p> \n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/icml-2020-highlights-a-transformer-based-rl-agent-causal-ml-for-increased-privacy-and-more/\">ICML 2020 highlights: A Transformer-based RL agent, causal ML for increased privacy, and more</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>","descriptionType":"html","publishedDate":"Tue, 04 Aug 2020 16:33:34 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1024x577.png","linkMd5":"d3a27d20f70cd143be812f433fce1f5d","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn99@2020_5/2020/08/24/21-39-24-608_28ae713392181e7a.webp","destWidth":1024,"destHeight":577,"sourceBytes":265007,"destBytes":46818,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1024x577.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn99@2020_5/2020/08/24/21-39-24-608_28ae713392181e7a.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Neural-network-3_-shorter.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn45@2020_5/2020/08/24/21-40-53-880_c1540f92a321fc8c.webp"},"publishedOrCreatedDate":1598305163714},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"A path to personalization: Using ML to subtype patients receiving digital mental health interventions","link":"https://www.microsoft.com/en-us/research/?p=675831","description":"<p><img class=\"aligncenter wp-image-676218 size-full\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_SilverCloud_NoLogo_animation_updated-final.gif\" alt=\"An animated diagram of a hidden Markov model employed by researchers to identify subtypes of patients based on their use of internet-delivered cognitive behavioral therapy. Diagram shows how the number of subtypes, represented by the letter “K,” is determined by defining a hidden state that captures true engagement, represented by the letter “x,” which is inferred from observed engagement “Y” and learning the probability of patients transitioning across these states “x” over time, represented by “abr.”\" width=\"1400\" height=\"788\" /></p>\n<p>Mental health experiences vary widely from individual to individual. Because of this, effective treatment is about identifying the <em>unique</em> set of tools that will help a person manage their mental health productively. And that rarely happens overnight. People experiencing symptoms of depression, anxiety, or other mental health conditions know the therapeutic process can be a long and arduous one. But as healthcare becomes more digital, we see an opportunity to help people key in on the treatments that might work for them sooner, eliminating some of the false hits along the way, through <em>subtyping</em>. Subtyping is already happening around us when we receive suggestions, for example, for reads or programs we might enjoy after purchasing a book online or streaming a show. The systems at work use past behavior to make these recommendations, creating a crude heuristic by which they could assume people who responded favorably to one thing will respond favorably to something similar.</p>\n<p>In their relatively young existence, internet-delivered mental health interventions have not only been shown to provide effective treatment at lower costs, but have also produced data from which to learn similar patterns of engagement. And while studies have shown how more engagement with the treatment leads to better outcomes, there’s an awareness that the types of engagement—check-ins with a clinician versus more self-directed work, let’s say—make a difference. Yet, how these different forms of engagement affect mental health outcomes is less well known.</p>\n<p>Through the continuing <a href=\"https://www.microsoft.com/en-us/research/project/project-talia/\">Project Talia</a> research collaboration between <a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/\">Microsoft Research Cambridge</a>, <a href=\"https://www.tcd.ie/\">Trinity College Dublin</a>, and <a href=\"https://www.silvercloudhealth.com/uk\">SilverCloud Health</a>, we’ve developed a proof of concept for identifying more nuanced patterns of engagement. SilverCloud Health, the world’s largest provider in digital mental health, offers a suite of internet-delivered cognitive behavioral therapy (iCBT) interventions for the treatment of depression, anxiety, and other mental health conditions. Leveraging anonymized long-term engagement data and clinical measures from 54,604 SilverCloud Health patients—the largest dataset of its kind—we built a machine learning framework that identifies subtypes of patients based on their use of the iCBT intervention over time and investigates how these subtypes predict clinical outcomes for patients with symptoms of both depression and anxiety.</p>\n<p>Our work, which was published in a <em>JAMA Open</em> article titled <a href=\"https://www.microsoft.com/en-us/research/publication/a-machine-learning-approach-to-understanding-patterns-of-engagement-with-internet-delivered-mental-health-interventions/\">“A Machine Learning Approach to Understanding Patterns of Engagement with Internet-Delivered Mental Health Interventions,”</a> helps pave the way for better understanding that if people engage with treatment differently and get different outcomes how we can optimize and personalize treatment so they receive the best results possible.</p>\n<p><div id=\"attachment_675894\" style=\"width: 1034px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-675894\" class=\"wp-image-675894 size-large\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-1024x762.png\" alt=\"A diagram of a hidden Markov model employed by researchers to identify subtypes of patients based on their use of internet-delivered cognitive behavioral therapy. Diagram shows how the number of subtypes, represented by the letter “K,” is determined by defining a hidden state that captures true engagement, represented by the letter “x,” which is inferred from observed engagement “Y” and learning the probability of patients transitioning across these states “x” over time, represented by “abr.”\" width=\"1024\" height=\"762\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-1024x762.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-300x223.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-768x571.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-80x60.png 80w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-240x180.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1.png 1184w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><p id=\"caption-attachment-675894\" class=\"wp-caption-text\">Figure 1: Researchers’ hidden Markov model represents how observed engagement (Y) with different sections (s) on the SilverCloud platform transitions over time (t) and whether these engagements with different sections are representative of a latent subtype of engagement (K) that can be learned from the observed data. Here, x is a hidden state representing true engagement each week; π is a uniform Dirichlet prior probability, which assumes each patient has equal probability of belonging to a particular state; a<sub>br</sub> is the probability of a patient moving across states over time; and q<sub>s</sub> and q'<sub>s</sub> are probabilities of state membership.</p></div></p>\n<h3>Identifying subtypes using probabilistic graphical models</h3>\n<p>To understand whether different types, or patterns, of patient behaviors exist in the way people engage with an iCBT intervention for depression and anxiety, we employed an unsupervised machine learning approach using probabilistic graphical modeling. Probabilistic graphical models provide an especially rich framework in which to represent complex data structures, such as those found in healthcare, where we’re trying to understand disease progression based on many different symptoms and factors. There are several examples of the use of probabilistic graphical models to understand disease heterogeneity in a longitudinal, observational clinical context, such as sepsis, asthma, and kidney failure. Here, we’re looking at a variety of different patient actions over an extended period of time across thousands of participants to try to identify subtypes. For 54,604 SilverCloud Health patients, that’s more than 3 million data points.</p>\n<p>The probabilistic graphical modeling we used is the hidden Markov model (Figure 1), which assumes behavior today is conditioned by behavior yesterday. So whatever action we observe someone doing on one day is conditioned by the action on the previous day. From this model, we want to learn the probability of moving from one state to another: If a patient is engaging with a specific tool today and yesterday they didn’t, what’s the probability they’ll engage with it on a particular day moving forward? We assume the probabilistic patterns of behavior are governed by something that’s not directly observed but rather is latent and can be inferred from the transition, or change in a person’s behavior from one day to the next, over time. The model seeks to capture what we can’t measure directly with only raw metrics like resource usage—<em>how engaged</em> a person is.</p>\n<p>Our model represents how observed engagement (Y) with different sections (s) on the SilverCloud platform transitions over time (t) and whether these engagements with different sections are representative of a latent subtype of engagement (K) that we can learn from the observed data. We define a hidden state (x) that represents true engagement each week. We assume a uniform initialization probability (π)—more specifically, a uniform Dirichlet prior probability—and learn the probability of patients transitioning across states over time (a<sub>br</sub>). These transition probabilities are governed by the overarching latent subtype (K). We don’t know the number or size of subtypes in K, but we learn the optimal number using penalized log-likelihood based on Bayesian information criterion, which identifies the most parsimonious number of subtypes that best describes the data, ranging K between 1 and 10.</p>\n<p>Prior to our work, SilverCloud Health patients had given permission for their anonymized data to be used for studies like ours. Omitted from the data were not only names but also age, gender, and other demographics and the content of information they provided as part of treatment. The data included only what they did on the program—sections they clicked, amount of time spent on particular sections, whether they had conversations with support staff, and the like.</p>\n<p><div id=\"attachment_675900\" style=\"width: 1034px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-675900\" class=\"wp-image-675900 size-large\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-1024x848.png\" alt=\"A line graph shows patient engagement with internet-delivered cognitive behavioral therapy by subtype, or class. On the y-axis is the percentage of engagement; on the x-axis is the week, marked in increments of two. A key identifies the color associated with each subtype, or class. Each class experiences a decrease in engagement probability over the 14 weeks. Class 1 was least likely to engage and over time experienced a steady decline in engagement. Class 2 had low engagement to start and decreased more slowly. Class 3 had the second highest probability of initial engagement and the sharpest drop in engagement. Class 4 was consistently high in its probability of engagement. Class 5 was most likely to engage over the 14 weeks. \" width=\"1024\" height=\"848\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-1024x848.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-300x249.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-768x636.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-1536x1273.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2.png 1680w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><p id=\"caption-attachment-675900\" class=\"wp-caption-text\">Figure 2: The ML framework developed by researchers to better understand patient engagement with internet-delivered cognitive behavioral therapy identified five subtypes, or classes, of engagement. The above illustrates the level of engagement over time per subtype. Over 14 weeks, Class 1 showed a steady decrease in engagement, while Class 3 engaged often to start before experiencing the steepest decline in engagement. Class 5 maintained the highest level of engagement. Of the 54,604 patients, a small number had equal probability of being assigned to two different subtypes and weren’t counted toward the totals of either.</p></div></p>\n<h3>Identified subtypes and their predicted clinical outcomes</h3>\n<p>Using our probabilistic model, we identified five subtypes of engagement (Figure 2), inferred based on patterns in how patients interacted with different program sections over 14 weeks (of the 54,604 patients, a small number had equal probability of being assigned to two different subtypes and weren’t counted toward the totals of either):</p>\n<ul>\n<li>Class 1 (“low engagers”; n = 19,930; 36.5%)</li>\n<li>Class 2 (“late engagers”; n = 11,674; 21.4%)</li>\n<li>Class 3 (“high engagers with rapid disengagement”; n = 13,936; 25.5%)</li>\n<li>Class 4 (“high engagers with moderate decrease”; n = 3,258; 6.0%)</li>\n<li>Class 5 (“highest engagers”; n = 5,799; 10.6%)</li>\n</ul>\n<p>With the end goal of delivering more effective, tailored treatment, we investigated whether these prototypical patient behaviors were associated with improvements in depression and anxiety, as assessed through patients’ regular completion of standardized clinical questionnaires for symptoms of depression (PHQ-9) and symptoms of anxiety (GAD-7).</p>\n<p>We found that these distinct subtypes had marked differences in PHQ-9 and GAD-7. Patients in Class 3, although they spent less time on the platform than Classes 4 and 5, had significantly greater weekly change in PHQ-9 over time. Class 2 had the least severe symptoms, indicated by the lowest mean initial PHQ-9, and saw the least amount of improvement despite the fact its initial mean PHQ-9 was not significantly different from the initial mean of Class 5. Trends were similar when it came to differences in anxiety symptoms. After 14 weeks, estimated gains in GAD-7 were greatest among Class 3 patients and lowest among Class 2 patients. The promising news is all subtypes—including low engagers—experienced positive results, as measured by PHQ-9 and GAD-7, suggesting any type of engagement can be effective in reducing symptoms of depression and anxiety. Those patients belonging to groups who engaged more, though, did see better outcomes.</p>\n<p>Further investigation revealed the specific usage patterns that contributed to the outcomes observed above. While Class 5 opted for such resources as the relaxation and mindfulness tools, Class 4 used goal-based activities and mood tracking more and a lot of the core CBT treatment contents. Patients in Class 3 were not only more inclined to complete components of the core contents, but they also did so in their first few weeks.</p>\n<h3>Implications for achieving more personalized mental health interventions</h3>\n<p>We believe this approach of understanding and predicting future interactions with iCBT can lead to earlier intervention strategies and consequently better clinical outcomes. This approach enables us to scope beyond an average treatment effect. It helps us move toward care that responds to each patient’s needs and desired outcomes and ultimately individualized delivery strategies that can maximize engagement, minimize dropout, and improve patients’ clinical scores. By identifying different subtypes of patients early on, for example, we may be able to recommend at the outset those resources that have been found to lead to better engagement and improve symptoms more effectively.</p>\n<p>The sensitive and fluctuating nature of mental health symptomology requires an increase in access to interventions that can be personalized in this way. We see online interventions as a means of achieving that goal. Further research and innovation and the continued identification of meaningful ways for machine learning to have a positive impact on mental well-being are necessary. We hope that through our partnership with SilverCloud Health, we can leverage advances in AI to help people get the specific help they need sooner.</p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/a-path-to-personalization-using-ml-to-subtype-patients-receiving-digital-mental-health-interventions/\">A path to personalization: Using ML to subtype patients receiving digital mental health interventions</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Fri, 17 Jul 2020 15:01:37 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_SilverCloud_NoLogo_animation_updated-final.gif","linkMd5":"e51ea0a4c5e4ddb7f86b61ead8e413e6","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn11@2020_2/2020/08/24/21-40-50-446_ff882ef063b0aedd.webp","destWidth":1400,"destHeight":788,"sourceBytes":1360942,"destBytes":2580902,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_SilverCloud_NoLogo_animation_updated-final.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn11@2020_2/2020/08/24/21-40-50-446_ff882ef063b0aedd.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-1024x762.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn12@2020_6/2020/08/24/21-40-53-294_aad22602db146881.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-1024x848.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn20@2020_2/2020/08/24/21-40-53-477_6fe45b8b34ae8429.webp"},"publishedOrCreatedDate":1598305163714},{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","title":"Research Collection: The Unseen History of Audio and Acoustics Research at Microsoft","link":"https://www.microsoft.com/en-us/research/?p=681651","description":"<h2>Audio and Acoustics Research at Microsoft</h2> \n<p>Getting the sound right is a crucial ingredient in natural user interfaces, immersive gaming, realistic virtual and mixed reality, and ubiquitous computing. Audio also plays an important role in assistive technologies for people who are blind or have low vision, and speech recognition and processing can help support those who are deaf or hard of hearing. Although computers have been capable of playing and processing high-fidelity audio for many decades, there are many frontiers left to explore in computational recognition, analysis and rendering of sound for speech or immersive sound fields.</p> \n<figure class=\"wp-block-image size-large\">\n <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1024x576.jpg\" alt=\"audio and acoustics: woman and man setting up a dummy in anachoic chamber\" class=\"wp-image-682323\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n</figure> \n<p>Audio has been a key research area since Microsoft Research was founded in 1991 – in its first year, researchers used audio data as well as other cues to explore automatic summarization of audiovisual presentations. Over the years, there have been steady and significant research advances in speech recognition, natural user interfaces, audio as a tool for collaboration and productivity, capturing and reproducing sound, spatial audio, acoustic simulation and audio analytics.</p> \n<p>Many of these advances have shipped in Microsoft products and services like Windows 10, Kinect, HoloLens and Teams, as well as Ford’s SYNC in-car infotainment system, Polycom’s videoconferencing devices, and major game titles such as Gears of War, Sea of Thieves and Borderlands 3. Still more are working their way into future products and services, and into the hands of developers.</p> \n<p><strong>Use the timelines below to explore several threads of audio and acoustics research as they evolved from theories and experiments to real-world applications.</strong></p> \n<hr class=\"wp-block-separator has-text-color has-background has-green-background-color has-green-color is-style-dots\" /> \n<h2 class=\"alignwide has-text-align-wide\">Speech recognition and natural user interfaces</h2> \n<div class=\"wp-block-msr-block-journey journey--date alignwide\" data-bi-area=\"block-journey\"> \n <span class=\"journey-circle\"></span> \n <ol class=\"list-style-display-date\"> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2002 \n    </div> \n   </div> <h3 class=\"moment__title\">Microsoft researchers establish the Sound Capture and Speech Enhancement project</h3> <p>The <a href=\"https://www.microsoft.com/en-us/research/project/sound-capture-speech-enhancement/\">Sound Capture and Speech Enhancement</a> project begins to explore areas such as acoustic echo reduction, microphone array processing and noise reduction.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/gain-self-calibration-procedure-for-microphone-arrays-2/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Gain Self-Calibration Procedure for Microphone Arrays\"> Gain Self-Calibration Procedure for Microphone Arrays </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This paper introduces one of the technologies that made microphone arrays feasible for manufacturing.</span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/a-new-beamformer-design-algorithm-for-microphone-arrays/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A New Beamformer Design Algorithm for Microphone Arrays\"> A New Beamformer Design Algorithm for Microphone Arrays </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <p></p> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/reverberation-reduction-for-better-speech-recognition/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Reverberation Reduction for Better Speech Recognition\"> Reverberation Reduction for Better Speech Recognition </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/microphone-array-post-processor-using-instantaneous-direction-of-arrival/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Microphone Array Post-Processor Using Instantaneous Direction of Arrival\"> Microphone Array Post-Processor Using Instantaneous Direction of Arrival </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2007 \n    </div> \n   </div> <h3 class=\"moment__title\">Ford releases SYNC</h3> <p>Ford releases the first version of its SYNC in-car infotainment system, with a speech enhancement audio pipeline first designed by Microsoft researchers.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-1024x576.jpg\" alt=\"audio and acoustics: man standing in front of Experience Ford SYNC kiosk\" class=\"wp-image-682314\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Video</span> <a href=\"https://www.microsoft.com/en-us/research/video/natural-language-moves-in-car-infotainment-forward/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Natural Language Moves In-Car Infotainment Forward\"> Natural Language Moves In-Car Infotainment Forward </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">February 2009</span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/unified-framework-for-single-channel-speech-enhancement/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Unified Framework for Single Channel Speech Enhancement\"> Unified Framework for Single Channel Speech Enhancement </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This paper introduced the parameter optimization approach used in Ford SYNC’s speech enhancement pipeline (August 2009).</span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2007 \n    </div> \n   </div> <h3 class=\"moment__title\">Windows support for microphone arrays</h3> <p>Microsoft releases Windows Vista, including support for four preselected microphone array geometries and standardized support for USB microphone arrays. Later, Windows 10 is updated to include support for microphone arrays with arbitrary geometry.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/sound-capture-and-processing-practical-approaches/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Sound Capture and Processing: Practical Approaches\"> Sound Capture and Processing: Practical Approaches </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This book includes the introduction of multichannel acoustic echo cancellation, which later ships as part of Microsoft Kinect (July 2009).</span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <p></p> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2010 \n    </div> \n   </div> <h3 class=\"moment__title\">Hands-free control in Kinect</h3> <p>Microsoft releases Kinect for Xbox 360, which includes the first hands-free open microphone command and control product with surround sound echo cancellation.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-1024x576.jpg\" alt=\"Kinect Voice Illustration 2010\" class=\"wp-image-684177\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/beamformer-design-using-measured-microphone-directivity-patterns-robustness-to-modelling-error/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Beamformer Design Using Measured Microphone Directivity Patterns: Robustness to Modelling Error\"> Beamformer Design Using Measured Microphone Directivity Patterns: Robustness to Modelling Error </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/optimal-3d-beamforming-using-measured-microphone-directivity-patterns/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Optimal 3D Beamforming Using Measured Microphone Directivity Patterns\"> Optimal 3D Beamforming Using Measured Microphone Directivity Patterns </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/data-driven-suppression-rule-for-speech-enhancement/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Data Driven Suppression Rule for Speech Enhancement\"> Data Driven Suppression Rule for Speech Enhancement </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/kinect-development-kit-toolkit-gesture-speech-based-human-machine-interaction-2/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Kinect Development Kit: A Toolkit for Gesture- and Speech-Based Human-Machine Interaction\"> Kinect Development Kit: A Toolkit for Gesture- and Speech-Based Human-Machine Interaction </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2016 \n    </div> \n   </div> <h3 class=\"moment__title\">Microsoft releases HoloLens</h3> <p>Microsoft releases HoloLens, which contains a four-element microphone array and a sophisticated sound capture and speech enhancement system for capturing the voice of the wearer and the ambient sound environment.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-1024x576.jpg\" alt=\"image of hands holding a Hololens device\" class=\"wp-image-684174\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2017 \n    </div> \n   </div> <h3 class=\"moment__title\">Researchers begin exploring neural networks for speech enhancement</h3> <p>In 2017, Microsoft researchers establish the <a href=\"https://www.microsoft.com/en-us/research/project/nn-speech-enhancement/\">Neural Networks-Based Speech Enhancement</a> project, which aims for more accurate and reliable speech processing, particularly on mobile, wearable, smart home and IoT devices – which, unlike previous devices, present new challenges such as noisier background environments, greater speaker-microphone distances, and limited edge processing abilities.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/causal-speech-enhancement-approach-combining-data-driven-learning-suppression-rule-estimation/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A Causal Speech Enhancement Approach Combining Data-driven Learning and Suppression Rule Estimation\"> A Causal Speech Enhancement Approach Combining Data-driven Learning and Suppression Rule Estimation </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/a-hybrid-approach-to-combining-conventional-and-deep-learning-techniques-for-single-channel-speech-enhancement-and-recognition/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A Hybrid Approach to Combining Conventional and Deep Learning Techniques for Single-channel Speech Enhancement and Recognition\"> A Hybrid Approach to Combining Conventional and Deep Learning Techniques for Single-channel Speech Enhancement and Recognition </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/convolutional-recurrent-neural-networks-for-speech-enhancement/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Convolutional-Recurrent Neural Networks for Speech Enhancement\"> Convolutional-Recurrent Neural Networks for Speech Enhancement </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/constrained-convolutional-recurrent-networks-for-improve-speech-quality-with-low-impact-on-recognition-accuracy/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Constrained Convolutional-recurrent Networks to Improve Speech Quality with Low Impact on Recognition Accuracy\"> Constrained Convolutional-recurrent Networks to Improve Speech Quality with Low Impact on Recognition Accuracy </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/limiting-numerical-precision-of-neural-networks-to-achieve-real-time-voice-activity-detection/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Limiting Numerical Precision of Neural Networks to Achieve Real-time Voice Activity Detection\"> Limiting Numerical Precision of Neural Networks to Achieve Real-time Voice Activity Detection </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2019 \n    </div> \n   </div> <h3 class=\"moment__title\">Microsoft releases HoloLens 2</h3> <p>The device contains a five-element microphone array and sophisticated sound capture and speech enhancement system for capturing the voice of the wearer as well as the ambient sound environment. Researchers explored key components of its speech enhancement technology earlier in the year.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/weighted-speech-distortion-losses-for-neural-network-based-real-time-speech-enhancement/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement\"> Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/acoustic-localization-using-spatial-probability-in-noisy-and-reverberant-environments/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Acoustic Localization using Spatial Probability in Noisy and Reverberant Environments\"> Acoustic Localization using Spatial Probability in Noisy and Reverberant Environments </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2020 \n    </div> \n   </div> <h3 class=\"moment__title\">Speech enhancement incorporated into Microsoft Teams</h3> <p>Microsoft CEO Satya Nadella <a href=\"https://www.linkedin.com/posts/satyanadella_tools-like-microsoft-teams-and-microsoft-activity-6646788172292456448-d0DD/\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">announces</a> that new improvements to Microsoft Teams will include a neural network-based speech enhancement algorithm.</p> </li> \n </ol> \n</div> \n<hr class=\"wp-block-separator has-text-color has-background has-teal-background-color has-teal-color is-style-dots\" /> \n<h2 class=\"alignwide has-text-align-wide\">Audio for collaboration and productivity</h2> \n<div class=\"wp-block-msr-block-journey journey--date alignwide\" data-bi-area=\"block-journey\"> \n <span class=\"journey-circle\"></span> \n <ol class=\"list-style-display-date\"> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      1991 \n    </div> \n   </div> <h3 class=\"moment__title\">First audio-related paper published</h3> <p>Microsoft researchers publish their first audio-related <a href=\"https://www.microsoft.com/en-us/research/publication/auto-summarization-of-audio-video-presentations/\">paper</a>, on the automatic summarization of multimedia presentations.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-1024x555.png\" alt=\"audio and acoustics: 1991 software testing window UI\" class=\"wp-image-682302\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-1024x555.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-300x162.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-768x416.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-1536x832.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot.png 1590w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n    <figcaption>\n     The slides, shown on right, are synchronized with summary-segment transitions derived from the presentation at left.\n    </figcaption>\n   </figure> <p></p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      1996 \n    </div> \n   </div> <h3 class=\"moment__title\">Seeing the sound</h3> \n   <div class=\"wp-block-image\">\n    <figure class=\"alignright size-full\">\n     <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR.png\" alt=\"audio and acoustics: man standing in front on a large VR screen with hands up\" class=\"wp-image-682305\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR.png 786w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR-300x219.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR-768x562.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR-80x60.png 80w\" sizes=\"(max-width: 786px) 100vw, 786px\" />\n    </figure>\n   </div> <p>In 1996, Microsoft researchers explore ways to use vision data to capture and render sound in interactive environments.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"annotations\" data-bi-area=\"citation\"> \n    <ul class=\"annotations__list \"> \n     <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/vision-steered-audio-interactive-environments/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Vision-Steered Audio for Interactive Environments\"> Vision-Steered Audio for Interactive Environments </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n    </ul> \n   </div> \n   <div style=\"height:124px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> <p></p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      1999 \n    </div> \n   </div> <h3 class=\"moment__title\">Progress in audio detection and classification</h3> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/detection-of-target-speakers-in-audio-databases-2/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Detection of target speakers in audio databases\"> Detection of target speakers in audio databases </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This paper introduces technology to detect individual speakers in audio, which is later implemented in Microsoft RoundTable.</span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/a-robust-audio-classification-and-segmentation-method/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A Robust Audio Classification and Segmentation Method\"> A Robust Audio Classification and Segmentation Method </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This paper introduces robust audio classification and segmentation – which is used to distinguish speech, music, environmental noise, and silence.</span> </li> \n      </ul> \n     </div> \n     <p></p> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2001 \n    </div> \n   </div> <h3 class=\"moment__title\">Project RingCam established</h3> <p>Microsoft researchers establish Project RingCam, to explore 360-degree videoconferencing.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-1024x576.jpg\" alt=\"audio and acoustics: project RingCam video conference screen\" class=\"wp-image-682311\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference.jpg 1280w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2016/02/Cutler_DMeetings_ACMMM_02.pdf\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Distributed Meetings: A Meeting Capture and Broadcasting System\"> Distributed Meetings: A Meeting Capture and Broadcasting System </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"></div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2007 \n    </div> \n   </div> <h3 class=\"moment__title\">Microsoft RoundTable ships with speaker detection technology</h3> \n   <div class=\"wp-block-image\">\n    <figure class=\"alignleft size-medium\">\n     <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849-300x217.jpg\" alt=\"photo of the Microsoft RoundTable video conferencing device\" class=\"wp-image-684744\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849-300x217.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849-1024x741.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849-768x556.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849.jpg 1400w\" sizes=\"(max-width: 300px) 100vw, 300px\" />\n    </figure>\n   </div> <p>Speaker detection technology developed by Microsoft researchers ships as part of the Microsoft Roundtable conferencing system.</p> <p>The technology is later sold to Polycom and released as the Polycom CX5000.</p> </li> \n </ol> \n</div> \n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n<hr class=\"wp-block-separator is-style-dots\" /> \n<h2 class=\"alignwide has-text-align-wide\">Capturing and reproducing sound</h2> \n<div class=\"wp-block-msr-block-journey journey--date alignwide\" data-bi-area=\"block-journey\"> \n <span class=\"journey-circle\"></span> \n <ol class=\"list-style-display-date\"> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      1998 \n    </div> \n   </div> <h3 class=\"moment__title\">Researchers begin experimenting with microphone arrays</h3> <p>Microsoft researchers build their first microphone array, using an Erector set.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-1024x447.jpg\" alt=\"audio and acoustics: prototype of microphone array\" class=\"wp-image-682308\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-1024x447.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-300x131.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-768x335.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-1536x671.jpg 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-2048x894.jpg 2048w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n    <figcaption>\n     This is one of the first prototypes of microphone arrays designed in the Signal Processing group by Rico Malvar and Dinei Fiorencio in 1998.\n    </figcaption>\n   </figure> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2005 \n    </div> \n   </div> <h3 class=\"moment__title\">USB microphone array prototypes</h3> <p>Microsoft researchers establish the <a href=\"https://www.microsoft.com/en-us/research/project/audio-devices/\">Audio Devices</a> project, and build and evaluate two USB microphone array prototypes: a four element linear array and an eight element circular array.</p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2007 \n    </div> \n   </div> <h3 class=\"moment__title\">An anechoic chamber in Building 99</h3> <p>Microsoft Research Redmond moves into its new home in Building 99. The building includes the company’s first anechoic chamber.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-1024x550.jpg\" alt=\"view of inside the anechoic chamber\" class=\"wp-image-682794\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-1024x550.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-300x161.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-768x413.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-710x380.jpg 710w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber.jpg 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> <p>Key publications from 2007:</p> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/robust-design-of-wideband-loudspeaker-arrays/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Robust Design of Wideband Loudspeaker Arrays\"> Robust Design of Wideband Loudspeaker Arrays </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/sound-capture-system-and-spatial-filter-for-small-devices/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Sound Capture System and Spatial Filter for Small Devices\"> Sound Capture System and Spatial Filter for Small Devices </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2009 \n    </div> \n   </div> <h3 class=\"moment__title\">Anechoic chamber retrofitted to measure sound in 3D</h3> <p>The anechoic chamber in Building 99 is retrofitted to automatically measure 3D directivity and radiation patterns, including human spatial hearing. It uses a 3D scanner with sub-millimeter accuracy to measure the head and torso. Among other things, this enables the advancement of head-related transfer functions (HRTFs), which can enable more realistic-sounding spatial audio.</p> \n   <figure class=\"wp-block-image size-full\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008_AnechoicChamberWithRigFor3Dscanning.jpg\" alt=\"The Microsoft Research anechoic chamber set for measuring human spatial hearing.\" class=\"wp-image-682374\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008_AnechoicChamberWithRigFor3Dscanning.jpg 556w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008_AnechoicChamberWithRigFor3Dscanning-300x197.jpg 300w\" sizes=\"(max-width: 556px) 100vw, 556px\" />\n    <figcaption>\n     The Microsoft Research anechoic chamber set for measuring human spatial hearing.\n    </figcaption>\n   </figure> \n   <figure>\n    <div class=\"video-wrapper\">\n     <iframe src=\"https://channel9.msdn.com/Series/CampusTours/Microsoft-Campus-Tours-Microsoft-Research-Part-1-The-Anechoic-Chamber/player\" width=\"960\" height=\"560\" allowfullscreen=\"\" frameborder=\"0\" title=\"Microsoft Campus Tours - Microsoft Research Part 1 - The Anechoic Chamber - Microsoft Channel 9 Video\"></iframe>\n    </div>\n    <figcaption>\n     Microsoft Campus Tours – Microsoft Research Part 1 – The Anechoic Chamber\n    </figcaption>\n   </figure> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2012 \n    </div> \n   </div> <h2 class=\"moment__title\">Progress in microphone arrays</h2> <p>Microsoft researchers build a spherical 16 channel microphone array and a cylindrical 16 channel microphone array to study sound field decomposition using spherical and cylindrical functions. In 2016, they build a 64-channel spherical microphone array.</p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2017 \n    </div> \n   </div> <h3 class=\"moment__title\">A new approach to gesture recognition</h3> <p><a href=\"https://www.microsoft.com/en-us/research/publication/ultrasound-based-gesture-recognition-2/\">Ultrasound-based Gesture Recognition</a> – This paper introduces a new approach to gesture recognition using ultrasound waves, which uses significantly less power than optical systems.</p> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures-1024x238.png\" alt=\"audio and acoustics: figures showing ultrasound based gesture recognition setup\" class=\"wp-image-683289\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures-1024x238.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures-300x70.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures-768x179.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n    <figcaption>\n     Figure 1: Left: Hardware set-up and close-up of the ultrasonic piezoelectric transducer at the center and an 8-element microphone array around it in a circular configuration.\n     <br><br>Figure 2: Right: Block diagram of the proposed approach.</br></br>\n    </figcaption>\n   </figure> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/hardware-and-algorithms-for-ultrasonic-depth-imaging/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Hardware and Algorithms for Ultrasonic Depth Imaging\"> Hardware and Algorithms for Ultrasonic Depth Imaging </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/video/multimodal-gesture-recognition/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Multimodal Gesture Recognition\"> Multimodal Gesture Recognition </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This paper further demonstrates live ultrasound sensing for gesture recognition.</span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2018 \n    </div> \n   </div> <h3 class=\"moment__title\">Live 360 audio and video streaming</h3> \n   <figure class=\"wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\">\n    <div class=\"wp-block-embed__wrapper\"> \n     <iframe title=\"Live 360 Audio and Video Streaming\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/hVrFALdY8Hc?feature=oembed&amp;rel=0\" frameborder=\"0\" allowfullscreen=\"\"></iframe> \n    </div>\n   </figure> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2019 \n    </div> \n   </div> <h3 class=\"moment__title\">Project Denmark established</h3> <p>Microsoft researchers establish <a href=\"https://www.microsoft.com/en-us/research/project/project-denmark/\">Project Denmark</a>, which aims to achieve high-quality capture of meeting conversations using virtual microphone arrays composed of ordinary consumer devices such as mobile phones and laptops.</p> </li> \n </ol> \n</div> \n<hr class=\"wp-block-separator has-text-color has-background has-teal-background-color has-teal-color is-style-dots\" /> \n<h2 class=\"alignwide has-text-align-wide\">Spatial audio</h2> \n<div class=\"wp-block-msr-block-journey journey--date alignwide\" data-bi-area=\"block-journey\"> \n <span class=\"journey-circle\"></span> \n <ol class=\"list-style-display-date\"> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2012 \n    </div> \n   </div> <h3 class=\"moment__title\">New directions for spatial audio</h3> <p>Microsoft researchers begin exploring new approaches to head-related transfer functions (HRTFs), which represent the acoustic transfer function from a sound source at a given location to the ear drums of a human. A potential consequence of this work is more realistic spatial audio that is tuned to the shape of the listener’s head and torso.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/hrtf-magnitude-modeling-using-a-non-regularized-least-squares-fit-of-spherical-harmonics-coefficients-on-incomplete-data/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"HRTF Magnitude Modeling Using a Non-Regularized Least-Squares Fit of Spherical Harmonics Coefficients on Incomplete Data\"> HRTF Magnitude Modeling Using a Non-Regularized Least-Squares Fit of Spherical Harmonics Coefficients on Incomplete Data </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/hrtf-magnitude-synthesis-via-sparse-representation-of-anthropometric-features/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"HRTF Magnitude Synthesis via Sparse Representation of Anthropometric Features\"> HRTF Magnitude Synthesis via Sparse Representation of Anthropometric Features </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">The HRTF personalization used in HoloLens.</span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/hrtf-phase-synthesis-via-sparse-representation-of-anthropometric-features/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"HRTF Phase Synthesis via Sparse Representation of Anthropometric Features\"> HRTF Phase Synthesis via Sparse Representation of Anthropometric Features </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Blog</span> <a href=\"https://www.windowscentral.com/microsoft-3d-audio-tech-makes-virtual-sounds-sound-real\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Microsoft 3D audio tech makes virtual sounds sound real\"> Microsoft 3D audio tech makes virtual sounds sound real </a> <span class=\"svg-icon icon-external-link\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> \n   <figure class=\"wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\">\n    <div class=\"wp-block-embed__wrapper\"> \n     <iframe title=\"3-D Audio Demo\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/wtFgLi6XKrY?feature=oembed&amp;rel=0\" frameborder=\"0\" allowfullscreen=\"\"></iframe> \n    </div>\n   </figure> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2015 \n    </div> \n   </div> <h3 class=\"moment__title\">Virtual surround sound in Windows 10</h3> <p>Microsoft releases Windows 10 with support for virtual surround sound, marketed as Windows Sonic. This spatial audio rendering system is later released as part of HoloLens.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/estimation-of-multipath-propagation-delays-and-interaural-time-differences-from-3-d-head-scans/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Estimation of Multipath Propagation Delays and Interaural Time Differences from 3-D Head Scans\"> Estimation of Multipath Propagation Delays and Interaural Time Differences from 3-D Head Scans </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/applications-of-3d-spherical-transforms-to-personalization-of-head-related-transfer-functions/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Applications of 3D Spherical Transforms To Personalization Of Head-Related Transfer Functions\"> Applications of 3D Spherical Transforms To Personalization Of Head-Related Transfer Functions </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2016 \n    </div> \n   </div> <h3 class=\"moment__title\">Personalized audio rendering in HoloLens</h3> <p>Microsoft releases HoloLens. The device features an audio rendering system with on-the-fly personalization of the wearer’s spatial hearing.</p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2016 \n    </div> \n   </div> <h3 class=\"moment__title\">Microsoft releases the Windows Mixed Reality platform</h3> <p>Windows 10 includes support for virtual and mixed reality headsets manufactured by other companies. The platform contains an extended and improved version of the spatial audio engine. </p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/head-related-transfer-function-personalization-needs-spatial-audio-mixed-virtual-reality/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Head-related transfer function personalization for the needs of spatial audio in mixed and virtual reality\"> Head-related transfer function personalization for the needs of spatial audio in mixed and virtual reality </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"></div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2017 \n    </div> \n   </div> <h3 class=\"moment__title\">A map delivered in 3D sound</h3> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-1024x576.jpg\" alt=\"Man holding a smart phone, standing next to his guide dog\" class=\"wp-image-470751\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-343x193.jpg 343w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> <p class=\"has-text-align-left\">Microsoft releases <a href=\"https://www.microsoft.com/en-us/research/product/soundscape/\">Soundscape</a> (in collaboration with Guide Dogs UK) – a helper app for visually impaired people, which includes a spatial audio rendering system. Read about the <a href=\"https://www.microsoft.com/en-us/research/project/soundscape-maps-delivered-in-3d-sound/\">research behind the product</a>.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/blind-reverberation-time-estimation-using-a-convolutional-neural-network/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Blind reverberation time estimation using a convolutional neural network\"> Blind reverberation time estimation using a convolutional neural network </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Video</span> <a href=\"https://www.microsoft.com/en-us/research/video/microsoft-soundscape-map-delivered-3d-sound/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Microsoft Soundscape: A Map Delivered in 3D Sound\"> Microsoft Soundscape: A Map Delivered in 3D Sound </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2018 \n    </div> \n   </div> <h3 class=\"moment__title\">Podcast: Hearing in 3D with Dr. Ivan Tashev</h3> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-1024x576.png\" alt=\"Ivan Tashev podcast\" class=\"wp-image-549597\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> \n   <figure class=\"wp-block-audio\">\n    <audio controls=\"\" src=\"https://media.blubrry.com/microsoftresearch/b/content.blubrry.com/microsoftresearch/msr_050_tashev.mp3\" preload=\"none\"></audio>\n    <figcaption>\n     <br>In this podcast, Dr. Tashev provides an overview of the quest for better sound processing and speech enhancement, describes the latest innovations in 3D audio, and explains why the research behind audio processing technology is, thanks to variations in human perception, equal parts science, art and craft.</br>\n    </figcaption>\n   </figure> \n   <div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> <p>Key publications from 2018:</p> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/a-sparsity-measure-for-echo-density-growth-in-general-environments/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A Sparsity Measure for Echo Density Growth in General Environments\"> A Sparsity Measure for Echo Density Growth in General Environments </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/blind-room-volume-estimation-from-single-channel-noisy-speech/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Blind Room Volume Estimation from Single-channel Noisy Speech\"> Blind Room Volume Estimation from Single-channel Noisy Speech </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/capture-representation-and-rendering-of-3d-audio-for-virtual-and-augmented-reality/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Capture, representation, and rendering of 3D audio for virtual and augmented reality\"> Capture, representation, and rendering of 3D audio for virtual and augmented reality </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/improving-binaural-ambisonics-decoding-by-spherical-harmonics-domain-tapering-and-coloration-compensation/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Improving Binaural Ambisonics Decoding by Spherical Harmonics Domain Tapering and Coloration Compensation\"> Improving Binaural Ambisonics Decoding by Spherical Harmonics Domain Tapering and Coloration Compensation </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/spectral-manipulation-improves-elevation-perception-with-non-individualized-head-related-transfer-functions/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Spectral manipulation improves elevation perception with non-individualized head-related transfer functions\"> Spectral manipulation improves elevation perception with non-individualized head-related transfer functions </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n </ol> \n</div> \n<hr class=\"wp-block-separator has-text-color has-background has-green-background-color has-green-color is-style-dots\" /> \n<h2 class=\"alignwide has-text-align-wide\">Acoustic simulation</h2> \n<div class=\"wp-block-msr-block-journey journey--date alignwide\" data-bi-area=\"block-journey\"> \n <span class=\"journey-circle\"></span> \n <ol class=\"list-style-display-date\"> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2010 \n    </div> \n   </div> <h3 class=\"moment__title\">Microsoft researchers establish Project Triton</h3> <p>Prior to 2010, a key challenge in interactive audio had been the fast modeling of wave effects in complex game scenes: smooth sound obstruction around doorways, or dynamic reverberation, responsive to both source and listener motion. In this <a href=\"https://www.microsoft.com/en-us/research/publication/precomputed-wave-simulation-real-time-sound-propagation-dynamic-sources-complex-scenes/\">paper</a>, Microsoft researchers introduced the idea of pre-computing physically accurate wave simulations, and showed that it was a viable path forward for interactive audio and games.</p> <p><a href=\"https://www.microsoft.com/en-us/research/project/project-triton/\">Project Triton</a> explores a physics-based approach to modeling virtual environments, for more realistic in-game audio.</p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2012 \n    </div> \n   </div> <h3 class=\"moment__title\">Researchers begin collaboration with game studios</h3> <p>Microsoft researchers begin collaborating with The Coalition Studio to incorporate this acoustic simulation work into Gears of War, transitioning from exploratory research to a targeted redesign focused on performance and flexibility.</p> \n   <ul>\n    <li>2013: The first working prototype of Project Triton is demonstrated internally.</li>\n   </ul> \n   <ul>\n    <li>2014: This <a href=\"https://www.microsoft.com/en-us/research/publication/parametric-wave-field-coding-precomputed-sound-propagation/\">paper</a> describes the core design of Project Triton, combining perceptual coding, spatial compression and parametric rendering. The design solves the problem of system resource usage, and integrates easily into existing audio tools. Later work has built on this core design, with various improvements.</li>\n    <li>2015: A Microsoft Research summer intern researches a <a href=\"https://www.microsoft.com/en-us/research/publication/adaptive-sampling-for-sound-propagation/\">novel adaptive sampling approach</a> to resolve a key robustness issue in Project Triton.</li>\n   </ul> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2016 \n    </div> \n   </div> <h3 class=\"moment__title\">Project Triton ships in Gears of War</h3> <p><a href=\"https://www.microsoft.com/en-us/research/project/project-triton/\">Project Triton</a> ships as part of Gears of War 4 – the first instance of game acoustics provided by accurate physics-based simulation.</p> \n   <figure class=\"wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio\">\n    <div class=\"wp-block-embed__wrapper\"> \n     <iframe title=\"Gears of War 4, Project Triton: Pre-Computed Environmental Wave Acoustics\" width=\"500\" height=\"281\" src=\"https://www.youtube-nocookie.com/embed/qCUEGvIgco8?feature=oembed&amp;rel=0\" frameborder=\"0\" allowfullscreen=\"\"></iframe> \n    </div>\n    <figcaption>\n     GDC 2017 talk on Gears of War integration\n    </figcaption>\n   </figure> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2017 \n    </div> \n   </div> <h3 class=\"moment__title\">Project Triton in Windows 10 Fall Creator’s Update </h3> <p>Project Triton is used in the Mixed Reality experience shipped as part of the Windows 10 Fall Creator’s Update. It provides a natural acoustic experience in the virtual “cliffhouse” space, with new directional acoustics features such as sound that is obstructed by virtual objects, or heard as if coming around corners or through doorways. This experience also incorporates advances in HRTFs described in the previous timeline.</p> <p>In 2018, Project Triton ships as part of Sea of Thieves, the second game to incorporate this technology. The game included custom modifications for evaluating acoustics modularly, illustrating the flexibility of the system.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/spatial-audio-for-immersive-sound-propagation/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Parametric Directional Coding for Precomputed Sound Propagation\"> Parametric Directional Coding for Precomputed Sound Propagation </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> <span class=\"annotations__caption\">This SIGGRAPH paper describes improvements to Triton for encoding and rendering directional acoustic effects.</span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"></div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2019 \n    </div> \n   </div> <h3 class=\"moment__title\">Podcast: Project Triton and the Physics of Sound with Dr. Nikunj Raghuvanshi</h3> \n   <figure class=\"wp-block-image size-large\">\n    <img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-1024x576.png\" alt=\"Nikunj Raghuvanshi\" class=\"wp-image-573663\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" />\n   </figure> \n   <figure class=\"wp-block-audio\">\n    <audio controls=\"\" src=\"https://media.blubrry.com/microsoftresearch/b/content.blubrry.com/microsoftresearch/msr_068_raghuvanshi.mp3\" preload=\"none\"></audio>\n    <figcaption>\n     <br>In this podcast, Dr. Raghuvanshi wants you to hear how sound really travels – in rooms, around corners, behind walls, out doors – and he’s using computational physics to do it.</br>\n    </figcaption>\n   </figure> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2019 \n    </div> \n   </div> <h3 class=\"moment__title\">Project Triton technology released as Project Acoustics</h3> <p>Microsoft makes Project Triton technology available to developers as <a href=\"https://docs.microsoft.com/en-us/gaming/acoustics/what-is-acoustics\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">Project Acoustics</a>, including Unity and Unreal plugins for easy integration into games and research prototypes.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Video</span> <a href=\"https://www.microsoft.com/en-us/research/video/project-triton-making-waves-with-acoustics/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Project Acoustics: Making Waves with Triton\"> Project Acoustics: Making Waves with Triton </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Talk</span> <a href=\"https://www.youtube.com/watch?v=uY4G-GUAQIE\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Project Acoustics | Game Developers Conference 2019\"> Project Acoustics | Game Developers Conference 2019 </a> <span class=\"svg-icon icon-external-link\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> \n   <ul>\n    <li>2019: Gears of War 5 ships, with an immersive audio experience that combines headphone rendering technologies such as Windows Sonic and Dolby Atmos with Triton’s scene-informed sound propagation.</li>\n    <li>2019: Borderlands 3 ships. This is the first game studio outside Microsoft to employ Project Triton.</li>\n   </ul> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> <p></p> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2020 \n    </div> \n   </div> <h3 class=\"moment__title\">Project Acoustics incorporated into HoloLens</h3> <p>This milestone marks the first demonstration of physical acoustics in augmented reality.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/cloud-enabled-interactive-sound-propagation-for-untethered-mixed-reality/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Cloud-Enabled Interactive Sound Propagation for Untethered Mixed Reality\"> Cloud-Enabled Interactive Sound Propagation for Untethered Mixed Reality </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Talk</span> <a href=\"https://aka.ms/mracoustics\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Using Project Acoustics with HoloLens 2\"> Using Project Acoustics with HoloLens 2 </a> <span class=\"svg-icon icon-external-link\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2020 \n    </div> \n   </div> \n   <figure class=\"wp-block-image size-large\">\n    <a href=\"https://note.microsoft.com/MSR-Webinar-Sound-Simulation-Registration-Live.html\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-1024x576.png\" alt=\"\" class=\"wp-image-682326\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788.png 1400w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></a>\n   </figure> <p>In this <a href=\"https://note.microsoft.com/MSR-Webinar-Sound-Simulation-Registration-Live.html\" target=\"_blank\" aria-label=\"undefined (opens in a new tab)\" rel=\"noreferrer noopener\">webinar</a>, Microsoft Principal Researcher <a href=\"https://www.microsoft.com/en-us/research/people/nikunjr/\">Dr. Nikunj Raghuvanshi</a> covers the ins and outs of creating practical, high-quality sound simulations. It includes an overview of the three components of sound simulation: synthesis, propagation, and spatialization, as well as a focus on <a href=\"https://www.microsoft.com/en-us/research/project/project-triton/\">Project Triton</a>. For each, he will review the underlying physics, research techniques, practical considerations, and open research questions.</p> </li> \n </ol> \n</div> \n<hr class=\"wp-block-separator has-text-color has-background has-teal-background-color has-teal-color is-style-dots\" /> \n<h2 class=\"alignwide has-text-align-wide\">Audio analytics</h2> \n<div class=\"wp-block-msr-block-journey journey--date alignwide\" data-bi-area=\"block-journey\"> \n <span class=\"journey-circle\"></span> \n <ol class=\"list-style-display-date\"> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2010 \n    </div> \n   </div> <h3 class=\"moment__title\">Audio Analytics project established</h3> <p>Microsoft researchers establish the <a href=\"https://www.microsoft.com/en-us/research/project/audio-analytics/\">Audio Analytics</a> project, to explore research directions such as extracting non-verbal cues from human speech, detecting specific audio events and background noise, and audio search and retrieval. Potential applications include customer satisfaction analysis from customer support calls, media content analysis and retrieval, medical diagnostic aids and patient monitoring, assistive technologies for people with hearing impairments, and audio analysis for public safety.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/a-new-speaker-identification-algorithm-for-gaming-scenarios/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A New Speaker Identification Algorithm for Gaming Scenarios\"> A New Speaker Identification Algorithm for Gaming Scenarios </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/speech-emotion-recognition-using-deep-neural-network-and-extreme-learning-machine/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine\"> Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/high-level-feature-representation-using-recurrent-neural-network-for-speech-emotion-recognition/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"High-level Feature Representation using Recurrent Neural Network for Speech Emotion Recognition\"> High-level Feature Representation using Recurrent Neural Network for Speech Emotion Recognition </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n  <li class=\"wp-block-msr-block-moment has-date\" data-bi-area=\"block-moment\"> \n   <div class=\"moment__date\"> \n    <div class=\"moment__date-year\">\n      2015 \n    </div> \n   </div> <h3 class=\"moment__title\">“Hey, Cortana” uses speaker identification</h3> <p>Microsoft releases Windows 10 with speaker identification as part of the “Hey, Cortana” wake-up feature.</p> \n   <div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"></div> \n   <div class=\"wp-block-columns\"> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/learning-utterance-level-representations-speech-emotion-agegender-recognition-using-deep-neural-networks/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Learning Utterance-level Representations for Speech Emotion and Age/Gender Recognition Using Deep Neural Networks\"> Learning Utterance-level Representations for Speech Emotion and Age/Gender Recognition Using Deep Neural Networks </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/video/a-cross-modal-audio-search-engine-based-on-joint-audio-text-embeddings/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"A Cross-modal Audio Search Engine based on Joint Audio-Text Embeddings\"> A Cross-modal Audio Search Engine based on Joint Audio-Text Embeddings </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n     <p></p> \n    </div> \n    <div class=\"wp-block-column\"> \n     <div class=\"annotations\" data-bi-area=\"citation\"> \n      <ul class=\"annotations__list \"> \n       <li class=\"annotations__list-item\"> <span class=\"annotations__type\">Publication</span> <a href=\"https://www.microsoft.com/en-us/research/publication/supervised-deep-hashing-for-efficient-audio-event-retrieval/\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Supervised Deep Hashing for Efficient Audio Event Retrieval\"> Supervised Deep Hashing for Efficient Audio Event Retrieval </a> <span class=\"svg-icon icon-chevron-right-med-blue\"></span> </li> \n      </ul> \n     </div> \n    </div> \n   </div> </li> \n </ol> \n</div> \n<p><a href=\"https://www.microsoft.com/en-us/research/?p=681651#top\">Back to top &gt;</a></p> \n<p>\n <!-- /wp:msr/block-journey --></p> \n<p>\n <!-- /wp:msr/block-journey --></p> \n<p>\n <!-- /wp:msr/block-journey --></p> \n<p>\n <!-- wp:paragraph --></p> \n<p>\n <!-- /wp:paragraph --></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/research-collection-the-unseen-history-of-audio-and-acoustics-research-at-microsoft/\">Research Collection: The Unseen History of Audio and Acoustics Research at Microsoft</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>","descriptionType":"html","publishedDate":"Wed, 12 Aug 2020 16:00:21 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1024x576.jpg","linkMd5":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn90@2020_6/2020/08/24/21-39-23-903_d036e60a659fb831.webp","destWidth":1024,"destHeight":576,"sourceBytes":114696,"destBytes":54862,"author":"Brenda Potts","enclosureType":"audio/mpeg","enclosureUrl":"https://media.blubrry.com/microsoftresearch/b/content.blubrry.com/microsoftresearch/msr_050_tashev.mp3","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1024x576.jpg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn90@2020_6/2020/08/24/21-39-23-903_d036e60a659fb831.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-1024x576.jpg":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn29@2020_2/2020/08/24/21-40-52-993_d02c3117384c7983.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-1024x576.jpg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn4@2020_6/2020/08/24/21-40-53-799_f61b4b100d470eb8.jpg","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-1024x576.jpg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn24@2020_3/2020/08/24/21-40-53-531_ee36da0411001639.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-1024x555.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn27@2020_1/2020/08/24/21-40-52-989_4f167131ee69ec75.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn95@2020_2/2020/08/24/21-40-53-038_f9095148a1f2ee6a.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-1024x576.jpg":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn87@2020_1/2020/08/24/21-40-53-035_8f06a735ff5c3fac.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849-300x217.jpg":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn36@2020_5/2020/08/24/21-40-53-150_6c2f4419289840c9.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-1024x447.jpg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn75@2020_2/2020/08/24/21-40-53-461_db583253b9f0185c.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-1024x550.jpg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn31@2020_1/2020/08/24/21-40-53-168_4778e1022ba604e9.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008_AnechoicChamberWithRigFor3Dscanning.jpg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn16@2020_1/2020/08/24/21-40-52-987_9b59f6ede6226121.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures-1024x238.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn79@2020_5/2020/08/24/21-40-54-007_3b8e21d5c5ae7c4e.webp","https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-1024x576.jpg":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn27@2020_2/2020/08/24/21-40-53-025_fac51e6987a5658e.webp","https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-1024x576.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn91@2020_4/2020/08/24/21-40-53-071_07d50eab0f5b46da.webp","https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-1024x576.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn49@2020_4/2020/08/24/21-40-52-992_08261eea5bda8562.webp","https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-1024x576.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn23@2020_1/2020/08/24/21-40-53-856_f058f3b6f852fc93.webp"},"publishedOrCreatedDate":1598305163718}],"record":{"createdTime":"2020-08-25 05:39:23","updatedTime":"2020-08-25 05:39:23","feedId":3611,"fetchDate":"Mon, 24 Aug 2020 21:39:23 +0000","fetchMs":242,"handleMs":1179,"totalMs":93517,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"54.197.246.194","hostName":"us-001*","requestId":"072fa7724bae48e5be431570c9739bf3_3611","contentType":"application/rss+xml; charset=UTF-8","totalBytes":7688484,"bgimgsTotal":10,"bgimgsGithubTotal":10,"articlesImgsTotal":41,"articlesImgsGithubTotal":41,"successGithubMap":{"myreaderx14":2,"myreaderx8":1,"myreaderx7":2,"myreaderx15":1,"myreaderx6":2,"myreaderx16":2,"myreaderx4":1,"myreaderx10":1,"myreaderx32":1,"myreaderx33":2,"myreaderx11":1,"myreaderx3":1,"myreaderx2":3,"myreaderx12":1,"myreaderx13":1,"myreaderx1":1,"myreaderx30":3,"myreaderx31":1,"myreaderx19":1,"myreaderx":1,"myreaderx25":2,"myreaderx27":1,"myreaderx21":1,"myreaderx22":2,"myreaderx23":2,"myreaderx24":1,"myreaderx5oss":1,"myreaderx29":2},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:29:28","updatedTime":"2020-08-25 04:29:28","id":3611,"name":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/feed/","subscriber":null,"website":null,"icon":"https://www.microsoft.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn28@2020_6/2020/08/24/21-39-22-864_ddd9473b17dec87a.ico","description":"","weekly":null,"link":"https://www.microsoft.com"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":5991534,"tmpBodyImgCdnBytes":1696950,"tmpBgImgCdnBytes":0,"extra4":{"start":1598305162226,"total":0,"statList":[{"spend":314,"msg":"获取xml内容"},{"spend":1179,"msg":"解释文章"},{"spend":1,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":2901,"msg":"正文链接上传到cdn"}]},"extra5":41,"extra6":41,"extra7ImgCdnFailResultVector":[null,null],"extra10_invalidATagHrefValue":{"https://www.microsoft.com/en-us/research/?p=681651_#top":"https://www.microsoft.com/en-us/research/?p=681651#top","https://www.microsoft.com/en-us/research/?p=680043_#Pretraining":"https://www.microsoft.com/en-us/research/?p=680043#Pretraining","https://www.microsoft.com/en-us/research/?p=680043_#AIModels":"https://www.microsoft.com/en-us/research/?p=680043#AIModels","https://www.microsoft.com/en-us/research/?p=680043_#Utility":"https://www.microsoft.com/en-us/research/?p=680043#Utility","https://www.microsoft.com/en-us/research/?p=680043_#Normalization":"https://www.microsoft.com/en-us/research/?p=680043#Normalization","https://www.microsoft.com/en-us/research/?p=670932_mailto:project-freta@microsoft.com":"mailto:project-freta@microsoft.com","https://www.microsoft.com/en-us/research/?p=680043_#Transformers":"https://www.microsoft.com/en-us/research/?p=680043#Transformers"},"extra111_proxyServerAndStatMap":{"http://us-037.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-53.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-60.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-52.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-017.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe21.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-029.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-013.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-039.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-55.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-59.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-51.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Stefan_podcast_1400x788_No-logos-1024x577.png","sourceStatusCode":200,"destWidth":1024,"destHeight":577,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn7@2020_4/2020/08/24/21-39-23-955_da7f7e726872235f.webp","sourceBytes":471839,"destBytes":39324,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1052,"convertSpendMs":23,"createdTime":"2020-08-25 05:39:23","host":"us-55*","referer":"https://www.microsoft.com/en-us/research/?p=670293","linkMd5ListStr":"7bd6175a3659baf642494020f7c1efb1,7bd6175a3659baf642494020f7c1efb1","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"460.8 KB","destSize":"38.4 KB","compressRate":"8.3%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure-1-1024x564.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":564,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn14@2020_4/2020/08/24/21-39-24-004_8dbed831dff23127.webp","sourceBytes":41620,"destBytes":25204,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1071,"convertSpendMs":62,"createdTime":"2020-08-25 05:39:23","host":"us-021*","referer":"https://www.microsoft.com/en-us/research/?p=678456","linkMd5ListStr":"dd61ab70c47710f9a8ebc4b9d75ca1c8,dd61ab70c47710f9a8ebc4b9d75ca1c8","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"40.6 KB","destSize":"24.6 KB","compressRate":"60.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_anachoic-chamber_1400x788-1024x576.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn90@2020_6/2020/08/24/21-39-23-903_d036e60a659fb831.webp","sourceBytes":114696,"destBytes":54862,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1135,"convertSpendMs":23,"createdTime":"2020-08-25 05:39:23","host":"us-028*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada,04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"112 KB","destSize":"53.6 KB","compressRate":"47.8%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig1-1024x693.png","sourceStatusCode":200,"destWidth":1024,"destHeight":693,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn97@2020_3/2020/08/24/21-39-24-046_f39cd952816cc6e9.webp","sourceBytes":70166,"destBytes":22168,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1101,"convertSpendMs":32,"createdTime":"2020-08-25 05:39:23","host":"europe65*","referer":"https://www.microsoft.com/en-us/research/?p=677052","linkMd5ListStr":"08c6b949e99b41201da1351546bb68f2,08c6b949e99b41201da1351546bb68f2","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"68.5 KB","destSize":"21.6 KB","compressRate":"31.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/MineRL-2020-Feat.-Image-1024x576.png","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn84@2020_3/2020/08/24/21-39-24-082_87008072687d8d5a.webp","sourceBytes":370379,"destBytes":20122,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1424,"convertSpendMs":36,"createdTime":"2020-08-25 05:39:23","host":"europe63*","referer":"https://www.microsoft.com/en-us/research/?p=685122","linkMd5ListStr":"472191d91b1c75b6e03cfa1008781f42,472191d91b1c75b6e03cfa1008781f42","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"361.7 KB","destSize":"19.7 KB","compressRate":"5.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ICML-Hero-1024x577.png","sourceStatusCode":200,"destWidth":1024,"destHeight":577,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn99@2020_5/2020/08/24/21-39-24-608_28ae713392181e7a.webp","sourceBytes":265007,"destBytes":46818,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1983,"convertSpendMs":32,"createdTime":"2020-08-25 05:39:23","host":"europe66*","referer":"https://www.microsoft.com/en-us/research/?p=680043","linkMd5ListStr":"d3a27d20f70cd143be812f433fce1f5d,d3a27d20f70cd143be812f433fce1f5d","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"258.8 KB","destSize":"45.7 KB","compressRate":"17.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_Freta_NoLogo-2.png","sourceStatusCode":200,"destWidth":5834,"destHeight":3284,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn88@2020_1/2020/08/24/21-39-24-773_73b2efeb2df2b752.webp","sourceBytes":2456048,"destBytes":492794,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":2724,"convertSpendMs":672,"createdTime":"2020-08-25 05:39:23","host":"europe67*","referer":"https://www.microsoft.com/en-us/research/?p=670932","linkMd5ListStr":"c269c53b18a032812af6dc89fa7ea483,c269c53b18a032812af6dc89fa7ea483","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.3 MB","destSize":"481.2 KB","compressRate":"20.1%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/1400x788_AirSim_noLogo5secs-1.gif","sourceStatusCode":200,"destWidth":1400,"destHeight":788,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_2/2020/08/24/21-39-41-148_eae27493c21b80f7.webp","sourceBytes":1610991,"destBytes":1809008,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":19814,"convertSpendMs":17076,"createdTime":"2020-08-25 05:39:23","host":"europe-25*","referer":"https://www.microsoft.com/en-us/research/?p=683157","linkMd5ListStr":"abfa94c375d1b9bbf2e239da4c6b7139,abfa94c375d1b9bbf2e239da4c6b7139","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.5 MB","destSize":"1.7 MB","compressRate":"112.3%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_No_Logo_DNA.gif","sourceStatusCode":200,"destWidth":1400,"destHeight":788,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn93@2020_2/2020/08/24/21-40-46-787_841a52cbf2ac8d9f.webp","sourceBytes":394551,"destBytes":900332,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":24262,"convertSpendMs":22711,"createdTime":"2020-08-25 05:40:23","host":"us-021*","referer":"https://www.microsoft.com/en-us/research/?p=676782","linkMd5ListStr":"6bf554702573476505a26f06881bee2b,6bf554702573476505a26f06881bee2b","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"385.3 KB","destSize":"879.2 KB","compressRate":"228.2%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/1400x788_SilverCloud_NoLogo_animation_updated-final.gif","sourceStatusCode":200,"destWidth":1400,"destHeight":788,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn11@2020_2/2020/08/24/21-40-50-446_ff882ef063b0aedd.webp","sourceBytes":1360942,"destBytes":2580902,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":28282,"convertSpendMs":24990,"createdTime":"2020-08-25 05:40:24","host":"europe63*","referer":"https://www.microsoft.com/en-us/research/?p=675831","linkMd5ListStr":"e51ea0a4c5e4ddb7f86b61ead8e413e6,e51ea0a4c5e4ddb7f86b61ead8e413e6","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"2.5 MB","compressRate":"189.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-2_DNA-1024x524.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":524,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn25@2020_1/2020/08/24/21-40-52-994_6a7bf8cd312ae883.webp","sourceBytes":52409,"destBytes":27640,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":930,"convertSpendMs":16,"createdTime":"2020-08-25 05:40:52","host":"us-55*","referer":"https://www.microsoft.com/en-us/research/?p=676782","linkMd5ListStr":"6bf554702573476505a26f06881bee2b","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.2 KB","destSize":"27 KB","compressRate":"52.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2019/03/Nikunj-Raghuvanshi-POD_Site_02_2019_1400x788-1024x576.png","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn49@2020_4/2020/08/24/21-40-52-992_08261eea5bda8562.webp","sourceBytes":379326,"destBytes":27920,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":982,"convertSpendMs":38,"createdTime":"2020-08-25 05:40:52","host":"us-039*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"370.4 KB","destSize":"27.3 KB","compressRate":"7.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Quantum-Figure2-1024x518.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":518,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn71@2020_6/2020/08/24/21-40-53-038_a8a20defada77229.webp","sourceBytes":48201,"destBytes":27196,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1011,"convertSpendMs":41,"createdTime":"2020-08-25 05:40:52","host":"us-021*","referer":"https://www.microsoft.com/en-us/research/?p=678456","linkMd5ListStr":"dd61ab70c47710f9a8ebc4b9d75ca1c8","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"47.1 KB","destSize":"26.6 KB","compressRate":"56.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MSR_Athens_blog_minecraft_image_v2-02-1024x599.png","sourceStatusCode":200,"destWidth":1024,"destHeight":599,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn8@2020_6/2020/08/24/21-40-52-971_8c2b0d8f0423cc7f.webp","sourceBytes":52656,"destBytes":16674,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1006,"convertSpendMs":29,"createdTime":"2020-08-25 05:40:52","host":"us-51*","referer":"https://www.microsoft.com/en-us/research/?p=677052","linkMd5ListStr":"08c6b949e99b41201da1351546bb68f2","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.4 KB","destSize":"16.3 KB","compressRate":"31.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1991-software-testing-UI-screenshot-1024x555.png","sourceStatusCode":200,"destWidth":1024,"destHeight":555,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn27@2020_1/2020/08/24/21-40-52-989_4f167131ee69ec75.webp","sourceBytes":330208,"destBytes":39302,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1017,"convertSpendMs":35,"createdTime":"2020-08-25 05:40:52","host":"us-54*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"322.5 KB","destSize":"38.4 KB","compressRate":"11.9%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2001-RingCam-video-conference-1024x576.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn87@2020_1/2020/08/24/21-40-53-035_8f06a735ff5c3fac.webp","sourceBytes":66603,"destBytes":35330,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1004,"convertSpendMs":20,"createdTime":"2020-08-25 05:40:52","host":"us-037*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"65 KB","destSize":"34.5 KB","compressRate":"53%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-1_Polymerase-based-stand-displacement.png","sourceStatusCode":200,"destWidth":979,"destHeight":680,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn35@2020_2/2020/08/24/21-40-53-026_004554824c419ee0.webp","sourceBytes":76064,"destBytes":32210,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1024,"convertSpendMs":62,"createdTime":"2020-08-25 05:40:52","host":"us-021*","referer":"https://www.microsoft.com/en-us/research/?p=676782","linkMd5ListStr":"6bf554702573476505a26f06881bee2b","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"74.3 KB","destSize":"31.5 KB","compressRate":"42.3%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Figure-4-_AirSim-blog.jpg","sourceStatusCode":200,"destWidth":988,"destHeight":352,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn37@2020_5/2020/08/24/21-40-53-027_2a1a1b24b9277200.webp","sourceBytes":68632,"destBytes":45248,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1029,"convertSpendMs":24,"createdTime":"2020-08-25 05:40:52","host":"us-038*","referer":"https://www.microsoft.com/en-us/research/?p=683157","linkMd5ListStr":"abfa94c375d1b9bbf2e239da4c6b7139","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"67 KB","destSize":"44.2 KB","compressRate":"65.9%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1996-vision-steered-audio-VR.png","sourceStatusCode":200,"destWidth":786,"destHeight":575,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn95@2020_2/2020/08/24/21-40-53-038_f9095148a1f2ee6a.webp","sourceBytes":496413,"destBytes":31534,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1024,"convertSpendMs":29,"createdTime":"2020-08-25 05:40:52","host":"us-013*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"484.8 KB","destSize":"30.8 KB","compressRate":"6.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_OfficeRoundtable_32849-300x217.jpg","sourceStatusCode":200,"destWidth":300,"destHeight":217,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn36@2020_5/2020/08/24/21-40-53-150_6c2f4419289840c9.webp","sourceBytes":24446,"destBytes":6686,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1015,"convertSpendMs":5,"createdTime":"2020-08-25 05:40:52","host":"us-040*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23.9 KB","destSize":"6.5 KB","compressRate":"27.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Figure-1-1024x211.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":211,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn32@2020_2/2020/08/24/21-40-52-991_dcd5e081da91fdd3.webp","sourceBytes":48553,"destBytes":32050,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1047,"convertSpendMs":25,"createdTime":"2020-08-25 05:40:52","host":"us-52*","referer":"https://www.microsoft.com/en-us/research/?p=683157","linkMd5ListStr":"abfa94c375d1b9bbf2e239da4c6b7139","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"47.4 KB","destSize":"31.3 KB","compressRate":"66%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2007-Ford-SYNC_1400x788-1024x576.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn29@2020_2/2020/08/24/21-40-52-993_d02c3117384c7983.webp","sourceBytes":123795,"destBytes":60810,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1040,"convertSpendMs":34,"createdTime":"2020-08-25 05:40:52","host":"us-53*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"120.9 KB","destSize":"59.4 KB","compressRate":"49.1%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008_AnechoicChamberWithRigFor3Dscanning.jpg","sourceStatusCode":200,"destWidth":556,"destHeight":366,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn16@2020_1/2020/08/24/21-40-52-987_9b59f6ede6226121.webp","sourceBytes":84549,"destBytes":38734,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1042,"convertSpendMs":19,"createdTime":"2020-08-25 05:40:52","host":"us-017*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"82.6 KB","destSize":"37.8 KB","compressRate":"45.8%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/Ivan-Tashev_Pod_Site_10_2018_1400x788-1024x576.png","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn91@2020_4/2020/08/24/21-40-53-071_07d50eab0f5b46da.webp","sourceBytes":449020,"destBytes":38306,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1053,"convertSpendMs":63,"createdTime":"2020-08-25 05:40:52","host":"us-025*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"438.5 KB","destSize":"37.4 KB","compressRate":"8.5%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2018/02/Soundscape-MCR-home-page-hero-1066-x-600-1024x576.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn27@2020_2/2020/08/24/21-40-53-025_fac51e6987a5658e.webp","sourceBytes":131680,"destBytes":97852,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1090,"convertSpendMs":28,"createdTime":"2020-08-25 05:40:52","host":"us-55*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"128.6 KB","destSize":"95.6 KB","compressRate":"74.3%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta_Rootkits_Figure_UpdatedV.jpg","sourceStatusCode":200,"destWidth":927,"destHeight":531,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_3/2020/08/24/21-40-52-964_d6fbdd8500e6a51c.webp","sourceBytes":87395,"destBytes":56566,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1131,"convertSpendMs":20,"createdTime":"2020-08-25 05:40:52","host":"us-033*","referer":"https://www.microsoft.com/en-us/research/?p=670932","linkMd5ListStr":"c269c53b18a032812af6dc89fa7ea483","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"85.3 KB","destSize":"55.2 KB","compressRate":"64.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_Fig2-5f1a016db0687-1024x363.png","sourceStatusCode":200,"destWidth":1024,"destHeight":363,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn39@2020_5/2020/08/24/21-40-53-180_c0f86eb838e846b3.webp","sourceBytes":109471,"destBytes":16762,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1109,"convertSpendMs":175,"createdTime":"2020-08-25 05:40:52","host":"us-009*","referer":"https://www.microsoft.com/en-us/research/?p=677052","linkMd5ListStr":"08c6b949e99b41201da1351546bb68f2","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"106.9 KB","destSize":"16.4 KB","compressRate":"15.3%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Figure-3_DNA-1024x418.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":418,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn33@2020_4/2020/08/24/21-40-53-122_dd4bee411a2bd1e8.webp","sourceBytes":51134,"destBytes":27660,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1178,"convertSpendMs":16,"createdTime":"2020-08-25 05:40:52","host":"us-51*","referer":"https://www.microsoft.com/en-us/research/?p=676782","linkMd5ListStr":"6bf554702573476505a26f06881bee2b","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"49.9 KB","destSize":"27 KB","compressRate":"54.1%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Freta-fig-1-_plane-300x229.jpg","sourceStatusCode":200,"destWidth":300,"destHeight":229,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn40@2020_6/2020/08/24/21-40-53-126_c45861889622432b.webp","sourceBytes":17041,"destBytes":7794,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1066,"convertSpendMs":4,"createdTime":"2020-08-25 05:40:52","host":"europe-22*","referer":"https://www.microsoft.com/en-us/research/?p=670932","linkMd5ListStr":"c269c53b18a032812af6dc89fa7ea483","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.6 KB","destSize":"7.6 KB","compressRate":"45.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2008-MSR-anechoic-chamber-1024x550.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":550,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn31@2020_1/2020/08/24/21-40-53-168_4778e1022ba604e9.webp","sourceBytes":106851,"destBytes":55960,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1248,"convertSpendMs":24,"createdTime":"2020-08-25 05:40:52","host":"us-033*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"104.3 KB","destSize":"54.6 KB","compressRate":"52.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/MentalHealthSilverCloud_Figure1-1024x762.png","sourceStatusCode":200,"destWidth":1024,"destHeight":762,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn12@2020_6/2020/08/24/21-40-53-294_aad22602db146881.webp","sourceBytes":92688,"destBytes":27010,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1249,"convertSpendMs":50,"createdTime":"2020-08-25 05:40:52","host":"us-029*","referer":"https://www.microsoft.com/en-us/research/?p=675831","linkMd5ListStr":"e51ea0a4c5e4ddb7f86b61ead8e413e6","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"90.5 KB","destSize":"26.4 KB","compressRate":"29.1%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-blog-_-figure-2.png","sourceStatusCode":200,"destWidth":468,"destHeight":140,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn99@2020_4/2020/08/24/21-40-53-187_d7bc199bc40dcf72.webp","sourceBytes":136225,"destBytes":16650,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1179,"convertSpendMs":8,"createdTime":"2020-08-25 05:40:52","host":"europe-25*","referer":"https://www.microsoft.com/en-us/research/?p=683157","linkMd5ListStr":"abfa94c375d1b9bbf2e239da4c6b7139","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"133 KB","destSize":"16.3 KB","compressRate":"12.2%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/RLGamingBlog_fig4-1024x430.png","sourceStatusCode":200,"destWidth":1024,"destHeight":430,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn84@2020_5/2020/08/24/21-40-53-194_4a6ed8450116c0ab.webp","sourceBytes":59979,"destBytes":19762,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1187,"convertSpendMs":29,"createdTime":"2020-08-25 05:40:52","host":"europe-59*","referer":"https://www.microsoft.com/en-us/research/?p=677052","linkMd5ListStr":"08c6b949e99b41201da1351546bb68f2","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"58.6 KB","destSize":"19.3 KB","compressRate":"32.9%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Mental-Health-blog-fig-2-1024x848.png","sourceStatusCode":200,"destWidth":1024,"destHeight":848,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn20@2020_2/2020/08/24/21-40-53-477_6fe45b8b34ae8429.webp","sourceBytes":194413,"destBytes":44086,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1504,"convertSpendMs":98,"createdTime":"2020-08-25 05:40:52","host":"us-005*","referer":"https://www.microsoft.com/en-us/research/?p=675831","linkMd5ListStr":"e51ea0a4c5e4ddb7f86b61ead8e413e6","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"189.9 KB","destSize":"43.1 KB","compressRate":"22.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/AirSim-Blog_figure-3-1024x340.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":340,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn43@2020_4/2020/08/24/21-40-53-155_62d0a37937f2d4db.webp","sourceBytes":89355,"destBytes":66178,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1397,"convertSpendMs":19,"createdTime":"2020-08-25 05:40:52","host":"europe21*","referer":"https://www.microsoft.com/en-us/research/?p=683157","linkMd5ListStr":"abfa94c375d1b9bbf2e239da4c6b7139","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"87.3 KB","destSize":"64.6 KB","compressRate":"74.1%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_1998_Large-Mic-Array-1024x447.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":447,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn75@2020_2/2020/08/24/21-40-53-461_db583253b9f0185c.webp","sourceBytes":96924,"destBytes":41504,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1567,"convertSpendMs":122,"createdTime":"2020-08-25 05:40:52","host":"us-009*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"94.7 KB","destSize":"40.5 KB","compressRate":"42.8%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Hololens_1400x788-1024x576.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn24@2020_3/2020/08/24/21-40-53-531_ee36da0411001639.webp","sourceBytes":86355,"destBytes":30416,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1654,"convertSpendMs":21,"createdTime":"2020-08-25 05:40:52","host":"europe67*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"84.3 KB","destSize":"29.7 KB","compressRate":"35.2%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2018/11/MSR_Nikunj_Raghuvanshi_Webinar_Hero_1400x788-1024x576.png","sourceStatusCode":200,"destWidth":1024,"destHeight":576,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn23@2020_1/2020/08/24/21-40-53-856_f058f3b6f852fc93.webp","sourceBytes":480674,"destBytes":36020,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":2010,"convertSpendMs":33,"createdTime":"2020-08-25 05:40:52","host":"europe21*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"469.4 KB","destSize":"35.2 KB","compressRate":"7.5%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/07/Neural-network-3_-shorter.png","sourceStatusCode":200,"destWidth":1024,"destHeight":363,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn45@2020_5/2020/08/24/21-40-53-880_c1540f92a321fc8c.webp","sourceBytes":662589,"destBytes":31358,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":2095,"convertSpendMs":51,"createdTime":"2020-08-25 05:40:52","host":"europe-60*","referer":"https://www.microsoft.com/en-us/research/?p=680043","linkMd5ListStr":"d3a27d20f70cd143be812f433fce1f5d","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"647.1 KB","destSize":"30.6 KB","compressRate":"4.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/Acoustics_2017-ultrasound-based-gesture-recognition-figures-1024x238.png","sourceStatusCode":200,"destWidth":1024,"destHeight":238,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn79@2020_5/2020/08/24/21-40-54-007_3b8e21d5c5ae7c4e.webp","sourceBytes":206219,"destBytes":28064,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":2208,"convertSpendMs":18,"createdTime":"2020-08-25 05:40:52","host":"europe21*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"201.4 KB","destSize":"27.4 KB","compressRate":"13.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/KinectVoice_2306_1400x788-1024x576.jpg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn4@2020_6/2020/08/24/21-40-53-799_f61b4b100d470eb8.jpg","sourceBytes":633668,"destBytes":633668,"feedId":3611,"totalSpendMs":2812,"convertSpendMs":3,"createdTime":"2020-08-25 05:40:52","host":"europe63*","referer":"https://www.microsoft.com/en-us/research/?p=681651","linkMd5ListStr":"04cfc2b8bfdfcd9a1c0ad4952cfa2ada","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"618.8 KB","destSize":"618.8 KB","compressRate":"100%"}],"successGithubMap":{"myreaderx14":2,"myreaderx8":1,"myreaderx7":2,"myreaderx15":1,"myreaderx6":2,"myreaderx16":2,"myreaderx4":1,"myreaderx10":1,"myreaderx32":1,"myreaderx33":2,"myreaderx11":1,"myreaderx3":1,"myreaderx2":3,"myreaderx12":1,"myreaderx13":1,"myreaderx1":1,"myreaderx30":3,"myreaderx31":1,"myreaderx19":1,"myreaderx":1,"myreaderx25":2,"myreaderx27":1,"myreaderx21":1,"myreaderx22":2,"myreaderx23":2,"myreaderx24":1,"myreaderx5oss":1,"myreaderx29":2},"failGithubMap":{}}