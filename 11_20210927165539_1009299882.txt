{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Making Aurora Write Latency 15x Higher (or More!) by Choosing a Bad Primary Key","link":"https://www.percona.com/blog/?p=78190","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Aurora MySQL Write Latency\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78223\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-300x168.png\" alt=\"Aurora MySQL Write Latency\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Primary Key design is an important thing for InnoDB performance, and choosing a poor PK definition will have an impact on performance and also write propagation in databases. When this comes to Aurora, this impact is even worse than you may notice.</p>\n<p>In short, we consider a poor definition of a Primary Key in InnoDB as “anything but quasi sequential values”, which may cause very random access to data and thus increase the IO dependency.</p>\n<p>In this post, I’ll try to demonstrate the potential impact of the primary key design when running on Aurora, and how a bad design can lead to a 15x write latency penalty (or more).</p>\n<h2>The Analysis</h2>\n<p>Recently I worked on a case where a customer was having issues with scaling writes in Aurora MySQL. While this is a known limitation in Aurora considering how the distributed storage layer syncs out data among all nodes of the cluster, we observed additional latency occurring when more clients were writing to the database.</p>\n<p>The first thing I noticed is that their main table had a poor definition of the Primary Key as they were using UUID-like columns based on VARCHAR data types. In this case, the nature of values for the Primary Key was very random, which is really bad for a b-tree based storage like InnoDB.</p>\n<p>With this in mind, I referred to a great post from my colleague Yves Trudeau explaining <a href=\"https://www.percona.com/blog/2019/11/22/uuids-are-popular-but-bad-for-performance-lets-discuss/\">why UUIDs are bad for performance</a>, so based on this premise I decided to try to measure how big this impact can be in the Aurora world.</p>\n<p>The set of tests I’ve run were using a db.r5.2xlarge Aurora MySQL Cluster (8vCPU and 64GB of ram) which is pretty similar to the cluster my customer was using.</p>\n<p>First, I’ve started with two very basic tables to avoid any extra overhead but something close to a real case:</p><pre class=\"crayon-plain-tag\">CREATE TABLE `test_sequential_PK` (\n`id` int(11) NOT NULL AUTO_INCREMENT,\n`number` int(10) NOT NULL,\nPRIMARY KEY (`id`),\nKEY `idx_number` (`number`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n\nCREATE TABLE `test_random_PK` (\n`id` varchar(26) NOT NULL,\n`number` int(10) NOT NULL,\nPRIMARY KEY (`id`),\nKEY `idx_number` (`number`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</pre><p>Then I created two simple lua scripts to execute with sysbench &#8211; super simple, I’d say &#8211; as they were just for doing inserts using either the auto_increment property in table test_sequential_PK or creating random values for test_random_PK table.</p>\n<p>The final purpose was not to measure Aurora performance as a whole but the write latency when the Primary Key is not optimal.</p>\n<p>I’ve started the process by warming up the instances for few days by running both sysbench scripts and pushing the instance really hard while I was filling up the InnoDB Buffer Pool, results were pretty good for some time until the traffic became IO-bound:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78191 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-1024x357.png\" alt=\"Amazon Aurora Latency\" width=\"900\" height=\"314\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-1024x357.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-300x104.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-200x70.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-367x128.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45.png 1430w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>It took a few days but after some time we started to see an increase in the write latency. I created an initial set of data using 50 concurrent threads, which means the graphic above is not very helpful for the analysis I’m going to make.</p>\n<p>After I was sure the buffer pool was filled and the instance was warmed up, I verified that the dataset is bigger than the buffer pool:</p><pre class=\"crayon-plain-tag\">SELECT concat(table_schema,'.',table_name) schema_table,concat(round(table_rows/1000000,2),'M') rows,concat(round(data_length/(1024*1024*1024),2),'G') Data,round(data_length/table_rows,0) DataRow ,concat(round(index_length/(1024*1024*1024),2),'G') idx,round(index_length/table_rows,0) as IdxRow,concat(round((data_length+index_length)/(1024*1024*1024),2),'G') totSize,round(index_length/data_length,2) idxfrac\nFROM information_schema.TABLES\nwhere table_schema in ('percona')\nORDER BY data_length+index_length DESC LIMIT 10;\n+-------------------------------+----------+---------+---------+--------+--------+---------+---------+\n| schema_table                  | rows     | Data    | DataRow | idx    | IdxRow | totSize | idxfrac |\n+-------------------------------+----------+---------+---------+--------+--------+---------+---------+\n| percona.test_random_PK        | 1586.25M | 104.53G |      71 | 73.04G |     49 | 177.57G |    0.70 |\n| percona.test_sequential_PK    | 1840.49M | 54.74G  |      32 | 24.54G |     14 | 79.27G  |    0.45 |\n+-------------------------------+----------+---------+---------+--------+--------+---------+---------+</pre><p>I’ll explain the difference between table sizes later in this post.</p>\n<p>After this, I started to run separate tests to check how the write latency is affected by our table design.</p>\n<p>First I ran sysbench as follows:</p><pre class=\"crayon-plain-tag\">sysbench /usr/share/sysbench/insert_sequence.lua    --mysql-user=admin --mysql-password=xxxxxx --mysql-host=francisco-sysbench.cluster --mysql-db=percona --report-interval=1   --threads=15  --events=0 --time=0 run</pre><p><img loading=\"lazy\" class=\"aligncenter wp-image-78193 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-1024x358.png\" alt=\"\" width=\"900\" height=\"315\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-1024x358.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-300x105.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-200x70.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-367x128.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18.png 1432w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78192 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-1024x365.png\" alt=\"MySQL Client Thread Activity\" width=\"900\" height=\"321\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-1024x365.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-300x107.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-200x71.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-367x131.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51.png 1427w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>We can see that performance remains steady roughly at 124 microseconds average with all threads running so it looks there is no impact for about 24 hours.</p>\n<p>Then I tested the random insert by running:</p><pre class=\"crayon-plain-tag\">sysbench /usr/share/sysbench/insert_random.lua    --mysql-user=admin --mysql-password=xxxxxx --mysql-host=francisco-sysbench.cluster --mysql-db=percona --report-interval=1   --threads=15  --events=0 --time=0 run</pre><p><img loading=\"lazy\" class=\"aligncenter wp-image-78195 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-1024x357.png\" alt=\"\" width=\"900\" height=\"314\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-1024x357.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-300x105.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-200x70.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-367x128.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21.png 1431w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78194 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-1024x356.png\" alt=\"MySQL InnoDB\" width=\"900\" height=\"313\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-1024x356.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-300x104.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-200x69.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-367x127.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39.png 1434w\" sizes=\"(max-width: 900px) 100vw, 900px\" /><br />\nThis is a huge impact if you consider that random access is causing the instance to suffer from performance, roughly 15x writing latency increase.</p>\n<p>These numbers were very impressive compared to my previous experience so, being extra curious, I checked what was reported in Cloudwatch for Aurora Write latency for the previous 3 days.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78196 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32.png\" alt=\"Cloudwatch for Aurora Write latency\" width=\"1000\" height=\"639\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32.png 1000w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32-300x192.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32-200x128.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32-367x235.png 367w\" sizes=\"(max-width: 1000px) 100vw, 1000px\" /></p>\n<p>It’s quite clear the 3 stages of the checks:</p>\n<ul>\n<li aria-level=\"1\">Warming up the instance and buffer pool by pushing really hard with sysbench in both tables (peak of load at the beginning)</li>\n<li aria-level=\"1\">First round of sequential inserts using sequential write (sequential writes)</li>\n<li aria-level=\"1\">Last round of random access (random writes)</li>\n</ul>\n<p>As I said, the code used on each script was very simple. This is on purpose to avoid adding overhead somewhere other than the database. For reference, this is the interesting portion of code for random writes:</p><pre class=\"crayon-plain-tag\">local inserts = {\n\"INSERT INTO percona.test_random_PK (id, number) VALUES ('%s', %i)\"\n}\n\nlocal rnd_str_tmpl = \"###########-###########\"\n\nfunction execute_inserts()\n             local id = sysbench.rand.string(rnd_str_tmpl)\n             con:query(string.format(inserts[1], id, sb_rand(1, sysbench.opt.table_size)))\nend</pre><p>And for sequential writes:</p><pre class=\"crayon-plain-tag\">local inserts = {\n\n\"INSERT INTO percona.test_sequential_PK (number) VALUES (%i)\"\n\n}\n\nfunction execute_inserts()\n\n-- INSERT\n\ncon:query(string.format(inserts[1], sb_rand(1, sysbench.opt.table_size)))\n\nend</pre><p></p>\n<h2>Conclusion</h2>\n<p>Primary key design for InnoDB tables was largely discussed in several posts, and specially UUID format impact was perfectly described in the post I mentioned, above so there are no surprises. What I’ve found interesting in this analysis is that in Aurora there seems to be an extra impact of this random access.</p>\n<p>Given that, what’s happening underneath is not quite clear. I just can try to elaborate a theory:</p>\n<p>In Aurora, there are no dirty pages. Basically, every commit is synchronized to disk immediately so all replicas can be virtually in sync with the source server. This is reducing the chance of hitting the same page in memory every time you perform a write operation so the whole sequence of grabbing the page, placing it in memory, updating information, flushing, and committing synchronously to the block storage is particularly expensive in Aurora.</p>\n<p>Additionally given that every secondary index adds an entry of the Primary Key, the increase in data size (and thus the disk footprint) can be impacted a lot as we have seen before, so this may also cause an extra disk utilization.</p>\n<p>So be warned, plan your PKs properly if you don’t want to see a huge performance degradation in AWS Aurora as the database grows and workload is increased.</p>\n<p><strong>Percona Distribution for MySQL is the most complete, stable, scalable, and secure, open-source MySQL solution available, delivering enterprise-grade database environments for your most critical business applications&#8230; and it&#8217;s free to use!</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mysql-database\" rel=\"noopener\">Download Percona Distribution for MySQL Today</a></p>\n","descriptionType":"html","publishedDate":"Tue, 21 Sep 2021 15:00:25 +0000","feedId":11,"bgimg":"","linkMd5":"3241d8e49d35b45e30cb4f284cf6fcc8","bgimgJsdelivr":"","metaImg":"","author":"Francisco Bordenave","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn47@2020_1/2021/09/27/16-55-23-739_339254e818ee4e02.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn83@2020_4/2021/09/27/16-55-25-109_2b02128c70e8ce5e.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-1024x357.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn20@2020_5/2021/09/27/16-55-33-842_b444304b0a4f858c.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-1024x358.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn32@2020_6/2021/09/27/16-55-33-827_42c0cf8f42c2bd16.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-1024x365.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn52@2020_2/2021/09/27/16-55-28-823_ad5205f378ea161c.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-1024x357.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn20@2020_1/2021/09/27/16-55-30-043_eb87e49f0725e623.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-1024x356.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn44@2020_6/2021/09/27/16-55-28-738_30ccef3481aa175c.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn100@2020_5/2021/09/27/16-55-32-532_4f93bb1f18eb6b88.webp"},"publishedOrCreatedDate":1632761712756},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Enhanced Password Management Systems in MySQL 8: Part 1","link":"https://www.percona.com/blog/?p=78231","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Enhanced Password Management Systems in MySQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78246\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-300x168.png\" alt=\"Enhanced Password Management Systems in MySQL\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />MySQL 8 comes with a lot of good features, and recently I explored its password management systems. I wanted to put together a series of blogs about it, and this is the first part. In this post, I am going to explain the following topics in detail.</p>\n<ul>\n<li aria-level=\"1\">Password Reuse Policy</li>\n<li aria-level=\"1\">Random Password Generation</li>\n</ul>\n<h2>Password Reuse Policy</h2>\n<p>MySQL has implemented restrictions on reusing passwords. Restriction can be established in two ways:</p>\n<ul>\n<li aria-level=\"1\">Number of password changes</li>\n<li aria-level=\"1\">Time elapsed</li>\n</ul>\n<h3>Number of Password Changes</h3>\n<p>From the MySQL documents:</p>\n<p><em>If an account is restricted on the basis of number of password changes, a new password cannot be chosen from a specified number of the most recent passwords.</em></p>\n<p>To test this, in my local environment I have created the user with “number of password changes = 2”.</p><pre class=\"crayon-plain-tag\">mysql&#62; create user 'herc'@'localhost' identified by 'Percona@321' password history 2;\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql&#62; select user, host, password_reuse_history from mysql.user where user='herc'\\G\n*************************** 1. row ***************************\n                  user: herc\n                  host: localhost\npassword_reuse_history: 2\n1 row in set (0.00 sec)</pre><p>Here “password history 2” will define the number of password changes. MySQL will track the password changes on the table “mysql.password_history”.</p><pre class=\"crayon-plain-tag\">mysql&#62; select * from mysql.password_history;\n+-----------+------+----------------------------+------------------------------------------------------------------------+\n| Host      | User | Password_timestamp         | Password                                                               |\n+-----------+------+----------------------------+------------------------------------------------------------------------+\n| localhost | herc | 2021-09-20 15:44:42.295778 | $A$005$=R:q'M(Kh#D];c~SdCLyluq2UVHFobjWOFTwn2JYVFDyI042sl56B7DCPSK5 |\n+-----------+------+----------------------------+------------------------------------------------------------------------+\n1 row in set (0.00 sec)</pre><p>Now, I am going to change the password for the account “herc@localhost”.</p><pre class=\"crayon-plain-tag\">mysql&#62; alter user 'herc'@'localhost' identified by 'MySQL@321';\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql&#62; select * from mysql.password_history\\G\n*************************** 1. row ***************************\n              Host: localhost\n              User: herc\nPassword_timestamp: 2021-09-20 15:49:15.459018\nCGeRQT31UUwtw194KOKGdNbgj3558VUB.dxcoS8r4IKpG8\n*************************** 2. row ***************************\n              Host: localhost\n              User: herc\nPassword_timestamp: 2021-09-20 15:44:42.295778\n          Password: $A$005$=R:q'M(Kh#D];c~SdCLyluq2UVHFobjWOFTwn2JYVFDyI042sl56B7DCPSK5\n2 rows in set (0.00 sec)</pre><p>It worked. After changing the password, I verified the “mysql.password_history” table. Now, the table has the track of the last two passwords.</p>\n<p>Now, I am going to change the password for the account “herc@localhost” again. This time, I am going to assign the same password which was assigned during the user creation “Percona@321”.</p><pre class=\"crayon-plain-tag\">mysql&#62; alter user 'herc'@'localhost' identified by 'Percona@321';\nERROR 3638 (HY000): Cannot use these credentials for 'herc@localhost' because they contradict the password history policy</pre><p>It doesn&#8217;t work; I am not able to reuse the first password. Because as per my reuse policy, I can’t reuse the last two passwords and they are being tracked in the “mysql.password_policy” table. So, in my case, if I want to reuse my first password again, then it cannot be in that list.</p>\n<p>So I assigned a different password. Now, my first password is removed from the list of the last two passwords) and I tried to assign the first password.</p><pre class=\"crayon-plain-tag\">mysql&#62; alter user 'herc'@'localhost' identified by 'Herc@321';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&#62; alter user 'herc'@'localhost' identified by 'Percona@321';\nQuery OK, 0 rows affected (0.02 sec)</pre><p>This is working now. This is the way you can restrict the reuse of the passwords based on the number of password changes.</p>\n<p>This can be implemented globally and during the startup for all the users using the variable “password_history”.</p><pre class=\"crayon-plain-tag\">#vi my.cnf\n[mysqld]\npassword_history=6\n\n#set global\nmysql&#62; set global password_history=5;\nQuery OK, 0 rows affected (0.00 sec)</pre><p></p>\n<h3>Password Reuse Policy Based on Time Elapsed</h3>\n<p>From the MySQL document:</p>\n<p><em>If an account is restricted based on time elapsed, a new password cannot be chosen from passwords in the history that are newer than a specified number of days.</em></p>\n<p>To test this in my local environment, I have created the user “sri@localhost” with a password reuse interval of five days.</p><pre class=\"crayon-plain-tag\">mysql&#62; create user 'sri'@'localhost' identified by 'Percona@321' password reuse interval 5 day;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&#62; select user, host, password_reuse_time from mysql.user where user='sri'\\G\n*************************** 1. row ***************************\n               user: sri\n               host: localhost\npassword_reuse_time: 5\n1 row in set (0.00 sec)</pre><p>So, this means for five days, I can’t reuse the password for the account “sri@localhost”.</p><pre class=\"crayon-plain-tag\">mysql&#62; select * from mysql.password_history where user='sri'\\G\n*************************** 1. row ***************************\n              Host: localhost\n              User: sri\nPassword_timestamp: 2021-09-20 16:09:27.918585\n          Password: $A$005$+B   e3!C9&#38;8m\n                                         eFRG~IqRWX4b6PtzLA8I4VsdYvWU3qRs/nip/QRhXXR5phT6\n1 row in set (0.00 sec)</pre><p>Now, I am going to do the ALTER to change the password.</p><pre class=\"crayon-plain-tag\">mysql&#62; alter user 'sri'@'localhost' identified by 'Herc@321';\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql&#62; select * from mysql.password_history where user='sri'\\G\n*************************** 1. row ***************************\n              Host: localhost\n              User: sri\nPassword_timestamp: 2021-09-20 16:17:51.840483\n          Password: $A$005$~k7qp8.OP=^#e79qwtiYd7/cmCFLvHM7MHFbvfX2WlhXqzjmrN03gGZ4\n*************************** 2. row ***************************\n              Host: localhost\n              User: sri\nPassword_timestamp: 2021-09-20 16:09:27.918585\n          Password: $A$005$+B   e3!C9&#38;8m\n                                         eFRG~IqRWX4b6PtzLA8I4VsdYvWU3qRs/nip/QRhXXR5phT6\n2 rows in set (0.00 sec)</pre><p>It is working. But, if I am going to reuse any of those passwords, based on the reuse policy, it will not be allowed for five days. Let me try with the first password now.</p><pre class=\"crayon-plain-tag\">mysql&#62; alter user 'sri'@'localhost' identified by 'Percona@321';\nERROR 3638 (HY000): Cannot use these credentials for 'sri@localhost' because they contradict the password history policy</pre><p>It gives the error as expected. This restriction can be implemented globally and during startup for all the users using the variable “password_reuse_interval”.</p><pre class=\"crayon-plain-tag\">#vi my.cnf\n[mysqld]\npassword_reuse_interval=365\n\n#set global\nmysql&#62; set global password_reuse_interval=365;\nQuery OK, 0 rows affected (0.00 sec)</pre><p></p>\n<h2>Random Password Generation</h2>\n<p>From MySQL 8.0.18, MySQL has the capability of creating random passwords for user accounts. This means we don&#8217;t need to assign the passwords and MySQL will take care of it. It has the support for the following statements:</p>\n<ul>\n<li aria-level=\"1\">CREATE USER</li>\n<li aria-level=\"1\">ALTER USER</li>\n<li aria-level=\"1\">SET PASSWORD</li>\n</ul>\n<p>We need to use the “RANDOM PASSWORD” instead of providing the password text, and the password will be displayed on the screen during the creation.</p>\n<p>For example:</p><pre class=\"crayon-plain-tag\">mysql&#62; create user 'sakthi'@'localhost' identified by random password;\n+--------+-----------+----------------------+\n| user   | host      | generated password   |\n+--------+-----------+----------------------+\n| sakthi | localhost | .vZYy+&#60;&#60;BO7l1;vtIufH |\n+--------+-----------+----------------------+\n1 row in set (0.01 sec)\n\nmysql&#62; alter user 'sri'@'localhost' identified by random password;\n+------+-----------+----------------------+\n| user | host      | generated password   |\n+------+-----------+----------------------+\n| sri  | localhost | 5wb&#62;2[]q*jbDsFvlN-i_ |\n+------+-----------+----------------------+\n1 row in set (0.02 sec)</pre><p>The password hashes will be stored in the “mysql.user” table.</p><pre class=\"crayon-plain-tag\">mysql&#62; select user, authentication_string from mysql.user where user in ('sakthi','sri')\\G\n*************************** 1. row ***************************\n                 user: sakthi\nauthentication_string: $A$005$L`PYcedj%3tz*J&#62;ioBP1.Rsrj7H8wtelqijvV0CFnXVnWLNIc/RZL0C06l4oA\n*************************** 2. row ***************************\n                 user: sri\nauthentication_string: $A$005$/k?aO&#38;ap.#b=\n                                          ^zt[E|x9q3w9uHn1oEumXUgnqNMH8xWo4xd/s26hTPKs1AbC2\n2 rows in set (0.00 sec)</pre><p>By default, the password length is 20 characters based on the variable “generated_random_password_length”. We can define the password length using that variable. and the allowed length is 5 to 255.</p><pre class=\"crayon-plain-tag\">mysql&#62; select @@generated_random_password_length;\n+------------------------------------+\n| @@generated_random_password_length |\n+------------------------------------+\n|                                 20 |\n+------------------------------------+\n1 row in set (0.00 sec)</pre><p>The random passwords will not mind the “validate_password” policy if the component is implemented in MySQL.</p>\n<p>Hopefully, this blog will be helpful for you to learn about the password reuse policy and random passwords in MySQL 8. There are a few more features to go over, which will be covered in the next part of the blog series. Stay tuned!</p>\n<p><strong>Percona Distribution for MySQL is the most complete, stable, scalable, and secure, open-source MySQL solution available, delivering enterprise-grade database environments for your most critical business applications&#8230; and it&#8217;s free to use!</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mysql-database\" rel=\"noopener\">Download Percona Distribution for MySQL Today</a></p>\n","descriptionType":"html","publishedDate":"Wed, 22 Sep 2021 13:44:38 +0000","feedId":11,"bgimg":"","linkMd5":"2a15e491a2f1ad9f856674470ad33a3b","bgimgJsdelivr":"","metaImg":"","author":"Sri Sakthivel","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn11@2020_6/2021/09/27/16-55-13-578_ff3b7983f62af910.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn64@2020_2/2021/09/27/16-55-31-423_a0fb4bd8a24f74f5.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Repoint Replica Servers in MySQL/Percona Server for MySQL 8.0","link":"https://www.percona.com/blog/?p=78056","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Repoint Replica Servers in MySQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>When doing migrations or failovers in MySQL, there is usually a need to do a topology change and repoint replica servers to obtain replication data from a different server.</p>\n<p>For example, given servers {A, B, and C} and the following topology:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78057 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_2replicas.jpg\" alt=\"MySQL Topology\" width=\"361\" height=\"344\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_2replicas.jpg 361w, https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_2replicas-300x286.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_2replicas-157x150.jpg 157w\" sizes=\"(max-width: 361px) 100vw, 361px\" /></p>\n<p>If you need to repoint C to be a replica of B, i.e:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78058 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep.jpg\" alt=\"repoint mysql\" width=\"601\" height=\"144\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep.jpg 601w, https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep-300x72.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep-200x48.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep-367x88.jpg 367w\" sizes=\"(max-width: 601px) 100vw, 601px\" /></p>\n<p>You can follow the next steps:</p>\n<p><strong>Note:</strong> log_replica_updates should be enabled on the soon-to-be primary as it is a prerequisite for chain replication.</p>\n<p><strong>Note:</strong> It is assumed that both replicas only stream from Server A and there are no <a href=\"https://dev.mysql.com/doc/refman/8.0/en/replication-rules-channel-based-filters.html\">conflicting replication filters</a> in place that might break replication later on.</p>\n<h2>If Using File/Position-Based Replication:</h2>\n<p></p><pre class=\"crayon-plain-tag\">1) Stop B and C\n\nSTOP REPLICA;\n\n2) If replicas are multi-threaded, correct MTS gaps and make them single-threaded until all changes are applied. To do so, execute the following commands on BOTH nodes:\n\nSTART REPLICA UNTIL SQL_AFTER_MTS_GAPS;\nSHOW REPLICA STATUS\\G -- repeat this until you see \"Replica_SQL_Running: No\"\nSTOP REPLICA;\nSELECT @@global.replica_parallel_workers; -- take note to restore later\nSET GLOBAL replica_parallel_workers=0; -- disable MTS during the operations\n\n3) Then check which is the node that is more up to date by looking at Relay_Source_Log_File and Exec_Source_Log_Pos. Run on BOTH nodes:\n\nSHOW REPLICA STATUS\\G\n# Take note of Relay_Source_Log_File/Exec_Source_Log_Pos from the most up to date node.\n\n4) Sync replicas with UNTIL. Run on the most delayed node with above outputs:\n\nSTART REPLICA UNTIL SOURCE_LOG_FILE='&#60;Relay_Source_Log_File&#62;', SOURCE_LOG_POS=&#60;Exec_Source_Log_Pos&#62;;\nSHOW REPLICA STATUS\\G -- repeat this until you see \"Replica_SQL_Running: No\"\n\n5) If followed above steps, at this point both replicas should have the exact same data set and should be in sync at the same point in time.\n# Double check that both replicas are stopped and with the same coords as doing topology changes while replication is ongoing and with diffs coords can cause inconsistencies:\n\nSHOW REPLICA STATUS\\G\n# Replica_IO_Running must be “NO” in both replicas\n# Replica_SQL_Running must be “NO” in both replicas\n# Relay_Source_Log_File must match in both replicas\n# Exec_Source_Log_Pos must match in both replicas\n\n6) Get current coordinates from B (new intermediate primary). Execute on B:\n\nSHOW MASTER STATUS \\G\n# Take note of File and Position\n\n7) Repoint C to B. Execute on C with coords from previous step:\n\nCHANGE REPLICATION SOURCE TO SOURCE_HOST='&#60;ip-address-of-B&#62;', SOURCE_LOG_FILE='&#60;File&#62;', SOURCE_LOG_POS='&#60;Position&#62;';\n\n8) If you had disabled MTS, you should re-enable here for both B and C;\n\nSET GLOBAL replica_parallel_workers=X; -- see output of step 2 for correct value\n\n9) Restart replication normally. Run on both nodes:\n\nSTART REPLICA;</pre><p></p>\n<h2>If Using GTID-Based Replication:</h2>\n<p></p><pre class=\"crayon-plain-tag\">1) Stop B and C:\nSTOP REPLICA;\n\n2) If replicas are multi-threaded, correct MTS gaps and make them single-threaded until all changes are applied. Run on BOTH nodes:\n\nSHOW REPLICA STATUS\\G -- repeat this until you see \"Replica_SQL_Running: No\"\nSTOP REPLICA;\nSELECT @@global.replica_parallel_workers; -- take note to restore later\nSET GLOBAL replica_parallel_workers=0; -- disable MTS during the operations\n\n3) Then check which is the node that is more up to date by looking at sequence numbers in Executed_Gtid_Set. Run on BOTH nodes:\n​\nSHOW REPLICA STATUS\\G\n# Take note of Executed_Gtid_Set with the largest sequence number. If there is a mismatch in the gtid sets it means there were either local writes or writes coming from some other server. In that case you should check data consistency between the servers, for example with pt-table-checksum . Then you need to fix gtid differences by either restoring the replica from scratch or fix errant transactions as explained on this other blogpost\n\n4) Bring up all nodes to the same point in time. Run on node with smallest GTID sequence number;\n\nSTART REPLICA UNTIL SQL_AFTER_GTIDS='&#60;Executed_Gtid_Set&#62;';\nSHOW REPLICA STATUS\\G -- repeat this until you see \"Replica_SQL_Running: No\"\n\n5) If followed above steps, at this point both replicas should have the exact same data set and should be in sync at the same point in time.\n# Double check that both replicas are stopped and with the same coords as doing topology changes while replication is ongoing and with diffs coords can cause inconsistencies:\n\nSHOW REPLICA STATUS\\G\n# Replica_IO_Running must be “NO” in both replicas\n# Replica_SQL_Running must be “NO” in both replicas\n# Executed_Gtid_Set must match in both replicas\n\n6) Now both replicas have identical data, so you can re-point C to replicate from B. Run on C:\n\nCHANGE REPLICATION SOURCE TO SOURCE_HOST='&#60;ip-address-of-B&#62;'\n\n7) If you had disabled MTS, you should re-enable here for both B and C;\n\nSET GLOBAL replica_parallel_workers=X; -- see output of step 2 for correct value\n\n8) Restart replication normally. Run on both nodes\n\nSTART REPLICA;</pre><p></p>\n<h3>Doing the opposite replication change from chain replication (A-&#62;B-&#62;C) into one primary with two replicas should be simpler:</h3>\n<h2>If Using File/Position-Based Replication:</h2>\n<p></p><pre class=\"crayon-plain-tag\">1) Stop replication on B and make sure B is not receiving any write activity:\n\nSTOP REPLICA;\n\n2) Check current binary log position on B:\n\nSHOW MASTER STATUS \\G\n\n3) On C check replication until C does catch up with B. On C:\n\nSHOW REPLICA STATUS \\G\n\n# For C to have catch up with B, the following conditions should be met:\n# “File” from B on step 2) should match Relay_Source_Log_File from 3)\n# “Position” from B on step2) should match Exec_Source_Log_Pos from 3)\n\n# After catchup, both servers will be in sync with the same data set.\n\n4) Check current replication coords from B:\n\nSHOW REPLICA STATUS \\G\n# Write down Relay_Source_Log_File and Exec_Source_Log_Pos from B, as we will be using this coords on C\n\n5) Re point C to replicate from A. File and positions used should be the ones taken from B on last step: \n\nCHANGE REPLICATION SOURCE TO SOURCE_HOST='&#60;ip-address-of-A&#62;', SOURCE_LOG_FILE='&#60;File&#62;', SOURCE_LOG_POS='&#60;Position&#62;'\n\n6) Restart replication normally. Run on both nodes:\n\nSTART REPLICA;</pre><p></p>\n<h2>If Using GTID-Based Replication:</h2>\n<p></p><pre class=\"crayon-plain-tag\">1) Stop replication on B and make sure B is not receiving any write activity:\n\nSTOP REPLICA;\n\n2) Check current binary log position on B:\n\nSHOW MASTER STATUS \\G\n\n3) On C check replication until C does catch up with B. On C:\n\nSHOW REPLICA STATUS \\G\n\n# For C to have catch up with B, the following conditions should be met:\n# Executed_Gtid_Set from B step 2) should match Executed_Gtid_Set from 3)\n# After catchup, both servers will be in sync with the same data set.\n\n4) Re point C to replicate from A:\n\nCHANGE REPLICATION SOURCE TO SOURCE_HOST='&#60;ip-address-of-A&#62;'\n\n5) Restart replication normally. Run on both nodes\n\nSTART REPLICA;</pre><p></p>\n<h3>Conclusion:</h3>\n<p>Doing topology changes might seem hard at first, but with the above procedure, it should be easy and error-free! If you do not want to do the manual approach, then you can consider using tools like <a href=\"https://www.percona.com/blog/2016/03/08/orchestrator-mysql-replication-topology-manager/\">Orchestrator</a> which allows for automatic failover and promotions.</p>\n<p><strong>Percona Distribution for MySQL is the most complete, stable, scalable, and secure, open-source MySQL solution available, delivering enterprise-grade database environments for your most critical business applications&#8230; and it&#8217;s free to use!</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mysql-database\" rel=\"noopener\">Download Percona Distribution for MySQL Today</a></p>\n","descriptionType":"html","publishedDate":"Thu, 16 Sep 2021 11:57:59 +0000","feedId":11,"bgimg":"","linkMd5":"d6b4ea19a58ca2a1e8a608f2981464f3","bgimgJsdelivr":"","metaImg":"","author":"Carlos Tutte","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn71@2020_5/2021/09/27/16-55-34-389_c200ef37f32e8ecc.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_2replicas.jpg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn35@2020_1/2021/09/27/16-55-34-205_14593bc81deef569.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep.jpg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn39@2020_6/2021/09/27/16-55-31-323_e88ed2bc0082d275.webp"},"publishedOrCreatedDate":1632761712740},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"PostgreSQL Custom Dashboards Ported to Percona Monitoring and Management 2","link":"https://www.percona.com/blog/?p=77580","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Custom Dashboards for PostgreSQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>At the recent Percona Live, Agustín Gallego and I did a presentation &#8220;<a href=\"https://www.percona.com/resources/videos/optimizing-and-troubleshooting-postgresql-pmm\">Optimizing and Troubleshooting PostgreSQL with PMM</a>&#8220;. While preparing for this talk, I’ve ported some of the older PostgreSQL custom dashboards that we published over the years <a href=\"https://grafana.com/orgs/perconalab\">over at grafana.com</a>. Initially made for Percona Monitoring and Management (PMM) 1, they are now capable of being added to PMM 2 easily. In this short blog post, I will show you how to set up two of these dashboards on PMM 2: &#8220;PostgreSQL Tuple Statistics&#8221; and &#8220;PostgreSQL DB/Table Size Details&#8221;. Technically, the DB/Table Size one is for PMM 2, but there’s an easier way to set it up in recent PMM versions.</p>\n<p>You can also check out the original blog post covering the installation of the Tuple Statistics dashboard for PMM 1: &#8220;<a href=\"https://www.percona.com/blog/2019/04/08/adding-postgresql-tuple-statistics-dashboard-to-the-pmm-plugin/\">Adding PostgreSQL Tuple Statistics Dashboard to the PMM Plugin</a>&#8220;.</p>\n<h2>Disclaimer About PMM Versions</h2>\n<p>PostgreSQL support is continuously improving in PMM, and the engineering team is busy with QAN and exporter improvements. In PMM release 2.16.0, <a href=\"https://jira.percona.com/browse/PMM-7344\">postgres_exporter was updated</a> to the community version 0.8.0, which among other things significantly improves the way custom queries are executed. In this post, I assume that you’re using PMM version 2.16.0 or above. If you’re using an older version of PMM 2, you will have to use the dblink approach proposed in the original blog post. Note that some parts of the new dashboards may not work properly with the older PMM versions.</p>\n<h2>PostgreSQL Custom Dashboards Overview</h2>\n<p>The two dashboards that were ported are &#8220;<strong>PostgreSQL Tuple Statistics</strong>&#8221; and &#8220;<strong>PostgreSQL DB/Table Size Details</strong>&#8220;, both adding important missing pieces of information to PMM. They were initially implemented by Vadim Yalovets.</p>\n<p>&#8220;PostgreSQL Tuple Statistics&#8221;, in addition to the breakdown of tuple operations, provides an overview of the dead/live tuples ratio and details on the history of vacuum executions. New addition with this port to PMM2 is that the dashboard now gives some insight into database age and current progress towards the “vacuum to prevent wraparound.” Having a view of operations happening on a per-table basis can highlight the hottest objects in the databases, and the vacuum details are a welcome addition to any monitoring system that works with PostgreSQL.</p>\n<p>The “PostgreSQL DB/Table Size Details” dashboard is somewhat simpler and narrower in its scope. This dashboard gives an overview of database and table sizes and their rate of growth. Helpful when you want to understand where all the disk space went.</p>\n<h2>Setting up Custom Queries</h2>\n<p>Once you have set up the PMM client to monitor your PostgreSQL instance, you will find three directories where you can put custom queries:</p><pre class=\"crayon-plain-tag\"># cd /usr/local/percona/pmm2/collectors/custom-queries/postgresql/\n# ls -l\ndrwxr-xr-x. 2 pmm-agent pmm-agent  69 Jul 12 21:16 high-resolution\ndrwxr-xr-x. 2 pmm-agent pmm-agent 126 Jul 12 21:24 low-resolution\ndrwxr-xr-x. 2 pmm-agent pmm-agent  34 Jul 12 21:16 medium-resolution</pre><p><span>Technical details can be found in the “</span><a href=\"https://www.percona.com/blog/2020/06/10/running-custom-queries-in-percona-monitoring-and-management/\"><span>Running Custom MySQL Queries in Percona Monitoring and Management</span></a><span>” blog post. However, all you need to do is to create two files. Download or otherwise copy the following files to the </span><i><span>low-resolution</span></i><span> directory:</span></p>\n<ul>\n<li aria-level=\"1\"><a href=\"https://raw.githubusercontent.com/Percona-Lab/pmm-custom-queries/master/postgresql/pg_tuple_statistics.yaml\">https://raw.githubusercontent.com/Percona-Lab/pmm-custom-queries/master/postgresql/pg_tuple_statistics.yaml</a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://raw.githubusercontent.com/Percona-Lab/pmm-custom-queries/master/postgresql/pg_table_size-details.yaml\"><span>https://raw.githubusercontent.com/Percona-Lab/pmm-custom-queries/master/postgresql/pg_table_size-details.yaml</span></a></li>\n</ul>\n<p><span>Once that’s done, restart the <em>pmm-agent</em> or kill the <em>postgres_exporter</em> process:</span></p><pre class=\"crayon-plain-tag\"># systemctl restart pmm-agent</pre><p>I recommend using the low resolution for these particular queries as, frankly, it doesn’t make a lot of sense to be checking database size and number of updates every 1 or 5 seconds.</p>\n<h2>Importing the Dashboards</h2>\n<p><span>Once the custom queries are prepared and the exporter is restarted, you can go ahead and import new dashboards in Grafana! See the official documentation on how to do that: </span><a href=\"https://grafana.com/docs/grafana/latest/dashboards/export-import/\"><span>Dashboards/Export and import</span></a><span>.</span></p>\n<p><span>You can pick up the dashboards on grafana.com: &#8220;</span><a href=\"https://grafana.com/grafana/dashboards/14839\"><span>PostgreSQL Tuple Statistics (Designed for PMM2)</span></a><span>&#8220;, &#8220;</span><a href=\"https://grafana.com/grafana/dashboards/11874\"><span>PostgreSQL DB/Table Size Details (Designed for PMM2)</span></a><span>&#8220;, or find raw sources </span><a href=\"https://github.com/arronax/scratch/tree/master/pmm2-pg\"><span>over at my GitHub</span></a><span>.</span></p>\n<p><span>If everything worked as expected, you should see your new dashboards showing the data:</span></p>\n<div id=\"attachment_77588\" style=\"width: 910px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1.png\"><img aria-describedby=\"caption-attachment-77588\" loading=\"lazy\" class=\"wp-image-77588 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-1024x546.png\" alt=\"PMM Dashboard: PostgreSQL Tuple Details pt1\" width=\"900\" height=\"480\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-1024x546.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-1536x818.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1.png 1920w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a><p id=\"caption-attachment-77588\" class=\"wp-caption-text\">PostgreSQL Tuple Details pt1</p></div>\n<div id=\"attachment_77587\" style=\"width: 910px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2.png\"><img aria-describedby=\"caption-attachment-77587\" loading=\"lazy\" class=\"wp-image-77587 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-1024x321.png\" alt=\"PMM Dashboard: PostgreSQL Tuple Details pt2\" width=\"900\" height=\"282\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-1024x321.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-300x94.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-200x63.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-1536x482.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-367x115.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2.png 1920w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a><p id=\"caption-attachment-77587\" class=\"wp-caption-text\">PostgreSQL Tuple Details pt2</p></div>\n<div id=\"attachment_77586\" style=\"width: 910px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2.png\"><img aria-describedby=\"caption-attachment-77586\" loading=\"lazy\" class=\"wp-image-77586 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-1024x916.png\" alt=\"PMM Dashboard: PostgreSQL Tuple Details pt3\" width=\"900\" height=\"805\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-1024x916.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-300x268.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-168x150.png 168w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-1536x1374.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-367x328.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2.png 1920w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a><p id=\"caption-attachment-77586\" class=\"wp-caption-text\">PostgreSQL Tuple Details pt3</p></div>\n<div id=\"attachment_77583\" style=\"width: 910px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details.png\"><img aria-describedby=\"caption-attachment-77583\" loading=\"lazy\" class=\"wp-image-77583 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-1024x794.png\" alt=\"PMM Dashboard: PostgreSQL DB/Table Size Details\" width=\"900\" height=\"698\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-1024x794.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-300x233.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-193x150.png 193w, https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-1536x1191.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-367x285.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details.png 1920w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a><p id=\"caption-attachment-77583\" class=\"wp-caption-text\">PostgreSQL DB/Table Size Details</p></div>\n<p><span>Enjoy monitoring with these PostgreSQL custom dashboards!</span></p>\n","descriptionType":"html","publishedDate":"Tue, 24 Aug 2021 12:13:05 +0000","feedId":11,"bgimg":"","linkMd5":"10f022279cf429a86442d1461019b2e0","bgimgJsdelivr":"","metaImg":"","author":"Sergey Kuzmichev","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn43@2020_6/2021/09/27/16-55-31-257_baa3c915773b56ba.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-1024x546.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn99@2020_3/2021/09/27/16-55-33-123_a93a92459f3d91f0.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-1024x321.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn44@2020_4/2021/09/27/16-55-33-714_b67089af13caf5d3.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-1024x916.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn51@2020_4/2021/09/27/16-55-36-461_3231b4f4b1f93ddb.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-1024x794.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn27@2020_1/2021/09/27/16-55-14-298_892ad7e4947f51ab.webp"},"publishedOrCreatedDate":1632761712751},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Percona and PostgreSQL: Better Together","link":"https://www.percona.com/blog/?p=78152","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-200x105.jpeg\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Choose Percona for PostgreSQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-200x105.jpeg 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-300x157.jpeg 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-1024x536.jpeg 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-1140x595.jpeg 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-367x192.jpeg 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628.jpeg 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>The future of technology adoption belongs to developers.</p>\n<p>The future of application deployment belongs in the cloud.</p>\n<p>The future of databases belongs to open source.</p>\n<p>That’s essentially the summary of Percona’s innovation strategy and that’s the reason I decided to join Percona &#8211; I believe the company can provide me with the perfect platform for my vision of the future of PostgreSQL.</p>\n<p><strong>So What Does That Mean, Really?</strong></p>\n<p>Gone are the days when deals were closed by executives on a golf course. That was Sales-Led Growth. The days of inbound leads based on content are also numbered. That is Marketing-Led Growth.</p>\n<p>We are living in an age where tech decision-making is increasingly bottom-up. Where end-users get to decide which technology will suit their needs best. Where the only way to ensure a future for your PostgreSQL offerings is to delight the end users &#8211; the developers.</p>\n<p><strong>That’s Product-Led Growth.</strong></p>\n<p>PostgreSQL has been rapidly gaining in popularity. The Stack Overflow Developer survey ranked it as <a href=\"https://insights.stackoverflow.com/survey/2021#most-loved-dreaded-and-wanted-database-want\">the most wanted database</a> and DB-Engines declaring it the <a href=\"https://db-engines.com/en/blog_post/85\">DBMS of the year 2020</a>. DB-Engines shows <a href=\"https://db-engines.com/en/ranking_trend/system/PostgreSQL\">steady growth in popularity of PostgreSQL</a>, which outpaces any other database out there. It is ACID compliant, secure, fast, reliable, with a liberal license, and backed by a vibrant community.</p>\n<p><strong>PostgreSQL is the World’s Most Advanced Open Source Relational Database.</strong></p>\n<p>Being a leader in open source databases, I believe Percona is well-positioned to drive a Product Led Growth strategy for PostgreSQL. We want to offer PostgreSQL that is freely available, fully supported, and certified. We want to offer you the flexibility of using it yourself, using it with our help, or letting us completely handle your database.</p>\n<p>Our plan is to focus on the end-users, make it easy for developers to build applications using our technology, delight with our user experience, and deliver value with our expertise to all businesses &#8211; large or small.</p>\n<p><em>Data is exciting. Data has the ability to tell stories. Data is here to stay. </em></p>\n<p><em>Data is increasingly stored in open source databases. </em></p>\n<p><em>PostgreSQL is the most wanted database by developers. </em></p>\n<p><strong>That is why I believe: <a href=\"https://learn.percona.com/why-customers-choose-percona-postgres\">Percona and PostgreSQL &#8211; Better Together</a>.</strong></p>\n<p><strong>In our latest white paper, we explore how customers can optimize their PostgreSQL databases, tune them for performance, and reduce complexity while achieving the functionality and security your business needs to succeed.</strong></p>\n<p style=\"text-align: center;\"><strong><a class=\"btn btn-primary btn-lg\" href=\"https://learn.percona.com/why-customers-choose-percona-postgres\" rel=\"noopener\">Download to Learn Why Percona and PostgreSQL are Better Together</a></strong></p>\n","descriptionType":"html","publishedDate":"Mon, 27 Sep 2021 13:25:31 +0000","feedId":11,"bgimg":"","linkMd5":"805e448a35bc112aae8e165558adf706","bgimgJsdelivr":"","metaImg":"","author":"Umair Shahid","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-200x105.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn28@2020_6/2021/09/27/16-55-30-535_1e017e538c7c8fdf.webp"},"publishedOrCreatedDate":1632761712736},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"A Horizontal Scalability Mindset for MySQL","link":"https://www.percona.com/blog/?p=78112","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Horizontal Scalability for MySQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78305\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-300x168.png\" alt=\"Horizontal Scalability for MySQL\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />As a Technical Account Manager at Percona, I get to work with many of our largest clients. While the industry verticals vary, one main core challenge generally remains the same &#8211; what do I do with all this data? Dealing with massive data sets in MySQL isn’t a new challenge, but the best approach still isn’t trivial. Each application is obviously different, but I wanted to discuss some of the main best practices around dealing with lakes of data.</p>\n<h2>Keep MySQL Instances Small</h2>\n<p>First and foremost, the architecture needs to be designed to keep each MySQL instance relatively small. A very common question I get from teams new to working with MySQL is: “So what is the largest instance size MySQL supports?”. My answer goes back to my time in consulting: “It depends”. Can my MySQL instance support a 20TB dataset? Maybe, but it depends on the workload pattern. Should I store 20TB of data in a single MySQL instance? In most cases, absolutely not.</p>\n<p>MySQL can definitely store massive amounts of data. But RDBMSs are designed to store, write, and read that data. When the data grows that large, often the read performance starts to suffer. But what if my working dataset still fits in RAM? This is often the critical consideration when it comes to sizing an instance. In this case, active read/write operations may remain fast, but what happens when you need to take a backup or ALTER a table? You are reading (and writing out) 20TB which will always be bounded by I/O.</p>\n<p>So what is the magic number for sizing? Many large-scale shops try to keep individual instance sizes under the 2-3TB mark. This results in a few major advantages:</p>\n<ul>\n<li>Predictable operational times (backups, alters, etc)</li>\n<li>Allows for optimized and standardized hardware</li>\n<li>Potential for parallelism in loading data</li>\n</ul>\n<p>If I know my instance will never exceed a couple of terabytes, I can fully optimize my systems for that data size. The results are predictable and repeatable operational actions. Now, when a backup is “slow”, it is almost assuredly due to hardware and not being an outlier instance that is double the size. This is a huge win for the operations team in managing the overall infrastructure.  In addition to backups, you have the additional consideration of restore time.  Massive backups will slow restoration and have a negative impact on RTO.</p>\n<h2>Store Less Data</h2>\n<p>Now that the negative impact of large, individual instances is known, let&#8217;s look at how we keep the sizes down.  While seemingly obvious, the best way to keep data sizes small is to store less data.  There are a few ways to approach this:</p>\n<ul>\n<li>Optimize data types\n<ul>\n<li>If data types are bigger than needed, it results in excess disk footprint (i.e. using bigint when int will suffice)</li>\n</ul>\n</li>\n<li>Review indexes for bloat\n<ul>\n<li>Limit composite Primary Keys (PKs)</li>\n<li>Find and remove redundant indexes (using <a href=\"https://www.percona.com/doc/percona-toolkit/LATEST/pt-duplicate-key-checker.html\">pt-duplicate-key-checker</a>)</li>\n<li>Avoid PKs using varchar</li>\n</ul>\n</li>\n<li>Purge old data\n<ul>\n<li>When possible, remove records not being read</li>\n<li>Tools like <a href=\"https://www.percona.com/doc/percona-toolkit/LATEST/pt-archiver.html\">pt-archiver</a> can really help in this process</li>\n</ul>\n</li>\n</ul>\n<p>These techniques can help you delay the need for more advanced techniques.  However, in some cases (due to compliance, limited flexibility, etc), the above options aren&#8217;t possible.  In other cases, you may already be doing them and are still hitting size limits.</p>\n<h2>Horizontal Sharding</h2>\n<p>So what is another way to deal with massive data sets in MySQL? When all other options are exhausted, you need to look at splitting the data horizontally and spreading it across multiple equally sized instances. Unfortunately, this is much easier said than done. While there are some tools and options out there for MySQL (such as Vitess), often the best and most flexible approach is building this sharding logic into your application directly. Sharding can be done statically (key modulus for example) or more dynamically (via a dictionary lookup) or some hybrid approach of the two:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78113 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-1024x626.png\" alt=\"Horizontal Scalability Mindset for MySQL\" width=\"900\" height=\"550\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-1024x626.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-300x183.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-200x122.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-367x224.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1.png 1080w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<h3>Sharding Considerations</h3>\n<p>When you finally have to bite the bullet and split data horizontally, there are definitely some things to keep in mind.  First and foremost, picking the correct sharding key is imperative.  With the wrong key, shards won&#8217;t be balanced and you&#8217;ll end up with sizes all over the board.  This then becomes the same problem where a shard can grow too large.</p>\n<p>Once you have the correct key, you need to understand that different workloads will be impacted by sharding differently.  When the data is split across shards, individual lookups are generally the easiest to implement.  You take the key, map to a shard, and fetch the results.  However, if the workload requires aggregate access (think reports, totals, etc), now you are dealing with combining multiple shards.  This is a primary and major challenge when looking at horizontal sharding.  As is the case in most architectures, the business requirements and workload will dictate the design.</p>\n<p>If your team is struggling with an exploding data set, the <a href=\"https://www.percona.com/services/consulting\">Professional Services team at Percona</a> can help you design a more flexible and scalable solution. Each case is unique and our team can work with your specific use case and business requirements to guide you in the right direction. The biggest thing to remember: please don’t just keep adding hard disk space to your instances while expecting it to scale. Proper design and horizontal sharding is the critical factor as your data grows!</p>\n<p><strong>Percona Distribution for MySQL is the most complete, stable, scalable, and secure, open-source MySQL solution available, delivering enterprise-grade database environments for your most critical business applications&#8230; and it&#8217;s free to use!</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mysql-database\" rel=\"noopener\">Download Percona Distribution for MySQL Today</a></p>\n","descriptionType":"html","publishedDate":"Mon, 27 Sep 2021 14:40:47 +0000","feedId":11,"bgimg":"","linkMd5":"ea953c59c8bf260d1c6cae3b7147a7b1","bgimgJsdelivr":"","metaImg":"","author":"Mike Benshoof","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn87@2020_2/2021/09/27/16-55-28-691_561700ba6ddbe560.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn87@2020_6/2021/09/27/16-55-35-478_f3c898be60183b82.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-1024x626.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn96@2020_6/2021/09/27/16-55-27-773_287df2a73071abf1.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Storing and Using JSON Within PostgreSQL Part One","link":"https://www.percona.com/blog/?p=77983","description":"<img width=\"200\" height=\"107\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-200x107.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Storing JSON PostgreSQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-1024x546.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>Continuing our series on storing JSON directly within your database, we are now looking at the JSON functionality built into PostgreSQL.  You can see the first two posts in this series exploring MySQL’s JSON functionality with <a href=\"https://www.percona.com/blog/storing-json-in-your-databases-tips-mysql/\">Storing JSON in Your Databases: Tips and Tricks For MySQL Part One</a> and <a href=\"https://www.percona.com/blog/storing-json-in-your-databases-tips-and-tricks-for-mysql-part-two/\">Storing JSON in Your Databases: Tips and Tricks For MySQL Part Two</a>.  I used the exact same table structures and datasets as I did within my MySQL tests.  You can get the instructions to follow along at home <a href=\"https://github.com/TheYonk/os-db-json-tester\">here on GitHub</a>.</p>\n<p>PostgreSQL has two JSON datatypes available to you.  First JSON, and second JSONB.  The <a href=\"https://www.postgresql.org/docs/13/datatype-json.html\">docs</a> highlight the differences pretty well:</p>\n<p><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM.png\"><img loading=\"lazy\" class=\"aligncenter wp-image-77987 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-1024x230.png\" alt=\"JSON JSON B Compare\" width=\"900\" height=\"202\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-1024x230.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-300x67.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-200x45.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-1536x344.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-2048x459.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-367x82.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<p>Basically, JSONB is stored decomposed, making it more efficient for the database to interact with, while the JSON type is stored as an exact text copy of what was input.  This means formatting, white space, etc., will be lost with JSONB.  That said, the suggestion is to generally use JSONB.  The trade-off being slightly slower inserts vs. faster overall read performance (on top of the storage differences mentioned above).</p>\n<p>In terms of performance, how much of a difference is there?  That is tricky, as it will depend heavily on the size and complexity of the JSON document being stored.  The larger, more complex the document, the larger the difference you could see.  But to illustrate an example, we will use our movie JSON to select the same data from both JSON and JSONB.</p>\n<p>I created the following tables and loaded them with movie-related JSON:</p><pre class=\"crayon-plain-tag\">create table movies_json (\n\tai_myid serial primary key, \n\timdb_id varchar(255),\n\tjson_column json not null\n);\n\ncreate unique index movies_json_imdb_idx on movies_json(imdb_id);\nCREATE INDEX gin_index ON movies_json USING gin (jsonb_column);\n\n\ncreate table movies_jsonb (\n\tai_myid serial primary key, \n\timdb_id varchar(255),\n\tjsonb_column jsonb not null\n);\n\ncreate unique index movies_jsonb_imdb_idx on movies_jsonb(imdb_id);\nCREATE INDEX movies_jsonb_gin_index ON movies_jsonb USING gin (json_column);</pre><p><span>Querying the JSON column results in slower retrieval:</span><span><br />\n</span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select * from movies_json where json_column-&#62;&#62;'title' = 'Avengers: Endgame (2019)';\n                                                                QUERY PLAN                                                                \n------------------------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..55775.85 rows=1880 width=1059) (actual time=694.047..2516.879 rows=1 loops=1)\n   Output: ai_myid, imdb_id, json_column\n   Workers Planned: 2\n   Workers Launched: 2\n   -&#62;  Parallel Seq Scan on public.movies_json  (cost=0.00..54587.85 rows=783 width=1059) (actual time=1905.511..2512.010 rows=0 loops=3)\n         Output: ai_myid, imdb_id, json_column\n         Filter: ((movies_json.json_column -&#62;&#62; 'title'::text) = 'Avengers: Endgame (2019)'::text)\n         Rows Removed by Filter: 125119\n         Worker 0:  actual time=2511.276..2511.277 rows=0 loops=1\n         Worker 1:  actual time=2511.322..2511.322 rows=0 loops=1\n Planning Time: 0.166 ms\n Execution Time: 2516.897 ms\n(12 rows)</pre><p><span>While the JSONB column is faster (3x):</span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select * from movies_jsonb where jsonb_column-&#62;&#62;'title' = 'Avengers: Endgame (2019)';\n                                                               QUERY PLAN                                                                \n-----------------------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..54116.60 rows=1873 width=1025) (actual time=723.324..726.914 rows=1 loops=1)\n   Output: ai_myid, imdb_id, jsonb_column\n   Workers Planned: 2\n   Workers Launched: 2\n   -&#62;  Parallel Seq Scan on public.movies_jsonb  (cost=0.00..52929.30 rows=780 width=1025) (actual time=548.982..721.730 rows=0 loops=3)\n         Output: ai_myid, imdb_id, jsonb_column\n         Filter: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) = 'Avengers: Endgame (2019)'::text)\n         Rows Removed by Filter: 125119\n         Worker 0:  actual time=720.995..720.995 rows=0 loops=1\n         Worker 1:  actual time=202.751..720.994 rows=1 loops=1\n Planning Time: 0.038 ms\n Execution Time: 726.933 ms\n(12 rows)</pre><p><span>To ensure this is not a single data point I wrote a </span><a href=\"https://github.com/TheYonk/os-db-json-tester/blob/main/PG/pg_json_test_json_vs_jsonb.py\"><span>small script to run this test</span></a><span> (with this title and other random titles).  Over 100 runs we had pretty consistent results:</span></p>\n<table>\n<tbody>\n<tr>\n<td><span>JSON</span></td>\n<td><span>JSONB</span></td>\n</tr>\n<tr>\n<td><span>Average Time: 2.5492</span></p>\n<p><span>Min Time: 2.5297428970225155</span></p>\n<p><span>Max Time: 2.56536191503983</span></td>\n<td><span>Average Time: 0.747</span></p>\n<p><span>Min Time: 0.7297536049736664</span></p>\n<p><span>Max Time: 0.7827945239841938</span></td>\n</tr>\n</tbody>\n</table>\n<p><span>As stated, insert/updates will lose some of that performance, so keep that in mind.  It is, however, easier to interact and update things in a JSONB datatype.  For instance, some functions are only available for JSONB data types, like the </span><a href=\"https://www.postgresql.org/docs/current/functions-json.html\"><span>JSONB_SET</span></a><span> function which allows you to in-place update your JSON. </span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) update movies_jsonb set jsonb_column= jsonb_set(jsonb_column, '{imdb_rating}', '9') where jsonb_column-&#62;&#62;'title' = 'Avengers: Endgame (2019)';\n                                                           QUERY PLAN                                                           \n--------------------------------------------------------------------------------------------------------------------------------\n Update on public.movies_jsonb  (cost=0.00..56211.80 rows=1873 width=52) (actual time=1928.691..1928.692 rows=0 loops=1)\n   -&#62;  Seq Scan on public.movies_jsonb  (cost=0.00..56211.80 rows=1873 width=52) (actual time=577.386..1928.246 rows=1 loops=1)\n         Output: ai_myid, imdb_id, jsonb_set(jsonb_column, '{imdb_rating}'::text[], '9'::jsonb, true), ctid\n         Filter: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) = 'Avengers: Endgame (2019)'::text)\n         Rows Removed by Filter: 375358\n Planning Time: 0.040 ms\n Execution Time: 1928.718 ms\n(7 rows)</pre><p>I have seen references in several places to the insert performance difference between JSON and JSONB, but I have not seen any concrete numbers.  I decided to run a few tests:</p>\n<ul>\n<li>Inserting 10K records into a JSON column (via insert into select from):  72.851 ms</li>\n<li>Inserting 10K records into a JSONB column (via insert into select from):  754.045 ms</li>\n</ul>\n<p><span>If you are doing a heavy insert workload, the difference is significant enough to take note and plan for it.  </span></p>\n<p><span>Another distinct advantage for JSONB is the ability to use </span><a href=\"https://www.postgresql.org/docs/13/textsearch-indexes.html\"><span>GIN indexes</span></a><span> over the JSON document.  Gin indexes are designed and optimized to work for text searches.  This focus lends itself well to JSON.  That said, the syntax may be a bit more complicated to make use of GIN indexes.</span></p>\n<p><span>Here is how you create a GIN index.  </span></p><pre class=\"crayon-plain-tag\">movie_json_test=# CREATE INDEX movies_jsonb_gin_index ON movies_jsonb USING gin (jsonb_column);\nCREATE INDEX</pre><p><span>You can see the original query is still not using this index:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column-&#62;&#62;'title' = 'Avengers: Endgame (2019)';\n                                                              QUERY PLAN                                                               \n---------------------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..54125.60 rows=1877 width=64) (actual time=716.059..719.346 rows=1 loops=1)\n   Output: ((jsonb_column -&#62;&#62; 'title'::text)), ((jsonb_column -&#62;&#62; 'imdb_rating'::text))\n   Workers Planned: 2\n   Workers Launched: 2\n   -&#62;  Parallel Seq Scan on public.movies_jsonb  (cost=0.00..52937.90 rows=782 width=64) (actual time=544.197..714.577 rows=0 loops=3)\n         Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n         Filter: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) = 'Avengers: Endgame (2019)'::text)\n         Rows Removed by Filter: 125119\n         Worker 0:  actual time=202.768..713.907 rows=1 loops=1\n         Worker 1:  actual time=713.890..713.891 rows=0 loops=1\n Planning Time: 0.041 ms\n Execution Time: 719.365 ms\n(12 rows)</pre><p><span>Why not?  The -&#62;&#62; returns the value as a text, so the conversion from JSON to Text causes some issues.  There are a couple of ways to work around this.  The first is to use @@ which returns the first JSON item:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column @@ '$.title == \"Avengers: Endgame (2019)\"';\n                                                           QUERY PLAN                                                            \n---------------------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on public.movies_jsonb  (cost=72.29..221.83 rows=38 width=64) (actual time=0.179..0.180 rows=1 loops=1)\n   Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n   Recheck Cond: (movies_jsonb.jsonb_column @@ '($.\"title\" == \"Avengers: Endgame (2019)\")'::jsonpath)\n   Heap Blocks: exact=1\n   -&#62;  Bitmap Index Scan on movies_jsonb_gin_index  (cost=0.00..72.28 rows=38 width=0) (actual time=0.068..0.068 rows=2 loops=1)\n         Index Cond: (movies_jsonb.jsonb_column @@ '($.\"title\" == \"Avengers: Endgame (2019)\")'::jsonpath)\n Planning Time: 0.145 ms\n Execution Time: 0.199 ms\n(8 rows)</pre><p><span>This works.  You can also use the @&#62; which checks if the JSON value entries exist. </span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column @&#62; '{ \"title\" : \"Avengers: Endgame (2019)\"}';\n                                                           QUERY PLAN                                                            \n---------------------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on public.movies_jsonb  (cost=44.29..193.94 rows=38 width=64) (actual time=0.199..0.200 rows=1 loops=1)\n   Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n   Recheck Cond: (movies_jsonb.jsonb_column @&#62; '{\"title\": \"Avengers: Endgame (2019)\"}'::jsonb)\n   Heap Blocks: exact=1\n   -&#62;  Bitmap Index Scan on movies_jsonb_gin_index  (cost=0.00..44.28 rows=38 width=0) (actual time=0.091..0.091 rows=1 loops=1)\n         Index Cond: (movies_jsonb.jsonb_column @&#62; '{\"title\": \"Avengers: Endgame (2019)\"}'::jsonb)\n Planning Time: 0.147 ms\n Execution Time: 0.214 ms\n(8 rows)</pre><p><span>Not only did this use the index it is now over 3x faster overall.  The PostgreSQL documentation offers a fairly detailed overview of the different options to interact with JSON, especially when it comes to </span><a href=\"https://www.postgresql.org/docs/13/functions-json.html#FUNCTIONS-JSON-PROCESSING\"><span>functions and operators</span></a><span> (note the </span><a href=\"https://www.postgresql.org/docs/12/functions-json.html#FUNCTIONS-JSON-PROCESSING\"><span>table in the PG 12 Docs</span></a><span> giving you an overview of the operators is a little more readable ) available to you. It’s important to note that not all operators and functions will work with GIN indexes.  For example, using ‘like_regex’:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column @@ '$.title like_regex \"^Avengers*\"';\n                                                              QUERY PLAN                                                              \n--------------------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..53546.87 rows=38 width=64) (actual time=218.550..795.063 rows=9 loops=1)\n   Output: ((jsonb_column -&#62;&#62; 'title'::text)), ((jsonb_column -&#62;&#62; 'imdb_rating'::text))\n   Workers Planned: 2\n   Workers Launched: 2\n   -&#62;  Parallel Seq Scan on public.movies_jsonb  (cost=0.00..52543.07 rows=16 width=64) (actual time=251.866..790.098 rows=3 loops=3)\n         Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n         Filter: (movies_jsonb.jsonb_column @@ '($.\"title\" like_regex \"^Avengers*\")'::jsonpath)\n         Rows Removed by Filter: 125117\n         Worker 0:  actual time=311.403..789.402 rows=3 loops=1\n         Worker 1:  actual time=225.825..789.408 rows=1 loops=1\n Planning Time: 0.204 ms\n Execution Time: 795.087 ms\n(12 rows)</pre><p><span>In addition to the GIN indexes, you can create Hash or Btree indexes on your JSON columns, however, they are only useful if you are comparing or searching the entire JSON document.  Instead, you want to use an expression index (often referred to as functional indexes in other databases).  </span></p>\n<h2>Generated Columns and Expression Indexes</h2>\n<p><span>While the GIN indexes work for some use cases, you often will find it easier and more convenient to use actual columns or some of the normal functions you are familiar with.  The simplest use of expression indexes is to simply pull out and index a field you will use often.  Let’s say we will often want to search for a title.  We can create an index on the extracted JSON from our previous example.</span></p><pre class=\"crayon-plain-tag\">movie_json_test=# create index movie_jsonb_title_index on movies_jsonb (((jsonb_column -&#62;&#62; 'title')::text));\nCREATE INDEX</pre><p><span>This creates a btree index on the jsonb_column-&#62;&#62; title path, allowing us to search for a title. </span></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column-&#62;&#62;'title' = 'Avengers: Endgame (2019)';\n                                                             QUERY PLAN                                                             \n------------------------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on public.movies_jsonb  (cost=66.97..6421.19 rows=1877 width=64) (actual time=0.138..0.140 rows=1 loops=1)\n   Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n   Recheck Cond: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) = 'Avengers: Endgame (2019)'::text)\n   Heap Blocks: exact=1\n   -&#62;  Bitmap Index Scan on movie_jsonb_title_index  (cost=0.00..66.50 rows=1877 width=0) (actual time=0.014..0.014 rows=1 loops=1)\n         Index Cond: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) = 'Avengers: Endgame (2019)'::text)\n Planning Time: 0.160 ms\n Execution Time: 0.159 ms\n(8 rows)</pre><p><span>Originally, when we searched for movies with the ‘Avengers Endgame (2019)’ title there was no index (unless you modified the SQL to make use of the GIN index).  Now with this new index, you can see our original query is no longer doing a sequential scan but is instead using the btree index.  If we want the index to also be used for ‘ like string%’ statements as well to help with partial matching we need something a bit extra added to the index.  Note: this is not something you can do by default using only the GIN index (however you can look into the </span><a href=\"https://www.postgresql.org/docs/13/pgtrgm.html\"><span>pg_trgm</span></a><span> extension potentially for this).  Here we will create the same index with the </span><a href=\"https://www.postgresql.org/docs/current/indexes-opclass.html\"><span>operator class “text_pattern_ops”</span></a><span> which is designed to work with Like and Regex.</span></p>\n<p><strong>Before:</strong></p><pre class=\"crayon-plain-tag\">movie_json_test=# explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column-&#62;&#62;'title' like 'Avengers%';\n                                                              QUERY PLAN                                                               \n---------------------------------------------------------------------------------------------------------------------------------------\n Gather  (cost=1000.00..54125.60 rows=1877 width=64) (actual time=371.463..723.743 rows=8 loops=1)\n   Output: ((jsonb_column -&#62;&#62; 'title'::text)), ((jsonb_column -&#62;&#62; 'imdb_rating'::text))\n   Workers Planned: 2\n   Workers Launched: 2\n   -&#62;  Parallel Seq Scan on public.movies_jsonb  (cost=0.00..52937.90 rows=782 width=64) (actual time=288.053..718.957 rows=3 loops=3)\n         Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n         Filter: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) ~~ 'Avengers%'::text)\n         Rows Removed by Filter: 125117\n         Worker 0:  actual time=204.176..718.288 rows=2 loops=1\n         Worker 1:  actual time=288.637..718.299 rows=3 loops=1\n Planning Time: 0.130 ms\n Execution Time: 723.762 ms\n(12 rows)</pre><p><strong>After:</strong></p><pre class=\"crayon-plain-tag\">movie_json_test=# create index movie_jsonb_title_index on movies_jsonb (((jsonb_column -&#62;&#62; 'title')::text) text_pattern_ops);\nCREATE INDEX\nmovie_json_test=# explain (verbose true, analyze true) select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column-&#62;&#62;'title' like 'Avengers%';\n                                                                              QUERY PLAN                                                                              \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Bitmap Heap Scan on public.movies_jsonb  (cost=71.66..6425.88 rows=1877 width=64) (actual time=0.195..0.498 rows=8 loops=1)\n   Output: (jsonb_column -&#62;&#62; 'title'::text), (jsonb_column -&#62;&#62; 'imdb_rating'::text)\n   Filter: ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) ~~ 'Avengers%'::text)\n   Heap Blocks: exact=8\n   -&#62;  Bitmap Index Scan on movie_jsonb_title_index  (cost=0.00..71.19 rows=1877 width=0) (actual time=0.015..0.015 rows=8 loops=1)\n         Index Cond: (((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) ~&#62;=~ 'Avengers'::text) AND ((movies_jsonb.jsonb_column -&#62;&#62; 'title'::text) ~&#60;~ 'Avengert'::text))\n Planning Time: 0.168 ms\n Execution Time: 0.519 ms</pre><p><span>Now you can also perform a similar operation, but store the results as a queryable generated column instead:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; \\d movies_jsonb\n                                         Table \"public.movies_jsonb\"\n    Column    |          Type          | Collation | Nullable |                    Default                    \n--------------+------------------------+-----------+----------+-----------------------------------------------\n ai_myid      | integer                |           | not null | nextval('movies_jsonb_ai_myid_seq'::regclass)\n imdb_id      | character varying(255) |           |          | \n jsonb_column | jsonb                  |           | not null | \nIndexes:\n    \"movies_jsonb_pkey\" PRIMARY KEY, btree (ai_myid)\n    \"movies_jsonb_imdb_idx\" UNIQUE, btree (imdb_id)\n\nmovie_json_test=&#62; alter table movies_jsonb add column title varchar(255) generated always as (((jsonb_column -&#62;&#62; 'title')::text)) stored;\nALTER TABLE\n\n\nmovie_json_test=&#62; \\d movies_jsonb\n                                                  Table \"public.movies_jsonb\"\n    Column    |          Type          | Collation | Nullable |                             Default                             \n--------------+------------------------+-----------+----------+-----------------------------------------------------------------\n ai_myid      | integer                |           | not null | nextval('movies_jsonb_ai_myid_seq'::regclass)\n imdb_id      | character varying(255) |           |          | \n jsonb_column | jsonb                  |           | not null | \n title        | character varying(255) |           |          | generated always as (((jsonb_column -&#62;&#62; 'title'::text))) stored\nIndexes:\n    \"movies_jsonb_pkey\" PRIMARY KEY, btree (ai_myid)\n    \"movies_jsonb_imdb_idx\" UNIQUE, btree (imdb_id)</pre><p><span>Now we have the option to add an index on the title column, select it without the JSON formatting, etc.  Note: generated columns can not be updated.</span></p>\n<p><span>We can create our entire movie_json table with extra columns and expression indexes and make it look a bit more like a normal table:</span></p><pre class=\"crayon-plain-tag\">create table movies_json_generated (\n\tai_myid serial primary key, \n\timdb_id varchar(255) generated always as (jsonb_column -&#62;&#62; 'imdb_id') stored,\n\ttitle varchar(255) generated always as (jsonb_column -&#62;&#62; 'title') stored,\n    imdb_rating decimal(5,2) generated always as ((jsonb_column  -&#62;&#62; 'imdb_rating')::numeric) stored,\n\toverview text generated always as (jsonb_column -&#62;&#62; 'overview') stored,\n\tdirector jsonb generated always as ((jsonb_column -&#62;&#62; 'director')::json) stored,\n\tcountry varchar(100) generated always as (jsonb_column -&#62;&#62; 'country') stored,\n\tjsonb_column jsonb,\n\tjson_column json\n);\n\ncreate unique index gen_imdb_idx on movies_json_generated(imdb_id);\ncreate index gen_title_idx on movies_json_generated(title);\ncreate index gen_func_title_index on movies_json_generated (((json_column -&#62;&#62; 'title')::varchar));\t\nCREATE INDEX Gen_gin_index ON movies_json_generated USING gin (jsonb_column);</pre><p><span>You may have noticed we had to explicitly cast the columns to a specific data type.  One of the challenges or at least the more difficult things to get used to with both MySQL and PostgreSQL.</span></p>\n<p><span>It&#8217;s worth remembering that a JSON document does not have explicit data types, so you will often find that some functions or indexes may not work as expected because you are comparing data of two different types (and will return different results from some operations, i.e. sorting ascii vs. numeric).  For example: to add 1 to our IMDB rating:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; update  movies_jsonb_generated_1 set jsonb_column= jsonb_set(jsonb_column::jsonb, '{imdb_rating}',(((((jsonb_column -&#62;&#62;'imdb_rating')))+1.1))) ;\nERROR:  operator does not exist: text + numeric\nLINE 1: ...imdb_rating}',(((((jsonb_column -&#62;&#62;'imdb_rating')))+1))) ;\n                                                              ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.</pre><p><span>Here the imdb_rating that is returned is not numeric so you can not add 1.  So logically, you want to cast the value to a numeric to allow for the addition.  </span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; update  movies_jsonb_generated_1 set jsonb_column= jsonb_set(jsonb_column::jsonb, '{imdb_rating}',(((((jsonb_column -&#62;&#62;'imdb_rating')::numeric))+1))) ;\nERROR:  function jsonb_set(jsonb, unknown, numeric) does not exist\nLINE 1: ...pdate  movies_jsonb_generated_1 set jsonb_column= jsonb_set(...\n                                                             ^\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.</pre><p><span>Again we run into a type issue.  This time the jsonb_set function can not pass in a numeric value (nor does it implicitly convert) when the function is looking for a JSONB value.  So let’s cast the result to JSONB:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; update  movies_jsonb_generated_1 set jsonb_column= jsonb_set(jsonb_column::jsonb, '{imdb_rating}',(((((jsonb_column -&#62;&#62;'imdb_rating')::numeric))+1)::jsonb)) ;\nERROR:  cannot cast type numeric to jsonb\nLINE 1: ...((((jsonb_column -&#62;&#62;'imdb_rating')::numeric))+1.1)::jsonb)) …</pre><p><span>Here we find we can not cast a numeric directly to JSONB.  So we have to convert the numeric to text, then to JSONB.</span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; update  movies_jsonb_generated_1 set jsonb_column= jsonb_set(jsonb_column::jsonb, '{imdb_rating}',(((((jsonb_column -&#62;&#62;'imdb_rating')::numeric))+1)::text::jsonb)) ;\nUPDATE 100000</pre><p><span>For some developers, this may prove a bit tricky to get the hang of.  This will also extend to things that work, but maybe just slow or may produce the wrong data.  For instance, when using compare, sort, or filter you may or may not pick up the index you are looking for and you may be comparing the ASCII value for numerics.  Below you can see us explicitly convert certain columns to compare them. You can also see that when you generate a column or create an expression index you need to do the same:</span></p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; explain analyze select jsonb_column-&#62;&#62;'title' as title,   jsonb_column-&#62;&#62;'imdb_rating' as rating,   jsonb_column-&#62;&#62;'imdb_id' as imdb_id  from movies_json_generated where (jsonb_column-&#62;&#62;'imdb_rating')::numeric &#62; 8 order by (jsonb_column-&#62;&#62;'imdb_rating')::numeric desc limit 5;\n                                                                        QUERY PLAN                                                                        \n----------------------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=65291.78..65292.36 rows=5 width=128) (actual time=1055.064..1058.740 rows=5 loops=1)\n   -&#62;  Gather Merge  (cost=65291.78..77449.06 rows=104198 width=128) (actual time=1055.062..1058.738 rows=5 loops=1)\n         Workers Planned: 2\n         Workers Launched: 2\n         -&#62;  Sort  (cost=64291.76..64422.00 rows=52099 width=128) (actual time=1053.392..1053.393 rows=4 loops=3)\n               Sort Key: (((jsonb_column -&#62;&#62; 'imdb_rating'::text))::numeric) DESC\n               Sort Method: top-N heapsort  Memory: 25kB\n               Worker 0:  Sort Method: top-N heapsort  Memory: 25kB\n               Worker 1:  Sort Method: top-N heapsort  Memory: 25kB\n               -&#62;  Parallel Seq Scan on movies_json_generated  (cost=0.00..63426.41 rows=52099 width=128) (actual time=0.993..1052.275 rows=2688 loops=3)\n                     Filter: (((jsonb_column -&#62;&#62; 'imdb_rating'::text))::numeric &#62; '8'::numeric)\n                     Rows Removed by Filter: 122432\n Planning Time: 0.061 ms\n Execution Time: 1058.769 ms\n(14 rows)\n \nmovie_json_test=&#62; \\d movies_json_generated\n                                                     Table \"public.movies_json_generated\"\n    Column    |          Type          | Collation | Nullable |                                    Default                                    \n--------------+------------------------+-----------+----------+-------------------------------------------------------------------------------\n ai_myid      | integer                |           | not null | nextval('movies_json_generated_ai_myid_seq'::regclass)\n imdb_id      | character varying(255) |           |          | generated always as (((json_column -&#62;&#62; 'imdb_id'::text))) stored\n title        | character varying(255) |           |          | generated always as (((json_column -&#62;&#62; 'title'::text))) stored\n imdb_rating  | numeric(5,2)           |           |          | generated always as (((json_column -&#62;&#62; 'imdb_rating'::text)::numeric)) stored\n overview     | text                   |           |          | generated always as (json_column -&#62;&#62; 'overview'::text) stored\n director     | jsonb                  |           |          | generated always as (((json_column -&#62;&#62; 'director'::text)::json)) stored\n country      | character varying(100) |           |          | generated always as (((json_column -&#62;&#62; 'country'::text))) stored\n jsonb_column | jsonb                  |           |          | \n json_column  | json                   |           |          | \nIndexes:\n    \"movies_json_generated_pkey\" PRIMARY KEY, btree (ai_myid)\n    \"gen_func_title_index\" btree (((json_column -&#62;&#62; 'title'::text)::character varying))\n    \"gen_gin_index\" gin (jsonb_column)\n    \"gen_imdb_idx\" UNIQUE, btree (imdb_id)\n    \"gen_title_idx\" btree (title)\n \nmovie_json_test=&#62; explain analyze select jsonb_column-&#62;&#62;'title' as title,   jsonb_column-&#62;&#62;'imdb_rating' as rating,   jsonb_column-&#62;&#62;'imdb_id' as imdb_id  from movies_json_generated where imdb_rating &#62; 8 order by imdb_rating desc limit 5;\n                                                                       QUERY PLAN                                                                       \n--------------------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=62548.12..62548.70 rows=5 width=102) (actual time=112.458..114.704 rows=5 loops=1)\n   -&#62;  Gather Merge  (cost=62548.12..63277.80 rows=6254 width=102) (actual time=112.457..114.702 rows=5 loops=1)\n         Workers Planned: 2\n         Workers Launched: 2\n         -&#62;  Sort  (cost=61548.09..61555.91 rows=3127 width=102) (actual time=110.807..110.808 rows=4 loops=3)\n               Sort Key: imdb_rating DESC\n               Sort Method: top-N heapsort  Memory: 25kB\n               Worker 0:  Sort Method: top-N heapsort  Memory: 25kB\n               Worker 1:  Sort Method: top-N heapsort  Memory: 26kB\n               -&#62;  Parallel Seq Scan on movies_json_generated  (cost=0.00..61496.16 rows=3127 width=102) (actual time=0.128..109.939 rows=2688 loops=3)\n                     Filter: (imdb_rating &#62; '8'::numeric)\n                     Rows Removed by Filter: 122432\n Planning Time: 0.146 ms\n Execution Time: 114.729 ms\n(14 rows)\n\nmovie_json_test=&#62; create index test_index_imdb_rating on movies_json_generated (imdb_rating);\nCREATE INDEX\nmovie_json_test=&#62; explain analyze select jsonb_column-&#62;&#62;'title' as title,   jsonb_column-&#62;&#62;'imdb_rating' as rating,   jsonb_column-&#62;&#62;'imdb_id' as imdb_id  from movies_json_generated where imdb_rating &#62; 8 order by imdb_rating desc limit 5;\n                                                                              QUERY PLAN                                                                              \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=0.42..19.41 rows=5 width=102) (actual time=0.094..0.134 rows=5 loops=1)\n   -&#62;  Index Scan Backward using test_index_imdb_rating on movies_json_generated  (cost=0.42..28521.09 rows=7510 width=102) (actual time=0.093..0.132 rows=5 loops=1)\n         Index Cond: (imdb_rating &#62; '8'::numeric)\n Planning Time: 0.136 ms\n Execution Time: 0.152 ms\n(5 rows)</pre><p>A quick side note:  there will be a performance difference for inserts/updates on JSON/JSONB columns with one or more generated columns or expression indexes.  Because of how JSONB is stored, you can see a boost in performance when you update a column that has a lot of generated columns/expression indexes.  In my tests, I saw 3-4x or more performance improvement in doing updates on a JSONB column with several generated/expression indexes vs. a JSON column with the same setup.  That said, the more generated columns and expression indexes you have on a table, the more it can impact performance… this is a trade-off.</p>\n<ul>\n<li aria-level=\"1\">A table with a single generated column, took 7650ms to load 100K rows.</li>\n<li aria-level=\"1\">A table with 6 generated columns, took 8576ms to load 100K rows.</li>\n</ul>\n<p>While this is a very simple setup with just a single JSONB column and the generated columns, you can see a slight overhead.  This held true with the same 100K records being updated, updating only one column in the table with six generated columns took 4.7 seconds vs. 3.6 seconds for a single generated column (a ~25% difference).  Again, not an in-depth test, but it illustrates that adding a ton of expression indexes or generated columns will not be without some cost.</p>\n<h2>Recap &#38; What’s Next</h2>\n<p>A few quick takeaways:</p>\n<ul>\n<li aria-level=\"1\">Using JSONB is probably going to be your best option in most use cases</li>\n<li aria-level=\"1\">Be very careful of type conversions and making assumptions on the data within your JSON/JSONB columns.  You may get errors or odd results.</li>\n<li aria-level=\"1\">Use the available indexes, generated columns, and expression indexes to gain substantial performance benefits</li>\n</ul>\n<p>Now you have the basics of JSON in PostgreSQL.  In <a href=\"https://www.percona.com/blog/storing-and-using-json-within-postgresql-part-two\">part two of this series</a>, we review some of the more advanced options and do a deeper dive on performance and explore a normalized schema -vs- one that is heavily JSON.</p>\n","descriptionType":"html","publishedDate":"Fri, 03 Sep 2021 11:32:21 +0000","feedId":11,"bgimg":"","linkMd5":"28cf360982906e2016e175d9735638e0","bgimgJsdelivr":"","metaImg":"","author":"Matt Yonkovit","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-200x107.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn8@2020_1/2021/09/27/16-55-27-823_b4c4d15d70e5e43a.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-1024x230.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn64@2020_6/2021/09/27/16-55-32-444_ed09c0166e577427.webp"},"publishedOrCreatedDate":1632761712736},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Percona Server for MongoDB 5.0.2 Release Candidate Is Now Available","link":"https://www.percona.com/blog/?p=77762","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Server for MongoDB 5.0.2\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77771\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-300x157.png\" alt=\"Percona Server for MongoDB 5.0.2\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />We’re happy to announce the first release candidate of <a href=\"https://www.percona.com/doc/percona-server-for-mongodb/5.0/release_notes/5.0.2-1.html\">Percona Server for MongoDB version 5.0.2 (PSMDB)</a>. It is now available for<a href=\"https://www.percona.com/downloads/percona-server-mongodb-5.0/#\"> download from the Percona website</a> and via the<a href=\"https://www.percona.com/doc/percona-server-for-mongodb/5.0/install/index.html\"> Percona Software Repositories</a>.</p>\n<p>Percona Server for MongoDB 5.0.2 is an enhanced, source-available, and highly scalable document-oriented database that is a fully compatible drop-in replacement for MongoDB 5.0.2 Community Edition. It includes<a href=\"https://docs.mongodb.com/v5.0/release-notes/5.0/#5.0.2---aug-06--2021\"> all the features of MongoDB 5.0.2 Community Edition</a>, as well as some <a href=\"https://www.percona.com/software/mongodb/feature-comparison\">additional enterprise-grade features</a>.</p>\n<p>The most notable features in version 5.0 include the following:</p>\n<ul>\n<li><a href=\"https://docs.mongodb.com/v5.0/release-notes/5.0/#resharding\">Resharding</a> allows you to select a new shard key for a collection and then works in the background to correct any data distribution problems caused by bad shard keys and improve performance.</li>\n<li><a href=\"https://docs.mongodb.com/v5.0/release-notes/5.0/#time-series-collections\">Time Series Collections</a> are aimed at storing sequences of measurements over a period of time. These specialized collections will store data in a highly optimized way that will improve query efficiency, allow data analysis in real-time, and optimize disk usage.</li>\n<li><a href=\"https://docs.mongodb.com/v5.0/release-notes/5.0/#interrupted-index-builds\">Resumable Index Builds</a> means that the index build for a collection continues if a primary node in a replica set is switched to another server or when a server restarts. The build process is saved to disk and resumes from the saved position. This allows DBAs to perform maintenance and not worry about losing the index build in the process.</li>\n<li><a href=\"https://docs.mongodb.com/v5.0/release-notes/5.0/#window-operators\">Window operators</a> allow operations on a specified span of documents known as a window. <a href=\"https://docs.mongodb.com/v5.0/reference/operator/aggregation/setWindowFields/#mongodb-pipeline-pipe.-setWindowFields\"><tt>$setWindowFields</tt></a> is a new pipeline stage to operate with these documents.</li>\n<li><a href=\"https://docs.mongodb.com/v5.0/reference/versioned-api/\">Versioned API</a> allows specifying which API version your application communicating with MongoDB runs against. Versioned API detaches the application’s lifecycle from that of the database. As a result, you modify the application only to introduce new features instead of having to maintain compatibility with the new version of MongoDB.</li>\n</ul>\n<p>Additionally,<a href=\"https://docs.mongodb.com/v5.0/release-notes/5.0/#new-aggregation-operators\"> new aggregation operators</a> such as <tt>$count</tt>, <tt>$dateAdd</tt>, <tt>$dateDiff</tt>, <tt>$dateSubtract</tt>, <tt>$sampleRate</tt> and <tt>$rand</tt> are available with this release.</p>\n<p><b>Note:</b> As with every major release, version 5.0 comes with a significant number of new features and is still being rapidly updated. At this point, we’re making this version available as a &#8220;Release Candidate&#8221; only and <b>we strongly suggest not to use it for production environments yet</b>. However, we do encourage the use of this version in test and development environments.</p>\n<p>We’re also still in the process of integrating support for version 5.0 into our other products. While <a href=\"https://www.percona.com/doc/percona-backup-mongodb/release-notes/1.6.0.html\">Percona Backup for MongoDB 1.6.0</a> has just been released to support this version, some other products still need to be updated and tested.</p>\n<p>For example, the <a href=\"https://www.percona.com/doc/kubernetes-operator-for-psmongodb/index.html\">Percona Distribution for MongoDB Operator</a> will have PSMDB 5.0 support from version 1.10.0, which is slated to happen in mid-September.</p>\n<p>On the <a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> side, <a href=\"https://jira.percona.com/browse/PMM-8469\">Percona Server for MongoDB 5.0 support</a> is scheduled to be included in version 2.22.0 (currently targeting the end of September).</p>\n<p>Because of these factors, we will not release version 5.0 of our <a href=\"https://www.percona.com/software/mongodb\">Percona Distribution for MongoDB</a> until we’ve updated these products and have gathered enough confidence to remove the “release candidate” label.</p>\n","descriptionType":"html","publishedDate":"Tue, 17 Aug 2021 12:37:52 +0000","feedId":11,"bgimg":"","linkMd5":"80883f465440ff80e26e48846522c5d0","bgimgJsdelivr":"","metaImg":"","author":"Lenz Grimmer","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn95@2020_1/2021/09/27/16-55-29-894_cd90abc4a01143f4.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn32@2020_3/2021/09/27/16-55-34-020_11e160f47e8a2cf8.webp"},"publishedOrCreatedDate":1632761712756},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"October 13, 2021: Open Mic on Open Source Takes on MongoDB","link":"https://www.percona.com/blog/?p=78118","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Open Source MongoDB\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><strong><img loading=\"lazy\" class=\"alignright size-medium wp-image-78125\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-300x168.png\" alt=\"Percona Open Source MongoDB\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Have a burning question about MongoDB?</strong></p>\n<p>Database experts will be leading an open forum discussion based on attendees’ interests. Are you ahead of the curve? Just trying to keep up? Get the best of MongoDB.</p>\n<p><strong>Live Stream: October 13 at 11:30 am CT (30 min)</strong></p>\n<p>Watch this upcoming Open Mic on Open Source to learn the latest from the experts. Will Fromme, Percona Solutions Engineer, and Michal Nosek, Percona Enterprise Architect, will share their insights into what&#8217;s facing MongoDB admins right now.</p>\n<p><strong>Will and Michal will bring you up to speed on:</strong></p>\n<ul>\n<li aria-level=\"1\">How to convert a standalone MongoDB Community Edition to Percona Server for MongoDB</li>\n<li aria-level=\"1\">How to convert an entire replica set running MongoDB Community Edition to <a href=\"https://www.percona.com/software/mongodb/percona-server-for-mongodb\">Percona Server for MongoDB</a></li>\n<li aria-level=\"1\">How to run Kubernetes Operators with Percona Server for MongoDB</li>\n</ul>\n<p>Percona’s pro pairing will bring together in-depth operational knowledge of MongoDB, Percona&#8217;s open source tools, and a fully-supported MongoDB Community distribution. They’ll also remind you how to best scale a cluster, use the self-healing feature, backup and restore your database, and modify parameters.</p>\n<p><strong>Hear what other open source enthusiasts want to know about!</strong></p>\n<p>This is an open forum for a reason. If you get stumped sometimes, don’t worry; you’re not alone! Our hosts will take your questions in REAL-TIME. And you can remain anonymous even if you want to ask a question.</p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://learn.percona.com/2021-open-mic-on-open-source\" rel=\"noopener\">Register to Attend!</a></p>\n<p>When you register, our hosts will make sure they provide a webinar worth the half-hour!</p>\n<p>Space is limited. Secure your spot now and we’ll see you on <strong>October 13 at 11:30 am</strong>.</p>\n","descriptionType":"html","publishedDate":"Tue, 14 Sep 2021 13:23:27 +0000","feedId":11,"bgimg":"","linkMd5":"bcf5931fb660a9a42d53f2b91979573e","bgimgJsdelivr":"","metaImg":"","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn52@2020_3/2021/09/27/16-55-13-778_a34ae542ce52dc46.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn47@2020_3/2021/09/27/16-55-28-490_b19895ebee0135c1.webp"},"publishedOrCreatedDate":1632761712716},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"New Features in Percona Distribution for MySQL Operator, Beta Percona Distribution for PostgreSQL Operator 0.2.0: Release Roundup August 16, 2021","link":"https://www.percona.com/blog/?p=77567","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Software Release AUg 16\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16.png 712w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2><img loading=\"lazy\" class=\"alignright size-medium wp-image-77600\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-300x169.png\" alt=\"Percona Software Release AUg 16\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16.png 712w\" sizes=\"(max-width: 300px) 100vw, 300px\" />It&#8217;s release roundup time again here at Percona!</h2>\n<p>Our Release Roundups <span class=\"s1\">showcase the latest Percona software updates, tools, and features to help you manage and deploy our software. It offers</span> highlights and critical information, as well as links to the full release notes and direct links to the software or service itself to download.</p>\n<p>Today&#8217;s post includes those releases and updates that have come out since August 2, 2021.</p>\n<p>&#160;</p>\n<h2>Percona Distribution for PostgreSQL Operator 0.2.0</h2>\n<p>On August 12, 2021, <a href=\"https://www.percona.com/doc/kubernetes-operator-for-postgresql/ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN0.2.0.html\">Percona Distribution for PostgreSQL Operator 0.2.0</a> was released. It is based on best practices for the configuration and setup of a Percona Distribution for PostgreSQL cluster. The benefits of the Operator are many, but saving time and delivering a consistent and vetted environment is key. New features and improvements include the Custom Resource structure was reworked to provide the same look and feel as other Percona Operators, there is no need to specify the name of the pgBackrest Pod in the backup manifest anymore as it is detected automatically by the Operator, and replicas management is now performed through the main Custom Resource manifest instead of creating separate Kubernetes resources.</p>\n<p><strong>Note: Version 0.2.0 of the Percona Distribution for PostgreSQL Operator is a Beta release, and it is not recommended for production environments.</strong></p>\n<p><a href=\"https://www.percona.com/software/percona-kubernetes-operators\">Download Percona Distribution for PostgreSQL Operator 0.2.0</a></p>\n<p>&#160;</p>\n<h2>Percona Distribution for MySQL Operator 1.9.0</h2>\n<p>Aug 9, 2021, saw the release of <a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/ReleaseNotes/Kubernetes-Operator-for-PXC-RN1.9.0.html\">Percona Distribution for MySQL Operator 1.9.0</a>, which automates the creation, alteration, or deletion of members in your Percona Distribution for MySQL environment. There are many new features,  improvements, and fixes, including (but not limited to) using Secrets to store custom configuration with sensitive data for <a class=\"reference internal\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/options.html#operator-configmaps\"><span class=\"std std-ref\">Percona XtraDB Cluster</span></a>, <a class=\"reference internal\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/haproxy-conf.html#haproxy-conf-custom\"><span class=\"std std-ref\">HAProxy</span></a>, and <a class=\"reference internal\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/proxysql-conf.html#proxysql-conf-custom\"><span class=\"std std-ref\">ProxySQL</span></a> Pods, now you can <a class=\"reference external\" href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/client/haproxy.html\">see HAProxy metrics</a> in your favorite Percona Monitoring and Management (PMM) dashboards automatically, and the <a class=\"reference internal\" href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/replication.html#operator-replication\"><span class=\"std std-ref\">cross-site replication</span></a> feature allows an asynchronous replication between two Percona XtraDB Clusters, including scenarios when one of the clusters is outside of the Kubernetes environment.</p>\n<p><a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Download Percona Distribution for MySQL Operator 1.9.0</a></p>\n<p>&#160;</p>\n<h2>Percona Monitoring and Management 2.20.0</h2>\n<p>On Tuesday, August 3, 2021, <a href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/release-notes/2.20.0.html\">Percona Monitoring and Management 2.20.0</a> (PMM) was released. It is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Included in this release is an important note for users of PMM who started out using the Docker image of 2.16.0, along with many new features and improvements.</p>\n<p><a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Download Percona Monitoring and Management 2.20.0</a></p>\n<p>&#160;</p>\n<h2>Percona Server for MongoDB 4.0.26-21</h2>\n<p><a href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.0/release_notes/4.0.26-21.html\">Percona Server for MongoDB 4.0.26-21</a> was released on August 9, 2021. It is an enhanced, open source, and highly scalable database that is a fully compatible, drop-in replacement for MongoDB 4.0.26 Community Edition, supporting MongoDB 4.0.26 protocols and drivers. An improvement in this release is the added status of hot backup, so one can check the status of a backup like running: true/false, start time, estimated time, etc.</p>\n<p><a href=\"https://www.percona.com/software/mongodb/percona-server-for-mongodb\">Download Percona Server for MongoDB 4.0.26-21</a></p>\n<p>&#160;</p>\n<h2>ProxySQL 2.2.0 and proxysql-admin</h2>\n<p>As of August 10, 2021, ProxySQL 2.2.0, released by ProxySQL, is now available for download in the Percona Repository along with an updated version of Percona‘s <strong class=\"command\">proxysql-admin</strong> tool. It is a high-performance proxy, currently for MySQL, and database servers in the MySQL ecosystem</p>\n<p><a href=\"https://www.percona.com/software/percona-software-repositories-for-mysql\">Download ProxySQL 2.2.0 and proxysql-admin</a></p>\n<p>&#160;</p>\n<p>That&#8217;s it for this roundup, and be sure to <a href=\"https://twitter.com/Percona\" target=\"_blank\" rel=\"noopener\">follow us on Twitter</a> to stay up-to-date on the most recent releases! Percona is a leader in providing best-of-breed enterprise-class support, consulting, managed services, training, and software for MySQL, MongoDB, PostgreSQL, MariaDB, and other open source databases in on-premises and cloud environments.</p>\n","descriptionType":"html","publishedDate":"Mon, 16 Aug 2021 11:40:31 +0000","feedId":11,"bgimg":"","linkMd5":"3a26926be8500c5b1856e9eed70b2987","bgimgJsdelivr":"","metaImg":"","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn76@2020_4/2021/09/27/16-55-31-795_62e598fbd4911813.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn56@2020_6/2021/09/27/16-55-29-301_e573a330d261a75a.webp"},"publishedOrCreatedDate":1632761712754},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"myloader Stops Causing Data Fragmentation","link":"https://www.percona.com/blog/?p=77742","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"myloader Stops Causing Data Fragmentation\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77888\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-300x157.png\" alt=\"myloader Stops Causing Data Fragmentation\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />During the development of the myloader &#8211;innodb-optimize-keys option, which was released in <a href=\"https://www.percona.com/blog/mydumper-0-10-7-is-now-available/\">version 0.10.7</a>, we found several issues and opportunities to improve the process. We had to change the approach, reimplement some of the core functionality and add a couple of data structures. That allowed us to implement, at a really low cost, a feature that executes the files that contain INSERT statements, sorted by Primary Key. This is desirable to <a href=\"https://www.percona.com/blog/2017/04/10/innodb-page-merging-and-page-splitting/\">reduce page splits</a>, which cause on-disk tablespace fragmentation.</p>\n<p>In this blog post, I will present the differences in data fragmentation for each version.</p>\n<h2>Test Details</h2>\n<p>These are local vm tests as there is no intention to show performance gain.</p>\n<p>The table that I used is:</p><pre class=\"crayon-plain-tag\">CREATE TABLE `perf_test` (\n `id` int(11) NOT NULL AUTO_INCREMENT,\n `val` varchar(108) DEFAULT NULL,\n PRIMARY KEY (`id`),\n KEY `val` (`val`(2)),\n KEY `val_2` (`val`(4)),\n KEY `val_3` (`val`(8))\n) ENGINE=InnoDB</pre><p>And I inserted the data with:</p><pre class=\"crayon-plain-tag\">INSERT INTO perf_test(val) SELECT concat(uuid(),uuid(),uuid()) FROM perf_test;</pre><p>The graphs below were made with <a href=\"https://github.com/jeremycole/innodb_ruby\">innodb_ruby</a> (more info about it in this <a href=\"https://www.percona.com/blog/2015/04/03/illustrating-primary-key-models-in-innodb-and-their-impact-on-disk-usage/\">blog post</a>) and based on a table of 131K rows with &#8211;rows 100. The intention of this test was to create a lot of files that will cause better spread in the Primary Key. The timings are over the same table structure but the table has 32M rows. Finally, I performed the test with 1 and 4 threads and with &#8211;innodb-optimize-keys when possible.</p>\n<h2>Tests Performed</h2>\n<p><span>In myloader v0.10.5 there was no file sorting, which is why we can see that lower Primary Key values were updated recently:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-77747 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-181x300.png\" alt=\"\" width=\"181\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-181x300.png 181w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-617x1024.png 617w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-90x150.png 90w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-926x1536.png 926w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-367x609.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM.png 1144w\" sizes=\"(max-width: 181px) 100vw, 181px\" />   <img loading=\"lazy\" class=\"alignnone wp-image-77750 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-180x300.png\" alt=\"\" width=\"180\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-180x300.png 180w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-615x1024.png 615w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-90x150.png 90w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-923x1536.png 923w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-367x611.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM.png 1134w\" sizes=\"(max-width: 180px) 100vw, 180px\" /></p>\n<p><span>It doesn’t matter the number of threads, we can see how pages, across the whole file, are being updated at any time. </span></p>\n<p><span>This is happening because mydumper exported the files in order with these min_id and max_id values:</span></p>\n<table>\n<tbody>\n<tr>\n<td><span>File</span></td>\n<td><span>min_id</span></td>\n<td><span>max_id</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00000.sql</span></td>\n<td><span>1</span></td>\n<td><span>21261</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00001.sql</span></td>\n<td><span>21262</span></td>\n<td><span>42522</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00002.sql</span></td>\n<td><span>42523</span></td>\n<td><span>49137</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00003.sql</span></td>\n<td><span>65521</span></td>\n<td><span>85044</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00004.sql</span></td>\n<td><span>85045</span></td>\n<td><span>98288</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00006.sql</span></td>\n<td><span>131056</span></td>\n<td><span>148827</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00007.sql</span></td>\n<td><span>148828</span></td>\n<td><span>170088</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00008.sql</span></td>\n<td><span>170089</span></td>\n<td><span>191349</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00009.sql</span></td>\n<td><span>191350</span></td>\n<td><span>196591</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00012.sql</span></td>\n<td><span>262126</span></td>\n<td><span>276393</span></td>\n</tr>\n</tbody>\n</table>\n<p><span>But, during import, there was no order, let’s see the log:</span></p><pre class=\"crayon-plain-tag\">** Message: 12:55:12.267: Thread 3 restoring `test`.`perf_test` part 1476. Progress 1 of 1589 .\n** Message: 12:55:12.269: Thread 1 restoring `test`.`perf_test` part 87. Progress 2 of 1589 .\n** Message: 12:55:12.269: Thread 2 restoring `test`.`perf_test` part 1484. Progress 3 of 1589 .\n** Message: 12:55:12.269: Thread 4 restoring `test`.`perf_test` part 1067. Progress 4 of 1589 .\n** Message: 12:55:13.127: Thread 1 restoring `test`.`perf_test` part 186. Progress 5 of 1589 .\n** Message: 12:55:13.128: Thread 4 restoring `test`.`perf_test` part 1032. Progress 6 of 1589 .</pre><p><span>With these max_id and max_id per file:</span></p>\n<table>\n<tbody>\n<tr>\n<td><span>File</span></td>\n<td><span>min_id</span></td>\n<td><span>max_id</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.01476.sql</span></td>\n<td><span>31381237</span></td>\n<td><span>31402497</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00087.sql</span></td>\n<td><span>1849708</span></td>\n<td><span>1870968</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.01484.sql</span></td>\n<td><span>31551325</span></td>\n<td><span>31572585</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.01067.sql</span></td>\n<td><span>22685488</span></td>\n<td><span>22706748</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.00186.sql</span></td>\n<td><span>3954547</span></td>\n<td><span>3975807</span></td>\n</tr>\n<tr>\n<td><span>test.perf_test.01032.sql</span></td>\n<td><span>21941353</span></td>\n<td><span>21962613</span></td>\n</tr>\n</tbody>\n</table>\n<p><span>With this kind of insert order, you can only imagine the amount of page splits that cause the fragmentation in the InnoDB datafile.</span></p>\n<p><span>Timings were:</span></p><pre class=\"crayon-plain-tag\">0.10.5/mydumper/myloader  -t 1 6:52\n0.10.5/mydumper/myloader  -t 4 4:55</pre><p><span>In v0.10.7-2 we have the same behavior:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-77746 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-184x300.png\" alt=\"\" width=\"184\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-184x300.png 184w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-628x1024.png 628w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-92x150.png 92w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-942x1536.png 942w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-367x598.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM.png 1166w\" sizes=\"(max-width: 184px) 100vw, 184px\" />  <img loading=\"lazy\" class=\"alignnone wp-image-77745 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-183x300.png\" alt=\"\" width=\"183\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-183x300.png 183w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-624x1024.png 624w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-91x150.png 91w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-936x1536.png 936w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-367x602.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM.png 1152w\" sizes=\"(max-width: 183px) 100vw, 183px\" /></p>\n<p><span>But we have a small performance increase:</span></p><pre class=\"crayon-plain-tag\">0.10.7-2/mydumper/myloader  -t 1 6:49 \n0.10.7-2/mydumper/myloader  -t 4 4:47</pre><p><span>We see the same pattern, even if we use the &#8211;innodb-optimize-keys:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-77752 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-209x300.png\" alt=\"\" width=\"209\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-209x300.png 209w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-715x1024.png 715w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-105x150.png 105w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-1073x1536.png 1073w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-367x526.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM.png 1141w\" sizes=\"(max-width: 209px) 100vw, 209px\" />  <img loading=\"lazy\" class=\"alignnone wp-image-77744 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-212x300.png\" alt=\"\" width=\"212\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-212x300.png 212w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-724x1024.png 724w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-106x150.png 106w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-1087x1536.png 1087w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-367x519.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM.png 1149w\" sizes=\"(max-width: 212px) 100vw, 212px\" /></p>\n<p><span>The main difference is the index creation stage.</span></p><pre class=\"crayon-plain-tag\">0.10.7-2/mydumper/myloader --innodb-optimize-keys -t 1 6:07 \n0.10.7-2/mydumper/myloader --innodb-optimize-keys -t 4 5:53</pre><p><span>Now, in v0.10.9, where we have table and file sorting, the graphs have a significant change: </span></p>\n<p data-wp-editing=\"1\"><img loading=\"lazy\" class=\"alignnone wp-image-77749 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-232x300.png\" alt=\"\" width=\"232\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-232x300.png 232w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-792x1024.png 792w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-116x150.png 116w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-367x474.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM.png 1160w\" sizes=\"(max-width: 232px) 100vw, 232px\" />  <img loading=\"lazy\" class=\"alignnone wp-image-77751 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-186x300.png\" alt=\"\" width=\"186\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-186x300.png 186w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-633x1024.png 633w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-93x150.png 93w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-950x1536.png 950w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-367x593.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM.png 1167w\" sizes=\"(max-width: 186px) 100vw, 186px\" /></p>\n<p><span>It is also a bit shocking the difference between the 2 graphs, not about color trending, but about the number of pages used which indicates a high fragmentation when multiple threads are used.</span></p><pre class=\"crayon-plain-tag\">master/mydumper/myloader  -t 1 5:50 \nmaster/mydumper/myloader  -t 4 4:29</pre><p><span>Let’s check now with &#8211;innodb-optimize-keys:</span></p>\n<p><img loading=\"lazy\" class=\"alignnone wp-image-77753 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-232x300.png\" alt=\"\" width=\"232\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-232x300.png 232w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-791x1024.png 791w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-116x150.png 116w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-367x475.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM.png 1156w\" sizes=\"(max-width: 232px) 100vw, 232px\" />  <img loading=\"lazy\" class=\"alignnone wp-image-77748 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-216x300.png\" alt=\"\" width=\"216\" height=\"300\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-216x300.png 216w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-738x1024.png 738w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-108x150.png 108w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-1107x1536.png 1107w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-367x509.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM.png 1175w\" sizes=\"(max-width: 216px) 100vw, 216px\" /></p>\n<p><span>This is what we are looking for! As you can see with 1 thread is perfect, but with 4 threads there is some odd distribution, but for sure, much better than the other options.</span></p>\n<p><span>However, the timings are not the best:</span></p><pre class=\"crayon-plain-tag\">master/mydumper/myloader --innodb-optimize-keys -t 1 5:33 \nmaster/mydumper/myloader --innodb-optimize-keys -t 4 5:10</pre><p><span>Let’s compare them:</span></p><pre class=\"crayon-plain-tag\">Data       | Index      | Total      | Table \n0 00:05:50 | 0 00:00:00 | 0 00:05:50 | `test`.`perf_test`  -t 1 \n0 00:04:29 | 0 00:00:00 | 0 00:04:29 | `test`.`perf_test`  -t 4 \n0 00:02:33 | 0 00:02:59 | 0 00:05:33 | `test`.`perf_test`  -t 1 --innodb-optimize-keys \n0 00:02:01 | 0 00:03:09 | 0 00:05:10 | `test`.`perf_test`  -t 4 --innodb-optimize-keys</pre><p><span>But that makes sense if you read </span><a href=\"https://www.percona.com/blog/2015/01/21/importing-big-tables-large-indexes-myloader/\"><span>this blog post</span></a><span>. Actually, it would be really nice to have a feature that </span><a href=\"https://github.com/maxbube/mydumper/issues/389\"><span>determines when &#8211;innodb-optimize-keys needs to be used</span></a><span>.</span></p>\n<h2>Conclusions</h2>\n<p><span>Version 0.10.9 of MyDumper will allow myloader to insert better than previous versions. </span><span>Multithreaded inserts sorted by Primary Key are now possible and faster than ever!</span></p>\n","descriptionType":"html","publishedDate":"Mon, 23 Aug 2021 14:55:39 +0000","feedId":11,"bgimg":"","linkMd5":"f20c364dc02652beb58d1e7d77ddf319","bgimgJsdelivr":"","metaImg":"","author":"David Ducos","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn24@2020_2/2021/09/27/16-55-34-513_2e3e00e58f543626.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn7@2020_4/2021/09/27/16-55-33-459_590bda8c07c61001.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-181x300.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn75@2020_1/2021/09/27/16-55-35-619_d38ad486d9db6478.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-180x300.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn3@2020_1/2021/09/27/16-55-35-637_d1cd1193ddf07328.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-184x300.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_6/2021/09/27/16-55-31-889_5dcd62278ecc0075.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-183x300.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn32@2020_6/2021/09/27/16-55-28-139_d163a4fa3142ec4c.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-209x300.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn96@2020_5/2021/09/27/16-55-35-611_0f9a8f91d6490028.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-212x300.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn88@2020_2/2021/09/27/16-55-32-697_ad0f915647b56c6b.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-232x300.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn8@2020_1/2021/09/27/16-55-35-180_2f9ad720f7cc5511.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-186x300.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn78@2020_5/2021/09/27/16-55-30-213_3a50842f22544b77.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-232x300.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn8@2020_2/2021/09/27/16-55-28-314_428e31c31611ab52.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-216x300.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn84@2020_2/2021/09/27/16-55-32-636_9cf23f3db9570823.webp"},"publishedOrCreatedDate":1632761712764},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Performance of Various EBS Storage Types in AWS","link":"https://www.percona.com/blog/?p=77786","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"EBS Storage Types in AWS\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77853\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-300x157.png\" alt=\"EBS Storage Types in AWS\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />EBS storage type choices in AWS can be impacted by a lot of factors. As a consultant, I get a lot of questions about choosing the best storage type for a workload. Let me share a few examples. Is io2 better than gp2/3 if the configured iops are the same? What can I expect when upgrading gp2 to gp3?</p>\n<p>In order to be able to answer questions like this, in this blog post, we will take a deeper look. We will compare storage devices that are “supposed to be the same”, in order to reveal the differences between these storage types. We will examine the following storage devices:</p>\n<ul>1 TB gp2 volume (has 3000 iops by definition)</ul>\n<ul>1 TB gp3 volume, with the iops set to 3000</ul>\n<ul>1 TB io1 volume, with the iops set to 3000</ul>\n<ul>1 TB io2 volume, with the iops set to 3000</ul>\n<p>So, all the volumes are 1TB with 3000 iops, so in theory, they are the same. Also, in theory, theory and practice are the same, but in practice, they are different. Storage performance is more complex than just capacity and the number of iops, as we will see soon. Note that this test is very limited to draw conclusions like io1 is better than gp2 or anything like that in general. These devices have very different scalability characteristics (the io devices are scaling to 64k iops, while the maximum for the gp devices is 16k). Measuring the scalability of these devices and testing them in the long run and in different availability zones are out of scope for these tests. The reason I chose devices that have the same “specs” is to gain an understanding of the difference in their behavior. The tests were only run in a single availability zone (eu-west-1a).</p>\n<p>For the tests, I used sysbench fileio, with the following prepare command.</p><pre class=\"crayon-plain-tag\">sysbench --test=fileio \\\n--file-total-size=700G \\\n--threads=16 \\\n--file-num=64 \\\n--file-block-size=16384 \\\nprepare</pre><p>The instances I used were r5.xlarge instances, which have up to 4750 Mbps bandwidth to EBS.</p>\n<p>I used the following command to run the tests:</p><pre class=\"crayon-plain-tag\">sysbench fileio \\\n--file-total-size=700G \\\n--time=1800 \\\n--max-requests=0 \\\n--threads=${th} \\\n--file-num=64 \\\n--file-io-mode=sync \\\n--file-test-mode=${test_mode} \\\n--file-extra-flags=direct \\\n--file-fsync-freq=0 \\\n--file-block-size=16384 \\\n--report-interval=1 \\\nrun</pre><p>In this command, the test mode can be rndwr (random writes only), rndrd (random reads only), and rndwr (random reads and writes mixed). The number of threads used were 1, 2, 4, 8, 16, 32, 64, and 128. All tests are using 16k io operations with direct io enabled (bypassing the filesystem cache), based on this, the peak theoretical throughput of the tests is 16k*3000 = 48 MB/s.</p>\n<h2>Random Writes</h2>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77787 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-1024x717.png\" alt=\"sysbench random writes\" width=\"900\" height=\"630\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-1024x717.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-300x210.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-200x140.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-367x257.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp.png 1363w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>The gp2 and io1 devices reached the peak throughput for this benchmark with 4 threads and the gp3 reached it with 2 threads (but with a larger variance). The io2 device has more consistent performance overall. The peak throughput in these tests is the expected peak throughput (16k*3000 iops = 46.8MB/sec).</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77788 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-1024x717.png\" alt=\"sysbench random mixed read/write latency\" width=\"900\" height=\"630\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-1024x717.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-300x210.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-200x140.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-367x257.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat.png 1363w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>At a low thread count, gp3 has the highest variation in latency, gp2’s performance is more consistent. The latencies of io1 and io2 are more consistent, especially io2 at a higher thread count.</p>\n<p>This means if the workload is mostly writes:</p>\n<p>&#8211; Prefer gp3 over gp2 (better performance, less price).<br />\n&#8211; Prefer io2 if the price is worth the consistency in performance at lower thread counts.<br />\n&#8211; If the workload is multithreaded, and there are always more than 4 threads, prefer gp3 (in this case, the performance is the same, gp3 is the cheapest option).</p>\n<h2>Random Reads</h2>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77789 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-1024x717.png\" alt=\"sysbench random reads\" width=\"900\" height=\"630\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-1024x717.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-300x210.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-200x140.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-367x257.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp.png 1363w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>The random read throughput shows a much bigger difference than writes. First of all, the performance is more inconsistent in the case of gp2 and gp3, but gp2 seems to be slightly more consistent. The io2 device has the same consistent performance even with a single thread.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77790 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-1024x717.png\" alt=\"sysbench random read latency\" width=\"900\" height=\"630\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-1024x717.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-300x210.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-200x140.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-367x257.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat.png 1363w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>Similarly, there is a much bigger variance in latency in the case of low thread counts between the gp2 and the gp3. Even at 64 threads, the io2 device has very consistent latency characteristics.</p>\n<p>This means if the workload is mostly reads:</p>\n<p>&#8211; The gp2 volumes can give slightly better performance, but they are also slightly more expensive.<br />\n&#8211; Above 16 parallel threads, the devices are fairly similar, prefer gp3 because of the price.<br />\n&#8211; Prefer io2 if performance and latency are important with a low thread count (even over io1).</p>\n<h2>Random Mixed Reads/Writes</h2>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77791 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-1024x717.png\" alt=\"random mixed reads/writes\" width=\"900\" height=\"630\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-1024x717.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-300x210.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-200x140.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-367x257.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp.png 1363w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>The mixed workload behavior is similar to the random read one, so the variance in the read performance will also show as a variance in the write performance. The more reads are added to the mix, the inconsistent the performance will become with the gp2/gp3 volumes. The io1 volume reaches peak throughput even with two threads, but with a high variance.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-large wp-image-77867\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-1024x717.png\" alt=\"\" width=\"900\" height=\"630\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-1024x717.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-300x210.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-200x140.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-367x257.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2.png 1363w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>&#160;</p>\n<p>In the case of the mixed workload, the gp3 has the least consistent performance. This can come as an unpleasant surprise when the volumes are upgraded to gp3, and the workload has a low concurrency. This can be an issue for not loaded, but latency-sensitive applications. Otherwise, for choosing storage, the same advice applies to random reads.</p>\n<p><strong>Conclusion</strong></p>\n<p>The difference between these seemingly similar devices is greatest when a low number of threads are used against the device. If the io workload is parallel enough, the devices behave very similarly.</p>\n<p>The raw data for these measurements are available on GitHub: <a href=\"https://github.com/pboros/aws_storage_blog\">https://github.com/pboros/aws_storage_blog</a>.</p>\n","descriptionType":"html","publishedDate":"Fri, 20 Aug 2021 13:36:54 +0000","feedId":11,"bgimg":"","linkMd5":"aade0fd597a5fabc26e919311ae75b78","bgimgJsdelivr":"","metaImg":"","author":"Peter Boros","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn72@2020_1/2021/09/27/16-55-28-754_b671a1899b08eb7f.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn23@2020_1/2021/09/27/16-55-28-003_76f9d051147f2166.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-1024x717.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn4@2020_6/2021/09/27/16-55-28-037_4a2f04c147196fbc.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-1024x717.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn84@2020_3/2021/09/27/16-55-24-414_5f3f01c7b24f1939.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-1024x717.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn40@2020_6/2021/09/27/16-55-31-282_246c9a5a168f9805.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-1024x717.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn52@2020_4/2021/09/27/16-55-36-427_7f4fd6865f37b512.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-1024x717.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn48@2020_5/2021/09/27/16-55-33-201_d0ed7c4e8f3a036d.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-1024x717.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn71@2020_1/2021/09/27/16-55-32-541_fc96e3635bc63be4.webp"},"publishedOrCreatedDate":1632761712743},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"First Packages for Debian 11 “bullseye” Now Available","link":"https://www.percona.com/blog/?p=77764","description":"<img width=\"200\" height=\"113\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-200x113.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Debian 11 bullseye\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-200x113.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-1024x576.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye.png 1280w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77776\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-300x169.png\" alt=\"Percona Debian 11 bullseye\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-1024x576.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-200x113.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye.png 1280w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Over the weekend, the Debian project <a href=\"https://www.debian.org/News/2021/20210814\">announced</a> the availability of their newest major distribution release, Debian 11 (code name “bullseye”). We’d like to congratulate the Debian project and the open source community for achieving this major milestone! With over two years in the making, it contains an impressive amount of new and updated software for a wide range of applications (check out the <a href=\"https://www.debian.org/releases/bullseye/amd64/release-notes/\">release notes</a> for details). The project’s emphasis on providing a stable Linux operating system makes Debian Linux a preferred choice for database workloads.</p>\n<p>The packaging, release, and QA teams here at Percona have been working on adding support for Debian 11 to our products for quite some time already.</p>\n<p>This week, we’ve released <a href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.4/release_notes/4.4.8-9.html\">Percona Server for MongoDB 4.4.8</a> and <a href=\"https://www.percona.com/doc/percona-server-for-mongodb/5.0/release_notes/5.0.2-1.html\">5.0.2 (Release Candidate)</a> as well as <a href=\"https://www.percona.com/doc/percona-backup-mongodb/release-notes/1.6.0.html\">Percona Backup for MongoDB 1.6.0</a>, including packages for Debian 11 as a new supported OS platform. Please follow the installation instructions in the respective product documentation to install these versions.</p>\n<p>We’ve also rebuilt a number of previously released products on Debian 11. At this point, the following products are available for download from our “testing” package repositories:</p>\n<ul>\n<li aria-level=\"1\">Percona Server for MySQL 5.7 and 8.0</li>\n<li aria-level=\"1\">Percona XtraDB Cluster 5.7 and 8.0</li>\n<li aria-level=\"1\">Percona XtraBackup 2.4 and 8.0</li>\n</ul>\n<p>As usual, you can use the <tt>percona-release</tt> tool to enable the testing repository for these products. Please follow the <a href=\"https://www.percona.com/doc/percona-repo-config/percona-release.html\">installation instructions</a> on how to install the tool and proceed.</p>\n<p>As an example, if you’d like to install the latest version of Percona Server for MySQL 8.0 on Debian 11, perform the following steps after completing the installation of the base operating system and installing the <tt>percona-release</tt> tool:</p><pre class=\"crayon-plain-tag\">$ sudo percona-release enable ps-80 testing\n$ sudo apt update\n$ sudo apt install percona-server-server percona-server-client</pre><p><strong>Percona Distribution for MongoDB is a freely available MongoDB database alternative, giving you a single solution that combines the best and most important enterprise components from the open source community, designed and tested to work together.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mongodb\" rel=\"noopener\">Download Percona Distribution for MongoDB Today!</a></p>\n","descriptionType":"html","publishedDate":"Tue, 17 Aug 2021 14:04:37 +0000","feedId":11,"bgimg":"","linkMd5":"f4569ae0b391d617606fda9b40343180","bgimgJsdelivr":"","metaImg":"","author":"Lenz Grimmer","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-200x113.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn98@2020_6/2021/09/27/16-55-28-149_1e69f3054112119d.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn64@2020_6/2021/09/27/16-55-34-790_ccaf270419e35c80.webp"},"publishedOrCreatedDate":1632761712758},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Percona Distribution for MySQL, DBaaS in Percona Monitoring and Management: Release Roundup September 27, 2021","link":"https://www.percona.com/blog/?p=78116","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Releases Sept 27\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27.png 712w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2><img loading=\"lazy\" class=\"alignright size-medium wp-image-78282\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-300x169.png\" alt=\"Percona Releases Sept 27\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27.png 712w\" sizes=\"(max-width: 300px) 100vw, 300px\" />It&#8217;s release roundup time again here at Percona!</h2>\n<p>Percona is a leading provider of unbiased open source database solutions that allow organizations to easily, securely, and affordably maintain business agility, minimize risks, and stay competitive.</p>\n<p>Our Release Roundups <span class=\"s1\">showcase the latest Percona software updates, tools, and features to help you manage and deploy our software. It offers</span> highlights and critical information, as well as links to the full release notes and direct links to the software or service itself to download.</p>\n<p>Today&#8217;s post includes those releases and updates that have come out since September 13, 2021. Take a look!</p>\n<p>&#160;</p>\n<h2>Percona Distribution for MySQL (PXC Variant) 8.0.23</h2>\n<p><a href=\"https://www.percona.com/doc/percona-distribution-mysql/8.0/release-notes-pxc-v8.0.23.upd.html\">Percona Distribution for MySQL (PXC variant) 8.0.23</a> was released on September 15, 2021. It provides better performance and concurrency for even the most demanding workload. It delivers greater value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility.</p>\n<p>This update of the <em>Percona XtraDB Cluster</em>-based variant of the Percona Distribution for MySQL includes the HAProxy version 2.3.14, which fixes the security vulnerability <a class=\"reference external\" href=\"https://security-tracker.debian.org/tracker/CVE-2021-40346\">CVE-2021-40346</a>.</p>\n<p><a href=\"https://www.percona.com/downloads/percona-distribution-mysql-pxc/LATEST/\">Download Percona Distribution for MySQL (PXC variant) 8.0.23</a></p>\n<p>&#160;</p>\n<h2>Percona Monitoring and Management 2.22.0</h2>\n<p><a href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/release-notes/2.22.0.html\">Percona Monitoring and Management 2.22.0</a> was released on September 24, 2021. It is a best-of-breed open source database monitoring solution, helping you reduce complexity, optimize performance, and improve the security of your business-critical database environments, no matter where they are located or deployed. A Release Highlight in this version is a Technical Preview, where DBaaS users can now use the PMM UI to upgrade existing Clusters to the newer version of the operator without interacting directly with Kubernetes. In addition, there are several new features, improvements, and bug fixes, which can be read about in the release notes.</p>\n<p><a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Download Percona Monitoring and Management 2.22.0</a></p>\n<p>&#160;</p>\n<h2>Percona XtraBackup 2.4.24</h2>\n<p>On September 14, 2021, we released <a href=\"https://www.percona.com/doc/percona-xtrabackup/2.4/release-notes/2.4/2.4.24.html\">Percona XtraBackup 2.4.24</a>, a free, online, open source, complete database backups solution for all versions of Percona Server for MySQL and MySQL, complete with enterprise features for free. It provides a method of performing a hot backup of your MySQL data while the system is running. Improvements in this release are that the xbcloud binary should retry on an error and utilize incremental backoff (Thanks to Baptiste Mille-Mathias for reporting this issue), and with the xbcloud binary, a chunk-upload on SSL connect error to Amazon S3 was not retried. (Thanks to Tim Vaillancourt for providing the patch).</p>\n<p><a href=\"https://www.percona.com/downloads/Percona-XtraBackup-LATEST/\">Download Percona XtraBackup 2.4.24</a></p>\n<p>&#160;</p>\n<p>That&#8217;s it for this roundup, and be sure to <a href=\"https://twitter.com/Percona\" target=\"_blank\" rel=\"noopener\">follow us on Twitter</a> to stay up-to-date on the most recent releases! Percona is a leader in providing best-of-breed enterprise-class support, consulting, managed services, training, and software for MySQL, MongoDB, PostgreSQL, MariaDB, and other open source databases in on-premises and cloud environments.</p>\n","descriptionType":"html","publishedDate":"Mon, 27 Sep 2021 12:01:45 +0000","feedId":11,"bgimg":"","linkMd5":"9abdd2fb0799c7aa92878f47bd27a89e","bgimgJsdelivr":"","metaImg":"","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn59@2020_2/2021/09/27/16-55-31-415_3026f8645fd08826.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn32@2020_4/2021/09/27/16-55-36-689_64c54b2d08a9aeef.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"MySQL/ZFS in the Cloud, Leveraging Ephemeral Storage","link":"https://www.percona.com/blog/?p=78039","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL/ZFS in the cloud\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright wp-image-78106 size-medium\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-300x168.png\" alt=\"MySQL/ZFS in the cloud\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Here’s a second post focusing on the performance of MySQL on ZFS in cloud environments. In the first post, <a href=\"https://www.percona.com/blog/mysql-zfs-performance-update/\">MySQL/ZFS Performance Update</a>, we compared the performances of ZFS and ext4. This time we’ll look at the benefits of using ephemeral storage devices. These devices, called <em>ephemeral</em> in AWS, <em>local</em> in Google cloud, and <em>temporary</em> in Azure, are provided directly by the virtualization host. They are not network-attached and are not IO throttled, at least compared to regular storage. Not only can they handle a high number of IOPs, but their IO latency is also very low. For simplicity, we’ll name these devices <em>local ephemeral</em>. They can be quite large: Azure <em>lsv2</em>, Google Cloud <em>n2, </em>and AWS <em>i3</em> instance types offer TBs of fast NVMe local ephemeral storage.</p>\n<p>The main drawback of local ephemeral devices is the loss of all the data if the VM is terminated. For that reason, the usage of local ephemeral devices is limited with databases like MySQL. Typical use cases are temporary reporting servers and <a href=\"https://www.percona.com/software/mysql-database/percona-xtradb-cluster\">Percona XtraDB Cluster</a> (PXC)/Galera cluster nodes. PXC is a bit of a wild case here: the well polished and automated full state transfer of Galera overcomes the issue caused by having to reload the dataset when a cluster node is recycled. Because of data compression, much more data can be stored on an ephemeral device. Actually, our TPCC dataset fits on the 75GB of temporary storage when compressed. Under such circumstances, the TPCC performance is stellar as shown below.</p>\n<div id=\"attachment_78040\" style=\"width: 871px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral.png\"><img aria-describedby=\"caption-attachment-78040\" loading=\"lazy\" class=\"wp-image-78040 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral.png\" alt=\"TPCC Transation Rate ZFS\" width=\"861\" height=\"484\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral.png 861w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral-367x206.png 367w\" sizes=\"(max-width: 861px) 100vw, 861px\" /></a><p id=\"caption-attachment-78040\" class=\"wp-caption-text\">TPCC results using ZFS on an ephemeral device</p></div>\n<p>On the local ephemeral device, the TPCC transaction rate is much higher, hovering close to 200 per minute. The ZFS results on the regular SSD Premium are included as a reference. The transaction rate during the last hour was around 50 per minute. Essentially, with the use of the local ephemeral device, the load goes from IO-bound to CPU-bound.</p>\n<p>Of course, it is not always possible to only use ephemeral devices. We’ll now explore a use case for an ephemeral device, as a caching device for the filesystem, using the ZFS L2ARC.</p>\n<h2>What is the ZFS L2ARC?</h2>\n<p>Like all filesystems, ZFS has a memory cache, called the ARC, to prevent disk IOPs from retrieving frequently used pieces of data. The ZFS ARC has a few additional tricks up its sleeve. First, when data compression is used on the filesystem, the compressed form is stored in the ARC. This helps store more data. The second ZFS trick is the ability to connect the ARC LRU eviction to a fast storage device, the L2ARC. L2 stands for “Level 2”, a bit like the leveled caches of CPUs.</p>\n<p>Essentially, the ZFS ARC is a level 1 cache, and records evicted from it can be inserted into a level 2 cache, the L2ARC. For the L2ARC to be efficient, the device used must have a low latency and be able to perform a high number of IOPs. Those are characteristics of cloud ephemeral devices.</p>\n<h2>Configuration for the L2ARC</h2>\n<p>The ZFS L2ARC has many tunables and many of these have been inherited from the recent past when flash devices were much slower for writes than for reads. So, let’s start by the beginning, here is how we add a L2ARC using the local ephemeral device, <em>/dev/sdb</em> to the ZFS pool <em>bench</em>:</p><pre class=\"crayon-plain-tag\"># zpool add bench cache /dev/sdb</pre><p>Then, the cache device appears in the zpool:</p><pre class=\"crayon-plain-tag\"># zpool status\n       pool: bench\n      state: ONLINE\n     config:\n         NAME      STATE  READ WRITE CKSUM\n         bench     ONLINE        0      0      0\n           sdc     ONLINE        0      0      0\n         cache\n           sdb     ONLINE        0      0      0</pre><p>Once the L2ARC is created, if we want data in it, we must start storing data in the ARC with:</p><pre class=\"crayon-plain-tag\"># zfs set primarycache=all bench/data</pre><p>This is all that is needed to get data flowing to the L2ARC, but the default parameters controlling the L2ARC have conservative values and it can be quite slow to warm up the L2ARC. In order to improve the L2ARC performance, I modified the following kernel module parameters:</p><pre class=\"crayon-plain-tag\">l2arc_headroom=4\nl2arc_write_boost=134217728\nl2arc_write_max=67108864\nzfs_arc_max=4294967296</pre><p>Essentially, I am boosting the ingestion rate of the L2ARC. I am also slightly increasing the size of the ARC because the pointers to the L2ARC data are kept in the ARC. If you don’t use a large enough ARC, you won’t be able to add data to the L2ARC. That ceiling frustrated me a few times until I realized the entry <em>l2_hdr_size</em> in <em>/proc/spl/kstat/zfs/arcstats</em> is data stored in the metadata section of the ARC. The ARC must be large enough to accommodate the L2ARC pointers.</p>\n<h2>L2ARC Impacts on TPCC Results</h2>\n<p>So, what happens to the TPCC transaction rate when we add a L2ARC? Since we copy the dataset is copied over every time, the L2ARC is fully warm at the beginning of a run. The figure below shows the ZFS results with and without a L2ARC in front of SSD premium Azure storage.</p>\n<div id=\"attachment_78041\" style=\"width: 871px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC.png\"><img aria-describedby=\"caption-attachment-78041\" loading=\"lazy\" class=\"wp-image-78041 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC.png\" alt=\"TPCC performance on ZFS with a L2ARC\" width=\"861\" height=\"484\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC.png 861w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC-367x206.png 367w\" sizes=\"(max-width: 861px) 100vw, 861px\" /></a><p id=\"caption-attachment-78041\" class=\"wp-caption-text\">TPCC performance on ZFS with a L2ARC</p></div>\n<p>The difference is almost incredible. Since the whole compressed dataset fits into the L2ARC, the behavior is somewhat similar to the direct use of the local ephemeral device. Actually, since the write load is now sent to the SSD premium storage, the performance is even higher. However, after 4000s, the performance starts to degrade.</p>\n<p>From what I found, this is caused by the thread feeding the L2ARC (l2arc_feed). As pages are updated by the TPCC workload, they are eventually flushed at a high rate to the storage. The L2ARC feed thread has to scan the ARC LRU to find suitable records before they are evited. This thread then writes it to the local ephemeral device, and updates the pointers in the ARC. Even if the write latency of the local ephemeral device is low, it is significant and it greatly limits the amount of work a single feed thread can do. Ideally, ZFS should be able to use more than a single L2ARC feed thread.</p>\n<p>In the event you end up in such a situation with a degraded L2ARC, you can refresh it when the write load goes down. Just run the following command when activity is low:</p><pre class=\"crayon-plain-tag\"># tar c /var/lib/mysql/data &#62; /dev/null</pre><p>It is important to keep in mind that a read-intensive or a moderately write-intensive workload will not degrade as much over time as the TPCC benchmark used here. Essentially, if a replica with one of even a few (2 or 3) replication threads can keep up with the write load, the ZFS L2ARC feed thread will also be able to keep up.</p>\n<h2>Comparison with bcache</h2>\n<p>The ZFS L2ARC is not the only option to use a local ephemeral device as a read cache; there are other options like <em>bcache</em> and <em>flashcache</em>. Since bcache is now part of the Linux kernel, we’ll focus on it.</p>\n<p>bcache is used as an ext4 read cache extension. Its content is uncompressed, unlike the L2ARC. The dataset is much larger than the size of the local ephemeral device so the impacts are expected to be less important.</p>\n<div id=\"attachment_78042\" style=\"width: 766px\" class=\"wp-caption aligncenter\"><a href=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache.png\"><img aria-describedby=\"caption-attachment-78042\" loading=\"lazy\" class=\"wp-image-78042 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache.png\" alt=\"\" width=\"756\" height=\"425\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache.png 756w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache-367x206.png 367w\" sizes=\"(max-width: 756px) 100vw, 756px\" /></a><p id=\"caption-attachment-78042\" class=\"wp-caption-text\">Comparison of the TPCC transaction rate between bcache and L2ARC</p></div>\n<p>As we can see in the above figure, it is exactly what we observe. The transaction rate with bcache is inferior to L2ARC because less data is cached. The L2ARC yielded more than twice the number of transactions over the 2h period than bcache. However, bcache is not without merit, it did help ext4 increase its performance by about 43%.</p>\n<h2>How to Recreate L2ARC if Missing</h2>\n<p>By nature, local ephemeral devices are… ephemeral. When a virtual machine is restarted, it could end up on a different host. In such a case, the L2ARC data on the local ephemeral device is lost. Since it is only a read cache, it doesn’t prevent ZFS from starting but you get a pool status similar to this:</p><pre class=\"crayon-plain-tag\"># zpool status\n  pool: bench\n state: ONLINE\nstatus: One or more devices could not be opened.  Sufficient replicas exist for\n    \tthe pool to continue functioning in a degraded state.\naction: Attach the missing device and online it using 'zpool online'.\n   see: http://zfsonlinux.org/msg/ZFS-8000-2Q\n  scan: none requested\nconfig:\n\n        NAME          \tSTATE \tREAD WRITE CKSUM\n    \tbench        \tONLINE   \t0 \t0 \t0\n      \tsdc         \tONLINE   \t0 \t0 \t0\n    \tcache\n      \t/dev/sdb        UNAVAIL  \t0 \t0 \t0  cannot open</pre><p>In such case, the L2ARC can be easily be fixed with:</p><pre class=\"crayon-plain-tag\"># zpool remove bench /dev/sdb\n# zpool add bench cache /dev/sdb</pre><p>These commands should be called from a startup script to ensure the L2ARC is sane after a restart.</p>\n<h2>Conclusion</h2>\n<p>In this post, we have explored the great potential of local ephemeral devices. These devices are means to improve MySQL performance and reduce the costs of cloud hosting. Either used directly or as a caching device, ZFS data compression and architecture allow nearly triple the number of TPCC transactions executed over a 2 hours period.</p>\n<p>There are still a few ZFS related topics I’d like to cover in the near future. Those posts may not be in that order but the topics are: “Comparison with InnoDB compression”, “Comparison with BTRFS”, “ZFS tuning for MySQL”. If some of these titles raise your interest, stay tuned.</p>\n<p><strong>Percona Distribution for MySQL is the most complete, stable, scalable, and secure, open-source MySQL solution available, delivering enterprise-grade database environments for your most critical business applications&#8230; and it&#8217;s free to use!</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mysql-database\" rel=\"noopener\">Download Percona Distribution for MySQL Today</a></p>\n","descriptionType":"html","publishedDate":"Mon, 13 Sep 2021 13:51:05 +0000","feedId":11,"bgimg":"","linkMd5":"c32ff24988146f7c0f1ecda919ae7452","bgimgJsdelivr":"","metaImg":"","author":"Yves Trudeau","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn40@2020_5/2021/09/27/16-55-33-517_6a6fb7dc2ed0fd1c.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn16@2020_6/2021/09/27/16-55-35-490_756b596a283aa975.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn48@2020_1/2021/09/27/16-55-33-798_a8c6513bb592c64f.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_6/2021/09/27/16-55-31-803_18e159f92cb738eb.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn35@2020_5/2021/09/27/16-55-32-462_025e05c7f615c187.webp"},"publishedOrCreatedDate":1632761712756},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Storing and Using JSON Within PostgreSQL Part Two","link":"https://www.percona.com/blog/?p=78018","description":"<img width=\"200\" height=\"107\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-200x107.jpg\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Storing and Using JSON Within PostgreSQL 2\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-200x107.jpg 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-300x160.jpg 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-1024x546.jpg 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-367x196.jpg 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2.jpg 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>We explored the basic functionality of <a href=\"https://www.percona.com/blog/storing-and-using-json-within-postgresql-part-one/\">JSON within PostgreSQL in Part One</a> of this series. Now we will look a little deeper into some things we may want to use regularly.  Most of the functionality we talked about in part one works well when you have a simple JSON format.  However, in real life, our documents may get a bit messy.  Let’s look at part of the JSON we are using for our tests:</p>\n<p>Example JSON:</p><pre class=\"crayon-plain-tag\">{\n  \"imdb_id\": \"tt2395427\",\n  \"tmdb_id\": \"99861\",\n  \"douban_id\": \"10741834\",\n  \"title\": \"Avengers: Age of Ultron (2015)\",\n  \"genres\": [\n    \"Action\",\n    \"Adventure\",\n    \"Sci-Fi\"\n  ],\n  \"country\": \"USA\",\n  \"version\": [\n    {\n      \"runtime\": \"141 min\",\n      \"description\": \"\"\n    }\n  ],\n  \"imdb_rating\": 7.5,\n  \"director\": [\n    {\n      \"id\": \"nm0923736\",\n      \"name\": \"Joss Whedon\"\n    }\n  ],\n  \"writer\": [\n    {\n      \"id\": \"nm0923736\",\n      \"name\": \"Joss Whedon\",\n      \"description\": \"written by\"\n    },\n    {\n      \"id\": \"nm0498278\",\n      \"name\": \"Stan Lee\",\n      \"description\": \"based on the Marvel comics by and\"\n    },\n    {\n      \"id\": \"nm0456158\",\n      \"name\": \"Jack Kirby\",\n      \"description\": \"based on the Marvel comics by\"\n    }\n  ],\n  \"cast\": [\n    {\n      \"id\": \"nm0000375\",\n      \"name\": \"Robert Downey Jr.\",\n      \"character\": \"Tony Stark\"\n    },\n    {\n      \"id\": \"nm1165110\",\n      \"name\": \"Chris Hemsworth\",\n      \"character\": \"Thor\"\n    },\n    {\n      \"id\": \"nm0749263\",\n      \"name\": \"Mark Ruffalo\",\n      \"character\": \"Bruce Banner\"\n    },\n    {\n      \"id\": \"nm0262635\",\n      \"name\": \"Chris Evans\",\n      \"character\": \"Steve Rogers\"\n    },\n    {\n      \"id\": \"nm0424060\",\n      \"name\": \"Scarlett Johansson\",\n      \"character\": \"Natasha Romanoff\"\n    },\n    {\n      \"id\": \"nm0719637\",\n      \"name\": \"Jeremy Renner\",\n      \"character\": \"Clint Barton\"</pre><p>You can see here that we have some nested arrays and a bit of multi-dimensional flair.  If we wanted to get all the characters or actors in this movie, we would have a challenge using the basic functions.  Thankfully, PostgreSQL has a deep set of <a href=\"https://www.postgresql.org/docs/current/functions-json.html\">functions for interacting with JSON</a>.</p>\n<p>First, let’s look at how to get all the movies starring Robert Downey Jr. The easiest way is to use one of the  following:</p><pre class=\"crayon-plain-tag\">select jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column @&#62; '{ \"cast\": [{ \"name\" : \"Robert Downey Jr.\" }]}'\n\nselect jsonb_column-&#62;&#62;'title', jsonb_column-&#62;&#62;'imdb_rating' from movies_jsonb where jsonb_column @@ '$.cast.name == \"Robert Downey Jr.\"'</pre><p>However, what if we also need to pull out the character from the movie?  For our needs of getting a full list of actors and characters who were in this particular movie, we can use the jsonb_to_rescordset (similar to MySQL’s json_table function we covered in the <a href=\"https://www.percona.com/blog/storing-json-in-your-databases-tips-mysql/\">MySQL part of this series</a>).</p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset(jsonb_column-&#62;'cast') as t(id text,name text,character text)  where imdb_id = 'tt2395427' limit 15;\n  imdb_id  |             title              | imdb_rating |    id     |         name         |    character     \n-----------+--------------------------------+-------------+-----------+----------------------+------------------\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0000375 | Robert Downey Jr.    | Tony Stark\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm1165110 | Chris Hemsworth      | Thor\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0749263 | Mark Ruffalo         | Bruce Banner\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0262635 | Chris Evans          | Steve Rogers\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0424060 | Scarlett Johansson   | Natasha Romanoff\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0719637 | Jeremy Renner        | Clint Barton\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0000652 | James Spader         | Ultron\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0000168 | Samuel L. Jackson    | Nick Fury\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0000332 | Don Cheadle          | James Rhodes\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm1093951 | Aaron Taylor-Johnson | Pietro Maximoff\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0647634 | Elizabeth Olsen      | Wanda Maximoff\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm0079273 | Paul Bettany         | Jarvis\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm1130627 | Cobie Smulders       | Maria Hill\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm1107001 | Anthony Mackie       | Sam Wilson\n tt2395427 | Avengers: Age of Ultron (2015) |        7.50 | nm2017943 | Hayley Atwell        | Peggy Carter\n(15 rows)</pre><p>This works fine &#8211; until it doesn’t.  If I do a similar search for all movies starring Robert Downey Jr., I get:</p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset(jsonb_column-&#62;'cast') as t(id text,name text,character text)  where name like 'Robert Downey%' limit 10; \nERROR:  cannot call jsonb_to_recordset on a non-array</pre><p>When we look at the results, you can see that the function expects an array, and several of our movies have no cast (or a NULL value in the json).</p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; select imdb_id, jsonb_typeof((jsonb_column-&#62;&#62;'cast')::jsonb) from movies_json_generated limit 10;\n  imdb_id  | jsonb_typeof \n-----------+--------------\n tt7620156 | \n tt0109524 | array\n tt0534466 | array\n tt0111091 | \n tt4167726 | array\n tt0638383 | array\n tt6346314 | array\n tt5877808 | array\n tt4098782 | \n tt0365100 | array\n(10 rows)</pre><p>You can work around this “Null” issue in a couple of different ways.  The easiest is by converting to text, then back into JSON.  For example:</p><pre class=\"crayon-plain-tag\">select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset((jsonb_column-&#62;&#62;'cast'::text)::jsonb) as t(id text,name varchar(100),character text) where name like 'Robert Downey%';</pre><p>Remember in part one how I harped on types and ensuring you cast to different data types when you needed to?  This is another example.  In this case, first taking the null as text, then taking the empty string, and then converting to a JSON object with a null inside.</p>\n<p>While this is the easiest way, let’s show some other interesting ways to work around this to highlight some of the other functions, indexes, etc., we learned in part one.  Because we know that we have NULL values for some cast entries, we could check and filter out the values where the type is not empty or null.  For example, here is a simple check if the text version of our jsonb_column-&#62;’cast’ is not equal to null.</p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset(jsonb_column-&#62;'cast') as t(id text,name varchar(100),character text)  where name like 'Robert Downey%' and jsonb_column-&#62;'cast'::text != 'null' limit 5;\n  imdb_id  |                   title                   | imdb_rating |    id     |       name        |               character                \n-----------+-------------------------------------------+-------------+-----------+-------------------+----------------------------------------\n tt0266220 | The 65th Annual Academy Awards (1993)     |             | nm0000375 | Robert Downey Jr. | Himself - Presenter\n tt1515091 | Sherlock Holmes: A Game of Shadows (2011) |        7.50 | nm0000375 | Robert Downey Jr. | Sherlock Holmes\n tt1231583 | Due Date (2010)                           |        6.60 | nm0000375 | Robert Downey Jr. | Peter Highman\n tt0343663 | Eros (2004)                               |        6.00 | nm0000375 | Robert Downey Jr. | Nick Penrose (segment \"Equilibrium\")\n tt4420868 | The EE British Academy Film Awards (2015) |        7.40 | nm0000375 | Robert Downey Jr. | Himself - Tribute to Lord Attenborough\n(5 rows)\n \nmovie_json_test=&#62; explain analyze select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset(jsonb_column-&#62;'cast') as t(id text,name varchar(100),character text)  where name like 'Robert Downey%' and jsonb_column-&#62;'cast'::text != 'null' limit 5;\n                                                                QUERY PLAN                                                                \n------------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=0.01..7.30 rows=5 width=332) (actual time=0.586..84.666 rows=5 loops=1)\n   -&#62;  Nested Loop  (cost=0.01..545198.71 rows=373482 width=332) (actual time=0.585..84.664 rows=5 loops=1)\n         -&#62;  Seq Scan on movies_json_generated  (cost=0.00..74611.38 rows=373482 width=272) (actual time=0.023..30.257 rows=3786 loops=1)\n               Filter: ((jsonb_column -&#62; 'cast'::text) &#60;&#62; 'null'::jsonb)\n               Rows Removed by Filter: 258\n         -&#62;  Function Scan on jsonb_to_recordset t  (cost=0.01..1.25 rows=1 width=282) (actual time=0.014..0.014 rows=0 loops=3786)\n               Filter: ((name)::text ~~ 'Robert Downey%'::text)\n               Rows Removed by Filter: 24\n Planning Time: 0.064 ms\n Execution Time: 84.692 ms\n(10 rows)</pre><p>This is not terribly fast, but it does work. This is basically working around the functionality built into the JSON functions, however.  To speed our query up, we can, of course, index this column; however, some of the data in our movie cast list is just too large:</p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; create index gen_func_index_cast on movies_json_generated (((jsonb_column-&#62;&#62;'cast')::text));\nERROR:  index row requires 10704 bytes, maximum size is 8191</pre><p>Of course, we can work around this as well.  But sometimes, it’s best to pivot.  Let’s look at another alternative to simply searching for a NULL value.  As we learned in the first post, we can use generated columns or expression indexes to do some nice things.  Here we could create either an expression index that checks for a null or add a new column that uses a case statement to flag null casts:</p><pre class=\"crayon-plain-tag\">movie_json_test=&#62; alter table movies_json_generated add column cast_is_null boolean generated always as  ((case when ((jsonb_column-&#62;&#62;'cast')::text) is null then true else false end)) stored; \nALTER TABLE\n\n\nmovie_json_test=&#62; select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset(jsonb_column-&#62;'cast') as t(id text,name varchar(100),character text)  where name like 'Robert Downey%' and cast_is_null is false limit 5;\n  imdb_id  |                          title                          | imdb_rating |    id     |       name        |                   character                    \n-----------+---------------------------------------------------------+-------------+-----------+-------------------+------------------------------------------------\n tt3473134 | Off Camera with Sam Jones (TV Series 2014– )            |             | nm0000375 | Robert Downey Jr. | Himself                  2 episodes, 2014-2019\n tt0092851 | Dear America: Letters Home from Vietnam (TV Movie 1987) |        7.90 | nm0000375 | Robert Downey Jr. | (voice)\n tt0426841 | The 1994 Billboard Music Awards (1994)                  |             | nm0000375 | Robert Downey Jr. | Himself\n tt1228705 | Iron Man 2 (2010)                                       |        7.00 | nm0000375 | Robert Downey Jr. | Tony Stark\n tt0821642 | The Soloist (2009)                                      |        6.70 | nm0000375 | Robert Downey Jr. | Steve Lopez\n(5 rows)\n\n\nmovie_json_test=&#62; explain analyze select imdb_id, title, imdb_rating, t.* from movies_json_generated, jsonb_to_recordset(jsonb_column-&#62;'cast') as t(id text,name varchar(100),character text)  where name like 'Robert Downey%' and cast_is_null is not true limit 5;\n                                                               QUERY PLAN                                                                \n-----------------------------------------------------------------------------------------------------------------------------------------\n Limit  (cost=0.01..8.24 rows=5 width=332) (actual time=0.912..30.550 rows=5 loops=1)\n   -&#62;  Nested Loop  (cost=0.01..309227.39 rows=187680 width=332) (actual time=0.912..30.548 rows=5 loops=1)\n         -&#62;  Seq Scan on movies_json_generated  (cost=0.00..72750.59 rows=187680 width=272) (actual time=0.007..1.069 rows=1789 loops=1)\n               Filter: (cast_is_null IS NOT TRUE)\n               Rows Removed by Filter: 106\n         -&#62;  Function Scan on jsonb_to_recordset t  (cost=0.01..1.25 rows=1 width=282) (actual time=0.016..0.016 rows=0 loops=1789)\n               Filter: ((name)::text ~~ 'Robert Downey%'::text)\n               Rows Removed by Filter: 23\n Planning Time: 0.068 ms\n Execution Time: 30.572 ms\n(10 rows)</pre><p>You can see there are several options here for dealing with the nulls, some way easier (and cleaner) than others.  I want to highlight some of the challenges this brings up with using unstructured data within a structured system.</p>\n<h2>Evolution Upsets the Balance (or Breaks Stuff)</h2>\n<p>All of the above solutions work for the existing data, but the wonderful thing about JSON is that you can evolve what you store over time.  Let’s use the above example.  Let’s say that for years, every movie that is fed into your system has a full cast listing of characters and actors.  Then, one day, the feed you get your data from allows movies without a cast listing.  Your application will still work, your load scripts will still work.  But every once in a while, your users will get a weird error, or you will see small flashes in your logs (if you are logging these).  But 99.9% of queries are fine.  It is these transient issues that drive people bonkers.</p>\n<p>Looking at this problem slightly differently, what if you start adding data or changing the order of certain items?  Back to our original JSON:<span><br />\n</span></p><pre class=\"crayon-plain-tag\">{\n  \"imdb_id\": \"tt2395427\",\n  \"tmdb_id\": \"99861\",\n  \"douban_id\": \"10741834\",\n  \"title\": \"Avengers: Age of Ultron (2015)\",\n  \"genres\": [\n    \"Action\",\n    \"Adventure\",\n    \"Sci-Fi\"\n  ],\n  \"country\": \"USA\",\n  \"version\": [\n    {\n      \"runtime\": \"141 min\",\n      \"description\": \"\"\n    }\n  ],\n  \"imdb_rating\": 7.5,\n  \"director\": [\n    {\n      \"id\": \"nm0923736\",\n      \"name\": \"Joss Whedon\"\n    }\n  ],\n  \"writer\": [\n    {\n      \"id\": \"nm0923736\",\n      \"name\": \"Joss Whedon\",\n      \"description\": \"written by\"\n    },\n    {\n      \"id\": \"nm0498278\",\n      \"name\": \"Stan Lee\",\n      \"description\": \"based on the Marvel comics by and\"\n    },\n    {\n      \"id\": \"nm0456158\",\n      \"name\": \"Jack Kirby\",\n      \"description\": \"based on the Marvel comics by\"\n    }\n  ],\n  \"cast\": [\n    {\n      \"id\": \"nm0000375\",\n      \"name\": \"Robert Downey Jr.\",\n      \"character\": \"Tony Stark\"\n    },\n    {\n      \"id\": \"nm1165110\",\n      \"name\": \"Chris Hemsworth\",\n      \"character\": \"Thor\"\n    },\n    {\n      \"id\": \"nm0749263\",\n      \"name\": \"Mark Ruffalo\",\n      \"character\": \"Bruce Banner\"\n    },\n    {\n      \"id\": \"nm0262635\",\n      \"name\": \"Chris Evans\",\n      \"character\": \"Steve Rogers\"\n    },\n    {\n      \"id\": \"nm0424060\",\n      \"name\": \"Scarlett Johansson\",\n      \"character\": \"Natasha Romanoff\"\n    },\n    {\n      \"id\": \"nm0719637\",\n      \"name\": \"Jeremy Renner\",\n      \"character\": \"Clint Barton\"</pre><p>What if we wanted to add another piece of data to the cast records? Let’s say we want to add a thumbs up, or thumbs down vote to each character, so something like this:</p><pre class=\"crayon-plain-tag\">{\n      \"id\": \"nm0749263\",\n      \"name\": \"Mark Ruffalo\",\n      “actcor_thumbs_up”: 10000,\n      “actor_thumbs_down”: 100,\n      \"character\": \"Bruce Banner\"\n    },</pre><p>Now we run into another challenge.  If we inject the new fields into the middle of the JSON, instead of the end, our call to <em><strong>jsonb_to_recordset(jsonb_column-&#62;&#8217;cast&#8217;) as t(id text,name varchar(100),character text</strong></em>) will cause use issues in the application.  We had to define the definition of the recordest; the first field returned is id, then actor name, and then character.  If we changed this record, the character returned in the SQL would be 10000, not Bruce Banner.  Now to make additions, you would need to start adding at the end like:</p><pre class=\"crayon-plain-tag\">{\n      \"id\": \"nm0749263\",\n      \"name\": \"Mark Ruffalo\",\n      \"character\": \"Bruce Banner\",\n      “actcor_thumbs_up”: 10000,\n      “actor_thumbs_down”: 100\n    },</pre><p>MySQL mitigates this with allowing you to defined specific paths in their equivalent function <em><strong>json_table(json_column, &#8216;$.cast[*]&#8217; columns( V_name varchar(200) path &#8216;$.name&#8217;, V_character varchar(200) path &#8216;$.character&#8217;)</strong></em> This allows you to define exactly which fields you want, not just the first X ones. That said, there are other limitations in the json_table method MYSQL uses.  Here this is a trade-off.  You need to be aware again that the order and structure of your JSON matter greatly if you are going to use built-in database functions to query this data.</p>\n<h2>To JSON or Not?  Database Design</h2>\n<p>While PostgreSQL’s JSON features are pretty in-depth (there are dozens of other functions I did not show), there are similar challenges I see when using these compared to a more classic database design. JSON functionality is NOT a replacement for actually designing your database and thinking about schema and design.  It is a great tool to offer additional flexibility.  If you treat PostgreSQL or any database simply as a dumping ground for data, bad things happen.  It would help to think about how JSON columns can augment your current database designs, not replace them.</p>\n<p>Let me give you a simple example of some of the dangers of simply forgoing database design in favor of just dumping in JSON.  While I think we have highlighted some of the complexity and setup challenges with different methods of storing JSON in PostgreSQL, I wanted to take this a step further and compare the performance of different database designs and their impact on performance and scalability.  For this, I built three separate designs and schemas for housing the same data:</p>\n<p>A.)  Minimal Database Design -&#62; single table, single column.  “The dump.”  With only a GIN index.</p>\n<p>B.)  Single Table -&#62; Generated columns and Expression indexes where needed.</p>\n<p>C.)  Old School Normalized Tables, with JSON stored for easy use</p>\n<p>The idea here is to illustrate a few of the trade-offs around performance when picking a specific design(1):</p>\n<table>\n<tbody>\n<tr>\n<td></td>\n<td><span>A.)  Minimal Database Design No Gin IDX</span></td>\n<td><span>A.)  Minimal Database Design with Gin IDX</span></td>\n<td><span>B.)  Single Table W Generated Column</span></td>\n<td><span>B.)  Single Table W Expression Indexes</span></td>\n<td><span>c.) Normalized</span></td>\n</tr>\n<tr>\n<td><span>Simple Query for Movie Title (random 10  titles)</span></td>\n<td><span>800ms</span></td>\n<td><span>0.3ms</span></td>\n<td><span>0.2ms</span></td>\n<td><span>0.2ms</span></td>\n<td><span>0.2ms</span></td>\n</tr>\n<tr>\n<td><span>Select top 10 movies</span></td>\n<td><span>841ms</span></td>\n<td><span>831ms</span></td>\n<td><span>0.9ms</span></td>\n<td><span>0.9ms</span></td>\n<td><span>0.3ms</span></td>\n</tr>\n<tr>\n<td><span>Select all movies for an actor </span></td>\n<td><span>1009.0ms</span></td>\n<td><span>228ms</span></td>\n<td><span>5588.0ms</span><span>(2)</span></td>\n<td><span>5588.0ms</span><span>(2)</span></td>\n<td><span>0.8ms</span></td>\n</tr>\n<tr>\n<td><span>Count movies for a given actor</span></td>\n<td><span>5071ms</span></td>\n<td><span>5075ms</span></td>\n<td><span>5544ms</span></td>\n<td><span>NA</span></td>\n<td><span>1ms</span></td>\n</tr>\n</tbody>\n</table>\n<ol>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><em><span>Each test was repeated 100 times, and the average results were listed.  Min/Max is available as well. </span></em></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><em><span>The nested json for our “cast info” was not easily indexed, there are some things we could do to mitigate this, but it is a bit more complicated.</span></em></li>\n</ol>\n<p>Interestingly, there are a few cases where we have problems with using only generated columns/expression indexes, generally because of nested data within the JSON.  We could do some tricks, like adding a GIN index and query the JSON to find the element or creating some funky nested generated column with an expression index.  I left these out for now.  Also, I could go deeper into the tests, testing inserts, updates, and, of course, assembling JSON from a normalized table, but I didn’t.  My goal here is merely to make you think about your design.  Choosing one way to implement and use JSON may be ok under certain use cases, but you may find yourself querying or aggregating the data, and things break.</p>\n<h2>Recap &#38; What’s Next</h2>\n<p>A few quick takeaways:</p>\n<p><strong>Recapping part 1:</strong></p>\n<ul>\n<li aria-level=\"1\">Using JSONB is probably going to be your best option in most use cases.</li>\n<li aria-level=\"1\">Be very careful of type conversions and making assumptions on the data within your JSON/JSONB columns.  You may get errors or odd results.</li>\n<li aria-level=\"1\">Use the available indexes, generated columns, and expression indexes to gain substantial performance benefits.</li>\n</ul>\n<p><strong>What we learned in part 2:</strong></p>\n<ul>\n<li aria-level=\"1\">The more straightforward the JSON, the easier it will be to pull out and interact with the data you need.</li>\n<li aria-level=\"1\">Nested JSON data can be pulled out in a few different ways. jsonb_to_rescordset is the easiest way I found to pull out the data I need.  However, using this function or others is very susceptible to the data structure in your document (data types matter!).</li>\n<li aria-level=\"1\">JSON data whose format changes ( elements added or removed ) may make using some functions difficult, if not impossible, to use.</li>\n<li aria-level=\"1\">JSON within a well-built, designed application and database can be wonderful and offer many great benefits.  JSON just dumped into databases won’t scale.  Database design still matters.</li>\n</ul>\n<p>Now that we have covered the basics and a few nuances of JSON with PostgreSQL, next up, it is time to look at the same functionality and data within MongoDB before finally comparing the three databases. Stay tuned!</p>\n","descriptionType":"html","publishedDate":"Wed, 08 Sep 2021 12:21:32 +0000","feedId":11,"bgimg":"","linkMd5":"f8e87d9dba4c97a4151bbaf355a6d80d","bgimgJsdelivr":"","metaImg":"","author":"Matt Yonkovit","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-200x107.jpg":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn4@2020_4/2021/09/27/16-55-31-294_ef8029c51317ad60.webp"},"publishedOrCreatedDate":1632761712756},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Percona Server for MongoDB 5.0.2-1 Release Candidate, Updated Percona Backup for MongoDB: Release Roundup August 30, 2021","link":"https://www.percona.com/blog/?p=77766","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Software Release Aug 30 2021\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021.png 712w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2><img loading=\"lazy\" class=\"alignright size-medium wp-image-77929\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-300x169.png\" alt=\"Percona Software Release Aug 30 2021\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021.png 712w\" sizes=\"(max-width: 300px) 100vw, 300px\" />It&#8217;s release roundup time again here at Percona!</h2>\n<p>Percona is a leading provider of unbiased open source database solutions that allow organizations to easily, securely, and affordably maintain business agility, minimize risks, and stay competitive.</p>\n<p>Our Release Roundups <span class=\"s1\">showcase the latest Percona software updates, tools, and features to help you manage and deploy our software. It offers</span> highlights and critical information, as well as links to the full release notes and direct links to the software or service itself to download.</p>\n<p>Today&#8217;s post includes those releases and updates that have come out since August 16, 2021. Take a look!</p>\n<p>&#160;</p>\n<h2>Percona Distribution for MongoDB 4.4.8</h2>\n<p><a href=\"https://www.percona.com/doc/percona-distribution-for-mongodb/4.4/release-notes-v4.4.8.html\">Percona Distribution for MongoDB 4.4.8</a> was released on August 16, 2021. It is a freely available MongoDB database alternative, giving you a single solution that combines enterprise components from the open source community, designed and tested to work together. Percona Distribution for MongoDB includes Percona Server for MongoDB and Percona Backup for MongoDB, enabling you to run and operate your MongoDB efficiently with the data being consistently backed up.</p>\n<p><a href=\"https://www.percona.com/software/mongodb\">Download Percona Distribution for MongoDB 4.4.8</a></p>\n<p>&#160;</p>\n<h2>Percona Server for MongoDB 5.0.2-1 (RC)</h2>\n<p>We are pleased to announce the release candidate of <a href=\"https://www.percona.com/doc/percona-server-for-mongodb/5.0/release_notes/5.0.2-1.html\">Percona Server for MongoDB 5.0.2-1 (RC)</a> on August 16, 2021. It is an enhanced, source-available, and highly scalable database that is a fully compatible, drop-in replacement for MongoDB 5.0.2 Community Edition and includes all features of MongoDB 5.0.2 Community Edition, fully supporting MongoDB 5.0.2 Community Edition protocols and drivers, requiring no changes to MongoDB applications or code.</p>\n<p class=\"first admonition-title\"><em><strong>Note</strong>: With a lot of new features and modifications introduced, we recommend using this release candidate in testing environments only</em></p>\n<p><a href=\"https://www.percona.com/downloads/percona-server-mongodb-5.0/#\">Download Percona Server for MongoDB 5.0.2-1 (RC)</a></p>\n<p>&#160;</p>\n<h2>Percona Server for MongoDB 4.4.8-9</h2>\n<p>On August 16, 2021, <a href=\"https://www.percona.com/doc/percona-server-for-mongodb/4.4/release_notes/4.4.8-9.html\">Percona Server for MongoDB 4.4.8-9</a> was released. It&#8217;s a fully compatible, drop-in replacement for MongoDB 4.4.8 Community Edition, supporting MongoDB 4.4.8 protocols and drivers. Along with some bug fixes, there are a few improvements in this release, including the ability to view the status of <span class=\"std std-ref\">hot backup</span> using the <code class=\"docutils literal\"><span class=\"pre\">mongo</span></code> shell and the removal of excessive log messages and improved error messages for various backup cases.</p>\n<p><a href=\"https://www.percona.com/software/mongodb/percona-server-for-mongodb\">Download Percona Server for MongoDB 4.4.8-9</a></p>\n<p>&#160;</p>\n<h2>Percona Backup for MongoDB 1.6.0</h2>\n<p>On August 16, 2021, we released <a href=\"https://www.percona.com/doc/percona-backup-mongodb/release-notes/1.6.0.html\">Percona Backup for MongoDB 1.6.0</a>, a distributed, low-impact solution for consistent backups of MongoDB sharded clusters and replica sets. This is a tool for creating consistent backups across a MongoDB sharded cluster (or a single replica set), and for restoring those backups to a specific point in time.</p>\n<p>Release highlights include:</p>\n<ul class=\"simple\">\n<li>Support for Percona Server for MongoDB and MongoDB Community 5.0</li>\n<li>Point-in-time recovery enhancements: ability to restore from any previous snapshot and configurable span of oplog events</li>\n<li>JSON output for PBM commands to simplify interfacing PBM with applications</li>\n</ul>\n<p><a href=\"https://www.percona.com/software/mongodb/percona-backup-for-mongodb\">Download Percona Backup for MongoDB 1.6.0</a></p>\n<p>&#160;</p>\n<h2>Percona Server for MySQL 5.7.35-38</h2>\n<p>August 18, 2021, saw the release of <a href=\"https://www.percona.com/doc/percona-server/5.7/release-notes/Percona-Server-5.7.35-38.html\">Percona Server for MySQL 5.7.35-38</a>, a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database.  It includes all the features and bug fixes available in MySQL 5.7.35 Community Edition in addition to <a class=\"reference external\" href=\"https://www.percona.com/software/mysql-database/percona-server/feature-comparison\">enterprise-grade features</a> developed by Percona.</p>\n<p><a href=\"https://www.percona.com/software/mysql-database/percona-server\">Download Percona Server for MySQL 5.7.35-38</a></p>\n<p>&#160;</p>\n<p>That&#8217;s it for this roundup, and be sure to <a href=\"https://twitter.com/Percona\" target=\"_blank\" rel=\"noopener\">follow us on Twitter</a> to stay up-to-date on the most recent releases! Percona is a leader in providing best-of-breed enterprise-class support, consulting, managed services, training, and software for MySQL, MongoDB, PostgreSQL, MariaDB, and other open source databases in on-premises and cloud environments.</p>\n","descriptionType":"html","publishedDate":"Mon, 30 Aug 2021 11:34:22 +0000","feedId":11,"bgimg":"","linkMd5":"7f8976aebdbb00e3a6971678bc614718","bgimgJsdelivr":"","metaImg":"","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn15@2020_4/2021/09/27/16-55-30-619_c8c365637062cb63.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn56@2020_1/2021/09/27/16-55-26-063_67d4b866ef648afe.webp"},"publishedOrCreatedDate":1632761712729},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Installing MongoDB With Docker","link":"https://www.percona.com/blog/?p=67094","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"installing MongoDB with Docker\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77951\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-300x168.png\" alt=\"installing MongoDB with Docker\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Following the series of blogs written with the intention to describe basic operations matching Docker and open source databases, in this article, I will demonstrate how to proceed with installing MongoDB with Docker.</p>\n<p>The first one, written by Peter Zaitsev, was <a href=\"https://www.percona.com/blog/2019/11/19/installing-mysql-with-docker/\">Installing MySQL with Docker</a>.</p>\n<p>Before proceeding, it is important to double warn by quoting Peter&#8217;s article:</p>\n<p><strong>&#8220;The following instructions are designed to get a test instance running quickly and easily; you do not want to use these for production deployments&#8221;. </strong></p>\n<p>Also, a full description of Docker is not the objective of this article, though I assume prior knowledge of Docker, and also you that already have it installed and configured to move on.</p>\n<p>Docker quickly deploys standalone MongoDB containers. If you are looking for fast deployments of both replica sets and shards, I suggest looking at the <a href=\"http://blog.rueckstiess.com/mtools/mlaunch.html\">mlaunch</a> tool.</p>\n<p>Peter mentioned, in his article, how MySQL has two different &#8220;official&#8221; repositories, and with MongoDB, it&#8217;s the same: MongoDB has one repository maintained by <a href=\"https://www.mongodb.com/products/mongodb-enterprise-advanced?jmp=docs\">MongoDB</a> and another maintained by <a href=\"https://hub.docker.com/_/mongo\">Docker</a>. I wrote this article based on the Docker-maintained repository.</p>\n<h2>Installing the Latest Version of MongoDB</h2>\n<p><span>The following snippet is one example of how to initiate a container of the latest MongoDB version from the Docker repository. </span></p><pre class=\"crayon-plain-tag\">docker run --name mongodb_dockerhub \\\n                -e MONGO_INITDB_ROOT_USERNAME=admin \n                -e MONGO_INITDB_ROOT_PASSWORD=secret \\\n                -d mongo:latest</pre><p>Now, if you want to check the container status right after creating it:</p><pre class=\"crayon-plain-tag\">docker ps\n\nCONTAINER ID        IMAGE COMMAND                  CREATED STATUS PORTS               NAMES\nfdef23c0f32a        mongo:latest \"docker-entrypoint...\"   4 seconds ago Up 4 seconds 27017/tcp           mongodb_dockerhub</pre><p></p>\n<h2>Connecting to MongoDB Server Docker Container</h2>\n<p><span>Having the container installed up and running, you will notice that no extra step or dependency installation was previously required, apart from the docker binaries. Now, it is time to access the container MongoDB shell and issue a basic command like &#8220;show dbs&#8221;. </span></p><pre class=\"crayon-plain-tag\">docker exec -it mongodb_dockerhub mongo -u admin -p secret\nMongoDB shell version v4.2.5\nconnecting to: mongodb://127.0.0.1:27017/?compressors=disabled&#38;gssapiServiceName=mongodb\nImplicit session: session { \"id\" : UUID(\"89c3fb4a-a8ed-4724-8363-09d65024e458\") }\nMongoDB server version: 4.2.5\nWelcome to the MongoDB shell.\n\n&#62; show dbs\nadmin   0.000GB\nconfig  0.000GB\nlocal   0.000GB\n\n&#62; exit\nbye</pre><p>It is also possible to connect to the containerized MongoDB using a host mongo shell. On the docker ps output, the container id has a field that informs its port mapping, and then it is a simple connection using that port.</p>\n<p>In the below example, we connected to the mongodb_dockerhub36. It&#8217;s up and running locally on port 27017  but mapped to the host 27018 port:</p><pre class=\"crayon-plain-tag\">docker ps\n\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                      NAMES\n60ffc759fab9        mongo:3.6           \"docker-entrypoint...\"   20 seconds ago      Up 19 seconds       0.0.0.0:27018-&#62;27017/tcp   mongodb_dockerhub36</pre><p>Hence, the mongo shell connection string will be executed against the external IP and port 27018</p><pre class=\"crayon-plain-tag\">mongo admin -u admin --host 172.16.0.10 --port 27018 -psecret</pre><p></p>\n<h2>Managing MongoDB Server in Docker Container</h2>\n<p><span>The following commands will demonstrate basic management operations when managing a MongoDB container. </span></p>\n<ul>\n<li><b>Starting the container and checking the Status</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker start mongodb_dockerhub\n\ndocker ps\n\nCONTAINER ID        IMAGE COMMAND                  CREATED STATUS PORTS               NAMES\nfdef23c0f32a        mongo:latest \"docker-entrypoint...\"   47 minutes ago Up 2 seconds 27017/tcp           mongodb_dockerhub</pre><p></p>\n<ul>\n<li><b>Stopping the container and checking the Status</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker stop mongodb_dockerhub\n\ndocker ps\n\nCONTAINER ID        IMAGE COMMAND</pre><p></p>\n<ul>\n<li><b>Taking a look at the MongoDB log entries</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker logs mongodb_dockerhub\n...\nabout to fork child process, waiting until server is ready for connections.\nforked process: 25\n2020-04-10T14:04:49.808+0000 I  CONTROL [main] ***** SERVER RESTARTED *****\n2020-04-10T14:04:49.812+0000 I  CONTROL [main] Automatically disabling TLS 1.0, to f\n...</pre><p></p>\n<h2>Passing Command Line Options to MongoDB Server in Docker Container</h2>\n<p><span>It is also possible to define the instance&#8217;s parameters when launching the MongoDB container. In the example below, I will show how to set WiredTiger cache.</span></p><pre class=\"crayon-plain-tag\">docker run --name mongodb_dockerhub \\\n                -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=secret \\\n                -d mongo:latest --wiredTigerCacheSizeGB 1.0</pre><p></p>\n<h2>Running Different MongoDB Server Versions in Docker</h2>\n<p><span>Another possibility is getting two MongoDB containers running in parallel but under different versions. The below snippet will describe how to build that scenario</span></p>\n<ul>\n<li><b>Launching a 4.0 container</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker run --name mongodb_dockerhub40 \\\n -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=secret \\\n -d mongo:4.0</pre><p></p>\n<ul>\n<li><b>Launching a 3.6 container</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker run --name mongodb_dockerhub36 \\\n -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=secret \\\n -d mongo:3.6</pre><p></p>\n<ul>\n<li><b>Checking the container&#8217;s status</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker ps\n\nCONTAINER ID        IMAGE COMMAND                  CREATED STATUS PORTS               NAMES\ne9f480497f2d        mongo:3.6 \"docker-entrypoint...\"   32 seconds ago Up 32 seconds 27017/tcp           mongodb_dockerhub36\n3a799f8a907c        mongo:4.0 \"docker-entrypoint...\"   41 seconds ago Up 41 seconds 27017/tcp           mongodb_dockerhub40</pre><p><span>If for some reason you need both containers running simultaneously and access them externally, then use different port mappings. In the below example, both MongoDB&#8217;s containers are deployed on the local 27017 port, nevertheless, I am setting different external port maps for each one.</span></p>\n<ul>\n<li><b>MongoDB 4.0 mapping the port 27017</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker run --name mongodb_dockerhub40 \\\n -p 27017:27017 \\\n -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=secret \\\n -d mongo:4.0</pre><p></p>\n<ul>\n<li><b>MongoDB 3.6 mapping the port 27018</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker run --name mongodb_dockerhub36 \\\n -p 27018:27017 \\\n -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=secret \\\n -d mongo:3.6</pre><p></p>\n<ul>\n<li><b>Checking both Container&#8217;s status</b></li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">docker ps\n\nCONTAINER ID        IMAGE COMMAND                  CREATED STATUS PORTS                      NAMES\n78a79e5606ae        mongo:4.0 \"docker-entrypoint...\"   3 seconds ago Up 2 seconds 0.0.0.0:27017-&#62;27017/tcp   mongodb_dockerhub40\n60ffc759fab9        mongo:3.6 \"docker-entrypoint...\"   20 seconds ago Up 19 seconds 0.0.0.0:27018-&#62;27017/tcp   mongodb_dockerhub36</pre><p>There are a lot of extra details on <a href=\"https://hub.docker.com/_/mongo\">Docker&#8217;s MongoDB Hub page</a> and you will find more options to use on your MongoDB Docker deployment. Enjoy it!</p>\n<p><strong>Percona Distribution for MongoDB is a freely available MongoDB database alternative, giving you a single solution that combines the best and most important enterprise components from the open source community, designed and tested to work together.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mongodb\" rel=\"noopener\">Download Percona Distribution for MongoDB Today!</a></p>\n","descriptionType":"html","publishedDate":"Tue, 31 Aug 2021 12:13:55 +0000","feedId":11,"bgimg":"","linkMd5":"409d87d45678dd7a1a89e474d21b7f32","bgimgJsdelivr":"","metaImg":"","author":"Rafael Galinari","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn40@2020_6/2021/09/27/16-55-35-612_069e324abaec8cf7.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn16@2020_2/2021/09/27/16-55-25-926_d603892886c805ef.webp"},"publishedOrCreatedDate":1632761712743},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"PostgreSQL Database Security: OS – Authentication","link":"https://www.percona.com/blog/?p=77706","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"PostgreSQL Database Security OS - Authentication\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span>Security is everybody&#8217;s concern when talking about data and information, and therefore it becomes the main foundation of every database. Security means protecting your data from unauthorized access. That means only authorized users can log in to a system called authentication; a user can only do what they are authorized to do (authorization) and log the user activity (accounting). I have explained these in my main security post, </span><a href=\"https://www.percona.com/blog/2021/01/04/postgresql-database-security-what-you-need-to-know/\"><span>PostgreSQL Database Security: What You Need To Know</span></a><span>.</span></p>\n<p><span>When we are talking about security, authentication is the first line of defense. PostgreSQL provides various methods of authentication, which are categorized into three categories.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://www.percona.com/blog/2021/02/01/postgresql-database-security-authentication/\"><span>PostgreSQL Internal Authentication </span></a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>OS-based Authentication</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>External Server-Based Authentication </span></li>\n</ul>\n<p><span>In most cases, PostgreSQL is configured to be used with internal authentication. Therefore I have discussed all internal authentication in the previous</span><a href=\"https://www.percona.com/blog/2021/02/01/postgresql-database-security-authentication/\"><span> blog post</span></a><span> I mentioned above. In this blog, we will discuss the operating system-based authentication methods for PostgreSQL. There are three methods to do OS-Based authentication.</span></p>\n<ul>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://www.postgresql.org/docs/current/auth-ident.html\"><span>Ident</span></a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://www.postgresql.org/docs/current/auth-pam.html\"><span>PAM (</span><span>Pluggable Authentication Modules</span><span>)</span></a></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><a href=\"https://www.postgresql.org/docs/current/auth-ident.html\"><span>Peer</span></a></li>\n</ul>\n<h2>Ident</h2>\n<p><span>Ident authentication only supports TCP/IP connections. Its ident server provides a mechanism to map the client’s operating system username onto the database username. It also has the option for username mapping.</span></p><pre class=\"crayon-plain-tag\"># TYPE  DATABASE        USER            ADDRESS                 METHOD\n# \"local\" is for Unix domain socket connections only\nlocal   all             all                                     trust\n# IPv4 local connections:\nhost    all             all             127.0.0.1/32            ident\n# IPv6 local connections:\nhost    all             all             ::1/128                 trust\n# Allow replication connections from localhost, by a user with the</pre><p></p><pre class=\"crayon-plain-tag\">$ psql postgres -h 127.0.0.1 -U postgres\npsql: error: connection to server at \"127.0.0.1\", port 5432 failed: FATAL:  Ident authentication failed for user \"postgres\"</pre><p>If no ident server is installed, you will need to install the <b><i>ident2</i></b> on your ubuntu box or <b><i>oidentd</i></b> on CentOS 7. Once you have downloaded and configured the ident server, it is now time to configure PostgreSQL. It starts with creating a user map in “pg_ident.conf” file.</p><pre class=\"crayon-plain-tag\"># Put your actual configuration here\n# ----------------------------------\n# MAPNAME       SYSTEM-USERNAME         PG-USERNAME\nPG_USER         vagrant                 postgres</pre><p><span>Here we have mapped our system user “vagrant” user with PostgreSQL’s “postgres.” Time to login using the user vagrant.</span></p><pre class=\"crayon-plain-tag\">$ psql postgres -h 127.0.0.1 -U postgres\npsql (15devel)\nType \"help\" for help.\n\npostgres=#</pre><p><a href=\"https://www.postgresql.org/docs/current/auth-ident.html\"><i>Note: The Identification Protocol is not intended as an authorization or access control protocol.</i></a></p>\n<h2>PAM (Pluggable Authentication Modules)</h2>\n<p>PAM (Pluggable Authentication Modules) authentication works similarly to “passwords.” You’d have to create a PAM service file that should enable PAM-based authentication. The service name should be set to “PostgreSQL.”</p>\n<p>Once the service is created, <a href=\"https://www.kernel.org/pub/linux/libs/pam/\">PAM</a> can now validate user name/password pairs and optionally the connected remote hostname or IP address. The user must already exist in the database for PAM authentication to work.</p><pre class=\"crayon-plain-tag\">$ psql postgres -h 127.0.0.1 -U postgres\nPassword for user postgres: \n2021-08-11 13:16:38.332 UTC [13828] LOG:  pam_authenticate failed: Authentication failure\n2021-08-11 13:16:38.332 UTC [13828] FATAL:  PAM authentication failed for user \"postgres\"\n2021-08-11 13:16:38.332 UTC [13828] DETAIL:  Connection matched pg_hba.conf line 91: \"host    all             all             127.0.0.1/32            pam\"\npsql: error: connection to server at \"127.0.0.1\", port 5432 failed: FATAL:  PAM authentication failed for user \"postgres\"</pre><p><span>Ensure that the PostgreSQL server supports PAM authentication. It is a compile-time option that must be set when the server binaries were built. You can check if your PostgreSQL server supports PAM authentication using the following command.<br />\n</span></p><pre class=\"crayon-plain-tag\">$ pg_config | grep with-pam\n\nCONFIGURE =  '--enable-tap-tests' '--enable-cassert' '--prefix=/usr/local/pgsql/' '--with-pam'</pre><p><span>In case there is no PAM server file for PostgreSQL under /etc/pam.d, you’d have to create it manually. You may choose any name for the file; however, I prefer to name it “postgresql.”<br />\n</span></p><pre class=\"crayon-plain-tag\">$ /etc/pam.d/PostgreSQL\n\n@include common-auth\n@include common-account\n@include common-session\n@include common-password</pre><p><span>Since the PostgreSQL user cannot read the password files, install sssd (</span><a href=\"https://sssd.io/\"><span>SSSD &#8211; System Security Services Daemon</span></a><span>) to bypass this limitation.</span></p><pre class=\"crayon-plain-tag\">sudo apt-get install sssd</pre><p><span>Add postgresql to the “ad_gpo_map_remote_interactive” to the “/etc/sssd/sssd.conf”</span></p><pre class=\"crayon-plain-tag\">$ cat /etc/sssd/sssd.conf\nad_gpo_map_remote_interactive = +postgresql</pre><p>Start sssd service, and check the status that it has properly started.</p><pre class=\"crayon-plain-tag\">$ sudo systemctl start sssd\n\n$ sudo systemctl status sssd\n\nsssd.service - System Security Services Daemon\n\n     Loaded: loaded (/lib/systemd/system/sssd.service; enabled; vendor preset: enabled)\n\n     Active: active (running) since Wed 2021-08-11 16:18:41 UTC; 12min ago\n\n   Main PID: 1393 (sssd)\n\n      Tasks: 2 (limit: 1071)\n\n     Memory: 5.7M\n\n     CGroup: /system.slice/sssd.service\n\n             ├─1393 /usr/sbin/sssd -i --logger=files\n\n             └─1394 /usr/libexec/sssd/sssd_be --domain shadowutils --uid 0 --gid 0 --logger=files</pre><p><span>Time now to configure pg_hba.conf to use the PAM authentication. We need to specify the PAM service name (pamservice) as part of authentication options. This should be the same as the file you have created in the /etc/pam.d folder, which in my case is postgresql.</span></p><pre class=\"crayon-plain-tag\"># \"local\" is for Unix domain socket connections only\n\nlocal   all             all                                     trust\n\n# IPv4 local connections:\n\nhost    all             all             127.0.0.1/32            pam pamservice=postgresql\n\n# IPv6 local connections:\n\nhost    all             all             ::1/128                 trust\n\n# Allow replication connections from localhost, by a user with the</pre><p><span>We must now reload (or restart) the PostgreSQL server. After this, you can try to login into the PostgreSQL server.</span></p><pre class=\"crayon-plain-tag\">vagrant@ubuntu-focal:~$ psql postgres -h 127.0.0.1 -U postgres\npsql (15devel)\nType \"help\" for help.\npostgres=&#62;</pre><p></p>\n<h3>Note</h3>\n<p><a href=\"https://www.postgresql.org/docs/current/auth-pam.html\"><i><span>If PAM is set up to read </span></i><i><span>/etc/shadow</span></i><i><span>, authentication will fail because the PostgreSQL server is started by a non-root user. However, this is not an issue when PAM is configured to use LDAP or other authentication methods.</span></i></a></p>\n<h2>Peer</h2>\n<p><span>Peer authentication is “</span><b><i>ident</i></b><span>”ical; i.e., Very much like the ident authentication! The only subtle differences are there are no ident servers, and this method works on local connections rather than over TCP/IP.</span></p>\n<p><span>The peer authentication provides a mechanism to map the client’s operating system username onto the database username. It also has the option for username mapping.</span><span>  The configuration is very similar to how we configured for ident authentication except that the authentication method is specified as “peer” instead of “ident.”</span></p>\n<p><span>$ cat $PGDATA/pg_hba.conf</span></p><pre class=\"crayon-plain-tag\"># TYPE  DATABASE        USER            ADDRESS                 METHOD\n\n# \"local\" is for Unix domain socket connections only\nlocal   all             all                                   peer map=PG_USER\n# IPv4 local connections:\nhost    all             all             127.0.0.1/32            trust</pre><p></p><pre class=\"crayon-plain-tag\">$ psql postgres -U postgres\n\n2021-08-12 10:51:11.855 UTC [1976] LOG:  no match in usermap \"PG_USER\" for user \"postgres\" authenticated as \"vagrant\"\n\n2021-08-12 10:51:11.855 UTC [1976] FATAL:  Peer authentication failed for user \"postgres\"\n\n2021-08-12 10:51:11.855 UTC [1976] DETAIL:  Connection matched pg_hba.conf line 89: \"local   all             all                                     peer map=PG_USER\"\n\npsql: error: connection to server on socket \"/tmp/.s.PGSQL.5432\" failed: FATAL:  Peer authentication failed for user \"postgres\"</pre><p><span>$PGDATA/pg_hba.conf configuration will look something like this:</span></p><pre class=\"crayon-plain-tag\">$ cat $PGDATA/pg_hba.conf\n# TYPE  DATABASE        USER            ADDRESS                 METHOD\n# \"local\" is for Unix domain socket connections only\n\nlocal   all             all                                     peer map=PG_USER\n# IPv4 local connections:</pre><p>$PGDATA/pg_ident.conf</p><pre class=\"crayon-plain-tag\"># Put your actual configuration here\n# ----------------------------------\n# MAPNAME       SYSTEM-USERNAME         PG-USERNAME\nPG_USER         vagrant                postgres</pre><p></p><pre class=\"crayon-plain-tag\">vagrant@ubuntu-focal:~$ psql postgres -U postgres\npsql (15devel)\nType \"help\" for help.\npostgres=&#62;</pre><p></p>\n<h2>Conclusion</h2>\n<p><span>We’ve covered several different authentication methods in this blog. These basic authentication methods involve the PostgreSQL server, kernel, and the ident server; options are available natively without any major external dependencies. It is, however, important that the database is secured properly to prevent unauthorized access to the data.</span></p>\n<p><strong>Percona Distribution for PostgreSQL provides the best and most critical enterprise components from the open-source community in a single distribution, designed and tested to work together.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/postgresql-distribution\" rel=\"noopener\">Download Percona Distribution for PostgreSQL Today!</a></p>\n","descriptionType":"html","publishedDate":"Mon, 23 Aug 2021 13:28:41 +0000","feedId":11,"bgimg":"","linkMd5":"090b64fde228ee0411a7015959a7d80b","bgimgJsdelivr":"","metaImg":"","author":"Ibrar Ahmed","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn7@2020_2/2021/09/27/16-55-13-572_579ed19e13870372.webp"},"publishedOrCreatedDate":1632761712754},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Storing JSON in Your Databases: Tips and Tricks For MySQL Part One","link":"https://www.percona.com/blog/?p=77805","description":"<img width=\"200\" height=\"107\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-200x107.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL JSON Databases\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-1024x546.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77825\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-300x160.png\" alt=\"MySQL JSON Databases\" width=\"300\" height=\"160\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-1024x546.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Database architecture and design are becoming an increasingly lost art. With new technologies and the push towards faster development cycles, people continue to take shortcuts, often to the detriment of long-term performance, scalability, and security. Designing how your application stores, accesses, and processes data is so fundamentally important, it can not be overlooked. I want people to understand that early design choices can have a profound impact on their applications. To that end, I will be exploring database design principles and practices over the next several months. I am starting with every developer’s favorite data format: JSON!</p>\n<p>It seems that almost every database over the last few years has introduced various degrees of support for storing and interacting with JSON objects directly. While these features are designed to make it easier for application developers to write code faster, the implementations of each implementation tend to vary wildly and can cause some, well, weirdness. Over the next few weeks/months, I will show you some methods, mistakes, and common ways developers store JSON. Just because you can use a database’s native JSON support does not always mean you should! I hope to show you which ones work best for which use cases.</p>\n<p>For part one of this series, I am going to focus on MySQL. MySQL’s implementation of the JSON data type was introduced back in 5.7 (Late 2015/Early 2016 timeframe). Since then, a few minor enhancements have made the implementation a bit more liveable. The current iteration MySQL 8 offers a fully functional implementation of JSON functions and features. Let me show you some examples of how to store and interact with your JSON documents within MySQL.</p>\n<h2>Setup</h2>\n<p>For all my tests, I wanted a reasonable amount of data to test the performance implications of certain functions.  I opted to use the metadata JSON from <a href=\"http://movienet.site/\">http://movienet.site/</a>, about 2.3GB of individual JSON files (one per movie).</p>\n<p>I wrote a small <a href=\"https://github.com/TheYonk/os-db-json-tester/blob/main/MySQL/mysql_load_tables.py\">python script to load</a> and iterate through the JSON files and load them into MySQL.</p>\n<p><img loading=\"lazy\" class=\"aligncenter size-large wp-image-77810\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-1024x470.png\" alt=\"metadata JSON\" width=\"900\" height=\"413\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-1024x470.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-300x138.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-200x92.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-367x168.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM.png 1208w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>I will walk through the examples and show you how I have seen many developers use MySQL to interact with JSON and point out why some of them may be incorrect or cause you issues you may not be aware of.  I will also show you a few other features you may want to look into and explore and offer some design advice.  Let us start with the following simple table definition:</p><pre class=\"crayon-plain-tag\">create table movies_json (\n   ai_myid int AUTO_INCREMENT primary key,\n   imdb_id varchar(255),\n   json_column json\n) engine = innodb;\n\ncreate unique index imdb_idx on movies_json(imdb_id);</pre><p>&#160;</p><pre class=\"crayon-plain-tag\">{\n  \"imdb_id\": \"tt8408760\",\n  \"tmdb_id\": null,\n  \"douban_id\": null,\n  \"title\": \"Rubes (2019)\",\n  \"genres\": [\n    \"Short\",\n    \"Comedy\",\n    \"Horror\"\n  ],\n  \"country\": \"USA\",\n  \"version\": [\n    {\n      \"runtime\": \"7 min\",\n      \"description\": \"\"\n    }\n  ],\n  \"imdb_rating\": null,\n  \"director\": [\n    {\n      \"id\": \"nm3216042\",\n      \"name\": \"Nathan Alan Bunker\"\n    }\n  ],\n  \"writer\": null,\n  \"cast\": [\n    {\n      \"id\": \"nm1899908\",\n      \"name\": \"Brendan Jennings\",\n      \"character\": \"Milton\"\n    },\n    {\n      \"id\": \"nm2384265\",\n      \"name\": \"Ben Begley\",\n      \"character\": \"Paul\"\n    },\n    {\n      \"id\": \"nm2287013\",\n      \"name\": \"Jerry Marr\",\n      \"character\": \"Professor Henson\"\n    },\n    {\n      \"id\": \"nm7529700\",\n      \"name\": \"Allene Prince\",\n      \"character\": \"Margaret\"\n    }\n  ],\n  \"overview\": null,\n  \"storyline\": \"Two disgruntled teachers use a Rube Goldberg machine to exact revenge on the people who have wronged them.\",\n  \"plot\": null,\n  \"synopsis\": null\n}</pre><p><i><span>You can see an example of the JSON format </span></i></p>\n<h2>101: Simple JSON Interactions in MySQL</h2>\n<p>Yes, a single column in a table with a key or two.  Each row would store one of the movies in the downloaded JSON files.  There is an auto_increment key and the IMDB ID that I extracted from the JSON during the load. This structure and setup is a straightforward design with minimal effort.  However, this design also means you generally rely on MySQL as merely the storage for your data.  Provided you are accessing everything by the imdb_id key, you can get and update your JSON to your application easily with a:</p><pre class=\"crayon-plain-tag\">select json_column from movies_json where imdb_id = ‘tt4154796’;\n\nupdate movies_json set json_column = ‘&#60;new JSON&#62;’ where imdb_id = ‘tt4154796’;</pre><p>Eventually, however, you will want to search within your JSON or just return a portion of the JSON Document. For example, let’s say you only want to find the title and IMDB rating for a specified movie. You can do this with functionality is built-in:</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title', json_column-&#62;&#62;'$.imdb_rating' from movies_json where json_column-&#62;&#62;'$.imdb_id'='tt2395427';\n+--------------------------------+-------------------------------+\n| json_column-&#62;&#62;'$.title'        | json_column-&#62;&#62;'$.imdb_rating' |\n+--------------------------------+-------------------------------+\n| Avengers: Age of Ultron (2015) | 7.5                           |\n+--------------------------------+-------------------------------+\n1 row in set (0.77 sec)\n\n\nmysql&#62; select json_column-&#62;&#62;'$.title', json_column-&#62;&#62;'$.imdb_rating' from movies_json where json_column-&#62;&#62;'$.imdb_id'='tt4154796';\n+--------------------------+-------------------------------+\n| json_column-&#62;&#62;'$.title'  | json_column-&#62;&#62;'$.imdb_rating' |\n+--------------------------+-------------------------------+\n| Avengers: Endgame (2019) | null                          |\n+--------------------------+-------------------------------+\n1 row in set (0.75 sec)</pre><p>Here you can see we can interact inside the JSON column just like we would with standard data via SQL by using the special syntax “<a href=\"https://dev.mysql.com/doc/refman/8.0/en/json-search-functions.html#operator_json-inline-path\">-&#62;&#62;’$.key</a>’”.   You can see Avengers: Endgame has a rating of null!  That is no good, and it was a much better movie than that.  Instead of updating and storing the entire JSON document again, MySQL provides a <a href=\"https://dev.mysql.com/doc/refman/8.0/en/json-modification-functions.html#function_json-set\">JSON_SET function</a> to set an element within a document.</p><pre class=\"crayon-plain-tag\">mysql&#62;  update movies_json \n       set json_column = JSON_SET(json_column, \"$.imdb_rating\", 9) \n       where json_column-&#62;&#62;'$.imdb_id'='tt4154796';\n Query OK, 1 row affected (0.93 sec)\n Rows matched: 1  Changed: 1  Warnings: 0\n\n mysql&#62; select json_column-&#62;&#62;'$.title', json_column-&#62;&#62;'$.imdb_rating' from movies_json where json_column-&#62;&#62;'$.imdb_id'='tt4154796';\n +--------------------------+-------------------------------+\n | json_column-&#62;&#62;'$.title'  | json_column-&#62;&#62;'$.imdb_rating' |\n +--------------------------+-------------------------------+\n | Avengers: Endgame (2019) | 9                             |\n +--------------------------+-------------------------------+\n 1 row in set (0.80 sec)</pre><p>We now have fixed the missing rating for Endgame!  But we may not know the IMDB ID when we are searching. Just like working with standard data types, you can use data from within your document in a where clause.  In this case, we will look for all movies that start with “Avengers”.</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title', json_column-&#62;&#62;'$.imdb_rating', json_column-&#62;&#62;'$.imdb_id' from movies_json \nwhere json_column-&#62;&#62;'$.title' like 'Avengers%';\n\n+------------------------------------------------------------+-------------------------------+---------------------------+\n| json_column-&#62;&#62;'$.title'                                    | json_column-&#62;&#62;'$.imdb_rating' | json_column-&#62;&#62;'$.imdb_id' |\n+------------------------------------------------------------+-------------------------------+---------------------------+\n| Avengers: Endgame (2019)                                   | 9.0                           | tt4154796                 |\n| Avengers Confidential: Black Widow &#38; Punisher (Video 2014) | 5.8                           | tt3482378                 |\n| Avengers of Justice: Farce Wars (2018)                     | null                          | tt6172666                 |\n| Avengers: Age of Ultron (2015)                             | 7.5                           | tt2395427                 |\n| Avengers: Infinity War (2018)                              | null                          | tt4154756                 |\n| Avengers Grimm: Time Wars (Video 2018)                     | null                          | tt8159584                 |\n| Avengers Assemble! (TV Series 2010– )                      | null                          | tt1779952                 |\n| Avengers Grimm (Video 2015)                                | 2.8                           | tt4296026                 |\n+------------------------------------------------------------+-------------------------------+---------------------------+\n8 rows in set (0.74 sec)</pre><p>Using the “json_column-&#62;’$.title’ in the where clause got us a nice list of Avengers titled movies and TV shows.  But, you can see from this query, we got more than just the blockbuster Avengers movies.  Let’s say you want to refine this a bit more and find just Avengers movies with Robert Downey Jr. in the cast.  This is a bit more difficult, honestly, because the format of our JSON documents uses an array for cast members.</p>\n<p>Here is what the JSON looks like:</p><pre class=\"crayon-plain-tag\">{\n  \"imdb_id\": \"tt2395427\",\n  \"tmdb_id\": \"99861\",\n  \"douban_id\": \"10741834\",\n  \"title\": \"Avengers: Age of Ultron (2015)\",\n  \"genres\": [\n    \"Action\",\n    \"Adventure\",\n    \"Sci-Fi\"\n  ],\n  \"country\": \"USA\",\n  \"version\": [\n    {\n      \"runtime\": \"141 min\",\n      \"description\": \"\"\n    }\n  ],\n  \"imdb_rating\": 7.5,\n  \"director\": [\n    {\n      \"id\": \"nm0923736\",\n      \"name\": \"Joss Whedon\"\n    }\n  ],\n  \"writer\": [\n    {\n      \"id\": \"nm0923736\",\n      \"name\": \"Joss Whedon\",\n      \"description\": \"written by\"\n    },\n    {\n      \"id\": \"nm0498278\",\n      \"name\": \"Stan Lee\",\n      \"description\": \"based on the Marvel comics by and\"\n    },\n    {\n      \"id\": \"nm0456158\",\n      \"name\": \"Jack Kirby\",\n      \"description\": \"based on the Marvel comics by\"\n    }\n  ],\n  \"cast\": [\n    {\n      \"id\": \"nm0000375\",\n      \"name\": \"Robert Downey Jr.\",\n      \"character\": \"Tony Stark\"\n    },\n    {\n      \"id\": \"nm1165110\",\n      \"name\": \"Chris Hemsworth\",\n      \"character\": \"Thor\"\n    },\n    {\n      \"id\": \"nm0749263\",\n      \"name\": \"Mark Ruffalo\",\n      \"character\": \"Bruce Banner\"\n    },\n    {\n      \"id\": \"nm0262635\",\n      \"name\": \"Chris Evans\",\n      \"character\": \"Steve Rogers\"\n    },\n    {\n      \"id\": \"nm0424060\",\n      \"name\": \"Scarlett Johansson\",\n      \"character\": \"Natasha Romanoff\"\n    },\n    {\n      \"id\": \"nm0719637\",\n      \"name\": \"Jeremy Renner\",\n      \"character\": \"Clint Barton\"</pre><p>&#160;</p>\n<p>You can access arrays in a JSON document by referencing the specific index for the element you want ( i.e. [0].name ), however, if you don’t know which one contains the data you are looking for you need to search for it.  MySQL has the function <a href=\"https://dev.mysql.com/doc/refman/8.0/en/json-search-functions.html#function_json-search\">json_search</a> to help with this (there are other functions such as json_contains as well).  json_search searches a provided value and will return the location if found and null if not found:</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title', \njson_column-&#62;&#62;'$.imdb_rating', \njson_column-&#62;&#62;'$.imdb_id' from movies_json  \nwhere json_column-&#62;&#62;'$.title' like 'Avengers%' and json_search(json_column-&#62;&#62;'$.cast', 'one','Robert Downey Jr.', NULL,'$[*].name' ) is not null;\n\n+--------------------------------+-------------------------------+---------------------------+\n| json_column-&#62;&#62;'$.title'        | json_column-&#62;&#62;'$.imdb_rating' | json_column-&#62;&#62;'$.imdb_id' |\n+--------------------------------+-------------------------------+---------------------------+\n| Avengers: Endgame (2019)       | 9                             | tt4154796                 |\n| Avengers: Age of Ultron (2015) | 7.5                           | tt2395427                 |\n| Avengers: Infinity War (2018)  | null                          | tt4154756                 |\n+--------------------------------+-------------------------------+---------------------------+\n3 rows in set (0.79 sec)</pre><p>You will notice that I used the parameter ‘one’, this finds the first value.  You can also use ‘all’ to return every value matched.  In case you are curious as to what the json_search actually returns here is the output:</p><pre class=\"crayon-plain-tag\">mysql&#62;  select json_column-&#62;&#62;'$.title' as title,\n        json_search(json_column-&#62;&#62;'$.cast', 'one','Robert Downey Jr.') as search_output\n    from movies_json  where json_column-&#62;&#62;'$.title' like 'Avengers%' \nand json_search(json_column-&#62;&#62;'$.cast', 'one','Robert Downey Jr.', NULL,'$[*].name' ) is not null;\n\n+--------------------------------+---------------+\n| title                          | search_output |\n+--------------------------------+---------------+\n| Avengers: Endgame (2019)       | \"$[0].name\"   |\n| Avengers: Age of Ultron (2015) | \"$[0].name\"   |\n| Avengers: Infinity War (2018)  | \"$[0].name\"   |\n+--------------------------------+---------------+\n3 rows in set (0.72 sec)</pre><p>You can see it returns the position and the property that contains the value.  This output is useful for a variety of reasons.  One is if you need to find which index value contains that particular text.  In the example of searching for Robert Downey JR movies, we can use this index information to return the character he played in each movie.  The first way I have seen this done requires a bit of unholy wrangling but:<span><br />\n</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title' as title, \n    json_column-&#62;&#62;'$.imdb_rating' as Rating, \n    json_column-&#62;&#62;'$.imdb_id' as IMDB_ID,\njson_extract(json_column-&#62;&#62;'$.cast',concat(substr(json_unquote(json_search(json_column-&#62;&#62;'$.cast', 'one','Robert Downey Jr.')),1,\n    -&#62; locate('.',json_unquote(json_search(json_column-&#62;&#62;'$.cast', 'one','Robert Downey Jr.')))),'character')) as Char_played\n    from movies_json  where json_column-&#62;&#62;'$.title' like 'Avengers%' and json_search(json_column-&#62;&#62;'$.cast', 'one','Robert Downey Jr.') is not null;\n\n\n+--------------------------------+--------+-----------+--------------------------------------+\n| title                          | Rating | IMDB_ID   | Char_played                          |\n+--------------------------------+--------+-----------+--------------------------------------+\n| Avengers: Endgame (2019)       | 9      | tt4154796 | \"Tony Stark /              Iron Man\" |\n| Avengers: Age of Ultron (2015) | 7.5    | tt2395427 | \"Tony Stark\"                         |\n| Avengers: Infinity War (2018)  | null   | tt4154756 | \"Tony Stark /              Iron Man\" |\n+--------------------------------+--------+-----------+--------------------------------------+\n3 rows in set (0.68 sec)</pre><p>Here I am finding where in the document Robert Downey Jr is listed, then extracting the index and using that with the JSON Extract function to pull out the value of the “$[0].character” instead of “$[0].name”.  While this works, it is ugly.  MySQL provides an alternative to doing this by using <a href=\"https://dev.mysql.com/doc/refman/8.0/en/json-table-functions.html\">json_table</a>.</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title',  json_column-&#62;&#62;'$.imdb_rating', t.* from movies_json, json_table(json_column, '$.cast[*]' columns( \n       V_name varchar(200) path '$.name',\n       V_character varchar(200) path '$.character')\n       ) t where t.V_name like 'Robert Downey Jr.%' and json_column-&#62;&#62;'$.title' like 'Avengers%';\n\n\n+--------------------------------+-------------------------------+-------------------+------------------------------------+\n| json_column-&#62;&#62;'$.title'        | json_column-&#62;&#62;'$.imdb_rating' | V_name            | V_character                        |\n+--------------------------------+-------------------------------+-------------------+------------------------------------+\n| Avengers: Endgame (2019)       | 9                             | Robert Downey Jr. | Tony Stark /              Iron Man |\n| Avengers: Age of Ultron (2015) | 7.5                           | Robert Downey Jr. | Tony Stark                         |\n| Avengers: Infinity War (2018)  | null                          | Robert Downey Jr. | Tony Stark /              Iron Man |\n+--------------------------------+-------------------------------+-------------------+------------------------------------+\n3 rows in set (0.74 sec)</pre><p>Basically, json_table takes an array and turns it into a table object, allowing you to join and query it.  You can also use this to list all the characters an actor played in any film in his career. Here is <a href=\"https://www.percona.com/blog/storing-json-in-your-databases-tips-and-tricks-for-mysql-part-two/\">part two of this series</a>, when we will show you some easier and faster ways to use JSON from inside MySQL.</p>\n","descriptionType":"html","publishedDate":"Thu, 19 Aug 2021 12:47:07 +0000","feedId":11,"bgimg":"","linkMd5":"26d1aeebbf957eb34b336a86fce47739","bgimgJsdelivr":"","metaImg":"","author":"Matt Yonkovit","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-200x107.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn44@2020_6/2021/09/27/16-55-35-344_e0af9e043c016c85.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-300x160.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn95@2020_5/2021/09/27/16-55-13-746_60faace583dffc0d.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-1024x470.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn11@2020_2/2021/09/27/16-55-30-030_d3a82ca7ad24c8f5.webp"},"publishedOrCreatedDate":1632761712751},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Installing PostgreSQL using Docker","link":"https://www.percona.com/blog/?p=67630","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"install postgresql with docker\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>Following the series of blogs started by Peter Zaitsev in <a href=\"https://www.percona.com/blog/2019/11/19/installing-mysql-with-docker/\">Installing MySQL with Docker</a>, on deploying Docker containers running open source databases, in this article, I&#8217;ll demonstrate how to install PostgreSQL using Docker.</p>\n<p><span>Before proceeding, it is important to remind you of Peter&#8217;s warning from his article, which applies here as well: </span></p>\n<p><strong>&#8220;The following instructions are designed to get a test instance running quickly and easily; you do not want to use these for production deployments.&#8221; </strong></p>\n<p>We intend to show how to bring up PostgreSQL instance(s) into Docker containers. Moreover, you will find some basic Docker management operations and some snippets containing examples, always based on the<a href=\"https://hub.docker.com/_/postgres\"> Docker Hub PostgreSQL official image</a>.</p>\n<h2>Installing</h2>\n<p>Starting with the installation, the following example shows how to bring up a Docker container within the latest PostgreSQL release from Docker Hub. It is required to provide the password of the Postgres user at the moment you are launching the container; otherwise, the container will not be created:</p><pre class=\"crayon-plain-tag\">$ docker run --name dockerPostgreSQL \\\n-e POSTGRES_PASSWORD=secret \\\n-d postgres:latest</pre><p>Once you start the container, you can check if it successfully started with <em>docker inspect</em>:</p><pre class=\"crayon-plain-tag\">$ docker inspect -f '{{.State.Running}}' dockerPostgreSQL\ntrue</pre><p>If not, you can try starting it manually (you will see how to do it on the <em>Managing the PostgreSQL Containers</em> section).</p>\n<h2>Getting Connected to the Containerized Instance</h2>\n<p>The snippet below will show you how to connect to your PostgreSQL containerized instance.</p>\n<ul>\n<li>Directly from docker:</li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">$ docker exec -it dockerPostgreSQL psql --user postgres\npostgres=# \\conninfo\n\nYou are connected to database \"postgres\" as user \"postgres\" via socket in \"/var/run/postgresql\" at port \"5432\".</pre><p></p>\n<ul>\n<li>Or from a PostgreSQL client connecting to its IP address, which you can obtain with the command <em>docker inspect</em>:</li>\n</ul>\n<p></p><pre class=\"crayon-plain-tag\">$ docker inspect -f '{{.NetworkSettings.IPAddress}}' dockerPostgreSQL\n172.16.0.12\n\n$ psql --user postgres --port 5432 --host 172.16.0.12\npostgres=# \\conninfo\nYou are connected to database \"postgres\" as user \"postgres\" on host \"172.16.0.12\" at port \"50432\".</pre><p></p>\n<h2>Managing the PostgreSQL Containers</h2>\n<p>The command below will show you how to list all active containers and their respective statuses:</p><pre class=\"crayon-plain-tag\">$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                     NAMES\n7f88656c4864        postgres:11         \"docker-entrypoint...\"   4 minutes ago       Up 4 minutes        0.0.0.0:50432-&#62;5432/tcp   dockerPostgreSQL11\n8aba8609dabc        postgres:11.5       \"docker-entrypoint...\"   21 minutes ago      Up 21 minutes       5432/tcp                  dockerPostgreSQL115\n15b9e0b789dd        postgres:latest     \"docker-entrypoint...\"   32 minutes ago      Up 32 minutes       5432/tcp                  dockerPostgreSQL</pre><p>To stop one of the containers, do:</p><pre class=\"crayon-plain-tag\">$ docker stop dockerPostgreSQL</pre><p>To start a container:</p><pre class=\"crayon-plain-tag\">$ docker start dockerPostgreSQL</pre><p>Finally, to remove a container, first stop it and then:</p><pre class=\"crayon-plain-tag\">$ docker rm dockerPostgreSQL</pre><p></p>\n<h2>Customizing PostgreSQL settings in the container</h2>\n<p>Let&#8217;s say you want to define different parameters for the PostgreSQL server running on Docker. You can create a custom postgresql.conf configuration file to replace the default one used with Docker:</p><pre class=\"crayon-plain-tag\">$ docker run --name dockerPostgreSQL11 -p 50433:5432 -v \"$PWD/my-postgres.conf\":/etc/postgresql/postgresql.conf -e POSTGRES_PASSWORD=scret -d postgres:11</pre><p>Another example of how to pass startup arguments is changing the directory which stores the wal files:</p><pre class=\"crayon-plain-tag\">$ docker run --name dockerPostgreSQL11 -p 50433:5432 -v \"$PWD/my-postgres.conf\":/etc/postgresql/postgresql.conf -e POSTGRES_PASSWORD=scret -e POSTGRES_INITDB_WALDIR=/backup/wal -d postgres:11</pre><p></p>\n<h2>Running Different PostgreSQL Versions in Different Containers</h2>\n<p>You can launch multiple PostgreSQL containers, each running a different PostgreSQL version. In the example below, you will find how to start up a 10.5 PostgreSQL version:</p><pre class=\"crayon-plain-tag\">$ docker run --name dockerPostgreSQL105 -p 50433:5432 -e POSTGRES_PASSWORD=scret  -d postgres:10.5\n7f51187d32f339688c2f450ecfda6b7552e21a93c52f365e75d36238f5905017</pre><p>Here is how you could launch a PostgreSQL 11 container:</p><pre class=\"crayon-plain-tag\">$  docker run --name dockerPostgreSQL11 -p 50432:5432 -e POSTGRES_PASSWORD=scret -d postgres:11\n7f88656c4864864953a5491705ac7ca882882df618623b1e5664eabefb662733</pre><p>After that, by reissuing the status command, you will see all of the already created ones:</p><pre class=\"crayon-plain-tag\">$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                     NAMES\ne06e637ae090        postgres:10.5       \"docker-entrypoint...\"   4 seconds ago       Up 4 seconds        0.0.0.0:50433-&#62;5432/tcp   dockerPostgreSQL105\n7f88656c4864        postgres:11         \"docker-entrypoint...\"   About an hour ago   Up About an hour    0.0.0.0:50432-&#62;5432/tcp   dockerPostgreSQL11\n15b9e0b789dd        postgres:latest     \"docker-entrypoint...\"   2 hours ago         Up 2 hours          5432/tcp                  dockerPostgreSQL</pre><p>You will notice that both 10.5 and 11 were created with different port mappings. Locally into the container, both are listening on 5432 but are externally mapped to different ports. Therefore, you will be able to access all of them externally.</p>\n<p>You will find even more details on the <a href=\"https://hub.docker.com/_/postgres\">PostgreSQL Hub Docker site</a>.</p>\n<p><strong>Percona Distribution for PostgreSQL provides the best and most critical enterprise components from the open-source community in a single distribution, designed and tested to work together.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/postgresql-distribution\" rel=\"noopener\">Download Percona Distribution for PostgreSQL Today!</a></p>\n","descriptionType":"html","publishedDate":"Fri, 03 Sep 2021 13:32:38 +0000","feedId":11,"bgimg":"","linkMd5":"3dd387a7c3c17d427550be17f081c4fc","bgimgJsdelivr":"","metaImg":"","author":"Rafael Galinari","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn28@2020_6/2021/09/27/16-55-33-741_623c27f2ff701cbc.webp"},"publishedOrCreatedDate":1632761712714},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Resharding in MongoDB 5.0","link":"https://www.percona.com/blog/?p=77591","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Resharding in MongoDB 5.0\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77904\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-300x157.png\" alt=\"Resharding in MongoDB 5.0\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" /><a href=\"https://www.percona.com/blog/mongodb-5-0-is-coming-in-hot-what-do-database-experts-across-the-community-think/\">MongoDB 5.0</a> has been released with a bunch of new features. An important one is the capability to completely redefine the shard key of a collection in a sharded cluster. Resharding was a feature frequently requested by the customers. In MongoDB 4.4 only the refining of the shard was available; from now on you can change your shard key using also different fields.</p>\n<p>When you use sharding in MongoDB and you fail to create the shard key of a collection, you can face issues like:</p>\n<ul>\n<li aria-level=\"1\">Unbalanced distribution of the data</li>\n<li aria-level=\"1\">Having jumbo chunks that cannot be split and cannot be migrated</li>\n</ul>\n<p>These issues can simply make your cluster very slow, making some of the shards extremely overloaded. Also, the cluster can be really unmanageable and crashes could happen at some point.</p>\n<h2>The Problem of Changing a Shard Key</h2>\n<p>I had a customer running a 5-shard cluster with more than 50TB of data. They had a few collections with sub-optimal shard keys. They had one of the shards having more than 75% of the data and workload, and they had thousands of jumbo chunks around. Some of the jumbo chunks reached 200GB in size; really a lot. The cluster was terribly slow, chunk migration attempts were a lot and they failed very frequently, and the application was most of the time unresponsive due to timeouts. Manual splits and migrations of jumbo chunks were impossible. The only solution was to create a different shard key for those collections, but this wasn&#8217;t possible with that MongoDB version. The only suggested way to change a shard key was to drop and recreate the collection. Yes, remember also to dump and reload the data. This requires you to stop your application for an impressive amount of time if the dataset is large. That was the case.</p>\n<p>In order to avoid stopping the application, we deployed a new empty cluster with a few more shards as well. We created all the empty collections in the new cluster with the optimal shard key. Then we moved the data out of the old cluster with custom scripts, one piece at the time, using boundaries on indexed timestamp fields. The solution required the customer to deploy some application changes. The application was instructed to read/write from a different cluster depending if the affected data was already migrated into the new cluster or not. The entire migration process took weeks to complete. In the end, it worked and the customer now has a balanced cluster with no jumbo chunks.</p>\n<p>Anyway, the solution came at some cost in terms of new machines, development effort, and time.</p>\n<p>Fortunately, MongoDB 5.0 introduced the <strong>reshardCollection</strong> command. From now on changing a shard key should be less expensive than in the past. That’s awesome.</p>\n<p>In this article, we&#8217;ll take a look at how resharding in MongoDB 5.0 works.</p>\n<h2>Internals</h2>\n<p>The way resharding works is simple. When you issue the <strong>reshardCollection</strong> command, a new implicit empty collection is created by MongoDB with the new shard key defined, and the data will be moved from the existing collection chunk by chunk. There is a two-second lock required by the balancer to determine the new data distribution, but during the copy phase, the application can continue to read and write the collection with no issues.</p>\n<p>Due to this copy phase, the larger the collection is, the more time the resharding takes.</p>\n<h2>Prerequisites for Resharding</h2>\n<p>There are some requirements you need to validate before starting the resharding, and some could be expensive, in certain cases.</p>\n<ul>\n<li aria-level=\"1\">Disk space: be sure to have at least 1.2x the size of the collection you’re going to reshard. If the collection is 1TB, you need at least 1.2TB free space in your disk.</li>\n<li aria-level=\"1\">I/O capacity should be below 50%</li>\n<li aria-level=\"1\">CPU load should be below 80%</li>\n</ul>\n<p>The resources available must be evaluated on each shard.</p>\n<p>If you fail to provide these resources the database will run out of space or the resharding could take longer than expected.</p>\n<p>Another important requirement is that you will need to deploy changes to your application queries. To let the database work best during the sharding process, the queries must use filters on both the current shard key and the new shard key. Only at the end of the resharding process you may drop all the filters regarding the old shard key out of your queries.</p>\n<p>This requirement is very important and in some cases it could be a little expensive. Temporarily changing the application code is sometimes a hard task.</p>\n<p>There also other limitations you need to consider:</p>\n<ul>\n<li aria-level=\"1\">Resharing is not permitted if an index built is running</li>\n<li aria-level=\"1\">Some queries will return error if you didn’t include both the current and new shard keys: <i>deleteOne(), findAnd Modify(), updateOne()</i> … check the manual for the full list</li>\n<li aria-level=\"1\">You can reshard only one collection at a time</li>\n<li aria-level=\"1\">You cannot use <i>addShard(), removeShard(), dropDatabase(), db.createCollection()</i> while resharding is running</li>\n<li aria-level=\"1\">The new shard key cannot have a uniqueness constraint</li>\n<li aria-level=\"1\">Resharding a collection with a uniqueness constraint is not supported</li>\n</ul>\n<h2>Let’s Test Resharding in MongoDB</h2>\n<p>To test the new feature, I created a sharded cluster using AWS instances, t2-large with 2 CPUs, 8 GB RAM, and EBS volume. I deployed a two-shard cluster using just a single member for each replica set.</p>\n<p>Let’s create a sharded collection and insert some sample data.</p>\n<p>At the beginning we create the shard key on the <strong>age</strong> field. To create the random data, we use some functions and a javascript loop to insert one million documents.</p><pre class=\"crayon-plain-tag\">[direct: mongos] testdb&#62; sh.shardCollection(\"testdb.testcoll\", { age: 1} )\n{\n  collectionsharded: 'testdb.testcoll',\n  ok: 1,\n  '$clusterTime': {\n    clusterTime: Timestamp({ t: 1628588848, i: 25 }),\n    signature: {\n      hash: Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),\n      keyId: Long(\"0\")\n    }\n  },\n  operationTime: Timestamp({ t: 1628588848, i: 21 })\n}\n\n[direct: mongos] testdb&#62; function getRandomInt(min, max) {\n...     return Math.floor(Math.random() * (max - min + 1)) + min;\n... }\n[Function: getRandomInt]\n\n[direct: mongos] testdb&#62; var day = 1000 * 60 * 60 * 24;\n\n[direct: mongos] testdb&#62; function getRandomDate() {\n... return new Date(Date.now() - (Math.floor(Math.random() * day)))\n... }\n[Function: getRandomDate]\n\n[direct: mongos] testdb&#62; function getRandomString() {\n... return (Math.random()+1).toString(36).substring(2)\n... }\n[Function: getRandomString]\n\n[direct: mongos] testdb&#62; for (var i=1; i&#60;=1000000; i++) {\n... db.testcoll.insert(\n..... {\n....... uid: i,\n....... name: getRandomString(),\n....... date_created: getRandomDate(),\n....... age: getRandomInt(1,100),\n....... address: getRandomString(),\n....... city: getRandomString(),\n....... country: getRandomString()\n....... }\n..... )\n... }</pre><p>Now we have our initial collection. Let’s take a look at the distribution of the chunks and the sharding status of the cluster.</p><pre class=\"crayon-plain-tag\">{\n  database: {\n    _id: 'testdb',\n    primary: 'sh1-rs',\n    partitioned: true,\n    version: {\n    uuid: UUID(\"efda5b69-1ffc-42c3-abed-ad20e08e321d\"),\n    timestamp: Timestamp({ t: 1628527701, i: 21 }),\n    lastMod: 1\n  }\n},\ncollections: {\n  'testdb.testcoll': {\n    shardKey: { age: 1 },\n    unique: false,\n    balancing: true,\n    chunkMetadata: [\n      { shard: 'sh0-rs', nChunks: 2 },\n      { shard: 'sh1-rs', nChunks: 3 }\n    ],\n    chunks: [\n      { min: { age: MinKey() }, max: { age: 1 }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 0 }) },\n      { min: { age: 1 }, max: { age: 42 }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 4, i: 0 }) },\n      { min: { age: 42 }, max: { age: 72 }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 2 }) },\n      { min: { age: 72 }, max: { age: 100 }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 3 }) },\n      { min: { age: 100 }, max: { age: MaxKey() }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 5, i: 1 }) }\n    ],\n    tags: []\n    }\n  }\n}</pre><p>With the current shard key on the <strong>age</strong> field we’ve got 5 chunks in total, 2 on sh0-rs and 3 on sh1-rs. The total size of the collection is around 200MB (uncompressed).</p>\n<p>It’s time to test the resharding. Let’s try to change the shard key to { <strong>name: 1</strong> }. The following is the right syntax where we provide the namespace to reshard and the new shard key:</p><pre class=\"crayon-plain-tag\">[direct: mongos] testdb&#62; db.adminCommand({\n... reshardCollection: \"testdb.testcoll\",\n... key: { name: 1 }\n... })</pre><p>From another connection we can monitor the status of the resharding:</p><pre class=\"crayon-plain-tag\">[direct: mongos] testdb&#62; db.getSiblingDB(\"admin\").aggregate([ \n{ $currentOp: { allUsers: true, localOps: false } }, \n{ $match: { type: \"op\", \"originatingCommand.reshardCollection\": \"testdb.testcoll\" } }])\n[\n  {\n    shard: 'sh0-rs',\n    type: 'op',\n    desc: 'ReshardingRecipientService f86967fe-36a9-4836-8972-1061a61230df',\n    op: 'command',\n    ns: 'testdb.testcoll',\n    originatingCommand: {\n      reshardCollection: 'testdb.testcoll',\n      key: { name: 1 },\n      unique: false,\n      collation: { locale: 'simple' }\n    },\n    totalOperationTimeElapsedSecs: Long(\"145\"),\n    remainingOperationTimeEstimatedSecs: Long(\"173\"),\n    approxDocumentsToCopy: Long(\"624135\"),\n    documentsCopied: Long(\"569868\"),\n    approxBytesToCopy: Long(\"95279851\"),\n    bytesCopied: Long(\"86996201\"),\n    totalCopyTimeElapsedSecs: Long(\"145\"),\n    oplogEntriesFetched: Long(\"0\"),\n    oplogEntriesApplied: Long(\"0\"),\n    totalApplyTimeElapsedSecs: Long(\"0\"),\n    recipientState: 'cloning',\n    opStatus: 'running'\n  },\n  {\n    shard: 'sh0-rs',\n    type: 'op',\n    desc: 'ReshardingDonorService f86967fe-36a9-4836-8972-1061a61230df',\n    op: 'command',\n    ns: 'testdb.testcoll',\n    originatingCommand: {\n      reshardCollection: 'testdb.testcoll',\n      key: { name: 1 },\n      unique: false,\n      collation: { locale: 'simple' }\n    },\n    totalOperationTimeElapsedSecs: Long(\"145\"),\n    remainingOperationTimeEstimatedSecs: Long(\"173\"),\n    countWritesDuringCriticalSection: Long(\"0\"),\n    totalCriticalSectionTimeElapsedSecs: Long(\"0\"),\n    donorState: 'donating-initial-data',\n    opStatus: 'running'\n  }, \n  {\n    shard: 'sh1-rs',\n    type: 'op',\n    desc: 'ReshardingDonorService f86967fe-36a9-4836-8972-1061a61230df',\n    op: 'command',\n    ns: 'testdb.testcoll',\n    originatingCommand: {\n      reshardCollection: 'testdb.testcoll',\n      key: { name: 1 },\n      unique: false,\n      collation: { locale: 'simple' }\n    },\n    totalOperationTimeElapsedSecs: Long(\"145\"),\n    remainingOperationTimeEstimatedSecs: Long(\"276\"),\n    countWritesDuringCriticalSection: Long(\"0\"),\n    totalCriticalSectionTimeElapsedSecs: Long(\"0\"),\n    donorState: 'donating-initial-data', \n    opStatus: 'running'\n  },\n  {\n    shard: 'sh1-rs',\n    type: 'op',\n    desc: 'ReshardingRecipientService f86967fe-36a9-4836-8972-1061a61230df',\n    op: 'command',\n    ns: 'testdb.testcoll',\n    originatingCommand: {\n      reshardCollection: 'testdb.testcoll',\n      key: { name: 1 },\n      unique: false,\n      collation: { locale: 'simple' }\n    },\n    totalOperationTimeElapsedSecs: Long(\"145\"),\n    remainingOperationTimeEstimatedSecs: Long(\"276\"),\n    approxDocumentsToCopy: Long(\"624135\"),\n    documentsCopied: Long(\"430132\"),\n    approxBytesToCopy: Long(\"95279851\"),\n    bytesCopied: Long(\"65663031\"),\n    totalCopyTimeElapsedSecs: Long(\"145\"),\n    oplogEntriesFetched: Long(\"0\"),\n    oplogEntriesApplied: Long(\"0\"),\n    totalApplyTimeElapsedSecs: Long(\"0\"),\n    recipientState: 'cloning',\n    opStatus: 'running'\n  }\n]</pre><p>With the <strong>totalOperationTimeElapsedSecs</strong> you can see the time elapsed and with the <strong>remainingOperationTimeEstimatedSecs</strong> the estimated time for completing the operation.</p>\n<p>While resharding is running we are allowed to write into the collection. For example, I tried the following and it worked:</p><pre class=\"crayon-plain-tag\">[direct: mongos] testdb&#62; db.testcoll.insert({ name: \"new doc\" })\n{\n  acknowledged: true,\n  insertedIds: { '0': ObjectId(\"61125fa53916a70b7fdd8f6c\") }\n}</pre><p>The full resharding of the collection took around 5 minutes. Let’s now see the new status of the sharding.</p><pre class=\"crayon-plain-tag\">{\n  database: {\n    _id: 'testdb',\n    primary: 'sh1-rs',\n    partitioned: true,\n    version: {\n      uuid: UUID(\"efda5b69-1ffc-42c3-abed-ad20e08e321d\"),\n      timestamp: Timestamp({ t: 1628527701, i: 21 }),\n      lastMod: 1\n    }\n  },\n  collections: {\n    'testdb.testcoll': {\n    shardKey: { name: 1 },\n    unique: false,\n    balancing: true,\n    chunkMetadata: [\n      { shard: 'sh0-rs', nChunks: 3 },\n      { shard: 'sh1-rs', nChunks: 3 }\n    ],\n    chunks: [\n      { min: { name: MinKey() }, max: { name: '7gbk73hf42g' }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 6, i: 0 }) },\n      { min: { name: '7gbk73hf42g' }, max: { name: 'cv1oqoetetf' }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 1 }) },\n      { min: { name: 'cv1oqoetetf' }, max: { name: 'kuk1p3z8kc' }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 2, i: 2 }) },\n      { min: { name: 'kuk1p3z8kc' }, max: { name: 'ppq4ubqwywh' }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 2, i: 3 }) },\n      { min: { name: 'ppq4ubqwywh' }, max: { name: 'zsods6qhqp' }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 1, i: 3 }) },\n      { min: { name: 'zsods6qhqp' }, max: { name: MaxKey() }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 6, i: 1 }) }\n    ],\n    tags: []\n    }\n  }\n}</pre><p>We can see the shard key has been changed, and the chunks are now evenly distributed.</p><pre class=\"crayon-plain-tag\">[direct: mongos] testdb&#62; db.testcoll.getShardDistribution()\nShard sh1-rs at sh1-rs/172.30.0.108:27017\n{\n  data: '92.72MiB',\n  docs: 636938,\n  chunks: 3,\n  'estimated data per chunk': '30.9MiB',\n  'estimated docs per chunk': 212312\n}\n---\nShard sh0-rs at sh0-rs/172.30.0.221:27017\n{\n  data: '82.96MiB',\n  docs: 569869,\n  chunks: 3,\n  'estimated data per chunk': '27.65MiB',\n  'estimated docs per chunk': 189956\n}\n---\nTotals\n{\n  data: '175.69MiB',\n  docs: 1206807,\n  chunks: 6,\n  'Shard sh1-rs': [\n    '52.77 % data',\n    '52.77 % docs in cluster',\n    '152B avg obj size on shard'\n  ],\n  'Shard sh0-rs': [\n    '47.22 % data',\n    '47.22 % docs in cluster',  \n    '152B avg obj size on shard'\n  ]\n}</pre><p>Good. It worked.</p>\n<p>Out of curiosity, during the resharding, the temporary collection being copied is visible. Just run <strong>sh.status()</strong>:</p><pre class=\"crayon-plain-tag\">    collections: {\n      'testdb.system.resharding.defda80d-ea5f-4e52-9a7f-9b3e2ed8ddf1': {\n        shardKey: { name: 1 },\n        unique: false,\n        balancing: true,\n        chunkMetadata: [\n          { shard: 'sh0-rs', nChunks: 3 },\n          { shard: 'sh1-rs', nChunks: 3 }\n        ],\n        chunks: [\n          { min: { name: MinKey() }, max: { name: '9bnfw8n23l' }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 2, i: 0 }) },\n          { min: { name: '9bnfw8n23l' }, max: { name: 'etd3ho9vfwg' }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 2, i: 1 }) },\n          { min: { name: 'etd3ho9vfwg' }, max: { name: 'kmht0br01l' }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 1, i: 2 }) },\n          { min: { name: 'kmht0br01l' }, max: { name: 'sljcb1s70g' }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 1, i: 3 }) },\n          { min: { name: 'sljcb1s70g' }, max: { name: 'zzi3ijuw2f' }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 1, i: 4 }) },\n          { min: { name: 'zzi3ijuw2f' }, max: { name: MaxKey() }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 1, i: 5 }) }\n        ],\n        tags: []\n      },\n      'testdb.testcoll': {\n        shardKey: { age: 1 },\n        unique: false,\n        balancing: true,\n        chunkMetadata: [\n          { shard: 'sh0-rs', nChunks: 2 },\n          { shard: 'sh1-rs', nChunks: 3 }\n        ],\n        chunks: [\n          { min: { age: MinKey() }, max: { age: 1 }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 0 }) },\n          { min: { age: 1 }, max: { age: 42 }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 4, i: 0 }) },\n          { min: { age: 42 }, max: { age: 72 }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 2 }) },\n          { min: { age: 72 }, max: { age: 100 }, 'on shard': 'sh1-rs', 'last modified': Timestamp({ t: 5, i: 3 }) },\n          { min: { age: 100 }, max: { age: MaxKey() }, 'on shard': 'sh0-rs', 'last modified': Timestamp({ t: 5, i: 1 }) }\n        ],\n        tags: []\n      }\n    }\n  }</pre><p><em>testdb.system.resharding.defda80d-ea5f-4e52-9a7f-9b3e2ed8ddf1</em> is the newly created collection.</p>\n<h2>Some Graphs from Percona Monitoring and Management</h2>\n<p>I deployed <a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) to monitor the cluster and for evaluating the impact of the resharding operation. As said, I used a little test environment with no workload at all, so what we can see on the following graphs is just the impact of the resharding.</p>\n<p>The cluster has one config server and two shards. Each replica set is just a single member running on a dedicated virtual host in AWS EC2.</p>\n<p>Let’s see the CPU usage</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77654 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-1024x223.png\" alt=\"percona monitoring and management mongodb\" width=\"900\" height=\"196\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-1024x223.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-300x65.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-200x44.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-1536x335.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-2048x447.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-367x80.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>The node in the middle is the config server. The data-bearing nodes had more impact in terms of CPU usage, quite relevant. The config server didn’t have any impact.</p>\n<p>Let’s see the Disk utilization in terms of IOPS and latency:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77655 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-1024x218.png\" alt=\"Disk utilization\" width=\"900\" height=\"192\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-1024x218.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-300x64.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-200x43.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-1536x327.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-2048x437.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-367x78.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77656 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-1024x187.png\" alt=\"\" width=\"900\" height=\"164\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-1024x187.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-300x55.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-200x37.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-1536x281.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-2048x374.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-367x67.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>The same as the CPU, the config server didn’t have any impact. Instead, the shards increased the IOPS as expected since a new collection has been created. The disk latency had spikes to more than 20 ms at the beginning and at the end of the process.</p>\n<p>MongoDB increased the latency of the read operations.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77657 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-1024x221.png\" alt=\"\" width=\"900\" height=\"194\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-1024x221.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-300x65.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-200x43.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-1536x332.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-2048x442.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-367x79.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>The WiredTiger storage engine increased the number of transactions processed and the cache activity as expected.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77658 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-1024x226.png\" alt=\"\" width=\"900\" height=\"199\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-1024x226.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-300x66.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-200x44.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-1536x339.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-2048x452.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-367x81.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>Other metrics increased but they are not included here. This behavior was expected and it’s normal for a resharding operation.</p>\n<p>The resharding is quite expensive, even for a small collection, in particular for the CPU utilization. Anyway, more tests are needed with larger collections, in the GB/TB magnitude.</p>\n<p>Remember the resharding time depends on the size of the collection. Be aware of that and expect the performance decrease for your servers for some time when you plan to do a resharding.</p>\n<h3>Conclusions</h3>\n<p>Resharding in MongoDB 5.0 is an amazing new feature but it comes with some additional costs and limitations. Indeed, resharding requires resources in terms of disk space, I/O, and CPU. Be aware of that and plan carefully your resharding operations.</p>\n<p>As a best practice, I suggest spending as much time as possible the first time you decide to shard a collection. If you choose the optimal shard key from the beginning, then you won’t need resharding. A good schema design is always the best approach when working with MongoDB.</p>\n<p>I suggest continuously monitoring the status of the chunk distribution in your collections. As soon as you notice there’s something wrong with the shard key and the distribution is uneven and it’s getting worse, then use reshardCollection as soon as you can, while the collection is not too large.</p>\n<p>Another suggestion is to do resharding only during a maintenance window because the performance impact for your cluster could be significant in the case of very large collections.</p>\n<p><em>Further readings:</em></p>\n<p><a href=\"https://docs.mongodb.com/manual/core/sharding-reshard-a-collection/\">https://docs.mongodb.com/manual/core/sharding-reshard-a-collection/</a></p>\n<p><a href=\"https://docs.mongodb.com/v5.0/core/sharding-choose-a-shard-key/\">https://docs.mongodb.com/v5.0/core/sharding-choose-a-shard-key/</a></p>\n<p><a href=\"https://docs.mongodb.com/manual/reference/command/refineCollectionShardKey/\">https://docs.mongodb.com/manual/reference/command/refineCollectionShardKey/</a></p>\n<p><a href=\"https://www.percona.com/blog/mongodb-5-0-is-coming-in-hot-what-do-database-experts-across-the-community-think/\">https://www.percona.com/blog/mongodb-5-0-is-coming-in-hot-what-do-database-experts-across-the-community-think/</a></p>\n<p><strong>Percona Distribution for MongoDB is a freely available MongoDB database alternative, giving you a single solution that combines the best and most important enterprise components from the open source community, designed and tested to work together.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mongodb\" rel=\"noopener\">Download Percona Distribution for MongoDB Today!</a></p>\n","descriptionType":"html","publishedDate":"Tue, 24 Aug 2021 13:55:05 +0000","feedId":11,"bgimg":"","linkMd5":"6faba7febc6c8ab4e0f3a1892e0bcf22","bgimgJsdelivr":"","metaImg":"","author":"Corrado Pandiani","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_4/2021/09/27/16-55-32-210_f0b6061f9200b01a.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn40@2020_1/2021/09/27/16-55-13-761_026f88f5b377aa3f.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-1024x223.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn68@2020_2/2021/09/27/16-55-14-483_5a7267e77a88ca59.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-1024x218.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn60@2020_1/2021/09/27/16-55-27-990_7b403c734bd0757c.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-1024x187.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn92@2020_6/2021/09/27/16-55-15-420_fe0e4557fcdc81e1.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-1024x221.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn92@2020_4/2021/09/27/16-55-33-585_4d45293c8683ac3b.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-1024x226.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn26@2020_6/2021/09/27/16-55-31-907_3c13c8e9381b6a01.webp"},"publishedOrCreatedDate":1632761712734},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Finding Undetected Jumbo Chunks in MongoDB","link":"https://www.percona.com/blog/?p=78021","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Jumbo Chunks in MongoDB\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78084\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-300x157.png\" alt=\"Jumbo Chunks in MongoDB\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />I recently came across an interesting case of performance issues during balancing in a MongoDB cluster. Digging through the logs, it became clear the problem was related to chunk moves taking a long time.</p>\n<p>As we know, the default maximum chunk size is 64 MB. So these migrations are supposed to be very fast in most of the hardware in use nowadays.</p>\n<p>In this case, there were several chunks way above that limit being moved around. How can this happen? Shouldn&#8217;t those chunks have been marked as Jumbo?</p>\n<h2>Recap on Chunk Moves</h2>\n<p>For starters, let&#8217;s review what we know about chunk moves.</p>\n<p>MongoDB does not move a chunk if the number of documents in it is greater than 1.3 times the result of dividing the configured chunk size (default 64 MB) by the average document size.</p>\n<p>If we have a collection with an average document size of 2 KB, a chunk with more than 42597 (65535 / 2 * 1.3) documents won&#8217;t be balanced. This might be an issue for collections with non-uniform document sizes, as the estimation won&#8217;t be very good in that case.</p>\n<p>Also, if a chunk grows past the maximum size, it is flagged as Jumbo. This means the balancer will not try to move it. You can check out the following article for more information about <a href=\"https://www.percona.com/blog/2020/06/11/dealing-with-jumbo-chunks-in-mongodb/\">dealing with jumbo chunks</a>.</p>\n<p>Finally, we must keep in mind that chunk moves <a href=\"https://github.com/mongodb/mongo/blob/master/src/mongo/db/s/README.md#the-live-migration-protocol\">take a metadata lock at a certain part of the process</a>, so write operations are blocked momentarily near the end of a chunk move.</p>\n<h2>The autoSplitter in Action</h2>\n<p>Before MongoDB 4.2, the autoSplitter process was responsible for ensuring chunks do not exceed the maximum size runs on the mongos router. The router process keeps some statistics about the operations it performs. These statistics are consulted to make chunk-splitting decisions.</p>\n<p>The problem is that in a production environment, there are usually many of these processes deployed. An individual mongos router only has a partial idea of what&#8217;s happening with a particular chunk of data.</p>\n<p>If multiple mongos routers change data on the same chunk, it could grow beyond the maximum size and still go unnoticed.</p>\n<p>There are many reported issues related to this topic (for example, see <a href=\"https://jira.mongodb.org/browse/SERVER-44088\">SERVER-44088</a>, <a href=\"https://jira.mongodb.org/browse/SERVER-13806\">SERVER-13806</a>, and <a href=\"https://jira.mongodb.org/browse/SERVER-16715\">SERVER-16715</a>).</p>\n<p>Frequently restarting mongos processes could also lead to the same problem. This causes the statistics used to make chunk-splitting decisions to be reset.</p>\n<p>Because of these issues, the autoSplitter process was changed to run on the primary member of each shard&#8217;s replica set. This happened in MongoDB version 4.2 (see <a href=\"https://jira.mongodb.org/browse/SERVER-9287\">SERVER-9287</a> for more details).</p>\n<p>The process should better prevent jumbo chunks now, as each shard has a more accurate view of what data it contains (and how it is being changed).</p>\n<p>The MongoDB version, in this case, was 3.6, and the autoSplitter was not doing as good a job as expected.</p>\n<h2>Finding Undetected Jumbo Chunks in MongoDB</h2>\n<p>For starters, I came across a script to print a summary of the chunks statistics <a href=\"https://gist.github.com/normgraham/8716338\">here</a>. We can build upon it to also display information about Jumbo chunks and generate the commands required to manually split any chunks exceeding the maximum size.</p><pre class=\"crayon-plain-tag\">var allChunkInfo = function(ns){\n    var chunks = db.getSiblingDB(\"config\").chunks.find({\"ns\" : ns}).sort({min:1}).noCursorTimeout(); //this will return all chunks for the ns ordered by min\n    var totalChunks = 0;\n    var totalJumbo = 0;\n    var totalSize = 0;\n    var totalEmpty = 0;\n \n    chunks.forEach( \n        function printChunkInfo(chunk) { \n          var db1 = db.getSiblingDB(chunk.ns.split(\".\")[0]) // get the database we will be running the command against later\n          var key = db.getSiblingDB(\"config\").collections.findOne({_id:chunk.ns}).key; // will need this for the dataSize call\n         \n          // dataSize returns the info we need on the data, but using the estimate option to use counts is less intensive\n          var dataSizeResult = db1.runCommand({datasize:chunk.ns, keyPattern:key, min:chunk.min, max:chunk.max, estimate:false});\n         \n          if(dataSizeResult.size &#62; 67108864) {\n            totalJumbo++;\n            print('sh.splitFind(\"' + chunk.ns.toString() + '\", ' + JSON.stringify(chunk.min) + ')' + ' // '+  chunk.shard + '    ' + Math.round(dataSizeResult.size/1024/1024) + ' MB    ' + dataSizeResult.numObjects );\n          }\n          totalSize += dataSizeResult.size;\n          totalChunks++;\n          \n          if (dataSizeResult.size == 0) { totalEmpty++ }; //count empty chunks for summary\n          }\n    )\n    print(\"***********Summary Chunk Information***********\");\n    print(\"Total Chunks: \"+totalChunks);\n    print(\"Total Jumbo Chunks: \"+totalJumbo);\n    print(\"Average Chunk Size (Mbytes): \"+(totalSize/totalChunks/1024/1024));\n    print(\"Empty Chunks: \"+totalEmpty);\n    print(\"Average Chunk Size (non-empty): \"+(totalSize/(totalChunks-totalEmpty)/1024/1024));\n}</pre><p>The script has to be called from a mongos router as follows:</p><pre class=\"crayon-plain-tag\">mongos&#62; allChunkInfo(\"db.test_col\")</pre><p>And it will print any commands to perform chunk splits as required:</p><pre class=\"crayon-plain-tag\">sh.splitFind(\"db.test_col\", {\"_id\":\"jhxT2neuI5fB4o4KBIASK1\"}) // shard-1    222 MB    7970\nsh.splitFind(\"db.test_col\", {\"_id\":\"zrAESqSZjnpnMI23oh5JZD\"}) // shard-2    226 MB    7988\nsh.splitFind(\"db.test_col\", {\"_id\":\"SgkCkfSDrY789e9nD4crk9\"}) // shard-1    218 MB    7986\nsh.splitFind(\"db.test_col\", {\"_id\":\"X5MKEH4j32OhmAhY7LGPMm\"}) // shard-1    238 MB    8338\n...\n***********Summary Chunk Information***********\nTotal Chunks: 5047\nTotal Jumbo Chunks: 120\nAverage Chunk Size (Mbytes): 19.29779934868946\nEmpty Chunks: 1107\nAverage Chunk Size (non-empty): 24.719795257064895</pre><p>You can see the script prints the commands to split the jumbo chunks, along with the shard where each one lives, the size of the chunk, and the number of documents on it. At the end, summary information is also displayed.</p>\n<p>The estimate: true option in the dataSize command causes it to use the average document size of the collection for the estimation. This is not very accurate if the document size has many variances but runs faster and is less resource-intensive. Consider setting that option if you know your document size is more or less uniform.</p>\n<p>All that is left is to run the commands generated by the script, and the jumbo chunks should be gone. The balancer will then move chunks during the next balancing window as required to even out the data. Consider scheduling the balancing window outside the peak workload hours to reduce the impact.</p>\n<h3>Conclusion</h3>\n<p>Even though MongoDB 3.6 has been EOL for some time now, many people are still running it, mostly due to outdated drivers on the application side that are not compatible with newer releases. Nowadays, the autoSplitter should be doing a better job of keeping chunk size at bay.</p>\n<p>If you are still running a MongoDB version older than 4.2, it would be a good idea to review your chunk stats from time to time to avoid some nasty surprises.</p>\n<p><strong>Percona Distribution for MongoDB is a freely available MongoDB database alternative, giving you a single solution that combines the best and most important enterprise components from the open source community, designed and tested to work together.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mongodb\" rel=\"noopener\">Download Percona Distribution for MongoDB Today!</a></p>\n","descriptionType":"html","publishedDate":"Fri, 10 Sep 2021 12:02:29 +0000","feedId":11,"bgimg":"","linkMd5":"b939b5f98c176b2d65c6cfdd64e52a4a","bgimgJsdelivr":"","metaImg":"","author":"Ivan Groenewold","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn59@2020_4/2021/09/27/16-55-28-882_f994c43eb1039982.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn56@2020_4/2021/09/27/16-55-33-670_fa7124fe9f8d6d51.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Migration of a MySQL Database to a Kubernetes Cluster Using Asynchronous Replication","link":"https://www.percona.com/blog/?p=78147","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Migration of a MySQL Database to a Kubernetes Cluster Using Asynchronous Replication\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78184\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-300x157.png\" alt=\"Migration of a MySQL Database to a Kubernetes Cluster Using Asynchronous Replication\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Nowadays, more and more companies are thinking about the migration of their infrastructure to Kubernetes. Databases are no exception. There were a lot of k8s operators that were created to simplify the different types of deployments and also perform routine day-to-day tasks like making the backups, renewing certificates, and so on.  If a few years ago nobody wanted to even listen about running databases in Kubernetes,  everything has changed now.</p>\n<p>At Percona, we created a few very featureful k8s operators for Percona Server for MongoDB, PostgreSQL, and MySQL databases. Today we will talk about using <a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/replication.html#set-up-percona-xtradb-cluster-cross-site-replication\">cross-site replication</a> &#8211; a new feature that was added to the latest release of <a href=\"https://www.percona.com/software/percona-kubernetes-operators\">Percona Distribution for MySQL Operator</a>. This feature is based on <a href=\"https://dev.mysql.com/doc/refman/8.0/en/replication-asynchronous-connection-failover.html\">synchronous connection failover mechanism</a>.<br />\nThe cross-site replication involves configuring one Percona XtraDB Cluster or a single/several MySQL servers as Source, and another Percona XtraDB Cluster (PXC) as a replica to allow asynchronous replication between them.  If an operator has several sources in custom resource (<a href=\"https://github.com/percona/percona-xtradb-cluster-operator/blob/v1.9.0/deploy/cr.yaml#L53-L55\">CR</a>), it will automatically handle connection failure of the source DB.<br />\nThis cross-site replication feature is supported only since MySQL 8.0.23, but you can read about migrating MySQL of earlier versions in this <a href=\"https://www.percona.com/blog/2021/06/14/migrating-into-kubernetes-running-the-percona-distribution-for-mysql-operator/https://www.percona.com/blog/2021/06/14/migrating-into-kubernetes-running-the-percona-distribution-for-mysql-operator/\">blog post</a>.</p>\n<h2>The Goal</h2>\n<p>Migrate the MySQL database, which is deployed on-prem or in the cloud, to the Percona Distribution for MySQL Operator using asynchronous replication. This approach helps you reduce downtime and data loss for your application.</p>\n<p>So, we have the following setup:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78153 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-1024x540.png\" alt=\"Migration of MySQL database to Kubernetes cluster using asynchronous replication\" width=\"900\" height=\"475\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-1024x540.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-300x158.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-200x106.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-1536x811.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-2048x1081.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-367x194.png 367w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><strong>The following components are used:</strong></p>\n<p><span>1. MySQL 8.0.23 database (in my case it is </span><a href=\"https://www.percona.com/software/mysql-database/percona-server\"><span>Percona Server for MySQL</span></a><span>) which is deployed in DO (as a Source) and </span><a href=\"https://www.percona.com/software/mysql-database/percona-xtrabackup\"><span>Percona XtraBackup</span></a><span> for the backup. In my test deployment, I use only one server as a Source to simplify the deployment. Depending on your topology of DB deployment, you can use several servers to use synchronous connection failover mechanism on the operator’s end.</span></p>\n<p><span>2. </span><span>Google Kubernetes Engine (GKE) cluster where </span><a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\"><span>Percona Distribution for MySQL Operator</span></a><span> is deployed with PXC cluster (as a target).</span></p>\n<p><span>3. AWS S3 bucket is used to save the backup from MySQL DB and then to restore the PXC cluster in k8s.</span></p>\n<p><strong>The following steps should be done to perform the migration procedure:</strong></p>\n<p>1. Make the MySQL database backup using <a href=\"https://www.percona.com/software/mysql-database/percona-xtrabackup\">Percona XtraBackup</a> and upload it to the S3 bucket using <a href=\"https://www.percona.com/doc/percona-xtrabackup/LATEST/xbcloud/xbcloud.html\">xbcloud</a>.</p>\n<p>2. Perform the restore of the MySQL database from the S3 bucket into the PXC cluster which is deployed in k8s.</p>\n<p>3. Configure asynchronous replication between MySQL server and PXC cluster managed by k8s operator.</p>\n<p>As a result, we have asynchronous replication between MySQL server and PXC cluster in k8s which is in read-only mode.</p>\n<h2>Migration</h2>\n<h3>Configure the target PXC cluster managed by k8s operator:</h3>\n<p>1. Deploy Percona Distribution for MySQL Operator on Kubernetes (I have used GKE 1.20).</p><pre class=\"crayon-plain-tag\"># clone the git repository\ngit clone -b v1.9.0 https://github.com/percona/percona-xtradb-cluster-operator\ncd percona-xtradb-cluster-operator\n\n# deploy the operator\nkubectl apply -f deploy/bundle.yaml</pre><p>2. Create PXC cluster using the default custom resource manifest (CR).</p><pre class=\"crayon-plain-tag\"># create my-cluster-secrets secret (do no use default passwords for production systems)\nkubectl apply -f deploy/secrets.yaml\n\n# create cluster by default it will be PXC 8.0.23\nkubectl apply -f deploy/cr.yaml</pre><p>3. Create the secret with credentials for the AWS S3 bucket which will be used for access to the S3 bucket during the restoration procedure.</p><pre class=\"crayon-plain-tag\"># create S3-secret.yaml file with following content, and use correct credentials instead of XXXXXX\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-s3-secret\ntype: Opaque\ndata:\n  AWS_ACCESS_KEY_ID: XXXXXX\n  AWS_SECRET_ACCESS_KEY: XXXXXX</pre><p></p><pre class=\"crayon-plain-tag\"># create secret\nkubectl apply -f S3-secret.yaml</pre><p></p>\n<h3>Configure the Source MySQL Server</h3>\n<p><span>1. Install Percona Server for MySQL 8.0.23 and Percona XtraBackup for the backup. Refer to the </span><a href=\"https://www.percona.com/doc/percona-server/LATEST/installation/yum_repo.html\"><span>Installing Percona Server for MySQL </span></a><span>and </span><a href=\"https://www.percona.com/doc/percona-xtrabackup/8.0/installation.html#installing-percona-xtrabackup-from-repositories\"><span>Installing Percona XtraBackup</span></a><span> chapters in the documentation for installation instructions.</span></p>\n<blockquote><p><span><br />\n</span><em><span>NOTE:</span><span><br />\n</span><span>You need to add the following options to my.cnf to enable </span><a href=\"https://dev.mysql.com/doc/refman/8.0/en/replication-options-gtids.html#sysvar_gtid_mode\"><span>GTID</span></a><span> support; otherwise, replication will not work because it is used by the PXC cluster  by default.</span></em><span><br />\n</span></p><pre class=\"crayon-plain-tag\">[mysqld]\nenforce_gtid_consistency=ON\ngtid_mode=ON</pre><p>\n</p></blockquote>\n<p>2. Create all needed users who will be used by k8s operator, the password should be the same as in <pre class=\"crayon-plain-tag\">deploy/secrets.yaml</pre>. Also, please note that the password for the root user should be the same as in deploy/secrets.yaml file for k8s the secret.  In my case, I used our default passwords from <pre class=\"crayon-plain-tag\">deploy/secrets.yaml</pre> file.</p><pre class=\"crayon-plain-tag\">CREATE USER 'monitor'@'%' IDENTIFIED BY 'monitory' WITH MAX_USER_CONNECTIONS 100;\nGRANT SELECT, PROCESS, SUPER, REPLICATION CLIENT, RELOAD ON *.* TO 'monitor'@'%';\nGRANT SERVICE_CONNECTION_ADMIN ON *.* TO 'monitor'@'%';\n\nCREATE USER 'operator'@'%' IDENTIFIED BY 'operatoradmin';\nGRANT ALL ON *.* TO 'operator'@'%' WITH GRANT OPTION;\n\nCREATE USER 'xtrabackup'@'%' IDENTIFIED BY 'backup_password';\nGRANT ALL ON *.* TO 'xtrabackup'@'%';\n\nCREATE USER 'replication'@'%' IDENTIFIED BY 'repl_password';\nGRANT REPLICATION SLAVE ON *.* to 'replication'@'%';\nFLUSH PRIVILEGES;</pre><p>2. Make the backup of MySQL database using XtraBackup tool and upload it to S3 bucket.</p><pre class=\"crayon-plain-tag\"># export aws credentials\nexport AWS_ACCESS_KEY_ID=XXXXXX\nexport AWS_SECRET_ACCESS_KEY=XXXXXX\n\n#make the backup\nxtrabackup --backup --stream=xbstream --target-dir=/tmp/backups/ --extra-lsndirk=/tmp/backups/  --password=root_password | xbcloud put --storage=s3 --parallel=10 --md5 --s3-bucket=\"mysql-testing-bucket\" \"db-test-1\"</pre><p><span>Now, everything is ready to perform the restore of the backup on the target database. So, let&#8217;s get back to our k8s cluster.</span></p>\n<h2>Configure the Asynchronous Replication to the Target PXC Cluster</h2>\n<p>If you have a completely clean source database (without any data), you can skip the points connected with backup and restoration of the database. Otherwise, do the following:</p>\n<p>1. Restore the backup from the S3 bucket using the following manifest:</p><pre class=\"crayon-plain-tag\"># create restore.yml file with following content\n\napiVersion: pxc.percona.com/v1\nkind: PerconaXtraDBClusterRestore\nmetadata:\n  name: restore1\nspec:\n  pxcCluster: cluster1\n  backupSource:\n    destination: s3://mysql-testing-bucket/db-test-1\n    s3:\n      credentialsSecret: aws-s3-secret\n      region: us-east-1</pre><p></p><pre class=\"crayon-plain-tag\"># trigger the restoration procedure\nkubectl apply -f restore.yml</pre><p>As a result, you will have a PXC cluster with data from the source DB. Now everything is ready to configure the replication.</p>\n<p><span>2. Edit custom resource manifest <pre class=\"crayon-plain-tag\">deploy/cr.yaml</pre> </span><span> to configure <pre class=\"crayon-plain-tag\">spec.pxc.replicationChannels</pre> </span><span>section.</span></p><pre class=\"crayon-plain-tag\">spec:\n  ...\n  pxc:\n    ...\n    replicationChannels:\n    - name: ps_to_pxc1\n      isSource: false\n      sourcesList:\n        - host: &#60;source_ip&#62;\n          port: 3306\n          weight: 100</pre><p></p><pre class=\"crayon-plain-tag\"># apply CR\nkubectl apply -f deploy/cr.yaml</pre><p></p>\n<h2><span><br />\n<strong>Verify the Replication </strong></span></h2>\n<p><span>In order to check the replication, you need to connect to the PXC node and run the following commands:</span></p><pre class=\"crayon-plain-tag\">kubectl exec -it cluster1-pxc-0 -c pxc -- mysql -uroot -proot_password -e \"show replica status \\G\"\n*************************** 1. row ***************************\n               Slave_IO_State: Waiting for master to send event\n                  Master_Host: &#60;ip-of-source-db&#62;\n                  Master_User: replication\n                  Master_Port: 3306\n                Connect_Retry: 60\n              Master_Log_File: binlog.000004\n          Read_Master_Log_Pos: 529\n               Relay_Log_File: cluster1-pxc-0-relay-bin-ps_to_pxc1.000002\n                Relay_Log_Pos: 738\n        Relay_Master_Log_File: binlog.000004\n             Slave_IO_Running: Yes\n            Slave_SQL_Running: Yes\n              Replicate_Do_DB:\n          Replicate_Ignore_DB:\n           Replicate_Do_Table:\n       Replicate_Ignore_Table:\n      Replicate_Wild_Do_Table:\n  Replicate_Wild_Ignore_Table:\n                   Last_Errno: 0\n                   Last_Error:\n                 Skip_Counter: 0\n          Exec_Master_Log_Pos: 529\n              Relay_Log_Space: 969\n              Until_Condition: None\n               Until_Log_File:\n                Until_Log_Pos: 0\n           Master_SSL_Allowed: No\n           Master_SSL_CA_File:\n           Master_SSL_CA_Path:\n              Master_SSL_Cert:\n            Master_SSL_Cipher:\n               Master_SSL_Key:\n        Seconds_Behind_Master: 0\nMaster_SSL_Verify_Server_Cert: No\n                Last_IO_Errno: 0\n                Last_IO_Error:\n               Last_SQL_Errno: 0\n               Last_SQL_Error:\n  Replicate_Ignore_Server_Ids:\n             Master_Server_Id: 1\n                  Master_UUID: 9741945e-148d-11ec-89e9-5ee1a3cf433f\n             Master_Info_File: mysql.slave_master_info\n                    SQL_Delay: 0\n          SQL_Remaining_Delay: NULL\n      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates\n           Master_Retry_Count: 3\n                  Master_Bind:\n      Last_IO_Error_Timestamp:\n     Last_SQL_Error_Timestamp:\n               Master_SSL_Crl:\n           Master_SSL_Crlpath:\n           Retrieved_Gtid_Set: 9741945e-148d-11ec-89e9-5ee1a3cf433f:1-2\n            Executed_Gtid_Set: 93f1e7bf-1495-11ec-80b2-06e6016a7c3d:1,\n9647dc03-1495-11ec-a385-7e3b2511dacb:1-7,\n9741945e-148d-11ec-89e9-5ee1a3cf433f:1-2\n                Auto_Position: 1\n         Replicate_Rewrite_DB:\n                 Channel_Name: ps_to_pxc1\n           Master_TLS_Version:\n       Master_public_key_path:\n        Get_master_public_key: 0\n            Network_Namespace:</pre><p>Also, you can verify the replication by checking that the data is changing.</p>\n<h2>Promote the PXC Cluster as a Primary</h2>\n<p>As soon as you are ready (your application was reconfigured and ready to work with the new DB) to stop the replication and promote the PXC cluster in k8s to be a primary DB, you need to perform the following simple actions:</p>\n<p><span>1. Edit the <pre class=\"crayon-plain-tag\">deploy/cr.yaml</pre> </span><span> </span><span>and comment the </span><span>replicationChannels</span></p><pre class=\"crayon-plain-tag\">spec:\n  ...\n  pxc:\n    ...\n    #replicationChannels:\n    #- name: ps_to_pxc1\n    #  isSource: false\n    #  sourcesList:\n    #    - host: &#60;source_ip&#62;\n    #      port: 3306\n    #      weight: 100</pre><p>2. Stop mysqld service on the source server to be sure that no new data is written.</p><pre class=\"crayon-plain-tag\"> systemctl stop mysqld</pre><p>3. Apply a new CR for k8s operator.</p><pre class=\"crayon-plain-tag\"># apply CR\nkubectl apply -f deploy/cr.yaml</pre><p>As a result, replication is stopped and the read-only mode is disabled for the PXC cluster.</p>\n<h3>Conclusion</h3>\n<p><span>Technologies are changing so fast that a migration procedure to k8s cluster, seeming very complex at first sight, turns out to be not so difficult and nor time-consuming. But you need to keep in mind that significant changes were made. Firstly, you migrate the database to the PXC cluster which has some peculiarities, and, of course, Kubernetes itself.  If you are ready, you can start the journey to Kubernetes right now.</span><span><br />\n</span><span><br />\n</span><span>The Percona team is ready to guide you during this journey. If you have any questions,  please raise the topic in the </span><a href=\"https://forums.percona.com/c/mysql-mariadb/percona-kubernetes-operator-for-mysql/28\"><span>community forum</span></a><span>.</span></p>\n<p><strong>The Percona Kubernetes Operators automate the creation, alteration, or deletion of members in your Percona Distribution for MySQL, MongoDB, or PostgreSQL environment. </strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/percona-kubernetes-operators\" rel=\"noopener\">Learn More About Percona Kubernetes Operators</a></p>\n","descriptionType":"html","publishedDate":"Fri, 17 Sep 2021 13:44:27 +0000","feedId":11,"bgimg":"","linkMd5":"4a519ab404d3f0653d0e6ea80e3a025b","bgimgJsdelivr":"","metaImg":"","author":"Slava Sarzhan","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn75@2020_4/2021/09/27/16-55-31-296_cbf45e530c2a40aa.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn12@2020_3/2021/09/27/16-55-33-880_d5195d28f2dd5e99.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-1024x540.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn23@2020_6/2021/09/27/16-55-30-827_8a8e426888f9f941.webp"},"publishedOrCreatedDate":1632761712736},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Dynamic User Creation with MySQL on Kubernetes and Hashicorp Cloud Platform Vault","link":"https://www.percona.com/blog/?p=77794","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MySQL Kubernetes Hashicorp Cloud\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p>You may have already seen this <a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/encryption.html\">document</a> which describes the integration between <a href=\"https://www.hashicorp.com/products/vault\">HashiCorp Vault</a> and <a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Percona Distribution for MySQL Operator</a> to enable data-at-rest encryption for self-managed Vault deployments.  In April 2021, HashiCorp announced a fully managed offering, <a href=\"https://cloud.hashicorp.com/\">HashiCorp Cloud Platform Vault</a> (HCP Vault), that simplifies deployment and management of the Vault.</p>\n<p>With that in mind, I’m going to talk about the integration between Percona and HCP Vault to provide dynamic user creation for MySQL.</p>\n<p>Without dynamic credentials, organizations are susceptible to a breach due to secrets sprawl across different systems, files, and repositories. Dynamic credentials provide a secure way of connecting to the database by using a unique password for every login or service account. With Vault, these just-in-time credentials are stored securely and it is also possible to set a lifetime for them.</p>\n<h2>Goal</h2>\n<p>My goal would be to provision users on my MySQL cluster deployed in Kubernetes with dynamic credentials through Hashicorp Vault.</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77801 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0.png\" alt=\"MySQL cluster deployed in Kubernetes with dynamic credentials through Hashicorp Vault\" width=\"924\" height=\"528\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0.png 924w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0-300x171.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0-200x114.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0-367x210.png 367w\" sizes=\"(max-width: 924px) 100vw, 924px\" /></p>\n<ol>\n<li aria-level=\"1\">Percona Operator deploys Percona XtraDB Cluster and HAProxy</li>\n<li aria-level=\"1\">HashiCorp Vault connects to MySQL through HAProxy and creates users with specific grants</li>\n<li aria-level=\"1\">Application or user can connect to myapp database using dynamic credentials created by vault</li>\n</ol>\n<h2>Before You Begin</h2>\n<h3>Prerequisites</h3>\n<ul>\n<li aria-level=\"1\">HCP Vault account</li>\n<li aria-level=\"1\">Kubernetes cluster</li>\n</ul>\n<h3>Networking</h3>\n<p>Right now HCP deploys Vault in Hashicorp’s Amazon account in a private Virtual Private Network. For now to establish a private connection between the Vault and your application you would need to have an AWS account, VPC, and either a peering or Transit Gateway connection:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77800 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-1024x395.png\" alt=\"\" width=\"900\" height=\"347\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-1024x395.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-300x116.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-200x77.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-367x142.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1.png 1254w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p><span>For the sake of simplicity in this blog post, I’m going to expose the Vault publicly, which is not recommended for production but allows me to configure Vault from anywhere.</span></p>\n<p><span>More clouds and networking configurations are on the way. Stay tuned to </span><a href=\"https://www.hashicorp.com/blog\"><span>HashiCorp news</span></a><span>.</span></p>\n<h2>Set it All Up</h2>\n<h3>MySQL</h3>\n<p>To deploy Percona Distribution for MySQL on Kubernetes please follow our <a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/kubernetes.html\">documentation</a>. The only requirement is to have HAProxy exposed via a public load balancer. The following fields should be set correctly in the Custom Resource &#8211;<span> <pre class=\"crayon-plain-tag\">deploy/cr.yaml</pre>:</span></p>\n<p>For simplicity, I have shared two required YAMLs in <a href=\"https://github.com/spron-in/blog-data/tree/master/hcp-mysql-kubernetes\">this</a> GitHub repository. Deploying them would provision the Percona XtraDB Cluster on Kubernetes exposed publicly:</p><pre class=\"crayon-plain-tag\">kubectl apply -f bundle.yaml\nkubectl apply -f cr.yaml</pre><p>Once the cluster is ready, get the public address:</p><pre class=\"crayon-plain-tag\">$ kubectl get pxc\nNAME       ENDPOINT       STATUS   PXC   PROXYSQL   HAPROXY   AGE\ncluster1   35.223.41.79   ready    3                3         4m43s</pre><p>Remember the ENDPOINT address, we will need to use it below to configure HCP Vault.</p>\n<h3>Create the User and the Database</h3>\n<p>I’m going to create a MySQL user which is going to be used by HCP Vault to create users dynamically. Also, an empty database called ‘myapp’ to which these users are going to have access.</p>\n<p>Get the current root password from the Secret object:</p><pre class=\"crayon-plain-tag\">$ kubectl get secrets my-cluster-secrets -o yaml | awk '$1~/root/ {print $2}' | base64 --decode &#38;&#38; echo\nJw6OYIsUJeAQQapk</pre><p>Connect to MySQL directly or by executing into the container:</p><pre class=\"crayon-plain-tag\">kubectl exec -ti cluster1-pxc-0 -c pxc bash\nmysql -u root -p -h 35.223.41.79</pre><p>Create the database user and the database:</p><pre class=\"crayon-plain-tag\">mysql&#62; create user hcp identified by 'superduper';\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql&#62; grant select, insert, update, delete, drop, create, alter, create user on *.* to hcp with grant option;\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql&#62; flush privileges;\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql&#62; create database myapp;\nQuery OK, 1 row affected (0.02 sec)</pre><p></p>\n<h2>Hashicorp Cloud Platform Vault</h2>\n<p>Setting up Vault on HCP is a few-click process that is described <a href=\"https://learn.hashicorp.com/tutorials/cloud/get-started-vault\">here</a>.</p>\n<p>As I mentioned before, for the sake of simplicity HCP Vault is going to be publicly accessible. To do that, go to your Vault cluster in HCP UI, click <em>Manage</em> and <em>Edit Configuration</em>:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77799 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-1024x227.png\" alt=\"vault cluster\" width=\"900\" height=\"200\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-1024x227.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-300x67.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-200x44.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-1536x341.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-367x82.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2.png 1562w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p>Enable the knob to expose the cluster publicly:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77798 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3.png\" alt=\"configure cluster\" width=\"649\" height=\"372\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3.png 649w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3-300x172.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3-200x115.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3-367x210.png 367w\" sizes=\"(max-width: 649px) 100vw, 649px\" /></p>\n<p>Now let&#8217;s get the Admin token for Vault. Navigate to your overview dashboard of your Vault cluster and click <em>Generate token</em>:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-77797 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4.png\" alt=\"\" width=\"518\" height=\"365\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4.png 518w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4-300x211.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4-200x141.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4-367x259.png 367w\" sizes=\"(max-width: 518px) 100vw, 518px\" /></p>\n<h3>Vault CLI</h3>\n<p>The Vault is publicly accessible and you have the Admin token. Let&#8217;s configure it with the vault CLI tool. Install it by following the manual <a href=\"https://learn.hashicorp.com/tutorials/vault/getting-started-install\">here</a>.</p>\n<p>Try to log in:</p><pre class=\"crayon-plain-tag\">export VAULT_ADDR=”https://vault-cluster.SOMEURL.hashicorp.cloud:8200”\nexport VAULT_NAMESPACE=\"admin\"\n\nvault login\nToken (will be hidden):\nSuccess! You are now authenticated. The token information displayed below\nis already stored in the token helper. You do NOT need to run \"vault login\"\nagain. Future Vault requests will automatically use this token.\n...</pre><p></p>\n<h2>Connecting the Dots</h2>\n<p>It is time to connect Vault with the MySQL database in Kubernetes and start provisioning users. We are going to rely on <a href=\"https://www.vaultproject.io/docs/secrets/databases\">Vault’s Databases Secrets engine</a>.</p>\n<p>1. Enable database secrets engine:</p><pre class=\"crayon-plain-tag\">vault secrets enable database</pre><p>2. Point Vault to MySQL and store the configuration:</p><pre class=\"crayon-plain-tag\">vault write database/config/myapp plugin_name=mysql-database-plugin \\\nconnection_url=”{{username}}:{{password}}@tcp(35.223.41.79:3306)/” \\\nallowed_roles=”mysqlrole” \\\nusername=”hcp” \\\npassword=”superduper”\nSuccess! Data written to: database/config/myapp</pre><p>3. Create the role:</p><pre class=\"crayon-plain-tag\">vault write database/roles/mysqlrole db_name=myapp \\\ncreation_statements=”CREATE USER ‘{{name}}’@’%’ IDENTIFIED BY ‘{{password}}’; GRANT select, insert, update, delete, drop, create, alter ON myapp.* TO ‘{{name}}’@’%’;” \\\ndefault_ttl=”1h” \\\nmax_ttl=”24h”\nSuccess! Data written to: database/roles/mysqlrole</pre><p>This role does the following:</p>\n<ul>\n<li aria-level=\"1\">Creates the user with a random name and password</li>\n<li aria-level=\"1\">The user has grants to myapp database</li>\n<li aria-level=\"1\">By default, the user exists for one hour, but time-to-live can be extended to 24 hours.</li>\n</ul>\n<p>Now to create the temporary user just execute the following:</p><pre class=\"crayon-plain-tag\">vault read database/creds/mysqlrole\nKey                Value\n---                -----\nlease_id           database/creds/mysqlrole/MpO5oMsd1A0uyXT8d7R6sxVe.slbaC                                                                                   lease_duration     1h\nlease_renewable    true\npassword           Gmx6fv89BL4qHbFokG-p\nusername           v-token-hcp--mysqlrole-EMt7xeECd</pre><p>It is now possible to connect to myapp database using the credentials provided above.</p>\n<h3>Conclusion</h3>\n<p>Dynamic credentials can be an essential part of your company&#8217;s security framework to avoid a breach due to secrets sprawl, data leaks, and maintain data integrity and consistency. You can similarly integrate HashiCorp Vault with any Percona Kubernetes Operator &#8211; for MongoDB, MySQL, and PostgreSQL.</p>\n<p>We encourage you to try it out to keep your data safe. Let us know if you faced any issues by submitting the topic to our <a href=\"https://forums.percona.com/\">Community Forum</a>.</p>\n<p><strong>Percona Distribution for MySQL Operator</strong></p>\n<p>The <a href=\"https://www.percona.com/doc/kubernetes-operator-for-pxc/index.html\">Percona Distribution for MySQL Operator</a> simplifies running Percona XtraDB Cluster on Kubernetes and provides automation for day-1 and day-2 operations. It&#8217;s based on the Kubernetes API and enables highly available environments. Regardless of where it is used, the Operator creates a member that is identical to other members created with the same Operator. This provides an assured level of stability to easily build test environments or deploy a repeatable, consistent database environment that meets Percona expert-recommended best practices.</p>\n<p><strong>Hashicorp Vault</strong></p>\n<p><a href=\"https://www.hashicorp.com/products/vault\">Hashicorp Vault</a> is an identity-based security solution that secures, stores, and tightly controls access to tokens, passwords, and other secrets with both open-source and enterprise offerings for self-managed security automation. In April 2021, HashiCorp announced a fully managed offering, <a href=\"https://cloud.hashicorp.com/\">HashiCorp Cloud Platform Vault</a> (HCP Vault), that simplifies deployment and management of the Vault.</p>\n","descriptionType":"html","publishedDate":"Thu, 19 Aug 2021 15:07:54 +0000","feedId":11,"bgimg":"","linkMd5":"f9d07f2139995f2c0bf057c6526f3e9e","bgimgJsdelivr":"","metaImg":"","author":"Sergey Pronin","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn20@2020_2/2021/09/27/16-55-31-768_f7702cda37f47992.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn27@2020_6/2021/09/27/16-55-33-838_fc711fd0a266a755.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-1024x395.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn3@2020_5/2021/09/27/16-55-33-427_b49b24bc9c32e527.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-1024x227.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn99@2020_1/2021/09/27/16-55-30-632_974f67deeaec1197.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn63@2020_6/2021/09/27/16-55-14-030_ffd6a2cc0cb038af.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn99@2020_4/2021/09/27/16-55-25-699_961658467a02b4bf.webp"},"publishedOrCreatedDate":1632761712748},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Q&A on Webinar “Using Open Source Software to Optimize and Troubleshoot Your MySQL Environment”","link":"https://www.percona.com/blog/?p=78046","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Optimize and Troubleshoot Your MySQL Environment\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78050\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-300x168.png\" alt=\"Optimize and Troubleshoot Your MySQL Environment\" width=\"300\" height=\"168\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Thanks to everyone who attended last week’s webinar on <strong>Using Open Source Software to Optimize and Troubleshoot Your MySQL Environment</strong>; hopefully you’ve found the time we spent in <a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) useful.</p>\n<p>We had a record-breaking number of questions during the talk and unfortunately weren’t able to answer them all live, so we decided to answer them separately. Also, there were several requests for best practices around installation and configuration. This is something we are considering for the next webinar in this series, so stay tuned!</p>\n<p>If you weren’t able to attend, the <a href=\"https://learn.percona.com/using-open-source-software-to-optimize-and-troubleshoot-your-mysql-environment\">recording</a> is available for viewing. But now, without further ado, here are the questions that we didn’t have time to cover during the presentation.</p>\n<p>&#160;</p>\n<p><strong>Q: Can PMM also be used for a web hosting server (Cpanel, Directadminetc)?</strong></p>\n<p>PMM by default can monitor a node to provide vital statistics on the health of the host.  From there, you can use <a href=\"https://github.com/ralfonso-directnic/cpanel_exporter\">external exporters </a>to monitor other applications and send the data to PMM to visualize and create alerts.</p>\n<p>&#160;</p>\n<p><strong>Q: Does it provide any query optimization suggestions if my query is bad? </strong></p>\n<p>Not at present&#8230;that’s planned for the future query advisor</p>\n<p>&#160;</p>\n<p><strong>Q: How soon we will be able to use the alerting manager in production?</strong></p>\n<p>We are looking at late Sept to early Oct. When it’s ready, you will hear about it!</p>\n<p>&#160;</p>\n<p><strong>Q: Capturing Queries Data for performance checking can be costly and some monitoring systems capture data every few seconds. At what level of data is captured here and analyzed&#8230;live systems with lots of database traffic? What percentage (all of it,  2 seconds, 1 second, etc.)?</strong></p>\n<p>We adhere to ‘do no harm&#8217; so the impact of PMM  is typically 1-4% of the busiest systems.  We offer custom resolutions to adjust the scrape frequency to balance the need for information with the need for performance.</p>\n<p>&#160;</p>\n<p><strong>Q: Are long-running queries captured that potentially slow down the system over time &#38; shown as graph/alert? Also, is there potentially more than one instance of these types running over again by a user.?</strong></p>\n<p>This is something we are going to include in our Alerting capabilities (coming soon, see above).</p>\n<p>&#160;</p>\n<p><strong>Q: Can more than one of the metrics be compared against each other to gain more insight into a problem in graphical form? Can you in effect play with these graphs?</strong></p>\n<p>Yes, you can, this is in fact how most of the dashboards are designed, where we connect different metric series together to drive graphs that explain system performance.  While you may be able to edit the existing graphs, Percona recommends that you instead make a copy of the dashboard you&#8217;d like to modify and make your changes on the copy.  The reason for this is if you modify a dashboard distributed by PMM, it will be overwritten on the next upgrade, and you&#8217;ll lose your changes.</p>\n<p>&#160;</p>\n<p><strong>Q: Could you list what can be monitored using PMM? And explain what recommended plugins are available and what they are used for? </strong></p>\n<p>Natively, any Linux system and pretty much all flavors of MySQL, MariaDB, MongoDB, and PostgreSQL. You can use external exporters to gather even more data than default and using Grafana as the basis for visualization of PMM allows you to create custom dashboards and a wealth of community plugins.</p>\n<p>&#160;</p>\n<p><strong>Q: Can you choose to monitor a particular set of users? Set of queries? Set of schema? </strong></p>\n<p>You can filter it down to view based on username, particular schema, and then filter those results by particular query strings.  We can monitor as much or as little about your database as the user you define to pull data.</p>\n<p>&#160;</p>\n<p><strong>Q: How can we work on optimization when using cloud-based services like RDS where we have limited access?</strong></p>\n<p>PMM can monitor RDS instances and has simplified the connection and selection process of its remote monitoring capabilities.  We can provide nearly the same data as an on-prem database however we don’t have access to the node level statistics.</p>\n<p>&#160;</p>\n<p><strong>Q: For Oracle MySQL 5.7.29, if you have many tables/objects in the database, will the PMM query information_schema and load the DB?</strong></p>\n<p>We have a predefined limit of 1000 tables that will disable polling information schema but you can configure this to your liking both with the client and with remote monitoring. This CAN have a more significant impact on your system though especially with large table and row counts.</p>\n<p>&#160;</p>\n<p><strong>Q: At what point do I know I&#8217;ve done enough optimization? </strong></p>\n<p>HA! It’s a never-ending game of cat and mouse considering the sheer volume of variables in play. It’s these times where monitoring data for before and after become vital.</p>\n<p>&#160;</p>\n<p><strong>Q: Can a database monitoring package be the source of database performance issues? In particular, mysqld_exporter is installed as a docker container, as I&#8217;m seeing &#8220;out of resources&#8221; on a trace on mysqld_exporter.</strong></p>\n<p>Of course, there are plenty of ways to generate database performance issues and it’s possible monitoring can result in some overhead. For an extreme example, here’s one way to replicate some overhead: start the pmm-client on a MySQL database and restore a blank DB from mysqldump. A few million rows at a time should generate LOTS of chaos and load between QAN and exporters. Our pmm client runs the exporter natively so no need to use a container.</p>\n<p>&#160;</p>\n<p><strong>Q: Is the query analytics somehow slowing down the database server as well? Or is it save to enable/use it without further impact?</strong></p>\n<p>The impact is minimal.  Most of the Query Analytics processing is done at the PMM server, the only impact to the client is retrieving the queries from slowlog or performance schema so this can have a bigger impact for the most extremely active DB’s but still should remain below 5% CPU hit.</p>\n<p>&#160;</p>\n<p><strong>Q: Did I understand correctly that PMM is not for RDS users and that AWS tools are available?</strong></p>\n<p>PMM certainly is for RDS! Since RDS is managed by AWS, PMM cannot collect CPU/Disk/Memory metrics but all MySQL metrics are still available even in RDS.</p>\n<p>&#160;</p>\n<p><strong>Q: Do you have any instructions/steps to install PMM to monitor MySQL RDS? </strong></p>\n<ul>\n<li aria-level=\"1\">Gear icon → PMM Inventory → Add Instance</li>\n<li aria-level=\"1\">Choose AWS/RDS Add Remote Instance</li>\n<li aria-level=\"1\">Use your AWS credentials to view your available RDS &#38; Aurora nodes</li>\n<li aria-level=\"1\">Ensure that performance_schema is enabled</li>\n</ul>\n<p>&#160;</p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://learn.percona.com/using-open-source-software-to-optimize-and-troubleshoot-your-mysql-environment\" rel=\"noopener\">Watch the Recording</a></p>\n","descriptionType":"html","publishedDate":"Thu, 09 Sep 2021 12:29:11 +0000","feedId":11,"bgimg":"","linkMd5":"4e76fe7de77593ea1899702f180afb79","bgimgJsdelivr":"","metaImg":"","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn91@2020_6/2021/09/27/16-55-31-887_478d41b283e9ee23.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-300x168.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn80@2020_6/2021/09/27/16-55-28-375_d242e2aa4836364a.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Congratulating Marcelo Altmann on his Promotion to Oracle ACE!","link":"https://www.percona.com/blog/?p=77972","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Marcelo Altmann promotion to Oracle ACE\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77979\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-300x157.png\" alt=\"Marcelo Altmann promotion to Oracle ACE\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />We’re excited to share that <a href=\"https://blog.marceloaltmann.com/\">Marcelo Altmann</a> from the Percona Server Engineering Team has just been promoted from Oracle ACE Associate to <a href=\"https://developer.oracle.com/ace/\">Oracle ACE</a>.</p>\n<p><strong>Congratulations!</strong></p>\n<p>The Oracle ACE Program recognizes and rewards community members for their technical contributions to the Oracle community.</p>\n<p>Marcelo initially joined Percona as a senior support engineer in our global services organization, where he helped customers with running their MySQL-based environments. In early 2020, he joined our Server Engineering team and has been actively involved in the development of <a href=\"https://www.percona.com/software/mysql-database/percona-server\">Percona Server for MySQL</a> and <a href=\"https://www.percona.com/software/mysql-database/percona-xtrabackup\">Percona XtraBackup</a> since then.</p>\n<p>Marcelo’s contributions to the MySQL ecosystem are countless &#8211; he’s an <a href=\"https://www.percona.com/blog/author/marcelo-altmann/\">active blogger</a>, he regularly <a href=\"https://bugs.mysql.com/search.php?cmd=display&#38;status=All&#38;severity=all&#38;reporter=5522392\">submits bug reports to the MySQL team</a>, organizes local and <a href=\"https://www.youtube.com/c/MyHandsOn/videos\">virtual meetups</a>, and also contributes patches and bug fixes.</p>\n<p>Congratulations again, Marcelo, we’re proud to have you on our team!</p>\n","descriptionType":"html","publishedDate":"Wed, 01 Sep 2021 12:07:46 +0000","feedId":11,"bgimg":"","linkMd5":"d5045f92ffbef5c18457d10a4a674bd8","bgimgJsdelivr":"","metaImg":"","author":"Lenz Grimmer","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn12@2020_3/2021/09/27/16-55-32-322_15b0f2136ba2054d.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn36@2020_5/2021/09/27/16-55-30-440_094639bd6412d0d5.webp"},"publishedOrCreatedDate":1632761712756},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Modern Web-Based Application Architecture 101","link":"https://www.percona.com/blog/?p=78201","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Modern Web-Based Application Architecture 101\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78238\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-300x157.png\" alt=\"Modern Web-Based Application Architecture 101\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />This article is meant to provide a high-level overview of how a web-based application is commonly structured nowadays. Keep in mind the topic presented is very simplified. It is meant as an introduction only and hopefully encourages the reader to investigate some of the concepts in more depth.</p>\n<h2>Monolith vs Microservices</h2>\n<p>With the rise of containerization and <a href=\"https://microservices.io/\">microservices</a>, applications nowadays tend to be composed of loosely coupled services that interact with each other. For example, an e-commerce site would have separate &#8220;microservices&#8221; for things like orders, questions, payments, and so on.</p>\n<p>In addition to this, data is oftentimes geographically distributed in an effort to bring it closer to the end-user.</p>\n<p>For example, instead of having a single database backend, we can have N smaller database backends distributed across different regions. Each of these would hold a portion of the data set related to the users located closer to it.</p>\n<h2>The Many Faces of Caching</h2>\n<p>As traffic grows, eventually the database + application servers combination is no longer the best cost-effective way to scale. Instead, we can do so by introducing some additional elements, each of these targeting a specific part of the user experience.</p>\n<p>Here&#8217;s how things would look like from a single microservice&#8217;s perspective:</p>\n<p><img loading=\"lazy\" class=\"aligncenter wp-image-78202 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM.png\" alt=\"Modern Application Architecture\" width=\"641\" height=\"422\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM.png 641w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM-300x198.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM-200x132.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM-367x242.png 367w\" sizes=\"(max-width: 641px) 100vw, 641px\" /></p>\n<p>Starting from what&#8217;s closest to the end-user, let&#8217;s briefly describe all these components.</p>\n<h4>Content Delivery Networks</h4>\n<p>You can think of the content delivery network (CDN) as a geo-distributed network of proxies with the goal of improving the response times for the end-user.</p>\n<p>CDN were traditionally used to cache &#8220;static&#8221; assets like web objects, downloadable files, streaming media, etc. Nowadays they can also be used to deliver some dynamic content as well.</p>\n<p>Popular providers include Akamai, Cloudflare, and Fastly.</p>\n<h4>HTTP Caches</h4>\n<p>The HTTP cache or web accelerator layer is typically deployed between the web servers and the CDN.</p>\n<p>Their purpose is to reduce the access times using a variety of techniques (like caching, prefetching, compression, etc) while also reducing resource utilization on the application servers.</p>\n<p>For example, Varnish is a web application accelerator for content-heavy dynamic websites that caches HTTP requests and functions as a reverse proxy. Other examples include nginx and squid.</p>\n<h4>Database Caches</h4>\n<p>Next comes the database cache layer. This usually consists of an in-memory key/value store that stores results of read database queries, allowing to scale reads without introducing additional database servers.</p>\n<p>Cache invalidation can be performed explicitly from the application or simply by defining a TTL for each object.</p>\n<p>One important point to consider in regards to caching is that there might be application flows that require strict read-after-write semantics.</p>\n<p>Some commonly used solutions for database caching are Redis and Memcached.</p>\n<h2>The Data Layer</h2>\n<p>Finally, the data layer can include online transaction processing (OLTP) and analytics/reporting (DW) components.</p>\n<p>To scale the database, we have to consider reads and writes separately. We can scale reads by introducing replicas, while for writes we can do data partitioning or sharding.</p>\n<p>For the data warehouse, there are many possibilities, even using a general-purpose database like MySQL. One might want to also consider a columnar storage, which is typically better for analytic queries (e.g. Clickhouse).</p>\n<p>The data warehouse is refreshed periodically via an ETL process. Some of the available solutions for ETL include Fivetran and Debezium.</p>\n<h2>Queues</h2>\n<p>There are some special considerations for write-intensive services. For example, data sources sending a continuous stream of data, or things like incoming orders which might have &#8220;spikes&#8221; at certain times.</p>\n<p>It is a common pattern to deploy a queueing system (like Activemq), or a more complex streams-processing system (like Apache Kafka) in front of the database layer.</p>\n<p>The queue acts as a &#8220;buffer&#8221; for the incoming data sent by the application. Since we can control the amount of write activity the queue does, we avoid overloading the database when there is a spike of requests.</p>\n<h3>Final Words</h3>\n<p>These are just some of the patterns used to architect modern web applications in a cost-effective and scalable way, some of the challenges frequently encountered, and a few of the available solutions to help overcome them.</p>\n","descriptionType":"html","publishedDate":"Wed, 22 Sep 2021 11:59:59 +0000","feedId":11,"bgimg":"","linkMd5":"3beda44dd1fc5760fd7b9f35eabacc9b","bgimgJsdelivr":"","metaImg":"","author":"Ivan Groenewold","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn33@2020_2/2021/09/27/16-55-25-571_1d36f819f68d7416.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn20@2020_5/2021/09/27/16-55-24-187_73cfc11b6a74458b.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn72@2020_2/2021/09/27/16-55-29-826_45b2aca5a67fec68.webp"},"publishedOrCreatedDate":1632761712756},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Percona Distribution for PostgreSQL Updates, Improvements to Percona XtraBackup: Release Roundup September 13, 2021","link":"https://www.percona.com/blog/?p=77995","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Software Update Sept 13 2021\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021.png 712w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2>It&#8217;s release roundup time again here at Percona!</h2>\n<p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78096\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-300x169.png\" alt=\"Percona Software Update Sept 13 2021\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-367x206.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021.png 712w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Percona is a leading provider of unbiased open source database solutions that allow organizations to easily, securely, and affordably maintain business agility, minimize risks, and stay competitive.</p>\n<p>Our Release Roundups <span class=\"s1\">showcase the latest Percona software updates, tools, and features to help you manage and deploy our software. It offers</span> highlights and critical information, as well as links to the full release notes and direct links to the software or service itself to download.</p>\n<p>Today&#8217;s post includes those releases and updates that have come out since August 30, 2021. Take a look!</p>\n<p>&#160;</p>\n<h2>Percona Distribution for PostgreSQL (11.13, 12.8 and 13.4)</h2>\n<p>On September 9, 2021, we released updates of <a href=\"https://www.percona.com/software/postgresql-distribution\">Percona Distribution for PostgreSQL</a>.  It provides the best and most critical enterprise components from the open-source community, in a single distribution, designed and tested to work together. Patroni, pgBackRest, pg_repack, and pgaudit are amongst the innovative components we utilize.</p>\n<p><a href=\"https://www.percona.com/doc/postgresql/11/release-notes-v11.13.html\">Percona Distribution for PostgreSQL 11.13 Release Notes</a><br />\n<a href=\"https://www.percona.com/downloads/percona-postgresql-11/LATEST/\">Download Percona Distribution for PostgreSQL 11.13</a></p>\n<p><a href=\"https://www.percona.com/doc/postgresql/12/release-notes-v12.8.html\">Percona Distribution for PostgreSQL 12.8 Release Notes</a><br />\n<a href=\"https://www.percona.com/downloads/postgresql-distribution-12/LATEST/\">Download Percona Distribution for PostgreSQL 12.8</a></p>\n<p><a href=\"https://www.percona.com/doc/postgresql/LATEST/release-notes-v13.4.html\">Percona Distribution for PostgreSQL 13.4 Release Notes</a><br />\n<a href=\"https://www.percona.com/downloads/postgresql-distribution-13/LATEST/#\">Download Percona Distribution for PostgreSQL 13.4</a></p>\n<p>&#160;</p>\n<h2>Percona XtraBackup 8.0.26-18.0</h2>\n<p>On September 2, 2021, we released <a href=\"https://www.percona.com/doc/percona-xtrabackup/LATEST/release-notes/8.0/8.0.26-18.0.html\">Percona XtraBackup 8.0.26-18.0</a>. It enables MySQL backups without blocking user queries, making it ideal for companies with large data sets and mission-critical applications that cannot tolerate long periods of downtime. Along with some bug fixes, this release features some improvements, including xbcloud should retry on an error and utilize incremental backoff (Thanks to Baptiste Mille-Mathias for reporting this issue), and removal of the obsolete LOCKLESS binary log functionality since the <code class=\"docutils literal\"><span class=\"pre\">performance_schema.log_status</span></code> table is now used to get the log information on all the storages without locking them.</p>\n<p><a href=\"https://www.percona.com/software/mysql-database/percona-xtrabackup\">Download Percona XtraBackup 8.0.26-18.0</a></p>\n<p>&#160;</p>\n<p>That&#8217;s it for this roundup, and be sure to <a href=\"https://twitter.com/Percona\" target=\"_blank\" rel=\"noopener\">follow us on Twitter</a> to stay up-to-date on the most recent releases! Percona is a leader in providing best-of-breed enterprise-class support, consulting, managed services, training, and software for MySQL, MongoDB, PostgreSQL, MariaDB, and other open source databases in on-premises and cloud environments.</p>\n","descriptionType":"html","publishedDate":"Mon, 13 Sep 2021 12:04:43 +0000","feedId":11,"bgimg":"","linkMd5":"6562f34bba189c0b252dfb95abb799ec","bgimgJsdelivr":"","metaImg":"","author":"David Quilty","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn16@2020_3/2021/09/27/16-55-35-254_53bce3ac7480c45f.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn11@2020_4/2021/09/27/16-55-26-017_21ce9097030c2e0b.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"MyDumper 0.11.1 is Now Available","link":"https://www.percona.com/blog/?p=78127","description":"<img width=\"200\" height=\"113\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-200x113.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"MyDumper 0.11.1\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-200x113.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-1024x576.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-1536x864.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-2048x1152.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-367x206.png 367w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><span><img loading=\"lazy\" class=\"alignright size-medium wp-image-78212\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-300x169.png\" alt=\"MyDumper 0.11.1\" width=\"300\" height=\"169\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-300x169.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-1024x576.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-200x113.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-1536x864.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-2048x1152.png 2048w, https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-367x206.png 367w\" sizes=\"(max-width: 300px) 100vw, 300px\" />The new</span><a href=\"https://github.com/maxbube/mydumper/releases/tag/v0.11.1\"><span> MyDumper 0.11.1</span></a><span> version, which includes many new features and bug fixes, is now available.  You can download the code from </span><a href=\"https://github.com/maxbube/mydumper/archive/refs/tags/v0.11.1.tar.gz\"><span>here</span></a><span>.</span></p>\n<p><span>For this release, there are three main changes: 1) we added config file functionality which allows users to set session-level variables (one of the most requested features!), 2) we developed a better and robust import mechanism, and 3) we fixed all the filename related issues.  Those changes and mostly the last one forced us to change the version number from 0.10.9 to 0.11.1 as a backup taken in 0.10.x will not work in 0.11.x and vice versa.</span></p>\n<h2>New Features:</h2>\n<ul>\n<li aria-level=\"1\">Adding order by part functionality <a href=\"https://github.com/maxbube/mydumper/pull/388\">#388</a></li>\n<li aria-level=\"1\">improve estimated_step <a href=\"https://github.com/maxbube/mydumper/pull/376\">#376</a> <a href=\"https://github.com/maxbube/mydumper/issues/373\">#373</a></li>\n<li aria-level=\"1\">Adding config file functionality <a href=\"https://github.com/maxbube/mydumper/pull/370\">#370</a>\n<ul>\n<li aria-level=\"2\">Use only a config file to load the parameters <a href=\"https://github.com/maxbube/mydumper/issues/318\">#318</a></li>\n<li aria-level=\"2\">We hope to add parameters and support custom SQL mode and character configuration <a href=\"https://github.com/maxbube/mydumper/issues/274\">#274</a></li>\n<li aria-level=\"2\">[myloader] Add option to disable Galera Cluster (Wsrep) replication before importing <a href=\"https://github.com/maxbube/mydumper/issues/159\">#159</a></li>\n<li aria-level=\"2\">mydumper can&#8217;t recreate mysql.proc + mysql.events due to 5.7 sql_mode <a href=\"https://github.com/maxbube/mydumper/issues/142\">#142</a></li>\n<li aria-level=\"2\">trouble with global sql_big_selects=0 <a href=\"https://github.com/maxbube/mydumper/issues/50\">#50</a></li>\n<li aria-level=\"2\">Enabling MAX_EXECUTION_TIME option <a href=\"https://github.com/maxbube/mydumper/issues/368\">#368</a></li>\n<li aria-level=\"2\">mydumper dump big table failed, because of dumping time longer than max_execution_time <a href=\"https://github.com/maxbube/mydumper/issues/257\">#257</a></li>\n<li aria-level=\"2\">trouble with global sql_big_selects=0 <a href=\"https://github.com/maxbube/mydumper/issues/50\">#50</a></li>\n<li aria-level=\"2\">We hope to add parameters and support custom SQL mode and character configuration <a href=\"https://github.com/maxbube/mydumper/issues/274\">#274</a></li>\n</ul>\n</li>\n<li aria-level=\"1\">Adding sync mechanism and constraint split <a href=\"https://github.com/maxbube/mydumper/pull/352\">#352</a>\n<ul>\n<li aria-level=\"2\">[fast index] Deadlock found when trying to get lock <a href=\"https://github.com/maxbube/mydumper/issues/342\">#342</a></li>\n<li aria-level=\"2\">usingFastIndex: indexes may be created while restore is in progress <a href=\"https://github.com/maxbube/mydumper/issues/292\">#292</a></li>\n<li aria-level=\"2\">[enhancement] optimized data loading order <a href=\"https://github.com/maxbube/mydumper/issues/285\">#285</a></li>\n<li aria-level=\"2\">Enhancement request: myloader option to prioritize size <a href=\"https://github.com/maxbube/mydumper/issues/78\">#78</a></li>\n</ul>\n</li>\n<li aria-level=\"1\">Table and database names changed to fix several issues <a href=\"https://github.com/maxbube/mydumper/pull/399\">#399</a>\n<ul>\n<li aria-level=\"2\">bug for overwrite-tables option <a href=\"https://github.com/maxbube/mydumper/issues/272\">#272</a></li>\n<li aria-level=\"2\">Problems related to log output when backing up table name with <a href=\"https://github.com/maxbube/mydumper/issues/179\">#179</a> <a href=\"https://github.com/maxbube/mydumper/issues/210\">#210</a></li>\n<li aria-level=\"2\">parallel schema restore? <a href=\"https://github.com/maxbube/mydumper/issues/169\">#169</a> <a href=\"https://github.com/maxbube/mydumper/issues/311\">#311</a></li>\n<li aria-level=\"2\">bug with table name <a href=\"https://github.com/maxbube/mydumper/issues/41\">#41</a> <a href=\"https://github.com/maxbube/mydumper/issues/56\">#56</a></li>\n<li aria-level=\"2\">Export with table folders <a href=\"https://github.com/maxbube/mydumper/issues/212\">#212</a></li>\n<li aria-level=\"2\">bug for overwrite-tables option <a href=\"https://github.com/maxbube/mydumper/issues/272\">#272</a></li>\n<li aria-level=\"2\">[BUG] table name make mydumper fail <a href=\"https://github.com/maxbube/mydumper/issues/391\">#391</a></li>\n<li aria-level=\"2\">[BUG] table name starts with checksum breaks myloader <a href=\"https://github.com/maxbube/mydumper/issues/382\">#382</a></li>\n<li aria-level=\"2\">Fix issue <a href=\"https://github.com/maxbube/mydumper/issues/182\">#182</a> for tables with quotes <a href=\"https://github.com/maxbube/mydumper/pull/258\">#258</a></li>\n</ul>\n</li>\n<li aria-level=\"1\">Dump procedures and functions using case sensitive database name <a href=\"https://github.com/maxbube/mydumper/pull/69\">#69</a></li>\n</ul>\n<h2>Bug Closed:</h2>\n<ul>\n<li aria-level=\"1\">[BUG] Error occurs when using &#8211;innodb-optimize-keys and there is a column having a foreign key and part of a generated column <a href=\"https://github.com/maxbube/mydumper/issues/395\">#395</a></li>\n<li aria-level=\"1\">Tables without indexes can cause segfault <a href=\"https://github.com/maxbube/mydumper/pull/386\">#386</a></li>\n</ul>\n<h2>Fixes:</h2>\n<ul>\n<li aria-level=\"1\">[ Fix ] on table name segfault and add free functions <a href=\"https://github.com/maxbube/mydumper/pull/401\">#401</a></li>\n<li aria-level=\"1\">Adding the set session execution <a href=\"https://github.com/maxbube/mydumper/pull/398\">#398</a></li>\n<li aria-level=\"1\">[ Fix ] When no index, alter_table_statement must be NULL <a href=\"https://github.com/maxbube/mydumper/pull/397\">#397</a></li>\n<li aria-level=\"1\">Avoid logging SQL data on error <a href=\"https://github.com/maxbube/mydumper/pull/392\">#392</a></li>\n<li aria-level=\"1\">[ fix ] The count was incorrect <a href=\"https://github.com/maxbube/mydumper/pull/390\">#390</a></li>\n<li aria-level=\"1\">Fix on Debian version <a href=\"https://github.com/maxbube/mydumper/pull/387\">#387</a></li>\n<li aria-level=\"1\">Fix package URLs <a href=\"https://github.com/maxbube/mydumper/pull/384\">#384</a></li>\n<li aria-level=\"1\">Change quotes to backticks in SQL statements <a href=\"https://github.com/maxbube/mydumper/pull/350\">#350</a></li>\n<li aria-level=\"1\">bug for overwrite-tables option <a href=\"https://github.com/maxbube/mydumper/issues/272\">#272</a></li>\n<li aria-level=\"1\">mydumper: fix an &#8216;SQL injection&#8217; issue when table name contains a &#8216; or <a href=\"https://github.com/maxbube/mydumper/pull/168\">#168</a></li>\n</ul>\n<h2>Refactoring</h2>\n<ul>\n<li aria-level=\"1\">Reading file list only one time <a href=\"https://github.com/maxbube/mydumper/pull/394\">#394</a></li>\n<li aria-level=\"1\">Export with table folders <a href=\"https://github.com/maxbube/mydumper/issues/212\">#212</a></li>\n</ul>\n<h2>Question Addressed:</h2>\n<ul>\n<li aria-level=\"1\">[QUESTION] Increase CPU usage Tuning restore <a href=\"https://github.com/maxbube/mydumper/issues/380\">#380</a></li>\n<li aria-level=\"1\">[Question] how to use mydumper to backup 2 tables with different where conditions? <a href=\"https://github.com/maxbube/mydumper/issues/377\">#377</a></li>\n<li aria-level=\"1\">column charset is ignored once it&#8217;s different from table charset. <a href=\"https://github.com/maxbube/mydumper/issues/156\">#156</a></li>\n<li aria-level=\"1\">mydumper/myloader feature requests <a href=\"https://github.com/maxbube/mydumper/issues/84\">#84</a></li>\n<li aria-level=\"1\">load Generated Columns error <a href=\"https://github.com/maxbube/mydumper/issues/175\">#175</a></li>\n</ul>\n<p style=\"text-align: center;\"><a href=\"https://github.com/maxbube/mydumper/archive/refs/tags/v0.11.1.tar.gz\"><strong>Download MyDumper 0.11.1 Today!</strong></a></p>\n","descriptionType":"html","publishedDate":"Tue, 21 Sep 2021 12:51:05 +0000","feedId":11,"bgimg":"","linkMd5":"4bf6e9f728499e7f0379036829919039","bgimgJsdelivr":"","metaImg":"","author":"David Ducos","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-200x113.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn68@2020_2/2021/09/27/16-55-27-737_a536c573d07da3c0.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn60@2020_1/2021/09/27/16-55-34-488_48bbe829c2074bc1.webp"},"publishedOrCreatedDate":1632761712755},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"My Favorite Percona Monitoring and Management Additional Dashboards","link":"https://www.percona.com/blog/?p=77955","description":"<img width=\"200\" height=\"112\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-200x112.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Monitoring and Management Dashboards\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-200x112.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-300x168.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-1024x572.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-367x205.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Management</a> (PMM) has dashboards that cover a lot of ground, yet <strong><em>PMM</em></strong> <strong><em>Superpowers</em></strong> come from the fact you do not need to stick to dashboards that are included with the product! You also can easily install additional dashboards provided by the Community, as well as implement your own.</p>\n<p>In this blog post, we will cover some of the additional dashboards which I find particularly helpful.</p>\n<h2>Node Processes Dashboard</h2>\n<p><a href=\"https://www.percona.com/blog/percona-monitoring-and-management-additional-dashboards/node-processes-dashboard/\"><img loading=\"lazy\" class=\"aligncenter wp-image-77961 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-1024x669.png\" alt=\"Node Processes Dashboard\" width=\"900\" height=\"588\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-1024x669.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-300x196.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-200x131.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-1536x1003.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-367x240.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-560x367.png 560w, https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard.png 1851w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<p>Get insights into the processes on the system to better understand resource usage by your database server vs other stuff on the system.   Unexpected resource hog processes are a quite common cause of downtime and performance issues.  More information in the <a href=\"https://www.percona.com/blog/2021/04/22/understanding-processes-running-on-linux-host-with-percona-monitoring-and-management/\">Understanding Processes on your Linux Host</a> blog post.</p>\n<h2>MySQL Memory Usage Details</h2>\n<p><a href=\"https://www.percona.com/blog/percona-monitoring-and-management-additional-dashboards/mysql-memory-usage-details/\"><img loading=\"lazy\" class=\"aligncenter wp-image-77962 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--1024x508.png\" alt=\"MySQL Memory Usage Details\" width=\"900\" height=\"446\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--1024x508.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--300x149.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--200x99.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--1536x762.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--367x182.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details-.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<p>Ever wondered where MySQL memory usage comes from? This dashboard can shed a light on this dark place, showing the top global memory consumers as well as what users and client hosts contribute to memory usage.  More details in the <a href=\"https://www.percona.com/blog/2020/11/02/understanding-mysql-memory-usage-with-performance-schema/\">Understanding MySQL Memory Usage with Performance Schema</a> blog post.</p>\n<h2>MySQL Query Performance Troubleshooting</h2>\n<p><a href=\"https://www.percona.com/blog/percona-monitoring-and-management-additional-dashboards/mysql-query-performance-troubleshooting-2/\"><img loading=\"lazy\" class=\"aligncenter wp-image-77963 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-1024x460.png\" alt=\"MySQL Query Performance Troubleshooting\" width=\"900\" height=\"404\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-1024x460.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-300x135.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-200x90.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-1536x690.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-367x165.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<p>Want to understand which queries are responsible for CPU, Disk, Memory, or Network Usage and get some other advanced MySQL Query Troubleshooting tools? Check out this dashboard.  Read more about it in the  <a href=\"https://www.percona.com/blog/2020/07/15/mysql-query-performance-troubleshooting-resource-based-approach/\">MySQL Query Performance Troubleshooting</a> blog post.</p>\n<h2>RED Method for MySQL Dashboard</h2>\n<p><a href=\"https://www.percona.com/blog/percona-monitoring-and-management-additional-dashboards/red-method-for-mysql-dashboard/\"><img loading=\"lazy\" class=\"aligncenter wp-image-77964 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-1024x400.png\" alt=\"RED Method for MySQL Dashboard\" width=\"900\" height=\"352\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-1024x400.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-300x117.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-200x78.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-1536x600.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-1140x445.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-367x143.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard.png 1999w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></a></p>\n<p>Want to apply the RED (Rate-Errors-Duration)  method to MySQL?  Check out this dashboard, and check out <a href=\"https://www.percona.com/blog/2020/06/17/red-method-for-mysql-performance-analyses/\">RED Method for MySQL Performance Analyses</a> for more details.</p>\n<p>OK, so let&#8217;s say you’re convinced and want to get those dashboards into your PMM install but manual installation does not excite you.  Here is how you can <a href=\"https://www.percona.com/blog/2021/03/30/how-to-automate-dashboard-importing-in-percona-monitoring-and-management/\">use custom dashboard provisioning</a>  to install all of them:</p><pre class=\"crayon-plain-tag\">curl -LJOs https://github.com/Percona-Lab/pmm-dashboards/raw/main/misc/import-dashboard-grafana-cloud.sh --output import-dashboard-grafana-cloud.sh\ncurl -LJOs https://github.com/Percona-Lab/pmm-dashboards/raw/main/misc/cleanup-dash.py --output cleanup-dash.py\n\nchmod a+x import-dashboard-grafana-cloud.sh\nchmod a+x cleanup-dash.py\n\n./import-dashboard-grafana-cloud.sh -s &#60;PMM_SERVER_IP&#62; -u admin:&#60;ADMIN_PASSWORD&#62; -f Custom -d 13266 -d 12630 -d 12470 -d 14239</pre><p>Note:  Node Processes and MySQL Memory Usage Details dashboards also require additional configuration on the client-side. Check out the blog posts mentioned for specifics.</p>\n<p>Enjoy!</p>\n<p><strong>Percona Monitoring and Management is a best-of-breed open source database monitoring solution. It helps you reduce complexity, optimize performance, and improve the security of your business-critical database environments, no matter where they are located or deployed.</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\" rel=\"noopener\">Download Percona Monitoring and Management Today</a></p>\n","descriptionType":"html","publishedDate":"Tue, 31 Aug 2021 13:40:56 +0000","feedId":11,"bgimg":"","linkMd5":"e6b3d0a4cb30eb097eded1b2264f4cf4","bgimgJsdelivr":"","metaImg":"","author":"Peter Zaitsev","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-200x112.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn47@2020_4/2021/09/27/16-55-31-791_3c0f55455af1a607.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-1024x669.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn88@2020_2/2021/09/27/16-55-13-820_40729e61da707652.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--1024x508.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_5/2021/09/27/16-55-37-269_890113497f1824e5.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-1024x460.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn35@2020_3/2021/09/27/16-55-27-789_718169d7891e9607.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-1024x400.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn5@2020_5/2021/09/27/16-55-28-741_af912f688a1ea139.webp"},"publishedOrCreatedDate":1632761712745},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"August Is Hot, but Not as Hot as Our Next Community Engineering Meeting!","link":"https://www.percona.com/blog/?p=77814","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Percona Community Engineering Meeting\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p class=\"code-line\" data-line=\"1\"><img loading=\"lazy\" class=\"alignright size-medium wp-image-77873\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-300x157.png\" alt=\"Percona Community Engineering Meeting\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />The engineering marvel that is <a href=\"https://www.percona.com/software/database-tools/percona-monitoring-and-management\">Percona Monitoring and Managemen</a>t (PMM), has to be, for me, one of the best examples out there of how a large number of vastly different community projects can successfully combine into something whose value and features far exceeds the sum of its component open-source parts.</p>\n<p class=\"code-line\" data-line=\"3\">That&#8217;s a lot of words for a simple thing: it&#8217;s a <em>metrics mover</em>—it&#8217;s PMM!</p>\n<p class=\"code-line\" data-line=\"5\">PMM has <em>exporters</em> that suck system stats from a server or database.</p>\n<p class=\"code-line\" data-line=\"7\">The numbers are safely shuffled through agents across a crowded network toward the sanctuary of the <a href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/server/index.html\" data-href=\"https://www.percona.com/doc/percona-monitoring-and-management/2.x/setting-up/server/index.html\">PMM Server</a>.</p>\n<p class=\"code-line\" data-line=\"9\">That&#8217;s where your valuable system values are collected and collated, charted and tabled, and where they emerge as <em>fabled knowledge</em>, logically organized and neatly arranged into <em>dashboards</em>.</p>\n<p class=\"code-line\" data-line=\"11\"><strong>Dashboards (What you see)</strong></p>\n<p class=\"code-line\" data-line=\"13\">PMM&#8217;s dashboards are based on Grafana, and PMM relies on many community-based projects.</p>\n<p class=\"code-line\" data-line=\"15\"><strong>Exporters (What you don&#8217;t see)</strong></p>\n<p class=\"code-line\" data-line=\"17\">Exporters are specialized programs that run and extract metrics data from a node or database. Every database is different, and so every exporter is too.</p>\n<p class=\"code-line\" data-line=\"19\"><strong>Community Meetings (What you may not know)</strong></p>\n<p class=\"code-line\" data-line=\"21\">I tell you what you may already know simply as a prelude to an announcement of what you may not yet know.</p>\n<p class=\"code-line\" data-line=\"23\">We&#8217;ve been running monthly <a href=\"https://percona.community/\" data-href=\"https://percona.community/\">community</a> meetings where our Engineering Gurus &#8220;talk technical&#8221; to share their knowledge of PMM and all its parts.</p>\n<p class=\"code-line\" data-line=\"25\">The format is fairly informal. We use our <a href=\"http://per.co.na/discord\" data-href=\"http://per.co.na/discord\">Discord server&#8217;s voice channel</a>. We grab a refreshing region-sensitive hot or cold drink, a <em>very</em> loose agenda, and we talk.</p>\n<p class=\"code-line\" data-line=\"27\"><strong>All we need now is someone to listen.</strong></p>\n<p class=\"code-line\" data-line=\"29\">The topics in <a href=\"https://percona.community/contribute/engineeringmeetings/2021-08-31/\" data-href=\"https://percona.community/contribute/engineeringmeetings/2021-08-31/\">this month&#8217;s meeting</a>:</p>\n<ul>\n<li class=\"code-line\" data-line=\"31\">\n<p class=\"code-line\" data-line=\"31\"><strong>Exporters</strong>: Focusing this month on <code>node_exporter</code>, and how and why we&#8217;re reverting <a href=\"https://github.com/percona/node_exporter\" data-href=\"https://github.com/percona/node_exporter\">our fork</a> of the <a href=\"https://github.com/prometheus/node_exporter\" data-href=\"https://github.com/prometheus/node_exporter\">Prometheus one</a>.</p>\n</li>\n<li class=\"code-line\" data-line=\"33\">\n<p class=\"code-line\" data-line=\"33\"><strong>Dashboards</strong>: How Percona&#8217;s differ from the community&#8217;s, and how to migrate them using the scripts in <a href=\"https://github.com/percona/grafana-dashboards/tree/main/misc\" data-href=\"https://github.com/percona/grafana-dashboards/tree/main/misc\">here</a>. Here&#8217;s a brief preview.</p>\n<ul>\n<li class=\"code-line\" data-line=\"35\">\n<p class=\"code-line\" data-line=\"35\"><strong>Convert a dashboard from PMM</strong></p>\n<p class=\"code-line\" data-line=\"37\"><img loading=\"lazy\" class=\"aligncenter wp-image-77821 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/image1-1024x232.png\" alt=\"PMM dashboard\" width=\"900\" height=\"204\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/image1-1024x232.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/image1-300x68.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/image1-200x45.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/image1-1536x348.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/image1-367x83.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/image1.png 1728w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p class=\"code-line\" data-line=\"39\">This script converts a PMM dashboard so it can be used in an external Prometheus + Grafana installation. It doesn&#8217;t need any input from you. It replaces PMM2 labels (<code>node_name</code>, <code>service_name</code>) that are used in variables with default labels (<code>instance</code>).</p>\n</li>\n<li class=\"code-line\" data-line=\"42\">\n<p class=\"code-line\" data-line=\"42\"><strong>Convert a dashboard to PMM<br />\n</strong><img loading=\"lazy\" class=\"aligncenter wp-image-77822 size-large\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/image2-1024x497.png\" alt=\"Convert a dashboard to PMM\" width=\"900\" height=\"437\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/image2-1024x497.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/image2-300x145.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/image2-200x97.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/image2-1536x745.png 1536w, https://www.percona.com/blog/wp-content/uploads/2021/08/image2-367x178.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/image2.png 1728w\" sizes=\"(max-width: 900px) 100vw, 900px\" /></p>\n<p class=\"code-line\" data-line=\"44\">\n</li>\n</ul>\n<p class=\"code-line\" data-line=\"46\">This one renames labels and variables interactively because it can&#8217;t know in advance the names of the labels you want to use. You must select names for renaming and provide a PMM2 label (<code>node_name</code>, <code>service_name</code>). The script collects a list of used variables and asks which have to be renamed in variables and expressions.</p>\n<p class=\"code-line\" data-line=\"50\">PMM2 labels can be checked in the VM configuration files:</p>\n<p class=\"code-line\" data-line=\"50\"><img loading=\"lazy\" class=\"aligncenter wp-image-77823 size-full\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/image3.png\" alt=\"VM configuration files\" width=\"690\" height=\"379\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/image3.png 690w, https://www.percona.com/blog/wp-content/uploads/2021/08/image3-300x165.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/image3-200x110.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/image3-367x202.png 367w\" sizes=\"(max-width: 690px) 100vw, 690px\" /></p>\n<p class=\"code-line\" data-line=\"52\">\n</li>\n</ul>\n<p class=\"code-line\" data-line=\"54\">As we&#8217;ve <a href=\"https://twitter.com/PerconaBytes\" data-href=\"https://twitter.com/PerconaBytes\">already tweeted</a>, our Gurus are not ashamed of showing their consoles. Some are bare and some are full, but they&#8217;re always fascinating to watch when they use them with such style.</p>\n<p class=\"code-line\" data-line=\"56\"><a href=\"https://percona.community/contribute/engineeringmeetings/\">Why not join us and see for yourself?</a></p>\n","descriptionType":"html","publishedDate":"Mon, 23 Aug 2021 11:48:40 +0000","feedId":11,"bgimg":"","linkMd5":"cd56b4a01b3e6ffde151f46fb441c68d","bgimgJsdelivr":"","metaImg":"","author":"Vadim Yalovets","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn84@2020_1/2021/09/27/16-55-31-732_86162ed39056c88a.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_3/2021/09/27/16-55-34-848_2b4249f2dd524cdd.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/image1-1024x232.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn23@2020_1/2021/09/27/16-55-34-608_f2e83986b78267c8.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/image2-1024x497.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn76@2020_2/2021/09/27/16-55-30-624_5f889c57fa94d51b.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn23@2020_2/2021/09/27/16-55-33-320_543eafd33af0c3c2.webp"},"publishedOrCreatedDate":1632761712750},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Storing JSON in Your Databases: Tips and Tricks For MySQL Part Two","link":"https://www.percona.com/blog/?p=77838","description":"<img width=\"200\" height=\"107\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-200x107.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"Storing JSON MySQL\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-1024x546.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><h2>JSON Structure, The Next Generation (Performance, Scale, and Fun)</h2>\n<p><img loading=\"lazy\" class=\"alignright size-medium wp-image-77847\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-300x160.png\" alt=\"Storing JSON MySQL\" width=\"300\" height=\"160\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-300x160.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-1024x546.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-200x107.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-367x196.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Now you know the basic JSON functionality available to you, as covered in <a href=\"https://www.percona.com/blog/storing-json-in-your-databases-tips-mysql/\">part one of this series</a>.  Let’s look deeper into performance, scale, and oddities that may occur.</p>\n<p>You can do almost anything you could do in SQL with standard data types and objects, but you may run into specific problems and limitations.  Here is an example of that.  Let’s say I want to get the top 10 rated movies of all time.</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title' as title,   json_column-&#62;&#62;'$.imdb_rating' as rating,   json_column-&#62;&#62;'$.imdb_id' as imdb_id  from movies_json where json_column-&#62;&#62;'$.imdb_rating' &#62; 8 order by json_column-&#62;&#62;'$.imdb_rating' desc limit 10;\n\nERROR 1038 (HY001): Out of sort memory, consider increasing server sort buffer size</pre><p>In this case, the size required to sort the data set is much larger than the available sort size.  Selecting this data from a normalized structure would not cause this (nor would generated columns).</p><pre class=\"crayon-plain-tag\">mysql&#62; select @@sort_buffer_size;\n+--------------------+\n| @@sort_buffer_size |\n+--------------------+\n|             262144 |\n+--------------------+\n1 row in set (0.00 sec)\n\nmysql&#62; set @@sort_buffer_size=26214400;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&#62; select json_column-&#62;&#62;'$.title' as title,   json_column-&#62;&#62;'$.imdb_rating' as rating,   json_column-&#62;&#62;'$.imdb_id' as imdb_id  from movies_json where json_column-&#62;&#62;'$.imdb_rating' &#62; 8 order by json_column-&#62;&#62;'$.imdb_rating' desc limit 10;\n+----------------------------------------------------+--------+-----------+\n| title                                              | rating | imdb_id   |\n+----------------------------------------------------+--------+-----------+\n| The Mountain II (2016)                             | 9.9    | tt5813916 |\n| Toxic Temptation (2016)                            | 9.9    | tt4621978 |\n| 1985 (2016)                                        | 9.9    | tt5932546 |\n| Jag har din rygg (2015)                            | 9.8    | tt3689312 |\n| My Head Hurts (2000)                               | 9.8    | tt1346290 |\n| Boz: Colors and Shapes (Video 2006)                | 9.8    | tt0876256 |\n| Foreclosed (Video 2010)                            | 9.8    | tt1648984 |\n| Nocturne in Black (2016)                           | 9.8    | tt4536608 |\n| Monrad &#38; Rislund: Det store triumftog (Video 2004) | 9.8    | tt0425266 |\n| Questione di Sguardi (2014)                        | 9.8    | tt4423586 |\n+----------------------------------------------------+--------+-----------+\n10 rows in set, 65535 warnings (0.78 sec)</pre><p>So we solved the issue with not having enough space to sort the dataset, but did you notice the time?  Almost 1 second.  Any website you are working on nowadays can not succeed if all your database queries take 0.5-1 second to return.  Yes, you can put a cache in front of these and probably will.  A better cache solves it all, right?  Well, not really.  In this case, you can speed up things drastically with a few important and small improvements.  Before moving on, notice the warnings?  Let’s ignore these for one moment. (These are because of the null values in the rating column (but these are not impacting performance).</p>\n<p>Let’s look at generated columns.   Generated columns allow you to create columns based on other columns or circumstances on the fly. Note, you can also use functional indexes, which I will cover later.  In the case of JSON, we can pull values out of our document and store them read-only in a column that can be indexed (we all know indexing is good, right!).  Let’s do this!  I am going to create a table with generated columns for various columns.  Then I am going to load the data via insert from our already loaded movies table.</p><pre class=\"crayon-plain-tag\">mysql&#62; create table movies_json_generated (\n     -&#62;         ai_myid int AUTO_INCREMENT primary key,\n     -&#62;         imdb_id varchar(255) generated always as (`json_column` -&#62;&#62; '$.imdb_id'),\n     -&#62;         title varchar(255) generated always as (`json_column` -&#62;&#62; '$.title'),\n     -&#62;         imdb_rating decimal(5,2) generated always as (`json_column` -&#62;&#62; '$.imdb_rating'),\n     -&#62;         overview text generated always as (`json_column` -&#62;&#62; '$.overview'),\n     -&#62;         director json generated always as (`json_column` -&#62;&#62; '$.director'),\n     -&#62;         cast json generated always as (`json_column` -&#62;&#62; '$.cast'),\n     -&#62;         json_column json\n     -&#62; ) engine = innodb;\n Query OK, 0 rows affected (0.04 sec)\n\n mysql&#62; create unique index imdb_idx on movies_json_generated(imdb_id);\n Query OK, 0 rows affected (0.04 sec)\n Records: 0  Duplicates: 0  Warnings: 0\n\n mysql&#62; insert into movies_json_generated (json_column ) select json_column from  movies_json;\n ERROR 1366 (HY000): Incorrect decimal value: 'null' for column 'imdb_rating' at row 1\n mysql&#62; Drop Table movies_json_generated;\n Query OK, 0 rows affected (0.04 sec)</pre><p><span>I wanted to take a moment and dive into what I have seen as a common issue. As you can see, the table was created without problem, but it failed when inserting data from our JSON objects.  The reason? I am storing the  IMDB rating in a decimal(5,2) field, but the JSON reference has ‘null’ (quoted) as text. Type conversions when working with JSON and MySQL can be a bit tricky for some people.  Remember those warnings above?  They were type conversion warnings.  Easy to ignore as they did not appear to cause an immediate issue (but they did).  Type conversions and character sets can be a bit of a problem when accessing JSON data. </span><span><br />\n</span></p>\n<blockquote><p><em>Allow a slight diverged rant:   If your application has an inconsistent or rapidly changing JSON format/schema you can make using standard database functions difficult ( if not impossible ).</em></p>\n<p><em>I often hear people talk about structure and the lack of flexibility as a massive drawback for relational databases. JSON is flexible; people love flexibility. The issue is flexibility comes at a cost, and you will pay that cost somewhere.  If you are validating and maintaining a structure in your database, your code must fit in the structure and changes to the structure maybe slow ( database migrations).  If you use a flexible database schema, then you need to validate in your code.  This validation in the code may be simple now, but as you grow, the more iterations or changes to your data, the more sprawling the code to check and validate data is. Whether you want to admit it or not, throwing any unstructured data into the wasteland that is a document is a recipe for problems. In the case of storing just a dump of JSON in your database, sometimes that data is complex to access or manipulate without writing code and having access to that validation. </em></p></blockquote>\n<p><span>In this case, the type conversion is pretty straightforward and you can solve this issue in a couple of different ways. Ultimately it is about ensuring the JSON value is converted correctly. I will use the json_value function to unquote the null.</span></p><pre class=\"crayon-plain-tag\">mysql&#62; create table movies_json_generated_stored (\n    -&#62;    ai_myid int AUTO_INCREMENT primary key,\n    -&#62;    imdb_id varchar(255) generated always as (`json_column` -&#62;&#62; '$.imdb_id')  ,\n    -&#62;    title varchar(255) generated always as (`json_column` -&#62;&#62; '$.title') ,\n    -&#62;    imdb_rating decimal(5,2) generated always as (json_value(json_column, '$.imdb_rating')) ,\n    -&#62;    overview text generated always as (`json_column` -&#62;&#62; '$.overview') ,\n    -&#62;    director json generated always as (`json_column` -&#62;&#62; '$.director') ,\n    -&#62;    cast json generated always as (`json_column` -&#62;&#62; '$.cast') ,\n    -&#62;    json_column json\n    -&#62; ) engine = innodb;\n\n mysql&#62; \n mysql&#62; create unique index imdb_idx on movies_json_generated(imdb_id);\n Query OK, 0 rows affected (0.01 sec)\n Records: 0  Duplicates: 0  Warnings: 0\n\n mysql&#62; insert into movies_json_generated (json_column ) select json_column from  movies_json;\n Query OK, 375359 rows affected (40.26 sec)\n Records: 375359  Duplicates: 0  Warnings: 0</pre><p>Now let’s compare searching for a movie using first the imdb_id from the JSON document and then from the generated table using the column we indexed:</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title', json_column-&#62;&#62;'$.imdb_rating' from movies_json where json_column-&#62;&#62;'$.imdb_id'='tt2395427';\n +--------------------------------+-------------------------------+\n | json_column-&#62;&#62;'$.title'        | json_column-&#62;&#62;'$.imdb_rating' |\n +--------------------------------+-------------------------------+\n | Avengers: Age of Ultron (2015) | 7.5                           |\n +--------------------------------+-------------------------------+\n 1 row in set (0.86 sec)\n\n mysql&#62; select title, imdb_rating from movies_json_generated where imdb_id='tt2395427';\n +--------------------------------+-------------+\n | title                          | imdb_rating |\n +--------------------------------+-------------+\n | Avengers: Age of Ultron (2015) |        7.50 |\n +--------------------------------+-------------+\n 1 row in set (0.01 sec)</pre><p>Great!  Let’s go back to our example using IMDB rating to get the top 10 movies.  To make this faster, we will need to first create an index on the generated column.</p><pre class=\"crayon-plain-tag\">mysql&#62; create index idx_rating on movies_json_generated ( imdb_rating );\n Query OK, 0 rows affected (1.45 sec)\n Records: 0  Duplicates: 0  Warnings: 0</pre><p>With that out of the way, let’s get the top 10 list:</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title' as title,   json_column-&#62;&#62;'$.imdb_rating' as rating,   json_column-&#62;&#62;'$.imdb_id' as imdb_id  from movies_json_generated where imdb_rating &#62; 8 order by imdb_rating desc limit 10;\n +--------------------------------------------------+--------+-----------+\n | title                                            | rating | imdb_id   |\n +--------------------------------------------------+--------+-----------+\n | Advent (IV) (2016)                               | 10.0   | tt6129028 |\n | 311 Live: 3/11 Day 2006 (2006)                   | 10.0   | tt0872240 |\n | Light Study (2013)                               | 10.0   | tt3130306 |\n | Future Boyfriend (2016)                          | 10.0   | tt3955652 |\n | Cory in the House: All Star Edition (Video 2007) | 10.0   | tt2402070 |\n | 1985 (2016)                                      | 9.9    | tt5932546 |\n | Toxic Temptation (2016)                          | 9.9    | tt4621978 |\n | The Mountain II (2016)                           | 9.9    | tt5813916 |\n | Questione di Sguardi (2014)                      | 9.8    | tt4423586 |\n | Foreclosed (Video 2010)                          | 9.8    | tt1648984 |\n +--------------------------------------------------+--------+-----------+\n 10 rows in set (0.01 sec)</pre><p>A very nice drop from 0.78 seconds to 0.01 seconds!  But wait… why is the data different?  Ahhh glad you noticed!  As discussed above, pulling data out of JSON often requires some type of conversion.  By default, values coming out of JSON are considered as text, not numeric, so it’s sorting based on the ASCII Value (oops).   So you can get the same results by forcing the type conversion:</p><pre class=\"crayon-plain-tag\">mysql&#62; select json_column-&#62;&#62;'$.title' as title,   json_column-&#62;&#62;'$.imdb_rating' as rating,   json_column-&#62;&#62;'$.imdb_id' as imdb_id  from movies_json_generated where json_column-&#62;&#62;'$.imdb_rating' &#62; 8 order by json_column-&#62;&#62;'$.imdb_rating'*1 desc limit 10;\n+--------------------------------------------------+--------+-----------+\n| title                                            | rating | imdb_id   |\n+--------------------------------------------------+--------+-----------+\n| 311 Live: 3/11 Day 2006 (2006)                   | 10.0   | tt0872240 |\n| Advent (IV) (2016)                               | 10.0   | tt6129028 |\n| Cory in the House: All Star Edition (Video 2007) | 10.0   | tt2402070 |\n| Light Study (2013)                               | 10.0   | tt3130306 |\n| Future Boyfriend (2016)                          | 10.0   | tt3955652 |\n| Toxic Temptation (2016)                          | 9.9    | tt4621978 |\n| The Mountain II (2016)                           | 9.9    | tt5813916 |\n| 1985 (2016)                                      | 9.9    | tt5932546 |\n| Nocturne in Black (2016)                         | 9.8    | tt4536608 |\n| My Head Hurts (2000)                             | 9.8    | tt1346290 |\n+--------------------------------------------------+--------+-----------+\n10 rows in set, 65535 warnings (0.89 sec)\n\nAlternative you can use:  cast(json_value(json_column,'$.imdb_rating') as float)</pre><p><span>So not only can you significantly speed up your performance with indexing with generated columns, you can also ensure that you are getting consistent types and the expected results.  The documentation has a detailed section on ordering and group by JSON values. Generated columns also improve most of the other queries we showed above.</span></p>\n<h2>Functional Indexes Without Generated Columns</h2>\n<p><span>Generated columns work well enough, but for our queries (In MySQL 8.0.13 or later), we can create indexes on the JSON functions we call regularly and forgo the generated columns altogether.   </span></p><pre class=\"crayon-plain-tag\">mysql&#62;  create index title_idx on movies_json ((json_value(json_column,'$.title')));\nQuery OK, 0 rows affected (2.90 sec)\nRecords: 0  Duplicates: 0  Warnings: 0</pre><p><span>This works well enough if you are trying to match an exact match; if you need to use a like or wild card the functional index won’t be used. As you explore using functional indexes with JSON, be mindful there are some character set and collation restrictions and restrictions on which functions can be used, so your mileage will vary.   I will avoid a deep dive here, but you can review the documentation here </span><a href=\"https://dev.mysql.com/doc/refman/8.0/en/create-index.html#create-index-functional-key-parts\"><span>https://dev.mysql.com/doc/refman/8.0/en/create-index.html#create-index-functional-key-parts</span></a><span> &#38; here:  </span><a href=\"https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html#json-column-indirect-index\"><span>https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html#json-column-indirect-index</span></a><span>.  In my testing, the more complicated or nested the JSON was, the more problems I ran into.   For this reason, I stuck with the generated columns for my examples above. </span></p>\n<h2>Multi-Valued Indexes</h2>\n<p><span>As of 8.0.17 MySQL also supports </span><a href=\"https://dev.mysql.com/doc/refman/8.0/en/create-index.html#create-index-multi-valued\"><span>Multi-Valued indexes</span></a><span>, a secondary index that allows you to index an array.  This is helpful if you have an array within your JSON (this does not appear to support arrays of characters at this time, the helpful “</span><span>This version of MySQL doesn&#8217;t yet support &#8216;CAST-ing data to array of char/binary BLOBs&#8217;”)</span><span>.  This can help with JSON designs that are straightforward, but as your JSON becomes more nested and complex I ran into problems.  </span></p>\n<p><b>Performance Summary</b></p>\n<p><span>Super unscientific, but these query run times hold over multiple iterations, look at the differences speed-wise: </span></p>\n<table>\n<tbody>\n<tr>\n<td><span>Query</span></td>\n<td><span>Access JSON Directly (seconds)</span></td>\n<td><span>Generated Column</span><span><br />\n</span><span> (seconds)</span></td>\n</tr>\n<tr>\n<td><span>Simple Search Via IMDB ID</span></td>\n<td><span>0.75 </span></td>\n<td><span>0.01</span></td>\n</tr>\n<tr>\n<td><span>Search for Avengers Titled Movies</span></td>\n<td><span>0.76</span></td>\n<td><span>0.01&#62;</span></td>\n</tr>\n<tr>\n<td><span>Updating a single value within the JSON searching via IMDB or title</span></td>\n<td><span>0.80</span></td>\n<td><span>0.01&#62;</span></td>\n</tr>\n<tr>\n<td><span>Find top 10 movies of all time</span></td>\n<td><span>0.89</span></td>\n<td><span>0.01</span></td>\n</tr>\n<tr>\n<td><span>Characters played by Robert Downey JR in the avengers’ movies</span></td>\n<td><span>0.74</span></td>\n<td><span>0.01&#62;</span></td>\n</tr>\n</tbody>\n</table>\n<h2>More Performance Needed? Normalizing Data for Query Patterns</h2>\n<p><span>So far we have done a lot of simple interactions and were able to speed up access to an acceptable level.  But not everything fits within the available toolbox.  When searching for movies or ratings for a specific cast member ( show me all the Avengers movies Robert Downey JR. played and the characters ), we used an index on the title generated column to reduce the JSON Documents we had to fully process to get the character he played.  See below:</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select title, imdb_rating, t.* from movies_json_generated, json_table(json_column, '$.cast[*]' columns( \n    -&#62;    V_name varchar(200) path '$.name',\n    -&#62;    V_character varchar(200) path '$.character')\n    -&#62;    ) t where t.V_name like 'Robert Downey Jr.%'  and title like 'Avengers%';\n+--------------------------------+-------------+-------------------+------------------------------------+\n| title                          | imdb_rating | V_name            | V_character                        |\n+--------------------------------+-------------+-------------------+------------------------------------+\n| Avengers: Age of Ultron (2015) |        7.50 | Robert Downey Jr. | Tony Stark                         |\n| Avengers: Endgame (2019)       |        9.00 | Robert Downey Jr. | Tony Stark /              Iron Man |\n| Avengers: Infinity War (2018)  |        NULL | Robert Downey Jr. | Tony Stark /              Iron Man |\n+--------------------------------+-------------+-------------------+------------------------------------+\n3 rows in set (0.00 sec)\n\nmysql&#62; explain select title, imdb_rating, t.* from movies_json_generated, json_table(json_column, '$.cast[*]' columns(     V_name varchar(200) path '$.name',    V_character varchar(200) path '$.character')    ) t where t.V_name like 'Robert Downey Jr.%'  and title like 'Avengers%';\n+----+-------------+-----------------------+------------+-------+---------------+-----------+---------+------+------+----------+----------------------------------------------------------+\n| id | select_type | table                 | partitions | type  | possible_keys | key       | key_len | ref  | rows | filtered | Extra                                                    |\n+----+-------------+-----------------------+------------+-------+---------------+-----------+---------+------+------+----------+----------------------------------------------------------+\n|  1 | SIMPLE      | movies_json_generated | NULL       | range | title_idx     | title_idx | 1023    | NULL |    8 |   100.00 | Using where                                              |\n|  1 | SIMPLE      | t                     | NULL       | ALL   | NULL          | NULL      | NULL    | NULL |    2 |    50.00 | Table function: json_table; Using temporary; Using where |\n+----+-------------+-----------------------+------------+-------+---------------+-----------+---------+------+------+----------+----------------------------------------------------------+\n2 rows in set, 1 warning (0.00 sec)</pre><p><span>But let’s assume you needed to get a list of all characters he played in his career (Will truncate the full result set).  </span></p><pre class=\"crayon-plain-tag\">mysql&#62; select title, imdb_rating, t.* from movies_json_generated, json_table(json_column, '$.cast[*]' columns( \n    -&#62;    V_name varchar(200) path '$.name',\n    -&#62;    V_character varchar(200) path '$.character')\n    -&#62;    ) t where t.V_name like 'Robert Downey Jr.%';\n\n\n+-------------------------------------------------------------------------------------------------------------+-------------+-------------------+------------------------------------------------------------------------------------------------+\n| title                                                                                                       | imdb_rating | V_name            | V_character                                                                                    |\n+-------------------------------------------------------------------------------------------------------------+-------------+-------------------+------------------------------------------------------------------------------------------------+\n| The 65th Annual Academy Awards (1993)                                                                       |        NULL | Robert Downey Jr. | Himself - Presenter                                                                            |\n| Sherlock Holmes: A Game of Shadows (2011)                                                                   |        7.50 | Robert Downey Jr. | Sherlock Holmes                                                                                |\n| Due Date (2010)                                                                                             |        6.60 | Robert Downey Jr. | Peter Highman                                                                                  |\n| Eros (2004)                                                                                                 |        6.00 | Robert Downey Jr. | Nick Penrose (segment \"Equilibrium\")                                                           |\n| The EE British Academy Film Awards (2015)                                                                   |        7.40 | Robert Downey Jr. | Himself - Tribute to Lord Attenborough                                                         |\n| \"Saturday Night Live\" John Lithgow/Mr. Mister (TV Episode 1985)                                             |        NULL | Robert Downey Jr. | Bruce Winston /              Rudy Randolph III /              Various       (as Robert Downey) |\n+-------------------------------------------------------------------------------------------------------------+-------------+-------------------+------------------------------------------------------------------------------------------------+\n213 rows in set (7.14 sec)\n\n\n\nmysql&#62; explain select title, imdb_rating, t.* from movies_json_generated, json_table(json_column, '$.cast[*]' columns(     V_name varchar(200) path '$.name',    V_character varchar(200) path '$.character')    ) t where t.V_name like 'Robert Downey Jr.%';\n+----+-------------+-----------------------+------------+------+---------------+------+---------+------+--------+----------+----------------------------------------------------------+\n| id | select_type | table                 | partitions | type | possible_keys | key  | key_len | ref  | rows   | filtered | Extra                                                    |\n+----+-------------+-----------------------+------------+------+---------------+------+---------+------+--------+----------+----------------------------------------------------------+\n|  1 | SIMPLE      | movies_json_generated | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 358174 |   100.00 | NULL                                                     |\n|  1 | SIMPLE      | t                     | NULL       | ALL  | NULL          | NULL | NULL    | NULL |      2 |    50.00 | Table function: json_table; Using temporary; Using where |\n+----+-------------+-----------------------+------------+------+---------------+------+---------+------+--------+----------+----------------------------------------------------------+\n2 rows in set, 1 warning (0.00 sec)</pre><p><span>Now our access pattern needs to start at a deeper nested element within the JSON document.  Here you are faced with a few options, but almost all of them lead to creating and maintaining a new table to get that level of data either via trigger, code to break out the data when being loaded or via batch job (or caching this evil slow query).  </span></p>\n<p><span>I ended up creating the following “Database Schema” for it:</span></p><pre class=\"crayon-plain-tag\">create table movies_normalized_meta (\n        ai_myid int AUTO_INCREMENT primary key,\n        imdb_id varchar(32),\n        title varchar(255),\n        imdb_rating decimal(5,2),\n        json_column json\n) engine = innodb;\n\ncreate unique index imdb_id_idx  on movies_normalized_meta (imdb_id);\ncreate index rating_idx  on movies_normalized_meta (imdb_rating);\n\ncreate table movies_normalized_actors (\n        ai_actor_id int auto_increment primary key,\n        actor_id varchar(50),\n        actor_name varchar(500)\n        ) engine = innodb;\ncreate index actor_id_idx  on movies_normalized_actors (actor_id);\ncreate index actor_name_idx  on movies_normalized_actors (actor_name);\n\ncreate table movies_normalized_cast (\n        ai_actor_id int,\n        ai_myid int,\n        actor_character varchar(500)\n        ) engine = innodb;\n\ncreate index cast_id_idx  on movies_normalized_cast (ai_actor_id,ai_myid);\ncreate index cast_id2_idx  on movies_normalized_cast (ai_myid);\ncreate index cast_character_idx  on movies_normalized_cast (actor_character);\ncreate unique index u_cast_idx  on movies_normalized_cast (ai_myid,ai_actor_id,actor_character);</pre><p><span>On loading the JSON into MySQL I added an actor table that will have a row for each new unique actor as well as a cast table that has the movie, actor, and name of the character they played in the movie (Note I could optimize the structure a bit, but that’s for another day ).  This gives me a ton of flexibility in reporting and a major performance boost.  Now to get all the movies Robert Downey JR was in and the characters he played I can it via:</span></p><pre class=\"crayon-plain-tag\">mysql&#62; select title, imdb_rating, actor_character from movies_normalized_meta a, movies_normalized_cast b,  movies_normalized_actors c where a.ai_myid=b.ai_myid and b.ai_actor_id = c.ai_actor_id and actor_name='Robert Downey Jr.';\n\n\n+-------------------------------------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------+\n| title                                                                                                       | imdb_rating | actor_character                                                                                |\n+-------------------------------------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------+\n| The 65th Annual Academy Awards (1993)                                                                       |        NULL | Himself - Presenter                                                                            |\n| Sherlock Holmes: A Game of Shadows (2011)                                                                   |        7.50 | Sherlock Holmes                                                                                |\n| Due Date (2010)                                                                                             |        6.60 | Peter Highman                                                                                  |\n| Eros (2004)                                                                                                 |        6.00 | Nick Penrose (segment \"Equilibrium\")     \n | Saturday Night Live in the '80s: Lost &#38; Found (2005)                                                        |        7.20 | Various       (archive footage)                                                                |\n| \"Saturday Night Live\" John Lithgow/Mr. Mister (TV Episode 1985)                                             |        NULL | Bruce Winston /              Rudy Randolph III /              Various       (as Robert Downey) |\n+-------------------------------------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------+\n213 rows in set (0.01 sec)\n                                           \n\n      mysql&#62; explain select title, imdb_rating, actor_character from movies_normalized_meta a, movies_normalized_cast b,  movies_normalized_actors c where a.ai_myid=b.ai_myid and b.ai_actor_id = c.ai_actor_id and actor_name='Robert Downey Jr.';\n+----+-------------+-------+------------+--------+-------------------------------------+----------------+---------+-------------------------------+------+----------+-----------------------+\n| id | select_type | table | partitions | type   | possible_keys                       | key            | key_len | ref                           | rows | filtered | Extra                 |\n+----+-------------+-------+------------+--------+-------------------------------------+----------------+---------+-------------------------------+------+----------+-----------------------+\n|  1 | SIMPLE      | c     | NULL       | ref    | PRIMARY,actor_name_idx              | actor_name_idx | 2003    | const                         |  213 |   100.00 | Using index           |\n|  1 | SIMPLE      | b     | NULL       | ref    | u_cast_idx,cast_id_idx,cast_id2_idx | cast_id_idx    | 5       | movie_json_test.c.ai_actor_id |    2 |   100.00 | Using index condition |\n|  1 | SIMPLE      | a     | NULL       | eq_ref | PRIMARY                             | PRIMARY        | 4       | movie_json_test.b.ai_myid     |    1 |   100.00 | NULL                  |\n+----+-------------+-------+------------+--------+-------------------------------------+----------------+---------+-------------------------------+------+----------+-----------------------+\n3 rows in set, 1 warning (0.00 sec)</pre><p><span>Not only is this 7 seconds faster than accessing the same data via the JSON functions, but I can also use these tables to easily do things like give me all the movies that had two or three actors tother, play six degrees, etc that would be a challenge only access the JSON directly. </span></p>\n<h2>Take-Aways and Lessons Learned</h2>\n<p><span>First,  think before you store your data.  Understanding what you are storing, why you are storing, and how you will access it is paramount. How you will access and use your data has a profound impact on the optimal database setup, design, and usage.  If you only access top-level properties in your document, a simple design relying only on using MySQL’s built-in JSON functions may be totally ok.  But as you want to dive deeper into the data and start looking at pulling our subsets of data or correlating different documents to ensure performance and scalability you may end up reducing your flexibility and build (hold your breath) an actual database schema that is normalized and everything.  </span></p>\n<p><span>Second, MySQL’s JSON functions get the job done for most basic use cases.  You can get what you need, but you need to understand the implementations and what is available and what is not.  As I perform these same processes with PostgreSQL and MongoDB you will see where some functions are better than others.   </span></p>\n<p><span>Finally,  don’t fear schemas!  Seriously, structure is good.  No matter how you access your data and use it, you will assume some structure (it all depends on where you want to enforce that structure).  </span></p>\n<p><span>Next up in this series, I will dive into JSON with PostgreSQL followed by JSON and MongoDB. Stay tuned!</span></p>\n","descriptionType":"html","publishedDate":"Fri, 20 Aug 2021 12:50:56 +0000","feedId":11,"bgimg":"","linkMd5":"5142508387f6865bb518416e22d1c088","bgimgJsdelivr":"","metaImg":"","author":"Matt Yonkovit","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-200x107.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_5/2021/09/27/16-55-28-463_1e3159e76fb6beb5.webp","https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-300x160.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn31@2020_2/2021/09/27/16-55-26-624_f9aba6131f3c0039.webp"},"publishedOrCreatedDate":1632761712757},{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","title":"Introducing xbcloud: Exponential Backoff Feature in Percona XtraBackup","link":"https://www.percona.com/blog/?p=77939","description":"<img width=\"200\" height=\"105\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-200x105.png\" class=\"webfeedsFeaturedVisual wp-post-image\" alt=\"xbcloud Percona XtraBackup\" loading=\"lazy\" style=\"display: block; margin-bottom: 5px; clear:both;max-width: 100%;\" link_thumbnail=\"\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup.png 1200w\" sizes=\"(max-width: 200px) 100vw, 200px\" /><p><img loading=\"lazy\" class=\"alignright size-medium wp-image-78001\" src=\"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-300x157.png\" alt=\"xbcloud Percona XtraBackup\" width=\"300\" height=\"157\" srcset=\"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-300x157.png 300w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-1024x536.png 1024w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-200x105.png 200w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-1140x595.png 1140w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-367x192.png 367w, https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup.png 1200w\" sizes=\"(max-width: 300px) 100vw, 300px\" />Storing your data locally can impose security and availability risks. Major cloud providers have object storage services available to allow you to upload and distribute data across different regions using various retention and restore policies.</p>\n<p>Percona XtraBackup delivers the xbcloud binary &#8211; an auxiliary tool to allow users to upload backups to different cloud providers directly.</p>\n<p>Today we are glad to announce the introduction of the Exponential Backoff feature to xbcloud.</p>\n<p>In short, this new feature will allow your backup upload/download to work better with unstable network connections by retrying each chunk and adding an exponential wait time in between retries, increasing the chances of completion in case of an unstable connection or network glitch.</p>\n<p>This new functionality is available on today&#8217;s release of <a href=\"https://www.percona.com/software/mysql-database/percona-xtrabackup\">Percona XtraBackup 8.0.26</a> and will be available in Percona XtraBackup 2.4.24.</p>\n<h2>How it Works &#8211; in General</h2>\n<p><span>Whenever one chunk upload or download fails to complete its operation, xbcloud will check the reason for the failure. It can be either a CURL / HTTP or a client-specific error. If the error is listed as retriable (more about that later in this post), xbcloud will backoff/sleep for a certain amount of time before trying again. It will retry the same chunk 10 times before aborting the whole process. 10 is the default retry amount and can be configured via <code>--max-retries</code> parameter.</span></p>\n<h2>How it Works &#8211; Backoff Algorithm</h2>\n<p>Network glitches/instabilities usually happen for a short period of time. To make xbcloud tool more reliable and increase the chances of a backup upload/download to complete during those instabilities, we pause for a certain period of time before retrying the same chunk. The algorithm chosen is known as <a href=\"https://en.wikipedia.org/wiki/Exponential_backoff\">exponential backoff</a>.</p>\n<p>In the case of a retry, we calculate the power of two using the number of retries we already did for that specific chunk as the exponential factor. Since xbcloud does multiple asynchronous requests in parallel, we factor in a random number of milliseconds between 1 and 1000 to each chunk. This is to avoid all asynchronous request backoff for the same amount of time and retry all at once, which could cause network congestion.</p>\n<p><span>The backoff time will keep increasing as the same chunk keeps failing to upload/download. Getting by example the default <code>--max-retry</code> of 10, that would mean the last backoff will be around 17 minutes. </span></p>\n<p><span>To overcome this, we have implemented the <code>--max-backoff</code> parameter. This parameter defines the maximum time the program can sleep in milliseconds between chunk retries &#8211; Default to 300000 (5 minutes).</span></p>\n<h2>How it Works &#8211; Retriable Errors</h2>\n<p>We have a set of errors that we know we should retry the operations. For CURL, we retry on:<span><br />\n</span><span><br />\n</span></p><pre class=\"crayon-plain-tag\">CURLE_GOT_NOTHING\nCURLE_OPERATION_TIMEDOUT\nCURLE_RECV_ERROR\nCURLE_SEND_ERROR\nCURLE_SEND_FAIL_REWIND\nCURLE_PARTIAL_FILE\nCURLE_SSL_CONNECT_ERROR</pre><p><span>For HTTP, we retry the operation in case of the following status codes:</span></p><pre class=\"crayon-plain-tag\">503\n500\n504\n408</pre><p><span>Each cloud provider might return a different CURL or HTTP error depending on the issue. To allow users to extend this list and not rely on us providing a new version of xbcloud, we created a mechanism to allow users to extend this list.</span></p>\n<p><span>One can add new errors by setting <code>--curl-retriable-errors</code> / <code>--http-retriable-errors</code> respectively.</span></p>\n<p><span>On top of that, we have enhanced the error handling when using <code>--verbose</code> output to specify in which error xbcloud failed and what parameter a user will have to add to retry on this error. Here is one example:</span></p><pre class=\"crayon-plain-tag\">210701 14:34:23 /work/pxb/ins/8.0/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data)\n210701 14:34:23 /work/pxb/ins/8.0/bin/xbcloud: Curl error (52) Server returned nothing (no headers, no data) is not configured as retriable. You can allow it by adding <code>--curl-retriable-errors=52</code> parameter</pre><p><span>Those options accept a comma list of error codes.</span></p>\n<h2><b>Example</b></h2>\n<p><span>Below is one example of xbcloud exponential backoff in practice used with <code>--max-retries=5 --max-backoff=10000</code>: </span></p><pre class=\"crayon-plain-tag\">210702 10:07:05 /work/pxb/ins/8.0/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data)\n210702 10:07:05 /work/pxb/ins/8.0/bin/xbcloud: Sleeping for 2384 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [1]\n\n. . .\n\n210702 10:07:23 /work/pxb/ins/8.0/bin/xbcloud: Operation failed. Error: Server returned nothing (no headers, no data)\n210702 10:07:23 /work/pxb/ins/8.0/bin/xbcloud: Sleeping for 4387 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [2]\n\n. . .\n\n210702 10:07:52 /work/pxb/ins/8.0/bin/xbcloud: Operation failed. Error: Failed sending data to the peer\n210702 10:07:52 /work/pxb/ins/8.0/bin/xbcloud: Sleeping for 8691 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [3]\n\n. . .\n\n210702 10:08:47 /work/pxb/ins/8.0/bin/xbcloud: Operation failed. Error: Failed sending data to the peer\n210702 10:08:47 /work/pxb/ins/8.0/bin/xbcloud: Sleeping for 10000 ms before retrying backup3/xtrabackup_logfile.00000000000000000006 [4]\n\n. . .\n\n210702 10:10:12 /work/pxb/ins/8.0/bin/xbcloud: successfully uploaded chunk: backup3/xtrabackup_logfile.00000000000000000006, size: 8388660</pre><p><span>Let&#8217;s analyze the snippet log above:</span></p>\n<ol>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>Chunk </span><span>xtrabackup_logfile.00000000000000000006 </span><span>failed to upload by the first time (as seen in the [1] above) and slept for 2384 milliseconds.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>Then the same chunk filed by the second time (as seen by the number within [] ) exponentially increasing the sleep time by 2</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>When the chunk failed by the third time, we continued exponentially increasing the sleep time to around 8 seconds</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>On the fourth time, we would originally increase the exponential time to around 16 seconds; however, we have used <code>--max-backoff=10000</code>which means that is the maximum sleep time between retries, resulting in the program waiting 10 seconds before trying the same chunk again.</span></li>\n<li style=\"font-weight: 400;\" aria-level=\"1\"><span>Then we can see that in the end, it successfully uploaded the chunk  </span><span>xtrabackup_logfile.00000000000000000006</span></li>\n</ol>\n<h2>Summary</h2>\n<p>Best practices recommend distributing your backups to different locations. Cloud providers have dedicated services for this purpose. Using xbcloud alongside Percona XtraBackup are the tools to ensure you meet this requirement when talking about MySQL backup. On the other hand, we know that network connectivity can be unstable at the worst times. The new version of xbcloud won&#8217;t stop you from completing your backups as it will be more resilient to those instabilities with a variety of options to tune the network transfer.</p>\n<p><strong>Percona Distribution for MySQL is the most complete, stable, scalable, and secure, open-source MySQL solution available, delivering enterprise-grade database environments for your most critical business applications&#8230; and it&#8217;s free to use!</strong></p>\n<p style=\"text-align: center;\"><a class=\"btn btn-primary btn-lg\" href=\"https://www.percona.com/software/mysql-database\" rel=\"noopener\">Download Percona Distribution for MySQL Today</a></p>\n","descriptionType":"html","publishedDate":"Thu, 02 Sep 2021 12:02:15 +0000","feedId":11,"bgimg":"","linkMd5":"7753ee447a6aa33243eabfd90c98fe3b","bgimgJsdelivr":"","metaImg":"","author":"Marcelo Altmann","articleImgCdnMap":{"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-200x105.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn92@2020_3/2021/09/27/16-55-32-433_8fcb9c315eb6e9f8.webp","https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-300x157.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn19@2020_6/2021/09/27/16-55-25-666_3cc9ddd77a9278a7.webp"},"publishedOrCreatedDate":1632761712754}],"record":{"createdTime":"2021-09-28 00:55:12","updatedTime":"2021-09-28 00:55:12","feedId":11,"fetchDate":"Mon, 27 Sep 2021 16:55:12 +0000","fetchMs":190,"handleMs":1814,"totalMs":28145,"newArticles":0,"totalArticles":40,"status":1,"type":0,"ip":"245d88d7b2d8f26704713c23b090d029","hostName":"us-012*","requestId":"3c8d5299ee5047ef9aca0576de52acf9_11","contentType":"application/rss+xml; charset=UTF-8","totalBytes":1753110,"bgimgsTotal":0,"bgimgsGithubTotal":0,"articlesImgsTotal":112,"articlesImgsGithubTotal":112,"successGithubMap":{"myreaderx14":4,"myreaderx8":3,"myreaderx7":3,"myreaderx15":4,"myreaderx16":4,"myreaderx6":3,"myreaderx10":4,"myreaderx32":4,"myreaderx4":4,"myreaderx3":4,"myreaderx11":4,"myreaderx33":4,"myreaderx12":4,"myreaderx2":4,"myreaderx1":4,"myreaderx13":4,"myreaderx30":4,"myreaderx31":4,"myreaderx18":4,"myreaderx19":4,"myreaderx":4,"myreaderx25":4,"myreaderx27":4,"myreaderx21":3,"myreaderx22":4,"myreaderx23":4,"myreaderx24":4,"myreaderx5oss":4,"myreaderx29":4},"failGithubMap":{}},"feed":{"createdTime":"2020-05-30 17:21:38","updatedTime":"2020-09-01 09:23:03","id":11,"name":"Percona Database Performance Blog","url":"https://www.percona.com/blog/feed/","subscriber":null,"website":null,"icon":"https://www.percona.com/blog/wp-content/uploads/2018/09/percona-32x32.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn89@2020_6/2020/09/01/01-23-01-358_b72bb3b39c378fe6.png","description":"","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":1753110,"tmpBgImgCdnBytes":0,"extra4":{"start":1632761710664,"total":0,"statList":[{"spend":292,"msg":"获取xml内容"},{"spend":1814,"msg":"解释文章"},{"spend":2,"msg":"上传封面图到cdn"},{"spend":1,"msg":"修正封面图上传失败重新上传"},{"spend":25777,"msg":"正文链接上传到cdn"}]},"extra5":112,"extra6":112,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-037.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-002.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-011.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-017.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe21.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200,200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-029.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-013.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-55.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200,200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-010.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-59.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-51.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-023.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Database-Security-OS-Authentication-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn7@2020_2/2021/09/27/16-55-13-572_579ed19e13870372.webp","sourceBytes":12363,"destBytes":3480,"targetWebpQuality":75,"feedId":11,"totalSpendMs":361,"convertSpendMs":4,"createdTime":"2021-09-28 00:55:13","host":"us-010*","referer":"https://www.percona.com/blog/?p=77706","linkMd5ListStr":"090b64fde228ee0411a7015959a7d80b","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"12.1 KB","destSize":"3.4 KB","compressRate":"28.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn11@2020_6/2021/09/27/16-55-13-578_ff3b7983f62af910.webp","sourceBytes":20689,"destBytes":5676,"targetWebpQuality":75,"feedId":11,"totalSpendMs":404,"convertSpendMs":6,"createdTime":"2021-09-28 00:55:13","host":"us-011*","referer":"https://www.percona.com/blog/?p=78231","linkMd5ListStr":"2a15e491a2f1ad9f856674470ad33a3b","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.2 KB","destSize":"5.5 KB","compressRate":"27.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn52@2020_3/2021/09/27/16-55-13-778_a34ae542ce52dc46.webp","sourceBytes":33407,"destBytes":5176,"targetWebpQuality":75,"feedId":11,"totalSpendMs":508,"convertSpendMs":30,"createdTime":"2021-09-28 00:55:13","host":"europe63*","referer":"https://www.percona.com/blog/?p=78118","linkMd5ListStr":"bcf5931fb660a9a42d53f2b91979573e","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"32.6 KB","destSize":"5.1 KB","compressRate":"15.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Node-Processes-Dashboard-1024x669.png","sourceStatusCode":200,"destWidth":1024,"destHeight":669,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn88@2020_2/2021/09/27/16-55-13-820_40729e61da707652.webp","sourceBytes":566544,"destBytes":40644,"targetWebpQuality":75,"feedId":11,"totalSpendMs":678,"convertSpendMs":48,"createdTime":"2021-09-28 00:55:13","host":"europe63*","referer":"https://www.percona.com/blog/?p=77955","linkMd5ListStr":"e6b3d0a4cb30eb097eded1b2264f4cf4","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"553.3 KB","destSize":"39.7 KB","compressRate":"7.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn40@2020_1/2021/09/27/16-55-13-761_026f88f5b377aa3f.webp","sourceBytes":24039,"destBytes":7690,"targetWebpQuality":75,"feedId":11,"totalSpendMs":893,"convertSpendMs":36,"createdTime":"2021-09-28 00:55:13","host":"us-004*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23.5 KB","destSize":"7.5 KB","compressRate":"32%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-300x160.png","sourceStatusCode":200,"destWidth":300,"destHeight":160,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn95@2020_5/2021/09/27/16-55-13-746_60faace583dffc0d.webp","sourceBytes":60122,"destBytes":8376,"targetWebpQuality":75,"feedId":11,"totalSpendMs":888,"convertSpendMs":8,"createdTime":"2021-09-28 00:55:13","host":"us-004*","referer":"https://www.percona.com/blog/?p=77805","linkMd5ListStr":"26d1aeebbf957eb34b336a86fce47739","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"58.7 KB","destSize":"8.2 KB","compressRate":"13.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-3.png","sourceStatusCode":200,"destWidth":649,"destHeight":372,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn63@2020_6/2021/09/27/16-55-14-030_ffd6a2cc0cb038af.webp","sourceBytes":23936,"destBytes":23434,"targetWebpQuality":75,"feedId":11,"totalSpendMs":847,"convertSpendMs":16,"createdTime":"2021-09-28 00:55:13","host":"europe21*","referer":"https://www.percona.com/blog/?p=77794","linkMd5ListStr":"f9d07f2139995f2c0bf057c6526f3e9e","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23.4 KB","destSize":"22.9 KB","compressRate":"97.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/db_table_size_details-1024x794.png","sourceStatusCode":200,"destWidth":1024,"destHeight":794,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn27@2020_1/2021/09/27/16-55-14-298_892ad7e4947f51ab.webp","sourceBytes":268736,"destBytes":45578,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1130,"convertSpendMs":40,"createdTime":"2021-09-28 00:55:13","host":"europe21*","referer":"https://www.percona.com/blog/?p=77580","linkMd5ListStr":"10f022279cf429a86442d1461019b2e0","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"262.4 KB","destSize":"44.5 KB","compressRate":"17%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/CPU-1024x223.png","sourceStatusCode":200,"destWidth":1024,"destHeight":223,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn68@2020_2/2021/09/27/16-55-14-483_5a7267e77a88ca59.webp","sourceBytes":142165,"destBytes":19832,"targetWebpQuality":75,"feedId":11,"totalSpendMs":937,"convertSpendMs":504,"createdTime":"2021-09-28 00:55:13","host":"us-004*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"138.8 KB","destSize":"19.4 KB","compressRate":"13.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/diosk_latency-1024x187.png","sourceStatusCode":200,"destWidth":1024,"destHeight":187,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn92@2020_6/2021/09/27/16-55-15-420_fe0e4557fcdc81e1.webp","sourceBytes":122045,"destBytes":20212,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1171,"convertSpendMs":13,"createdTime":"2021-09-28 00:55:14","host":"europe21*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"119.2 KB","destSize":"19.7 KB","compressRate":"16.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn20@2020_5/2021/09/27/16-55-24-187_73cfc11b6a74458b.webp","sourceBytes":22271,"destBytes":6222,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1583,"convertSpendMs":9,"createdTime":"2021-09-28 00:55:23","host":"europe-25*","referer":"https://www.percona.com/blog/?p=78201","linkMd5ListStr":"3beda44dd1fc5760fd7b9f35eabacc9b","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.7 KB","destSize":"6.1 KB","compressRate":"27.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn47@2020_1/2021/09/27/16-55-23-739_339254e818ee4e02.webp","sourceBytes":10626,"destBytes":2842,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1505,"convertSpendMs":6,"createdTime":"2021-09-28 00:55:23","host":"europe-25*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"10.4 KB","destSize":"2.8 KB","compressRate":"26.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Aurora-MySQL-Write-Latency-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn83@2020_4/2021/09/27/16-55-25-109_2b02128c70e8ce5e.webp","sourceBytes":22613,"destBytes":5798,"targetWebpQuality":75,"feedId":11,"totalSpendMs":458,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:25","host":"us-021*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"22.1 KB","destSize":"5.7 KB","compressRate":"25.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-1024x717.png","sourceStatusCode":200,"destWidth":1024,"destHeight":717,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn84@2020_3/2021/09/27/16-55-24-414_5f3f01c7b24f1939.webp","sourceBytes":278762,"destBytes":23348,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2112,"convertSpendMs":91,"createdTime":"2021-09-28 00:55:23","host":"europe-25*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"272.2 KB","destSize":"22.8 KB","compressRate":"8.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn19@2020_6/2021/09/27/16-55-25-666_3cc9ddd77a9278a7.webp","sourceBytes":27347,"destBytes":10814,"targetWebpQuality":75,"feedId":11,"totalSpendMs":363,"convertSpendMs":10,"createdTime":"2021-09-28 00:55:25","host":"us-021*","referer":"https://www.percona.com/blog/?p=77939","linkMd5ListStr":"7753ee447a6aa33243eabfd90c98fe3b","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"26.7 KB","destSize":"10.6 KB","compressRate":"39.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-4.png","sourceStatusCode":200,"destWidth":518,"destHeight":365,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn99@2020_4/2021/09/27/16-55-25-699_961658467a02b4bf.webp","sourceBytes":17212,"destBytes":11790,"targetWebpQuality":75,"feedId":11,"totalSpendMs":434,"convertSpendMs":8,"createdTime":"2021-09-28 00:55:25","host":"us-002*","referer":"https://www.percona.com/blog/?p=77794","linkMd5ListStr":"f9d07f2139995f2c0bf057c6526f3e9e","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.8 KB","destSize":"11.5 KB","compressRate":"68.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Modern-Web-Based-Application-Architecture-101-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn33@2020_2/2021/09/27/16-55-25-571_1d36f819f68d7416.webp","sourceBytes":14109,"destBytes":3372,"targetWebpQuality":75,"feedId":11,"totalSpendMs":862,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:25","host":"us-016*","referer":"https://www.percona.com/blog/?p=78201","linkMd5ListStr":"3beda44dd1fc5760fd7b9f35eabacc9b","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"13.8 KB","destSize":"3.3 KB","compressRate":"23.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn56@2020_1/2021/09/27/16-55-26-063_67d4b866ef648afe.webp","sourceBytes":86343,"destBytes":10680,"targetWebpQuality":75,"feedId":11,"totalSpendMs":898,"convertSpendMs":9,"createdTime":"2021-09-28 00:55:25","host":"us-021*","referer":"https://www.percona.com/blog/?p=77766","linkMd5ListStr":"7f8976aebdbb00e3a6971678bc614718","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"84.3 KB","destSize":"10.4 KB","compressRate":"12.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn11@2020_4/2021/09/27/16-55-26-017_21ce9097030c2e0b.webp","sourceBytes":86440,"destBytes":10654,"targetWebpQuality":75,"feedId":11,"totalSpendMs":925,"convertSpendMs":7,"createdTime":"2021-09-28 00:55:25","host":"europe-25*","referer":"https://www.percona.com/blog/?p=77995","linkMd5ListStr":"6562f34bba189c0b252dfb95abb799ec","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"84.4 KB","destSize":"10.4 KB","compressRate":"12.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn16@2020_2/2021/09/27/16-55-25-926_d603892886c805ef.webp","sourceBytes":22914,"destBytes":6260,"targetWebpQuality":75,"feedId":11,"totalSpendMs":709,"convertSpendMs":7,"createdTime":"2021-09-28 00:55:25","host":"europe63*","referer":"https://www.percona.com/blog/?p=67094","linkMd5ListStr":"409d87d45678dd7a1a89e474d21b7f32","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"22.4 KB","destSize":"6.1 KB","compressRate":"27.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-300x160.png","sourceStatusCode":200,"destWidth":300,"destHeight":160,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn31@2020_2/2021/09/27/16-55-26-624_f9aba6131f3c0039.webp","sourceBytes":61039,"destBytes":8778,"targetWebpQuality":75,"feedId":11,"totalSpendMs":514,"convertSpendMs":13,"createdTime":"2021-09-28 00:55:26","host":"us-004*","referer":"https://www.percona.com/blog/?p=77838","linkMd5ListStr":"5142508387f6865bb518416e22d1c088","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"59.6 KB","destSize":"8.6 KB","compressRate":"14.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-200x113.png","sourceStatusCode":200,"destWidth":200,"destHeight":113,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn68@2020_2/2021/09/27/16-55-27-737_a536c573d07da3c0.webp","sourceBytes":11974,"destBytes":4878,"targetWebpQuality":75,"feedId":11,"totalSpendMs":596,"convertSpendMs":3,"createdTime":"2021-09-28 00:55:27","host":"europe-59*","referer":"https://www.percona.com/blog/?p=78127","linkMd5ListStr":"4bf6e9f728499e7f0379036829919039","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"11.7 KB","destSize":"4.8 KB","compressRate":"40.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-200x113.png","sourceStatusCode":200,"destWidth":200,"destHeight":113,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn98@2020_6/2021/09/27/16-55-28-149_1e69f3054112119d.webp","sourceBytes":17421,"destBytes":3652,"targetWebpQuality":75,"feedId":11,"totalSpendMs":349,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:28","host":"us-003*","referer":"https://www.percona.com/blog/?p=77764","linkMd5ListStr":"f4569ae0b391d617606fda9b40343180","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"17 KB","destSize":"3.6 KB","compressRate":"21%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Sharding-Theory-1-1024x626.png","sourceStatusCode":200,"destWidth":1024,"destHeight":626,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn96@2020_6/2021/09/27/16-55-27-773_287df2a73071abf1.webp","sourceBytes":249822,"destBytes":77368,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1713,"convertSpendMs":56,"createdTime":"2021-09-28 00:55:26","host":"europe-59*","referer":"https://www.percona.com/blog/?p=78112","linkMd5ListStr":"ea953c59c8bf260d1c6cae3b7147a7b1","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"244 KB","destSize":"75.6 KB","compressRate":"31%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Open-Source-MongoDB-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn47@2020_3/2021/09/27/16-55-28-490_b19895ebee0135c1.webp","sourceBytes":69836,"destBytes":9880,"targetWebpQuality":75,"feedId":11,"totalSpendMs":447,"convertSpendMs":8,"createdTime":"2021-09-28 00:55:28","host":"us-021*","referer":"https://www.percona.com/blog/?p=78118","linkMd5ListStr":"bcf5931fb660a9a42d53f2b91979573e","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"68.2 KB","destSize":"9.6 KB","compressRate":"14.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_tp-1024x717.png","sourceStatusCode":200,"destWidth":1024,"destHeight":717,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn4@2020_6/2021/09/27/16-55-28-037_4a2f04c147196fbc.webp","sourceBytes":239517,"destBytes":15734,"targetWebpQuality":75,"feedId":11,"totalSpendMs":887,"convertSpendMs":82,"createdTime":"2021-09-28 00:55:27","host":"us-023*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"233.9 KB","destSize":"15.4 KB","compressRate":"6.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.54-PM-183x300.png","sourceStatusCode":200,"destWidth":183,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn32@2020_6/2021/09/27/16-55-28-139_d163a4fa3142ec4c.webp","sourceBytes":42796,"destBytes":12338,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1305,"convertSpendMs":7,"createdTime":"2021-09-28 00:55:27","host":"europe-59*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.8 KB","destSize":"12 KB","compressRate":"28.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn59@2020_4/2021/09/27/16-55-28-882_f994c43eb1039982.webp","sourceBytes":35640,"destBytes":6724,"targetWebpQuality":75,"feedId":11,"totalSpendMs":486,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:28","host":"europe-59*","referer":"https://www.percona.com/blog/?p=78021","linkMd5ListStr":"b939b5f98c176b2d65c6cfdd64e52a4a","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"34.8 KB","destSize":"6.6 KB","compressRate":"18.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.51-1024x365.png","sourceStatusCode":200,"destWidth":1024,"destHeight":365,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn52@2020_2/2021/09/27/16-55-28-823_ad5205f378ea161c.webp","sourceBytes":272108,"destBytes":19974,"targetWebpQuality":75,"feedId":11,"totalSpendMs":967,"convertSpendMs":66,"createdTime":"2021-09-28 00:55:28","host":"us-008*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"265.7 KB","destSize":"19.5 KB","compressRate":"7.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/disk_ops-1024x218.png","sourceStatusCode":200,"destWidth":1024,"destHeight":218,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn60@2020_1/2021/09/27/16-55-27-990_7b403c734bd0757c.webp","sourceBytes":193311,"destBytes":28506,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1326,"convertSpendMs":82,"createdTime":"2021-09-28 00:55:27","host":"us-008*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"188.8 KB","destSize":"27.8 KB","compressRate":"14.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn87@2020_2/2021/09/27/16-55-28-691_561700ba6ddbe560.webp","sourceBytes":13971,"destBytes":4040,"targetWebpQuality":75,"feedId":11,"totalSpendMs":483,"convertSpendMs":29,"createdTime":"2021-09-28 00:55:28","host":"us-008*","referer":"https://www.percona.com/blog/?p=78112","linkMd5ListStr":"ea953c59c8bf260d1c6cae3b7147a7b1","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"13.6 KB","destSize":"3.9 KB","compressRate":"28.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn23@2020_1/2021/09/27/16-55-28-003_76f9d051147f2166.webp","sourceBytes":15848,"destBytes":6578,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1420,"convertSpendMs":147,"createdTime":"2021-09-28 00:55:27","host":"us-008*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"15.5 KB","destSize":"6.4 KB","compressRate":"41.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-JSON-PostgreSQL-200x107.png","sourceStatusCode":200,"destWidth":200,"destHeight":107,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn8@2020_1/2021/09/27/16-55-27-823_b4c4d15d70e5e43a.webp","sourceBytes":39808,"destBytes":5516,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1728,"convertSpendMs":10,"createdTime":"2021-09-28 00:55:27","host":"europe67*","referer":"https://www.percona.com/blog/?p=77983","linkMd5ListStr":"28cf360982906e2016e175d9735638e0","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"38.9 KB","destSize":"5.4 KB","compressRate":"13.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Query-Performance-Troubleshooting-1024x460.png","sourceStatusCode":200,"destWidth":1024,"destHeight":460,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn35@2020_3/2021/09/27/16-55-27-789_718169d7891e9607.webp","sourceBytes":366847,"destBytes":45386,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2856,"convertSpendMs":121,"createdTime":"2021-09-28 00:55:26","host":"europe67*","referer":"https://www.percona.com/blog/?p=77955","linkMd5ListStr":"e6b3d0a4cb30eb097eded1b2264f4cf4","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"358.2 KB","destSize":"44.3 KB","compressRate":"12.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Storing-JSON-MySQL-200x107.png","sourceStatusCode":200,"destWidth":200,"destHeight":107,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn43@2020_5/2021/09/27/16-55-28-463_1e3159e76fb6beb5.webp","sourceBytes":31785,"destBytes":4962,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1813,"convertSpendMs":17,"createdTime":"2021-09-28 00:55:27","host":"europe67*","referer":"https://www.percona.com/blog/?p=77838","linkMd5ListStr":"5142508387f6865bb518416e22d1c088","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31 KB","destSize":"4.8 KB","compressRate":"15.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn80@2020_6/2021/09/27/16-55-28-375_d242e2aa4836364a.webp","sourceBytes":19647,"destBytes":6182,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1236,"convertSpendMs":751,"createdTime":"2021-09-28 00:55:27","host":"us-013*","referer":"https://www.percona.com/blog/?p=78046","linkMd5ListStr":"4e76fe7de77593ea1899702f180afb79","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"19.2 KB","destSize":"6 KB","compressRate":"31.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/EBS-Storage-Types-in-AWS-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn72@2020_1/2021/09/27/16-55-28-754_b671a1899b08eb7f.webp","sourceBytes":10696,"destBytes":3614,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1378,"convertSpendMs":6,"createdTime":"2021-09-28 00:55:28","host":"europe67*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"10.4 KB","destSize":"3.5 KB","compressRate":"33.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.15.39-1024x356.png","sourceStatusCode":200,"destWidth":1024,"destHeight":356,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn44@2020_6/2021/09/27/16-55-28-738_30ccef3481aa175c.webp","sourceBytes":187530,"destBytes":19324,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1546,"convertSpendMs":114,"createdTime":"2021-09-28 00:55:27","host":"us-013*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"183.1 KB","destSize":"18.9 KB","compressRate":"10.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.46.00-PM-232x300.png","sourceStatusCode":200,"destWidth":232,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn8@2020_2/2021/09/27/16-55-28-314_428e31c31611ab52.webp","sourceBytes":30291,"destBytes":5640,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1965,"convertSpendMs":722,"createdTime":"2021-09-28 00:55:27","host":"us-013*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"29.6 KB","destSize":"5.5 KB","compressRate":"18.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn56@2020_6/2021/09/27/16-55-29-301_e573a330d261a75a.webp","sourceBytes":86324,"destBytes":10678,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1057,"convertSpendMs":7,"createdTime":"2021-09-28 00:55:28","host":"europe21*","referer":"https://www.percona.com/blog/?p=77567","linkMd5ListStr":"3a26926be8500c5b1856e9eed70b2987","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"84.3 KB","destSize":"10.4 KB","compressRate":"12.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn95@2020_1/2021/09/27/16-55-29-894_cd90abc4a01143f4.webp","sourceBytes":18983,"destBytes":5406,"targetWebpQuality":75,"feedId":11,"totalSpendMs":333,"convertSpendMs":22,"createdTime":"2021-09-28 00:55:29","host":"us-004*","referer":"https://www.percona.com/blog/?p=77762","linkMd5ListStr":"80883f465440ff80e26e48846522c5d0","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"18.5 KB","destSize":"5.3 KB","compressRate":"28.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/RED-Method-for-MySQL-Dashboard-1024x400.png","sourceStatusCode":200,"destWidth":1024,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn5@2020_5/2021/09/27/16-55-28-741_af912f688a1ea139.webp","sourceBytes":403715,"destBytes":37126,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2758,"convertSpendMs":102,"createdTime":"2021-09-28 00:55:27","host":"us-013*","referer":"https://www.percona.com/blog/?p=77955","linkMd5ListStr":"e6b3d0a4cb30eb097eded1b2264f4cf4","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"394.3 KB","destSize":"36.3 KB","compressRate":"9.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-20-at-1.20.06-PM.png","sourceStatusCode":200,"destWidth":641,"destHeight":422,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn72@2020_2/2021/09/27/16-55-29-826_45b2aca5a67fec68.webp","sourceBytes":42388,"destBytes":10772,"targetWebpQuality":75,"feedId":11,"totalSpendMs":705,"convertSpendMs":55,"createdTime":"2021-09-28 00:55:29","host":"us-013*","referer":"https://www.percona.com/blog/?p=78201","linkMd5ListStr":"3beda44dd1fc5760fd7b9f35eabacc9b","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.4 KB","destSize":"10.5 KB","compressRate":"25.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-19-at-7.22.28-AM-1024x470.png","sourceStatusCode":200,"destWidth":1024,"destHeight":470,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn11@2020_2/2021/09/27/16-55-30-030_d3a82ca7ad24c8f5.webp","sourceBytes":220095,"destBytes":32344,"targetWebpQuality":75,"feedId":11,"totalSpendMs":420,"convertSpendMs":25,"createdTime":"2021-09-28 00:55:29","host":"us-021*","referer":"https://www.percona.com/blog/?p=77805","linkMd5ListStr":"26d1aeebbf957eb34b336a86fce47739","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"214.9 KB","destSize":"31.6 KB","compressRate":"14.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.08.21-1024x357.png","sourceStatusCode":200,"destWidth":1024,"destHeight":357,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn20@2020_1/2021/09/27/16-55-30-043_eb87e49f0725e623.webp","sourceBytes":282768,"destBytes":14710,"targetWebpQuality":75,"feedId":11,"totalSpendMs":516,"convertSpendMs":26,"createdTime":"2021-09-28 00:55:29","host":"europe21*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"276.1 KB","destSize":"14.4 KB","compressRate":"5.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn36@2020_5/2021/09/27/16-55-30-440_094639bd6412d0d5.webp","sourceBytes":20802,"destBytes":6088,"targetWebpQuality":75,"feedId":11,"totalSpendMs":398,"convertSpendMs":8,"createdTime":"2021-09-28 00:55:30","host":"us-013*","referer":"https://www.percona.com/blog/?p=77972","linkMd5ListStr":"d5045f92ffbef5c18457d10a4a674bd8","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.3 KB","destSize":"5.9 KB","compressRate":"29.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.26-PM-186x300.png","sourceStatusCode":200,"destWidth":186,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn78@2020_5/2021/09/27/16-55-30-213_3a50842f22544b77.webp","sourceBytes":27795,"destBytes":6164,"targetWebpQuality":75,"feedId":11,"totalSpendMs":927,"convertSpendMs":6,"createdTime":"2021-09-28 00:55:29","host":"europe62*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"27.1 KB","destSize":"6 KB","compressRate":"22.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-Aug-30-2021-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn15@2020_4/2021/09/27/16-55-30-619_c8c365637062cb63.webp","sourceBytes":42294,"destBytes":5050,"targetWebpQuality":75,"feedId":11,"totalSpendMs":909,"convertSpendMs":12,"createdTime":"2021-09-28 00:55:29","host":"us-008*","referer":"https://www.percona.com/blog/?p=77766","linkMd5ListStr":"7f8976aebdbb00e3a6971678bc614718","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.3 KB","destSize":"4.9 KB","compressRate":"11.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-async-replication-k8s-1024x540.png","sourceStatusCode":200,"destWidth":1024,"destHeight":540,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn23@2020_6/2021/09/27/16-55-30-827_8a8e426888f9f941.webp","sourceBytes":170013,"destBytes":27234,"targetWebpQuality":75,"feedId":11,"totalSpendMs":941,"convertSpendMs":26,"createdTime":"2021-09-28 00:55:30","host":"europe-59*","referer":"https://www.percona.com/blog/?p=78147","linkMd5ListStr":"4a519ab404d3f0653d0e6ea80e3a025b","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"166 KB","destSize":"26.6 KB","compressRate":"16%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/image2-1024x497.png","sourceStatusCode":200,"destWidth":1024,"destHeight":497,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn76@2020_2/2021/09/27/16-55-30-624_5f889c57fa94d51b.webp","sourceBytes":231017,"destBytes":51742,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1458,"convertSpendMs":174,"createdTime":"2021-09-28 00:55:29","host":"europe63*","referer":"https://www.percona.com/blog/?p=77814","linkMd5ListStr":"cd56b4a01b3e6ffde151f46fb441c68d","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"225.6 KB","destSize":"50.5 KB","compressRate":"22.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-2-1024x227.png","sourceStatusCode":200,"destWidth":1024,"destHeight":227,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn99@2020_1/2021/09/27/16-55-30-632_974f67deeaec1197.webp","sourceBytes":115290,"destBytes":16246,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1562,"convertSpendMs":131,"createdTime":"2021-09-28 00:55:29","host":"europe67*","referer":"https://www.percona.com/blog/?p=77794","linkMd5ListStr":"f9d07f2139995f2c0bf057c6526f3e9e","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"112.6 KB","destSize":"15.9 KB","compressRate":"14.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/PostgreSQL_Sculling_Blog_1200x628-200x105.jpeg","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn28@2020_6/2021/09/27/16-55-30-535_1e017e538c7c8fdf.webp","sourceBytes":9310,"destBytes":7846,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1003,"convertSpendMs":4,"createdTime":"2021-09-28 00:55:30","host":"us-037*","referer":"https://www.percona.com/blog/?p=78152","linkMd5ListStr":"805e448a35bc112aae8e165558adf706","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.1 KB","destSize":"7.7 KB","compressRate":"84.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/PostgreSQL-Custom-Dashboards-Percona-Monitoring-and-Management-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn43@2020_6/2021/09/27/16-55-31-257_baa3c915773b56ba.webp","sourceBytes":15802,"destBytes":3818,"targetWebpQuality":75,"feedId":11,"totalSpendMs":794,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:30","host":"europe63*","referer":"https://www.percona.com/blog/?p=77580","linkMd5ListStr":"10f022279cf429a86442d1461019b2e0","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"15.4 KB","destSize":"3.7 KB","compressRate":"24.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn59@2020_2/2021/09/27/16-55-31-415_3026f8645fd08826.webp","sourceBytes":42411,"destBytes":5060,"targetWebpQuality":75,"feedId":11,"totalSpendMs":395,"convertSpendMs":11,"createdTime":"2021-09-28 00:55:31","host":"us-004*","referer":"https://www.percona.com/blog/?p=78116","linkMd5ListStr":"9abdd2fb0799c7aa92878f47bd27a89e","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.4 KB","destSize":"4.9 KB","compressRate":"11.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_tp-1024x717.png","sourceStatusCode":200,"destWidth":1024,"destHeight":717,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn40@2020_6/2021/09/27/16-55-31-282_246c9a5a168f9805.webp","sourceBytes":328322,"destBytes":29314,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1040,"convertSpendMs":37,"createdTime":"2021-09-28 00:55:30","host":"europe-25*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"320.6 KB","destSize":"28.6 KB","compressRate":"8.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Software-Release-AUg-16-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn76@2020_4/2021/09/27/16-55-31-795_62e598fbd4911813.webp","sourceBytes":42273,"destBytes":5056,"targetWebpQuality":75,"feedId":11,"totalSpendMs":353,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:31","host":"us-021*","referer":"https://www.percona.com/blog/?p=77567","linkMd5ListStr":"3a26926be8500c5b1856e9eed70b2987","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.3 KB","destSize":"4.9 KB","compressRate":"12%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_with_L2ARC.png","sourceStatusCode":200,"destWidth":861,"destHeight":484,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_6/2021/09/27/16-55-31-803_18e159f92cb738eb.webp","sourceBytes":60228,"destBytes":40006,"targetWebpQuality":75,"feedId":11,"totalSpendMs":404,"convertSpendMs":25,"createdTime":"2021-09-28 00:55:31","host":"us-008*","referer":"https://www.percona.com/blog/?p=78039","linkMd5ListStr":"c32ff24988146f7c0f1ecda919ae7452","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"58.8 KB","destSize":"39.1 KB","compressRate":"66.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Enhanced-Password-Management-Systems-in-MySQL-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn64@2020_2/2021/09/27/16-55-31-423_a0fb4bd8a24f74f5.webp","sourceBytes":39145,"destBytes":11854,"targetWebpQuality":75,"feedId":11,"totalSpendMs":981,"convertSpendMs":29,"createdTime":"2021-09-28 00:55:31","host":"us-037*","referer":"https://www.percona.com/blog/?p=78231","linkMd5ListStr":"2a15e491a2f1ad9f856674470ad33a3b","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"38.2 KB","destSize":"11.6 KB","compressRate":"30.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Storing-and-Using-JSON-Within-PostgreSQL-2-200x107.jpg","sourceStatusCode":200,"destWidth":200,"destHeight":107,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn4@2020_4/2021/09/27/16-55-31-294_ef8029c51317ad60.webp","sourceBytes":7643,"destBytes":5704,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2196,"convertSpendMs":189,"createdTime":"2021-09-28 00:55:30","host":"us-55*","referer":"https://www.percona.com/blog/?p=78018","linkMd5ListStr":"f8e87d9dba4c97a4151bbaf355a6d80d","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.5 KB","destSize":"5.6 KB","compressRate":"74.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Marcelo-Altmann-promotion-to-Oracle-ACE-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn12@2020_3/2021/09/27/16-55-32-322_15b0f2136ba2054d.webp","sourceBytes":14321,"destBytes":3564,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1026,"convertSpendMs":107,"createdTime":"2021-09-28 00:55:31","host":"us-55*","referer":"https://www.percona.com/blog/?p=77972","linkMd5ListStr":"d5045f92ffbef5c18457d10a4a674bd8","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"14 KB","destSize":"3.5 KB","compressRate":"24.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn75@2020_4/2021/09/27/16-55-31-296_cbf45e530c2a40aa.webp","sourceBytes":29912,"destBytes":5642,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1388,"convertSpendMs":167,"createdTime":"2021-09-28 00:55:30","host":"us-55*","referer":"https://www.percona.com/blog/?p=78147","linkMd5ListStr":"4a519ab404d3f0653d0e6ea80e3a025b","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"29.2 KB","destSize":"5.5 KB","compressRate":"18.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_chainrep.jpg","sourceStatusCode":200,"destWidth":601,"destHeight":144,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn39@2020_6/2021/09/27/16-55-31-323_e88ed2bc0082d275.webp","sourceBytes":20576,"destBytes":10972,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1344,"convertSpendMs":142,"createdTime":"2021-09-28 00:55:30","host":"us-55*","referer":"https://www.percona.com/blog/?p=78056","linkMd5ListStr":"d6b4ea19a58ca2a1e8a608f2981464f3","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.1 KB","destSize":"10.7 KB","compressRate":"53.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Resharding-in-MongoDB-5.0-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_4/2021/09/27/16-55-32-210_f0b6061f9200b01a.webp","sourceBytes":14211,"destBytes":3512,"targetWebpQuality":75,"feedId":11,"totalSpendMs":985,"convertSpendMs":9,"createdTime":"2021-09-28 00:55:31","host":"us-55*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"13.9 KB","destSize":"3.4 KB","compressRate":"24.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Screen-Shot-2021-09-01-at-8.41.08-AM-1024x230.png","sourceStatusCode":200,"destWidth":1024,"destHeight":230,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn64@2020_6/2021/09/27/16-55-32-444_ed09c0166e577427.webp","sourceBytes":196346,"destBytes":45812,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1556,"convertSpendMs":398,"createdTime":"2021-09-28 00:55:31","host":"europe67*","referer":"https://www.percona.com/blog/?p=77983","linkMd5ListStr":"28cf360982906e2016e175d9735638e0","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"191.7 KB","destSize":"44.7 KB","compressRate":"23.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.43-PM-216x300.png","sourceStatusCode":200,"destWidth":216,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn84@2020_2/2021/09/27/16-55-32-636_9cf23f3db9570823.webp","sourceBytes":31743,"destBytes":6532,"targetWebpQuality":75,"feedId":11,"totalSpendMs":847,"convertSpendMs":21,"createdTime":"2021-09-28 00:55:32","host":"europe21*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31 KB","destSize":"6.4 KB","compressRate":"20.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.10-PM-212x300.png","sourceStatusCode":200,"destWidth":212,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn88@2020_2/2021/09/27/16-55-32-697_ad0f915647b56c6b.webp","sourceBytes":42504,"destBytes":11886,"targetWebpQuality":75,"feedId":11,"totalSpendMs":876,"convertSpendMs":8,"createdTime":"2021-09-28 00:55:32","host":"europe-59*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.5 KB","destSize":"11.6 KB","compressRate":"28%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/xbcloud-Percona-XtraBackup-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn92@2020_3/2021/09/27/16-55-32-433_8fcb9c315eb6e9f8.webp","sourceBytes":18687,"destBytes":5780,"targetWebpQuality":75,"feedId":11,"totalSpendMs":431,"convertSpendMs":22,"createdTime":"2021-09-28 00:55:32","host":"us-037*","referer":"https://www.percona.com/blog/?p=77939","linkMd5ListStr":"7753ee447a6aa33243eabfd90c98fe3b","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"18.2 KB","destSize":"5.6 KB","compressRate":"30.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_l2arc_bcache.png","sourceStatusCode":200,"destWidth":756,"destHeight":425,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn35@2020_5/2021/09/27/16-55-32-462_025e05c7f615c187.webp","sourceBytes":64006,"destBytes":41312,"targetWebpQuality":75,"feedId":11,"totalSpendMs":963,"convertSpendMs":57,"createdTime":"2021-09-28 00:55:32","host":"us-037*","referer":"https://www.percona.com/blog/?p=78039","linkMd5ListStr":"c32ff24988146f7c0f1ecda919ae7452","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"62.5 KB","destSize":"40.3 KB","compressRate":"64.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-16.50.32.png","sourceStatusCode":200,"destWidth":1000,"destHeight":639,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn100@2020_5/2021/09/27/16-55-32-532_4f93bb1f18eb6b88.webp","sourceBytes":156390,"destBytes":40232,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1002,"convertSpendMs":51,"createdTime":"2021-09-28 00:55:32","host":"us-037*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"152.7 KB","destSize":"39.3 KB","compressRate":"25.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_1-1024x546.png","sourceStatusCode":200,"destWidth":1024,"destHeight":546,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn99@2020_3/2021/09/27/16-55-33-123_a93a92459f3d91f0.webp","sourceBytes":296930,"destBytes":51636,"targetWebpQuality":75,"feedId":11,"totalSpendMs":428,"convertSpendMs":40,"createdTime":"2021-09-28 00:55:32","host":"us-013*","referer":"https://www.percona.com/blog/?p=77580","linkMd5ListStr":"10f022279cf429a86442d1461019b2e0","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"290 KB","destSize":"50.4 KB","compressRate":"17.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/rndwr_lat-2-1024x717.png","sourceStatusCode":200,"destWidth":1024,"destHeight":717,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn71@2020_1/2021/09/27/16-55-32-541_fc96e3635bc63be4.webp","sourceBytes":278762,"destBytes":23348,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1253,"convertSpendMs":60,"createdTime":"2021-09-28 00:55:32","host":"us-037*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"272.2 KB","destSize":"22.8 KB","compressRate":"8.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/image3.png","sourceStatusCode":200,"destWidth":690,"destHeight":379,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn23@2020_2/2021/09/27/16-55-33-320_543eafd33af0c3c2.webp","sourceBytes":101828,"destBytes":31146,"targetWebpQuality":75,"feedId":11,"totalSpendMs":417,"convertSpendMs":26,"createdTime":"2021-09-28 00:55:33","host":"us-004*","referer":"https://www.percona.com/blog/?p=77814","linkMd5ListStr":"cd56b4a01b3e6ffde151f46fb441c68d","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"99.4 KB","destSize":"30.4 KB","compressRate":"30.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn40@2020_5/2021/09/27/16-55-33-517_6a6fb7dc2ed0fd1c.webp","sourceBytes":11921,"destBytes":3756,"targetWebpQuality":75,"feedId":11,"totalSpendMs":387,"convertSpendMs":5,"createdTime":"2021-09-28 00:55:33","host":"us-021*","referer":"https://www.percona.com/blog/?p=78039","linkMd5ListStr":"c32ff24988146f7c0f1ecda919ae7452","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"11.6 KB","destSize":"3.7 KB","compressRate":"31.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-1-1024x395.png","sourceStatusCode":200,"destWidth":1024,"destHeight":395,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn3@2020_5/2021/09/27/16-55-33-427_b49b24bc9c32e527.webp","sourceBytes":126778,"destBytes":21682,"targetWebpQuality":75,"feedId":11,"totalSpendMs":763,"convertSpendMs":16,"createdTime":"2021-09-28 00:55:33","host":"europe-25*","referer":"https://www.percona.com/blog/?p=77794","linkMd5ListStr":"f9d07f2139995f2c0bf057c6526f3e9e","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"123.8 KB","destSize":"21.2 KB","compressRate":"17.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn7@2020_4/2021/09/27/16-55-33-459_590bda8c07c61001.webp","sourceBytes":28348,"destBytes":9910,"targetWebpQuality":75,"feedId":11,"totalSpendMs":770,"convertSpendMs":37,"createdTime":"2021-09-28 00:55:33","host":"europe63*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"27.7 KB","destSize":"9.7 KB","compressRate":"35%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrw_tp-1024x717.png","sourceStatusCode":200,"destWidth":1024,"destHeight":717,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn48@2020_5/2021/09/27/16-55-33-201_d0ed7c4e8f3a036d.webp","sourceBytes":346579,"destBytes":34138,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1990,"convertSpendMs":227,"createdTime":"2021-09-28 00:55:31","host":"us-55*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"338.5 KB","destSize":"33.3 KB","compressRate":"9.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.59-PM-184x300.png","sourceStatusCode":200,"destWidth":184,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn55@2020_6/2021/09/27/16-55-31-889_5dcd62278ecc0075.webp","sourceBytes":41317,"destBytes":11986,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2027,"convertSpendMs":36,"createdTime":"2021-09-28 00:55:31","host":"us-51*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"40.3 KB","destSize":"11.7 KB","compressRate":"29%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn84@2020_1/2021/09/27/16-55-31-732_86162ed39056c88a.webp","sourceBytes":29410,"destBytes":4686,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2164,"convertSpendMs":9,"createdTime":"2021-09-28 00:55:30","host":"us-51*","referer":"https://www.percona.com/blog/?p=77814","linkMd5ListStr":"cd56b4a01b3e6ffde151f46fb441c68d","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"28.7 KB","destSize":"4.6 KB","compressRate":"15.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/wt-transactions-1024x226.png","sourceStatusCode":200,"destWidth":1024,"destHeight":226,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn26@2020_6/2021/09/27/16-55-31-907_3c13c8e9381b6a01.webp","sourceBytes":110653,"destBytes":15832,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1999,"convertSpendMs":24,"createdTime":"2021-09-28 00:55:31","host":"us-51*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"108.1 KB","destSize":"15.5 KB","compressRate":"14.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Kubernetes-Hashicorp-Cloud-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn20@2020_2/2021/09/27/16-55-31-768_f7702cda37f47992.webp","sourceBytes":12483,"destBytes":4252,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1996,"convertSpendMs":23,"createdTime":"2021-09-28 00:55:30","host":"us-51*","referer":"https://www.percona.com/blog/?p=77794","linkMd5ListStr":"f9d07f2139995f2c0bf057c6526f3e9e","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"12.2 KB","destSize":"4.2 KB","compressRate":"34.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Monitoring-and-Management-Dashboards-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn47@2020_4/2021/09/27/16-55-31-791_3c0f55455af1a607.webp","sourceBytes":17957,"destBytes":3924,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2155,"convertSpendMs":60,"createdTime":"2021-09-28 00:55:30","host":"us-51*","referer":"https://www.percona.com/blog/?p=77955","linkMd5ListStr":"e6b3d0a4cb30eb097eded1b2264f4cf4","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"17.5 KB","destSize":"3.8 KB","compressRate":"21.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Optimize-and-Troubleshoot-Your-MySQL-Environment-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn91@2020_6/2021/09/27/16-55-31-887_478d41b283e9ee23.webp","sourceBytes":11025,"destBytes":3056,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2009,"convertSpendMs":37,"createdTime":"2021-09-28 00:55:31","host":"us-51*","referer":"https://www.percona.com/blog/?p=78046","linkMd5ListStr":"4e76fe7de77593ea1899702f180afb79","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"10.8 KB","destSize":"3 KB","compressRate":"27.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.1_2-1024x321.png","sourceStatusCode":200,"destWidth":1024,"destHeight":321,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn44@2020_4/2021/09/27/16-55-33-714_b67089af13caf5d3.webp","sourceBytes":295381,"destBytes":42362,"targetWebpQuality":75,"feedId":11,"totalSpendMs":453,"convertSpendMs":22,"createdTime":"2021-09-28 00:55:33","host":"us-008*","referer":"https://www.percona.com/blog/?p=77580","linkMd5ListStr":"10f022279cf429a86442d1461019b2e0","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"288.5 KB","destSize":"41.4 KB","compressRate":"14.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/blog-hcp-0.png","sourceStatusCode":200,"destWidth":924,"destHeight":528,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn27@2020_6/2021/09/27/16-55-33-838_fc711fd0a266a755.webp","sourceBytes":53799,"destBytes":30570,"targetWebpQuality":75,"feedId":11,"totalSpendMs":894,"convertSpendMs":49,"createdTime":"2021-09-28 00:55:33","host":"europe67*","referer":"https://www.percona.com/blog/?p=77794","linkMd5ListStr":"f9d07f2139995f2c0bf057c6526f3e9e","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"52.5 KB","destSize":"29.9 KB","compressRate":"56.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Server-for-MongoDB-5.0.2-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn32@2020_3/2021/09/27/16-55-34-020_11e160f47e8a2cf8.webp","sourceBytes":28112,"destBytes":10132,"targetWebpQuality":75,"feedId":11,"totalSpendMs":836,"convertSpendMs":26,"createdTime":"2021-09-28 00:55:33","host":"us-55*","referer":"https://www.percona.com/blog/?p=77762","linkMd5ListStr":"80883f465440ff80e26e48846522c5d0","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"27.5 KB","destSize":"9.9 KB","compressRate":"36%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/tpcc_ZFS_on_ephemeral.png","sourceStatusCode":200,"destWidth":861,"destHeight":484,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn48@2020_1/2021/09/27/16-55-33-798_a8c6513bb592c64f.webp","sourceBytes":60592,"destBytes":38658,"targetWebpQuality":75,"feedId":11,"totalSpendMs":682,"convertSpendMs":26,"createdTime":"2021-09-28 00:55:33","host":"europe21*","referer":"https://www.percona.com/blog/?p=78039","linkMd5ListStr":"c32ff24988146f7c0f1ecda919ae7452","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"59.2 KB","destSize":"37.8 KB","compressRate":"63.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Migration-of-a-MySQL-Database-to-a-Kubernetes-Cluster-Using-Asynchronous-Replication-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn12@2020_3/2021/09/27/16-55-33-880_d5195d28f2dd5e99.webp","sourceBytes":52978,"destBytes":10352,"targetWebpQuality":75,"feedId":11,"totalSpendMs":969,"convertSpendMs":33,"createdTime":"2021-09-28 00:55:33","host":"us-51*","referer":"https://www.percona.com/blog/?p=78147","linkMd5ListStr":"4a519ab404d3f0653d0e6ea80e3a025b","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.7 KB","destSize":"10.1 KB","compressRate":"19.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/install-postgresql-with-docker-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn28@2020_6/2021/09/27/16-55-33-741_623c27f2ff701cbc.webp","sourceBytes":12247,"destBytes":3762,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1084,"convertSpendMs":17,"createdTime":"2021-09-28 00:55:33","host":"us-017*","referer":"https://www.percona.com/blog/?p=67630","linkMd5ListStr":"3dd387a7c3c17d427550be17f081c4fc","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"12 KB","destSize":"3.7 KB","compressRate":"30.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Jumbo-Chunks-in-MongoDB-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn56@2020_4/2021/09/27/16-55-33-670_fa7124fe9f8d6d51.webp","sourceBytes":67357,"destBytes":13960,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1023,"convertSpendMs":21,"createdTime":"2021-09-28 00:55:33","host":"us-017*","referer":"https://www.percona.com/blog/?p=78021","linkMd5ListStr":"b939b5f98c176b2d65c6cfdd64e52a4a","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"65.8 KB","destSize":"13.6 KB","compressRate":"20.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/mongodb-latency-1024x221.png","sourceStatusCode":200,"destWidth":1024,"destHeight":221,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn92@2020_4/2021/09/27/16-55-33-585_4d45293c8683ac3b.webp","sourceBytes":131350,"destBytes":13206,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1658,"convertSpendMs":61,"createdTime":"2021-09-28 00:55:32","host":"us-017*","referer":"https://www.percona.com/blog/?p=77591","linkMd5ListStr":"6faba7febc6c8ab4e0f3a1892e0bcf22","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"128.3 KB","destSize":"12.9 KB","compressRate":"10.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.10.45-1024x357.png","sourceStatusCode":200,"destWidth":1024,"destHeight":357,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn20@2020_5/2021/09/27/16-55-33-842_b444304b0a4f858c.webp","sourceBytes":221643,"destBytes":15994,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1362,"convertSpendMs":57,"createdTime":"2021-09-28 00:55:33","host":"us-017*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"216.4 KB","destSize":"15.6 KB","compressRate":"7.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Captura-de-Pantalla-2021-09-17-a-las-11.18.18-1024x358.png","sourceStatusCode":200,"destWidth":1024,"destHeight":358,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn32@2020_6/2021/09/27/16-55-33-827_42c0cf8f42c2bd16.webp","sourceBytes":109594,"destBytes":12782,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1360,"convertSpendMs":41,"createdTime":"2021-09-28 00:55:33","host":"us-017*","referer":"https://www.percona.com/blog/?p=78190","linkMd5ListStr":"3241d8e49d35b45e30cb4f284cf6fcc8","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"107 KB","destSize":"12.5 KB","compressRate":"11.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Debian-11-bullseye-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn64@2020_6/2021/09/27/16-55-34-790_ccaf270419e35c80.webp","sourceBytes":34742,"destBytes":6662,"targetWebpQuality":75,"feedId":11,"totalSpendMs":880,"convertSpendMs":21,"createdTime":"2021-09-28 00:55:34","host":"us-017*","referer":"https://www.percona.com/blog/?p=77764","linkMd5ListStr":"f4569ae0b391d617606fda9b40343180","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"33.9 KB","destSize":"6.5 KB","compressRate":"19.2%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/image1-1024x232.png","sourceStatusCode":200,"destWidth":1024,"destHeight":232,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn23@2020_1/2021/09/27/16-55-34-608_f2e83986b78267c8.webp","sourceBytes":118083,"destBytes":23140,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1970,"convertSpendMs":22,"createdTime":"2021-09-28 00:55:33","host":"us-040*","referer":"https://www.percona.com/blog/?p=77814","linkMd5ListStr":"cd56b4a01b3e6ffde151f46fb441c68d","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"115.3 KB","destSize":"22.6 KB","compressRate":"19.6%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-JSON-Databases-200x107.png","sourceStatusCode":200,"destWidth":200,"destHeight":107,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn44@2020_6/2021/09/27/16-55-35-344_e0af9e043c016c85.webp","sourceBytes":31253,"destBytes":4908,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1905,"convertSpendMs":6,"createdTime":"2021-09-28 00:55:34","host":"us-033*","referer":"https://www.percona.com/blog/?p=77805","linkMd5ListStr":"26d1aeebbf957eb34b336a86fce47739","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"30.5 KB","destSize":"4.8 KB","compressRate":"15.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Software-Update-Sept-13-2021-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn16@2020_3/2021/09/27/16-55-35-254_53bce3ac7480c45f.webp","sourceBytes":42393,"destBytes":5076,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1997,"convertSpendMs":39,"createdTime":"2021-09-28 00:55:34","host":"us-033*","referer":"https://www.percona.com/blog/?p=77995","linkMd5ListStr":"6562f34bba189c0b252dfb95abb799ec","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.4 KB","destSize":"5 KB","compressRate":"12%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.32-PM-232x300.png","sourceStatusCode":200,"destWidth":232,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn8@2020_1/2021/09/27/16-55-35-180_2f9ad720f7cc5511.webp","sourceBytes":17798,"destBytes":6486,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2062,"convertSpendMs":782,"createdTime":"2021-09-28 00:55:34","host":"us-033*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"17.4 KB","destSize":"6.3 KB","compressRate":"36.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Topology_2replicas.jpg","sourceStatusCode":200,"destWidth":361,"destHeight":344,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn35@2020_1/2021/09/27/16-55-34-205_14593bc81deef569.webp","sourceBytes":22181,"destBytes":10856,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1893,"convertSpendMs":659,"createdTime":"2021-09-28 00:55:33","host":"us-033*","referer":"https://www.percona.com/blog/?p=78056","linkMd5ListStr":"d6b4ea19a58ca2a1e8a608f2981464f3","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.7 KB","destSize":"10.6 KB","compressRate":"48.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Repoint-Replica-Servers-in-MySQL-3-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn71@2020_5/2021/09/27/16-55-34-389_c200ef37f32e8ecc.webp","sourceBytes":12273,"destBytes":3736,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2002,"convertSpendMs":10,"createdTime":"2021-09-28 00:55:34","host":"us-033*","referer":"https://www.percona.com/blog/?p=78056","linkMd5ListStr":"d6b4ea19a58ca2a1e8a608f2981464f3","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"12 KB","destSize":"3.6 KB","compressRate":"30.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/MyDumper-0.11.1-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn60@2020_1/2021/09/27/16-55-34-488_48bbe829c2074bc1.webp","sourceBytes":21405,"destBytes":9562,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2239,"convertSpendMs":12,"createdTime":"2021-09-28 00:55:33","host":"us-029*","referer":"https://www.percona.com/blog/?p=78127","linkMd5ListStr":"4bf6e9f728499e7f0379036829919039","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.9 KB","destSize":"9.3 KB","compressRate":"44.7%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Horizontal-Scalability-for-MySQL-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn87@2020_6/2021/09/27/16-55-35-478_f3c898be60183b82.webp","sourceBytes":25606,"destBytes":7300,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2448,"convertSpendMs":291,"createdTime":"2021-09-28 00:55:34","host":"us-029*","referer":"https://www.percona.com/blog/?p=78112","linkMd5ListStr":"ea953c59c8bf260d1c6cae3b7147a7b1","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"25 KB","destSize":"7.1 KB","compressRate":"28.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/MySQL-ZFS-Ephemeral-Cloud-Storage-300x168.png","sourceStatusCode":200,"destWidth":300,"destHeight":168,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn16@2020_6/2021/09/27/16-55-35-490_756b596a283aa975.webp","sourceBytes":21659,"destBytes":7016,"targetWebpQuality":75,"feedId":11,"totalSpendMs":3269,"convertSpendMs":858,"createdTime":"2021-09-28 00:55:33","host":"us-029*","referer":"https://www.percona.com/blog/?p=78039","linkMd5ListStr":"c32ff24988146f7c0f1ecda919ae7452","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.2 KB","destSize":"6.9 KB","compressRate":"32.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/myloader-Stops-Causing-Data-Fragmentation-200x105.png","sourceStatusCode":200,"destWidth":200,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn24@2020_2/2021/09/27/16-55-34-513_2e3e00e58f543626.webp","sourceBytes":17065,"destBytes":5264,"targetWebpQuality":75,"feedId":11,"totalSpendMs":1765,"convertSpendMs":37,"createdTime":"2021-09-28 00:55:34","host":"us-029*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.7 KB","destSize":"5.1 KB","compressRate":"30.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/tuple_details.2-1024x916.png","sourceStatusCode":200,"destWidth":1024,"destHeight":916,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn51@2020_4/2021/09/27/16-55-36-461_3231b4f4b1f93ddb.webp","sourceBytes":350175,"destBytes":62460,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2786,"convertSpendMs":415,"createdTime":"2021-09-28 00:55:34","host":"us-029*","referer":"https://www.percona.com/blog/?p=77580","linkMd5ListStr":"10f022279cf429a86442d1461019b2e0","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"342 KB","destSize":"61 KB","compressRate":"17.8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/rndrd_lat-1024x717.png","sourceStatusCode":200,"destWidth":1024,"destHeight":717,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn52@2020_4/2021/09/27/16-55-36-427_7f4fd6865f37b512.webp","sourceBytes":332802,"destBytes":31396,"targetWebpQuality":75,"feedId":11,"totalSpendMs":3332,"convertSpendMs":92,"createdTime":"2021-09-28 00:55:34","host":"us-033*","referer":"https://www.percona.com/blog/?p=77786","linkMd5ListStr":"aade0fd597a5fabc26e919311ae75b78","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"325 KB","destSize":"30.7 KB","compressRate":"9.4%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/MySQL-Memory-Usage-Details--1024x508.png","sourceStatusCode":200,"destWidth":1024,"destHeight":508,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_5/2021/09/27/16-55-37-269_890113497f1824e5.webp","sourceBytes":397061,"destBytes":31812,"targetWebpQuality":75,"feedId":11,"totalSpendMs":2369,"convertSpendMs":50,"createdTime":"2021-09-28 00:55:35","host":"us-033*","referer":"https://www.percona.com/blog/?p=77955","linkMd5ListStr":"e6b3d0a4cb30eb097eded1b2264f4cf4","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"387.8 KB","destSize":"31.1 KB","compressRate":"8%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.44.36-PM-181x300.png","sourceStatusCode":200,"destWidth":181,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn75@2020_1/2021/09/27/16-55-35-619_d38ad486d9db6478.webp","sourceBytes":40407,"destBytes":11828,"targetWebpQuality":75,"feedId":11,"totalSpendMs":3994,"convertSpendMs":750,"createdTime":"2021-09-28 00:55:33","host":"us-025*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"39.5 KB","destSize":"11.6 KB","compressRate":"29.3%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.43.51-PM-180x300.png","sourceStatusCode":200,"destWidth":180,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn3@2020_1/2021/09/27/16-55-35-637_d1cd1193ddf07328.webp","sourceBytes":42214,"destBytes":12446,"targetWebpQuality":75,"feedId":11,"totalSpendMs":3974,"convertSpendMs":41,"createdTime":"2021-09-28 00:55:33","host":"us-025*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.2 KB","destSize":"12.2 KB","compressRate":"29.5%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/installing-MongoDB-with-Docker-200x112.png","sourceStatusCode":200,"destWidth":200,"destHeight":112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn40@2020_6/2021/09/27/16-55-35-612_069e324abaec8cf7.webp","sourceBytes":11968,"destBytes":3248,"targetWebpQuality":75,"feedId":11,"totalSpendMs":3954,"convertSpendMs":790,"createdTime":"2021-09-28 00:55:33","host":"us-025*","referer":"https://www.percona.com/blog/?p=67094","linkMd5ListStr":"409d87d45678dd7a1a89e474d21b7f32","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"11.7 KB","destSize":"3.2 KB","compressRate":"27.1%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Percona-Community-Engineering-Meeting-300x157.png","sourceStatusCode":200,"destWidth":300,"destHeight":157,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn67@2020_3/2021/09/27/16-55-34-848_2b4249f2dd524cdd.webp","sourceBytes":52481,"destBytes":8330,"targetWebpQuality":75,"feedId":11,"totalSpendMs":4557,"convertSpendMs":9,"createdTime":"2021-09-28 00:55:33","host":"us-025*","referer":"https://www.percona.com/blog/?p=77814","linkMd5ListStr":"cd56b4a01b3e6ffde151f46fb441c68d","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.3 KB","destSize":"8.1 KB","compressRate":"15.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/08/Screen-Shot-2021-08-04-at-3.45.21-PM-209x300.png","sourceStatusCode":200,"destWidth":209,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn96@2020_5/2021/09/27/16-55-35-611_0f9a8f91d6490028.webp","sourceBytes":40881,"destBytes":11396,"targetWebpQuality":75,"feedId":11,"totalSpendMs":4646,"convertSpendMs":1019,"createdTime":"2021-09-28 00:55:32","host":"us-025*","referer":"https://www.percona.com/blog/?p=77742","linkMd5ListStr":"f20c364dc02652beb58d1e7d77ddf319","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"39.9 KB","destSize":"11.1 KB","compressRate":"27.9%"},{"code":1,"isDone":false,"source":"https://www.percona.com/blog/wp-content/uploads/2021/09/Percona-Releases-Sept-27-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn32@2020_4/2021/09/27/16-55-36-689_64c54b2d08a9aeef.webp","sourceBytes":86545,"destBytes":10644,"targetWebpQuality":75,"feedId":11,"totalSpendMs":4980,"convertSpendMs":968,"createdTime":"2021-09-28 00:55:33","host":"us-025*","referer":"https://www.percona.com/blog/?p=78116","linkMd5ListStr":"9abdd2fb0799c7aa92878f47bd27a89e","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"84.5 KB","destSize":"10.4 KB","compressRate":"12.3%"}],"successGithubMap":{"myreaderx14":4,"myreaderx8":3,"myreaderx7":3,"myreaderx15":4,"myreaderx16":4,"myreaderx6":3,"myreaderx10":4,"myreaderx32":4,"myreaderx4":4,"myreaderx3":4,"myreaderx11":4,"myreaderx33":4,"myreaderx12":4,"myreaderx2":4,"myreaderx1":4,"myreaderx13":4,"myreaderx30":4,"myreaderx31":4,"myreaderx18":4,"myreaderx19":4,"myreaderx":4,"myreaderx25":4,"myreaderx27":4,"myreaderx21":3,"myreaderx22":4,"myreaderx23":4,"myreaderx24":4,"myreaderx5oss":4,"myreaderx29":4},"failGithubMap":{}}