{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2022-03-14 17:10:58","updatedTime":"2022-03-14 17:10:58","title":"Transformer将在AI领域一统天下？现在下结论还为时过早","link":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","description":"<div><div><div id=\"media\" class=\"rich_media_thumb_wrp\">\n\n            <img class=\"rich_media_thumb\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcau8iaZibIxzPWhJnsrUDcFSa3rSmTibf6SON7uNF2UGAuibicRjY2LicTD0ib2Q/0?wx_fmt=jpeg?imageView2/1/w/600\">\n        </div>\n    \n\n    \n\n    <div class=\"rich_media_content\" id=\"js_content\">\n                    \n\n                    \n                    \n                    \n                    <section data-mpa-powered-by=\"yiban.io\" data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" data-style=\"white-space: normal; max-width: 100%; letter-spacing: 0.544px; text-size-adjust: auto; background-color: rgb(255, 255, 255); font-family: \" helvetica neue sans gb yahei arial sans-serif box-sizing: border-box overflow-wrap: break-word class=\"js_darkmode__0\" style=\"white-space: normal;outline: 0px;max-width: 100%;letter-spacing: 0.544px;background-color: rgb(255, 255, 255);text-size-adjust: auto;font-family: \" visible><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-id=\"85660\" data-custom=\"rgb(117, 117, 118)\" data-color=\"rgb(117, 117, 118)\" data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-id=\"85660\" data-custom=\"rgb(117, 117, 118)\" data-color=\"rgb(117, 117, 118)\" data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" style=\"outline: 0px;max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-16095509242984=\"rgb(25, 25, 25)\" data-darkmode-original-bgcolor-16095509242984=\"rgb(255, 255, 255)\" data-style=\"margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; max-width: 100%; border-style: solid none; text-decoration: inherit; border-top-color: rgb(204, 204, 204); border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-bottom-width: 1px; box-sizing: border-box !important; overflow-wrap: break-word !important;\" class=\"js_darkmode__1\" style=\"margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;outline: 0px;max-width: 100%;border-bottom: 1px solid rgb(204, 204, 204);border-top: 1px solid rgb(204, 204, 204);border-right-style: none;border-left-style: none;text-decoration: inherit;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section style=\"margin-top: -1.2em;outline: 0px;max-width: 100%;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;color: rgb(163, 163, 163) !important;\"><span style=\"outline: 0px;max-width: 100%;text-decoration: inherit;visibility: visible;color: rgb(255, 255, 255);letter-spacing: 0.544px;background-color: rgb(117, 117, 118);font-size: 15px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"font-size: 15px;\">选自 Quanta Magazine</span></span></section><section style=\"margin-top: 10px;margin-bottom: 5px;outline: 0px;max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">机器之心编译</strong></span></strong></span></section><section style=\"margin-top: 10px;margin-bottom: 5px;outline: 0px;max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">作者：</strong></span></strong></span><span style=\"outline: 0px;max-width: 100%;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">Stephen Ornes</strong></span></strong></span></section><section style=\"margin-top: 10px;margin-bottom: 5px;outline: 0px;max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"outline: 0px;max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"outline: 0px;max-width: 100%;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">机器之心编辑部</strong></span></strong></span></section></section></section></section></section></section></section></section></section></section></section></section><blockquote data-type=\"2\" data-url=\"\" data-author-name=\"\" data-content-utf8-length=\"136\" data-source-title=\"\" style=\"color: rgba(0, 0, 0, 0.5);white-space: normal;outline: 0px;max-width: 100%;font-family: -apple-system, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif rgb border-box break-word><section style=\"outline: 0px;max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section style=\"outline: 0px;max-width: 100%;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"letter-spacing: 0.544px;\"><span style=\"font-size: 15px;\">从</span><span style=\"font-size: 15px;\">自然语言处理任务起家，又在图像分类和生成领域大放异彩，所向披靡的 Transformer 会成为下一个神话吗？</span></span><span style=\"color: rgb(51, 51, 51);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif></span></section></section></blockquote><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">想象一下你走进一家本地的五金店，在货架上看到一种新型的锤子。你听说过这种锤子：它比其他锤子敲得更快、更准确，而且在过去的几年里，在大多数用途中，它已经淘汰了许多其他锤子。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">此外，通过一些调整，比如这里加一个附件，那里拧一个螺丝，这种锤子还能变成一把锯，其切割速度能媲美其他任何替代品。一些处于工具开发前沿的专家表示，这把锤子可能预示着所有工具将融合到一个设备中。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">类似的故事正在人工智能领域上演。这种多功能的新锤子是一种人工神经网络——一种在现有数据上进行训练以「学习」如何完成某些任务的节点网络——称为 Transformer。它最初用于处理语言任务，但最近已经开始影响其他 AI 领域。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Transformer 最初出现在 2017 年的一篇论文中：《Attention Is All You Need》。在其他人工智能方法中，系统会首先关注输入数据的局部 patch，然后构建整体。例如，在语言模型中，邻近的单词首先会被组合在一起。相比之下，Transformer 运行程序以便输入数据中的每个元素都连接或关注其他元素。研究人员将此称为「自注意力」。这意味着一旦开始训练，Transformer 就可以看到整个数据集的迹。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">在 Transformer 出现之前，人工智能在语言任务上的进展一直落后于其他领域的发展。「在过去 10 年发生的这场深度学习革命中，自然语言处理在某种程度上是后来者，」马萨诸塞大学洛厄尔分校的计算机科学家 Anna Rumshisky 说，「从某种意义上说，NLP 曾落后于计算机视觉，而 Transformer 改变了这一点。」</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Transformer 很快成为专注于分析和预测文本的单词识别等应用程序的引领者。它引发了一波工具浪潮，比如 OpenAI 的 GPT-3 可以在数千亿个单词上进行训练并生成连贯的新文本。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Transformer 的成功促使人工智能领域的研究者思考：这个模型还能做些什么？</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">答卷正在徐徐展开——Transformer 被证明具有惊人的丰富功能。在某些视觉任务中，例如图像分类，使用 Transformer 的神经网络比不使用 Transformer 的神经网络更快、更准确。对于其他人工智能领域的新兴研究，例如一次处理多种输入或完成规划任务，Transformer 也可以处理得更多、更好。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">「Transformer 似乎在机器学习领域的许多问题上具有相当大的变革性，包括计算机视觉，」在慕尼黑宝马公司从事与自动驾驶汽车计算机视觉工作的 Vladimir Haltakov 说。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">就在十年前，AI 的不同子领域之间还几乎是互不相通的，但 Transformer 的到来表明了融合的可能性。「我认为 Transformer 之所以如此受欢迎，是因为它展示出了通用的潜力，」德克萨斯大学奥斯汀分校的计算机科学家 Atlas Wang 说：「我们有充分的理由尝试在整个 AI 任务范围内尝试使用 Transformer。」</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: center;line-height: 1.75em;\"><strong><span style=\"font-size: 16px;\">从「语言」到「视觉」</span></strong><span style=\"font-size: 15px;text-align: justify;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">在《Attention Is All You Need》发布几个月后，扩展 Transformer 应用范围的最有希望的动作就开始了。Alexey Dosovitskiy 当时在谷歌大脑柏林办公室工作，正在研究计算机视觉，这是一个专注于教授计算机如何处理和分类图像的 AI 子领域。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages wxw-img js_insertlocalimg\" data-ratio=\"1.018032786885246\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"1220\" style=\"width: 427px; height: 435px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauibBQKzicMNo49jkTLJMthrL4Z56B57k0PR5zgvicA1oLXdicN21N5UD4Rg/640?wx_fmt=jpeg\"></p><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);\"><em><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"> Alexey Dosovitskiy。</span></em></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">与该领域的几乎所有其他人一样，他一直使用卷积神经网络 (CNN) 。多年来，正是 CNN 推动了深度学习，尤其是计算机视觉领域的所有重大飞跃。CNN 通过对图像中的像素重复应用滤波器来进行特征识别。基于 CNN，照片应用程序可以按人脸给你的照片分门别类，或是将牛油果与云区分开来。因此，CNN 被认为是视觉任务必不可少的。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">当时，Dosovitskiy 正在研究该领域最大的挑战之一，即在不增加处理时间的前提下，将 CNN 放大：在更大的数据集上训练，表示更高分辨率的图像。但随后他看到，Transformer 已经取代了以前几乎所有与语言相关的 AI 任务的首选工具。「我们显然从正在发生的事情中受到了启发，」他说，「我们想知道，是否可以在视觉上做类似的事情？」 这个想法某种程度上说得通——毕竟，如果 Transformer 可以处理大数据集的单词，为什么不能处理图片呢？</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">最终的结果是：在 2021 年 5 月的一次会议上，一个名为 Vision Transformer（ViT）的网络出现了。该模型的架构与 2017 年提出的第一个 Transformer 的架构几乎相同，只有微小的变化，这让它能够做到分析图像，而不只是文字。「语言往往是离散的，」Rumshisky 说：「所以必须使图像离散化。」</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">ViT 团队知道，语言的方法无法完全模仿，因为每个像素的自注意力在计算时间上会非常昂贵。所以，他们将较大的图像划分为正方形单元或 token。大小是任意的，因为 token 可以根据原始图像的分辨率变大或变小（默认为一条边 16 像素），但通过分组处理像素，并对每个像素应用自注意力，ViT 可以快速处理大型训练数据集，从而产生越来越准确的分类。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Transformer 能够以超过 90% 的准确率对图像进行分类，这比 Dosovitskiy 预期的结果要好得多，并在 ImageNet 图像数据集上实现了新的 SOTA Top-1 准确率。ViT 的成功表明，卷积可能不像研究人员认为的那样对计算机视觉至关重要。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">与 Dosovitskiy 合作开发 ViT 的谷歌大脑苏黎世办公室的 Neil Houlsby 说：「我认为 CNN 很可能在中期被视觉 Transformer 或其衍生品所取代。」他认为，未来的模型可能是纯粹的 Transformer，或者是为现有模型增加自注意力的方法。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">一些其他结果验证了这些预测。研究人员定期在 ImageNet 数据库上测试他们的图像分类模型，在 2022 年初，ViT 的更新版本仅次于将 CNN 与 Transformer 相结合的新方法。而此前长期的冠军——没有 Transformer 的 CNN，目前只能勉强进入前 10 名。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: center;line-height: 1.75em;\"><strong><span style=\"font-size: 16px;\">Transformer 的工作原理</span></strong><span style=\"font-size: 15px;text-align: justify;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">ImageNet 结果表明，Transformer 可以与领先的 CNN 竞争。但谷歌大脑加州山景城办公室的计算机科学家 Maithra Raghu 想知道，它们是否和 CNN 一样「看到」图像。神经网络是一个难以破译的「黑盒子」，但有一些方法可以窥探其内部——例如通过逐层检查网络的输入和输出了解训练数据如何流动。Raghu 的团队基本上就是这样做的——他们将 ViT 拆开了。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages wxw-img js_insertlocalimg\" data-ratio=\"0.840983606557377\" data-s=\"300,640\" data-type=\"png\" data-w=\"1220\" style=\"width: 424px; height: 357px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcau7oOaiaMNZI8UeXicQqOMHK6Vhjc7SwCqw07icVhfWibibtbPbq9qoVeXUqg/640?wx_fmt=png\"></p><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);\"><em><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">Maithra Raghu</span></em></span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">她的团队确定了自注意力在算法中导致不同感知的方式。归根结底，Transformer 的力量来自于它处理图像编码数据的方式。「在 CNN 中，你是从非常局部的地方开始，然后慢慢获得全局视野，」Raghu 说。CNN 逐个像素地识别图像，通过从局部到全局的方式来识别角或线等特征。但是在带有自注意力的 Transformer 中，即使是信息处理的第一层也会在相距很远的图像位置之间建立联系（就像语言一样）。如果说 CNN 的方法就像从单个像素开始并用变焦镜头缩小远处物体的像的放大倍数，那么 Transformer 就是慢慢地将整个模糊图像聚焦。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">这种差异在 Transformer 最初专注的语言领域更容易理解，思考一下这些句子：「猫头鹰发现了一只松鼠。它试图用爪子抓住它，但只抓住了尾巴的末端。」第二句的结构令人困惑：「它」指的是什么？只关注「它」邻近的单词的 CNN 会遇到困难，但是将每个单词与其他单词连接起来的 Transformer 可以识别出猫头鹰在抓松鼠，而松鼠失去了部分尾巴。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages wxw-img js_insertlocalimg\" data-ratio=\"1.46015625\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"width: 484px; height: 707px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauhMiaunjNaib84iak0f3lFP1DMk5taFsHOfuglbBmia8ZV8VzWCw5y6rBibA/640?wx_fmt=png\"><span style=\"font-size: 15px;text-align: justify;\"></span></p><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">显然，Transformer 处理图像的方式与卷积网络有着本质上的不同，研究人员变得更加兴奋。Transformer 在将数据从一维字符串（如句子）转换为二维数组（如图像）方面的多功能性表明，这样的模型可以处理许多其他类型的数据。例如，Wang 认为，Transformer 可能是朝着实现神经网络架构的融合迈出的一大步，从而产生了一种通用的计算机视觉方法——也许也适用于其他 AI 任务。「当然，要让它真正发生是有局限性的，但如果有一种可以通用的模型，让你可以将各种数据放在一台机器上，那肯定是非常棒的。」</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: center;line-height: 1.75em;\"><strong><span style=\"font-size: 16px;\">关于 ViT 的展望</span></strong><span style=\"font-size: 15px;text-align: justify;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">现在研究人员希望将 Transformer 应用于一项更艰巨的任务：创造新图像。GPT-3 等语言工具可以根据其训练数据生成新文本。在去年发表的一篇论文《TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up》中，Wang 组合了两个 Transformer 模型，试图对图像做同样的事情，但这是一个困难得多的问题。当双 Transformer 网络在超过 200000 个名人的人脸上进行训练时，它以中等分辨率合成了新的人脸图像。根据初始分数（一种评估神经网络生成的图像的标准方法），生成的名人面孔令人印象深刻，并且至少与 CNN 创建的名人一样令人信以为真。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Wang 认为，Transformer 在生成图像方面的成功比 ViT 在图像分类方面的能力更令人惊讶。「生成模型需要综合能力，需要能够添加信息以使其看起来合理，」他说。与分类领域一样，Transformer 方法正在生成领域取代卷积网络。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Raghu 和 Wang 还看到了 Transformer 在多模态处理中的新用途。「以前做起来比较棘手，」Raghu 说，因为每种类型的数据都有自己的专门模型，方法之间是孤立的。但是 Transformer 提出了一种组合多个输入源的方法。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">「有很多有趣的应用程序可以结合其中一些不同类型的数据和图像。」例如，多模态网络可能会为一个系统提供支持，让系统除了听一个人的声音外，还可以读取一个人的唇语。「你可以拥有丰富的语言和图像信息表征，」Raghu 说，「而且比以前更深入。」</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages wxw-img js_insertlocalimg\" data-ratio=\"0.3328125\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauIL4sVgn7uNp6ib5iaK0tkAsnO9ttCcPb7FLhyZFqS5WE69QscCiciazbBg/640?wx_fmt=png\"></p><section style=\"text-align: left;line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);\"><em><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">这些面孔是在对超过 200000 张名人面孔的数据集进行训练后，由基于 Transformer 的网络创建的。</span></em></span><span style=\"font-size: 15px;text-align: justify;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">新的一系列研究表明了 Transformer 在其他人工智能领域的一系列新用途，包括教机器人识别人体运动、训练机器识别语音中的情绪以及检测心电图中的压力水平。另一个带有 Transformer 组件的程序是 AlphaFold，它以快速预测蛋白质结构的能力，解决了五十年来蛋白质分子折叠问题，成为了名噪一时的头条新闻。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: center;line-height: 1.75em;\"><strong><span style=\"font-size: 16px;\">Transformer isn't all you need</span></strong></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">即使 Transformer 有助于整合和改进 AI 工具，但和其他新兴技术一样，Transformer 也存在代价高昂的特点。一个 Transformer 模型需要在预训练阶段消耗大量的计算能力，才能击败之前的竞争对手。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">这可能是个问题。「人们对高分辨率的图像越来越感兴趣，」Wang 表示。训练费用可能是阻碍 Transformer 推广开来的一个不利因素。然而，Raghu 认为，训练障碍可以借助复杂的滤波器和其他工具来克服。</span><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Wang 还指出，尽管视觉 transformer 已经在推动 AI 领域的进步，但许多新模型仍然包含了卷积的最佳部分。他说，这意味着未来的模型更有可能同时使用这两种模式，而不是完全放弃 CNN。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: justify;line-height: 1.75em;\"><span style=\"font-size: 15px;\">同时，这也表明，一些混合架构拥有诱人的前景，它们以一种当前研究者无法预测的方式利用 transformer 的优势。「也许我们不应该急于得出结论，认为 transformer 就是最完美的那个模型，」Wang 说。但越来越明显的是，transformer 至少会是 AI shop 里所有新型超级工具的一部分。</span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section style=\"text-align: left;line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);\"><em><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">原文链接：https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/</span></em></span></section><section style=\"text-align: justify;line-height: 1.75em;\"><br></section><section mpa-from-tpl=\"t\" style=\"white-space: normal;\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\"><section data-id=\"90835\" mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\" style=\"padding: 10px;\"><section mpa-from-tpl=\"t\" style=\"padding: 15px;box-shadow: rgb(204, 204, 204) 0px 0px 5px;\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\" style=\"padding-top: 0.8em;padding-right: 0.5em;padding-left: 0.5em;\"><section data-autoskip=\"1\" mpa-from-tpl=\"t\" style=\"line-height: 1.5em;text-align: left;\"><p style=\"margin-top: 15px;max-width: 100%;font-family: -apple-system-font, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif break-word border-box><span style=\"font-size: 17px;\"><strong style=\"color: rgb(0, 0, 0);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif><strong style=\"color: rgb(19, 72, 177);font-size: 16px;letter-spacing: 0.544px;\"><strong style=\"outline: 0px;max-width: 100%;font-family: -apple-system, \" system-ui neue sc sans gb yahei ui arial sans-serif center rgb border-box break-word>3月23日北京——首席智行官大会</strong></strong></strong></span><img class=\"rich_pages wxw-img js_insertlocalimg\" data-ratio=\"0.6283185840707964\" data-s=\"300,640\" data-type=\"png\" data-w=\"678\" style=\"height: 29px; text-align: left; white-space: normal; width: 46px; float: left; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9uc365d2e0IVdribEf1tAe9GzicPVYfeyaf2HXPB9SXefvneaord38K1lMwcIt3kOiaVrKbGbcLByrw/640?wx_fmt=png\"></p><p><br></p><section style=\"line-height: 1.75em;\"><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif>机器之</span><span style=\"font-size: 15px;background-color: rgb(255, 255, 255);font-family: -apple-system, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif>心AI科技年会将于3月23日</span><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif>举办</span><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif>，「首席智行官大会」也将一同开幕。</span></section><ul class=\"list-paddingleft-1\" style=\"list-style-type: disc;\"><li><p style=\"margin-top: 5px;\"><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif>举办时间：2022年3月23日13:30-17:00</span></p></li><li><p style=\"margin-top: 5px;\"><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif>举办地址：北京望京凯悦酒店</span></p></li></ul><p style=\"line-height: 1.75em;margin-top: 15px;\"><span style=\"font-family: -apple-system, \" system-ui neue sc sans gb yahei ui arial sans-serif rgb>「首席智行官大会」</span><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, \" system-ui neue sc sans gb yahei ui arial sans-serif>将邀请智慧出行领域的领袖级人物，他们将来自当下热度最高的智能汽车、车规级芯片、Robotaxi 及无人物流等领域，所涉及议题覆盖了汽车机器人、大算力时代汽车芯片展望、无人驾驶商业化等多个前沿方向。</span></p><p style=\"line-height: 1.75em;margin-top: 15px;\"><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, \" system-ui neue sc sans gb yahei ui arial sans-serif>欢迎点击「阅读原文」报名活动。</span></p><p style=\"line-height: 1.75em;margin-top: 15px;\"><span style=\"background-color: rgb(255, 255, 255);font-family: -apple-system, \" system-ui neue sc sans gb yahei ui arial sans-serif><br></span></p><section style=\"text-align: center;margin-top: 10px;\"><img class=\"rich_pages wxw-img js_insertlocalimg\" data-cropselx1=\"0\" data-cropselx2=\"511\" data-cropsely1=\"0\" data-cropsely2=\"303\" data-ratio=\"1.96640625\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"1280\" style=\"width: 511px; height: 1005px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9uc365d2e0IVdribEf1tAe9brCibZ6JFoOHpDamozu0mBZUEP1ibkETVlM0CZ4dKQoIhQoH4dcFUHiaA/640?wx_fmt=jpeg\"></section></section></section></section></section></section></section></section></section></section></section></section><p style=\"margin-top: 5px;white-space: normal;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">© THE END </span></p><p style=\"margin-top: 5px;white-space: normal;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">转载请联系本公众号获得授权</span></p><p style=\"margin-top: 5px;white-space: normal;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">投稿或寻求报道：content@jiqizhixin.com</span></p>\n                </div>\n\n    \n        <br>\n        <div id=\"js_toobar3\" class=\"rich_media_tool\">\n            <a target=\"_blank\" href=\"https://8802505718417.huodongxing.com/event/7637735381200\" id=\"js_view_source\" class=\"media_tool_meta meta_primary\">阅读原文</a>\n        </div>\n    \n    <br>\n\n    \n        <a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650840250&amp;idx=1&amp;sn=adccda53b5678b59583133899ce7026e&amp;chksm=84e558c4b392d1d23ea603c3cc412100fcf8afe62760afe387d0e8d75776085fd035b6bf2a11#rd\" style=\"color: blue\" class=\"media_tool_meta meta_primary\">文章原文</a>\n        <br>\n    \n\n    \n\n    <img alt=\"\" width=\"1px\" height=\"1px\" class=\"\" style=\"width:1px;height:1px;display:none\" src=\"http://www.jintiankansha.me/rss_static/5409/IUF7BvIT13\"></div></div>","descriptionType":"html","publishedDate":"Sun, 13 Mar 2022 04:34:00 +0000","feedId":1837,"bgimg":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcau8iaZibIxzPWhJnsrUDcFSa3rSmTibf6SON7uNF2UGAuibicRjY2LicTD0ib2Q/0?wx_fmt=jpeg?imageView2/1/w/600","linkMd5":"bbb3757bb695b8a2b9767064cd25a660","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn72@2020_4/2022/03/14/09-10-59-818_2be0a81f73ebf1c7.webp","destWidth":1080,"destHeight":459,"sourceBytes":119546,"destBytes":96566,"author":"","articleImgCdnMap":{"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcau8iaZibIxzPWhJnsrUDcFSa3rSmTibf6SON7uNF2UGAuibicRjY2LicTD0ib2Q/0?wx_fmt=jpeg?imageView2/1/w/600":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn72@2020_4/2022/03/14/09-10-59-818_2be0a81f73ebf1c7.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauibBQKzicMNo49jkTLJMthrL4Z56B57k0PR5zgvicA1oLXdicN21N5UD4Rg/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn83@2020_1/2022/03/14/09-11-02-177_089b49b32a882845.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcau7oOaiaMNZI8UeXicQqOMHK6Vhjc7SwCqw07icVhfWibibtbPbq9qoVeXUqg/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauhMiaunjNaib84iak0f3lFP1DMk5taFsHOfuglbBmia8ZV8VzWCw5y6rBibA/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn91@2020_6/2022/03/14/09-11-06-473_b59427cff8bf3ff8.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauIL4sVgn7uNp6ib5iaK0tkAsnO9ttCcPb7FLhyZFqS5WE69QscCiciazbBg/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9uc365d2e0IVdribEf1tAe9GzicPVYfeyaf2HXPB9SXefvneaord38K1lMwcIt3kOiaVrKbGbcLByrw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn100@2020_1/2022/03/14/09-11-16-280_feb95b1af345aa26.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9uc365d2e0IVdribEf1tAe9brCibZ6JFoOHpDamozu0mBZUEP1ibkETVlM0CZ4dKQoIhQoH4dcFUHiaA/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn80@2020_4/2022/03/14/09-11-12-199_136a6258ef0b50f7.webp","http://www.jintiankansha.me/rss_static/5409/IUF7BvIT13":null},"publishedOrCreatedDate":1647249058329}],"record":{"createdTime":"2022-03-14 17:10:58","updatedTime":"2022-03-14 17:10:58","feedId":1837,"fetchDate":"Mon, 14 Mar 2022 09:10:58 +0000","fetchMs":332,"handleMs":44,"totalMs":126622,"newArticles":0,"totalArticles":5,"status":1,"type":0,"ip":"bc1eecef1292254c09de6c1e66b750f3","hostName":"europe-23*","requestId":"c1b68a0fcf554440874ca3493a18441b_1837","contentType":"application/rss+xml","totalBytes":471824,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":8,"articlesImgsGithubTotal":5,"successGithubMap":{"myreaderx25":1,"myreaderx15":1,"myreaderx3":1,"myreaderx11":1,"myreaderx24":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-24 21:31:33","updatedTime":"2020-09-01 09:54:29","id":1837,"name":"机器之心","url":"http://feedmaker.kindle4rss.com/feeds/almosthuman2014.weixin.xml","subscriber":null,"website":null,"icon":"http://www.sogou.com/images/logo/new/favicon.ico?v=4","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn87@2020_3/2020/09/01/01-54-30-263_d24121c9beed1de6.ico","description":"专业的人工智能媒体和产业服务平台","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2022-03-14 17:13:03","updatedTime":"2022-03-14 17:13:03","id":null,"feedId":1837,"linkMd5":"bbb3757bb695b8a2b9767064cd25a660"}],"tmpCommonImgCdnBytes":96566,"tmpBodyImgCdnBytes":375258,"tmpBgImgCdnBytes":0,"extra4":{"start":1647249057231,"total":0,"statList":[{"spend":1056,"msg":"获取xml内容"},{"spend":44,"msg":"解释文章"},{"spend":1,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":123566,"msg":"正文链接上传到cdn"}]},"extra5":8,"extra6":6,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5409/IUF7BvIT13","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":649,"convertSpendMs":0,"createdTime":"2022-03-14 17:11:00","host":"us-51*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5409/IUF7BvIT13","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":588,"convertSpendMs":0,"createdTime":"2022-03-14 17:11:01","host":"europe68*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},null,null,null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://europe68.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]},"http://us-55.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-004.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-033.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-005.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-51.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-029.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcau8iaZibIxzPWhJnsrUDcFSa3rSmTibf6SON7uNF2UGAuibicRjY2LicTD0ib2Q/0?wx_fmt=jpeg?imageView2/1/w/600","sourceStatusCode":200,"destWidth":1080,"destHeight":459,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn72@2020_4/2022/03/14/09-10-59-818_2be0a81f73ebf1c7.webp","sourceBytes":119546,"destBytes":96566,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":1927,"convertSpendMs":21,"createdTime":"2022-03-14 17:10:58","host":"europe63*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660,bbb3757bb695b8a2b9767064cd25a660","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"116.7 KB","destSize":"94.3 KB","compressRate":"80.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauibBQKzicMNo49jkTLJMthrL4Z56B57k0PR5zgvicA1oLXdicN21N5UD4Rg/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":1080,"destHeight":1099,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn83@2020_1/2022/03/14/09-11-02-177_089b49b32a882845.webp","sourceBytes":83557,"destBytes":50832,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":2092,"convertSpendMs":252,"createdTime":"2022-03-14 17:11:00","host":"us-004*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"81.6 KB","destSize":"49.6 KB","compressRate":"60.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicjSlj5hqhqZn7LJH5sjcauhMiaunjNaib84iak0f3lFP1DMk5taFsHOfuglbBmia8ZV8VzWCw5y6rBibA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":1577,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn91@2020_6/2022/03/14/09-11-06-473_b59427cff8bf3ff8.webp","sourceBytes":738040,"destBytes":158248,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":6733,"convertSpendMs":117,"createdTime":"2022-03-14 17:11:00","host":"europe67*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"720.7 KB","destSize":"154.5 KB","compressRate":"21.4%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9uc365d2e0IVdribEf1tAe9brCibZ6JFoOHpDamozu0mBZUEP1ibkETVlM0CZ4dKQoIhQoH4dcFUHiaA/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":1080,"destHeight":2124,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn80@2020_4/2022/03/14/09-11-12-199_136a6258ef0b50f7.webp","sourceBytes":226790,"destBytes":144956,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":12172,"convertSpendMs":109,"createdTime":"2022-03-14 17:11:00","host":"us-029*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"221.5 KB","destSize":"141.6 KB","compressRate":"63.9%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9uc365d2e0IVdribEf1tAe9GzicPVYfeyaf2HXPB9SXefvneaord38K1lMwcIt3kOiaVrKbGbcLByrw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":678,"destHeight":426,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn100@2020_1/2022/03/14/09-11-16-280_feb95b1af345aa26.webp","sourceBytes":42665,"destBytes":21222,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":16150,"convertSpendMs":68,"createdTime":"2022-03-14 17:11:00","host":"us-033*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+Transformer%E5%B0%86%E5%9C%A8AI%E9%A2%86%E5%9F%9F%E4%B8%80%E7%BB%9F%E5%A4%A9%E4%B8%8B%EF%BC%9F%E7%8E%B0%E5%9C%A8%E4%B8%8B%E7%BB%93%E8%AE%BA%E8%BF%98%E4%B8%BA%E6%97%B6%E8%BF%87%E6%97%A9","linkMd5ListStr":"bbb3757bb695b8a2b9767064cd25a660","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.7 KB","destSize":"20.7 KB","compressRate":"49.7%"}],"successGithubMap":{"myreaderx25":1,"myreaderx15":1,"myreaderx3":1,"myreaderx11":1,"myreaderx24":1},"failGithubMap":{}}