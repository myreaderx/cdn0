{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2022-03-14 19:30:49","updatedTime":"2022-03-14 19:30:49","title":"Clustering Similar Stories Using LDA","link":"http://engineering.flipboard.com//2017/02/storyclustering","description":"<p>There is more to a story than meets the eye, and some stories deserve to be presented from more than just one perspective. With Flipboard 4.0, we have released story roundups, a new feature that adds coverage from multiple sources to a story and provides you with a fuller picture of an event. \n <!--break--></p> \n<p>Here’s how it looks:</p> \n<div class=\"row\" style=\"text-align: center\"> \n <img src=\"http://engineering.flipboard.com/assets/storyclustering/storycluster.gif\" style=\"max-width:50%;\" /> \n</div> \n<p>With our scale of millions of articles and constant stream of documents, it’s impossible to generate these roundups manually. So, we have developed a clustering algorithm that’s both fast and scalable, and in this blog post, I will explain how we create these roundups on Flipboard.</p> \n<h2 id=\"why-is-this-difficult\">Why is this difficult?</h2> \n<p>Although there are many sophisticated automatic clustering algorithms, such as <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">K-means</a> or <a href=\"https://en.wikipedia.org/wiki/Hierarchical_clustering\">Agglomerative clustering</a>, story clustering is a non-trivial problem. Because each text document can contain any word from our vocabulary, most text document representations are extremely high-dimensional. In high-dimensional spaces, even basic clustering or similarity measures fail or are very slow.</p> \n<p>Additionally, two very similar documents often have very different word usages. For example, one article may use the term <em>kitten</em> and another may use <em>feline</em>, but both articles could be referring to the same <em>cat</em>.</p> \n<p>Furthermore, we don’t know the number of roundups that we expect to see beforehand. This makes it difficult for us to directly use parameteric algorithms such as K-means. Our clustering algorithm also needs to be fast and easy to update, because there is a constant stream of documents coming into our system.</p> \n<h2 id=\"overview\">Overview</h2> \n<p>Since even the most basic distance measures fail in high dimensions, the first thing we do is lower the problem’s dimensionality. We represent each of our text documents as a <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">bag-of-words</a>, and remove stop-words and rare words from our vocabulary. Even after an aggressive trimming, the documents are still very high-dimensional. We then we use <a href=\"https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf\">Latent Dirichlet Allocation</a> (LDA) to further lower the documents’ dimensionality. We use LDA because this algorithm is amenable for text modeling and provides us with interpretable lower dimensional representations of documents.</p> \n<p>LDA is generally used for <a href=\"http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf\">qualitative understanding</a> of big text corpora. The <em>latent</em> topics that the model learns are highly interpretable and provide deep insights to the data. However, that’s not the only thing it can be used for; it is also a very logical algorithm to use for dimensionality reduction as it learns a mapping of sparse document-term vectors to sparse document-topic vectors in an unsupervised setting.</p> \n<div class=\"row\" style=\"text-align: center\"> \n <img src=\"http://engineering.flipboard.com/assets/storyclustering/docfactors.gif\" style=\"max-width:100%;\" /> \n</div> \n<p>Once the documents are represented over a tractable number of dimensions, all similarity and distance measures come into play. For our story clustering, we simply map all the documents to this conceptual level (latent topics), and look at the neighbours for each document within a certain distance. We use an approximate <a href=\"https://en.wikipedia.org/wiki/Nearest_neighbor_search\">nearest neighbour</a> model because it only requires us to look at a small neighbourhood of documents to generate these clusters.</p> \n<p>The following sections explain how we use LDA for this problem, and then introduce some tricks to optimize LDA using Alias tables and Metropolis-Hastings tests.</p> \n<h2 id=\"high-level-overview-of-lda\">High-level overview of LDA</h2> \n<p>LDA is a probabilistic <a href=\"https://en.wikipedia.org/wiki/Generative_model\">generative model</a> that extracts the thematic structure in a big document collection. The model assumes that every topic is a distribution of words in the vocabulary, and every document (described over the same vocabulary) is a distribution of a small subset of these topics. This is a statistical way to say that each topic (e.g. <em>space</em>) has some representative words (<em>star</em>, <em>planet</em>, etc.), and each document is only about a handful of these topics.</p> \n<div class=\"row\" style=\"text-align: center\"> \n <img src=\"http://engineering.flipboard.com/assets/storyclustering/lda.png\" style=\"max-width:100%;\" /> \n</div> \n<p>For example, let’s assume that we have a few topics as shown in the figure above. Knowing these topics, when we see a document explaining <em>Detecting and classifying pets using deep learning</em>, we can confidently say that the document is mostly about <em>Topic 2</em> and a little about <em>Topic 1</em> but not at all about <em>Topic 3</em> and <em>Topic 4</em>.</p> \n<p>LDA automatically infers these topics given a large collection of documents and expresses those (and future) documents in terms of topics instead of raw terms. The key advantage of doing this is that we allow term variability as the document is represented at a higher conceptual (topic) level rather than at the raw word level. This is akin to many success stories in image classification tasks using deep learning, where the classification is done on a higher conceptual level instead of on the pixel level.</p> \n<p>To infer the above <em>latent</em> topics, we do posterior inference on the model using Gibbs Sampling. What it essentially comes down to is to estimate the <em>best</em> topic for each word seen in every document. This is estimated by:</p> \n<script type=\"math/tex; mode=display\">p (Z_{d,n} = k) \\propto (C_k^d + \\alpha) \\times\\frac{C_k^w + \\beta}{C_k + V\\beta}</script> \n<p>where <script type=\"math/tex\">w</script> is the word seen in the <script type=\"math/tex\">n</script>th position in document <script type=\"math/tex\">d</script>, <script type=\"math/tex\">C_k^d</script> is the number of times the topic <script type=\"math/tex\">k</script> has appeared in document <script type=\"math/tex\">d</script>, <script type=\"math/tex\">C_k^w</script> is the number of times the word <script type=\"math/tex\">w</script> has been estimated with topic <script type=\"math/tex\">k</script> in the whole corpus, and <script type=\"math/tex\">C_k</script> is the number of times the topic <script type=\"math/tex\">k</script> has been assigned in the corpus. It is a sensible equation which suggests that a topic is more likely to be assigned if it has been assigned to other words in the same document or when the term has been assigned to that same topic several times in the whole document corpus.</p> \n<p>We calculate the above equation for each topic and define a <a href=\"https://en.wikipedia.org/wiki/Multinomial_distribution\">multinomial distribution</a> (a weighted dice roll), and generate a random topic from that distribution. The code for LDA’s inference (CGS-LDA) looks like this:</p> \n<style> .smallrow { width: 100% !important; } pre { word-wrap: normal; word-break: normal; } </style> \n<div class=\"language-c highlighter-rouge\">\n <div class=\"highlight\">\n  <pre class=\"highlight\"><code>\n    <table class=\"rouge-table\">\n     <tbody>\n      <tr>\n       <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n</pre></td>\n       <td class=\"rouge-code\"><pre><span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">d</span> <span class=\"o\">&lt;</span> <span class=\"n\">D</span><span class=\"p\">;</span> <span class=\"n\">d</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n  <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">n</span> <span class=\"o\">&lt;</span> <span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">]);</span> <span class=\"n\">n</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">n</span><span class=\"p\">];</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">Z</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">n</span><span class=\"p\">];</span>\n    <span class=\"n\">decrement_count_matrices</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">);</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">k</span> <span class=\"o\">&lt;</span> <span class=\"n\">K</span><span class=\"p\">;</span> <span class=\"n\">k</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"n\">p</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">CDK</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">alpha</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">CWK</span><span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">][</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">CK</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">V</span> <span class=\"o\">*</span> <span class=\"n\">beta</span><span class=\"p\">);</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">multinomial_distribution</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">);</span>\n    <span class=\"n\">increment_count_matrices</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">);</span>\n    <span class=\"n\">Z</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">n</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre></td>\n      </tr>\n     </tbody>\n    </table></code></pre>\n </div>\n</div> \n<p>This computation can be very expensive if we try to capture more than a thousand topics, because the algorithmic complexity becomes <script type=\"math/tex\">O(DNK)</script> per iteration. What we would ideally like is to get rid of <script type=\"math/tex\">O(K)</script> loop for each word in a document.</p> \n<h2 id=\"optimizing-lda-using-alias-tables-and-metropolis-hastings-tests\">Optimizing LDA using Alias Tables and Metropolis-Hastings Tests</h2> \n<p>Fortunately, there has been a lot of new research (<a href=\"https://arxiv.org/pdf/1412.1576v1\">LightLDA</a>, <a href=\"http://www.sravi.org/pubs/fastlda-kdd2014.pdf\">AliasLDA</a>) to speed up the sampling process and reduce the computational complexity to <script type=\"math/tex\">O(DN)</script>. The key question here is: Is it really possible to generate a single sample from a weighted multinomial distribution in under <script type=\"math/tex\">O(K)</script> time?</p> \n<p>The answer, not surprisingly, is “no” because generating a <script type=\"math/tex\">K</script> dimensional multinomial probability array <script type=\"math/tex\">p</script> takes at least <script type=\"math/tex\">O(K)</script> time because we need to know the weight for each index. But once this array is created, generating a sample is simply a matter of generating a random number from <script type=\"math/tex\">[0, sum(p)]</script> and checking which index of the array the number falls in. And if we needed more samples from the same distribution, all future samples would only require <script type=\"math/tex\">O(1)</script> time.</p> \n<div class=\"row\" style=\"text-align: center\"> \n <img src=\"http://engineering.flipboard.com/assets/storyclustering/prob_vector.gif\" style=\"max-width:100%;\" /> \n</div> \n<p>Instead of taking a single sample each time from the distribution, if we take <script type=\"math/tex\">K</script> samples each time the table is generated, then the amortized sampling complexity would be <script type=\"math/tex\">O(1)</script>. But this method has huge memory implications because the sizes of these arrays are dependent on the sum of the weights, and in LDA’s case, they can get extremely massive due to dependencies on count matrices (<script type=\"math/tex\">C_d^k</script>, <script type=\"math/tex\">C_k^w</script> and <script type=\"math/tex\">C_k</script>).</p> \n<h3 id=\"alias-sampling\">Alias Sampling</h3> \n<p>Walker’s <a href=\"https://en.wikipedia.org/wiki/Alias_method\">Alias method</a> is an effective way to compactly store these probability vectors (Space complexity: <script type=\"math/tex\">O(K)</script>), while keeping the sampling complexity at <script type=\"math/tex\">O(1)</script>. Instead of defining a long row vector, Alias sampling defines a completely filled 2-dimensional table (dim: <script type=\"math/tex\">K \\times 2)</script> from which its easy to sample from in constant time.</p> \n<p>These tables are generated by first multiplying each element by <script type=\"math/tex\">K</script>, followed by a Robin Hood algorithm and maintaining two lists: <em>rich</em> and <em>poor</em>. <em>rich</em> contains all the elements that have a greater weight than <script type=\"math/tex\">1.0</script>, and the <em>poor</em> stores the rest. This is followed by a simple iteration of putting the poor elements in the table cell first, and then filling up the height if needed by “stealing” from the rich:</p> \n<div id=\"aliastable\"></div> \n<div style=\"text-align:center\">\n <div style=\"display:inline-block;\"> \n  <button type=\"button\" class=\"btn btn-default\" id=\"randomize_aliastable\"> <span class=\"glyphicon glyphicon-refresh\"></span> Randomize Probabilities </button> \n </div>\n</div> \n<p>Each column has a height of 1.0 with a maximum of two different array indices. For sample generation, a random number is generated between <script type=\"math/tex\">[0, len(table)]</script> to pick the column, and then we generate a random floating point between <script type=\"math/tex\">[0, 1.0]</script> and see which region the decimal number falls in.</p> \n<h3 id=\"metropolis-hastings-algorithm--test\">Metropolis Hastings Algorithm / Test</h3> \n<p>Coming back to our LDA case, every time we need to sample a topic for a word in a document, we could generate <script type=\"math/tex\">K</script> samples. However, that would be wrong because the update equation is dependent on the count matrices, and stale samples don’t represent the <strong>true</strong> probability distribution of the inference.</p> \n<p>Here’s where <a href=\"https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\">Metropolis-Hastings</a> (MH) algorithm comes into play. MH algorithm is a <a href=\"https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov-Chain Monte Carlo</a> (MCMC) method to move around the probability space with the intention of converging to an objective. Given an objective function <script type=\"math/tex\">O(\\cdot)</script>, a proposal function <script type=\"math/tex\">P(\\cdot)</script> and a proposed position in the probability space, MH algorithm acts as a guide telling the algorithm whether it is a good or bad idea to move from the current position <script type=\"math/tex\">x</script> to the proposed point <script type=\"math/tex\">x'</script>. The acceptance of a new proposed position is computed by:</p> \n<script type=\"math/tex; mode=display\">A(x' | x) = min(1, \\frac{O(x')}{O(x)}\\frac{P(x | x')}{P (x' | x)})</script> \n<p>If the algorithm thinks that it’s a great idea to move, MH will always accept the proposal.</p> \n<p>The following toy example shows how we can approximate the area of a random function with Metropolis-Hastings sampling. In this example, we try to approximate the shape of a complicated non-linear function and use a standard Gaussian distribution (<script type=\"math/tex\">\\mathcal{N}(0, 1)</script>) multiplied with a step-size to generate proposals.</p> \n<script src=\"https://d3js.org/d3.v4.min.js\"></script> \n<div id=\"mh_vis\" style=\"text-align:center;\"></div> \n<div id=\"mh_vis_text\" style=\"text-align:center;\"></div> \n<div id=\"mh_vis_stepsize\" style=\"text-align:center;\"></div> \n<div id=\"mh_vis_slider\" style=\"text-align:center;\"> \n <input id=\"mh_vis_s\" type=\"range\" min=\"0.1\" max=\"0.9\" step=\"0.1\" value=\"0.3\" oninput=\"updateStepsize(value)\" list=\"stepsizes\" style=\"text-align:center;\" /> \n <datalist id=\"stepsizes\"> <option>0.1</option> <option>0.3</option> <option>0.5</option> <option>0.7</option> <option>0.9</option> </datalist> \n</div> \n<script src=\"/assets/storyclustering/mh.js\"></script> \n<script src=\"/assets/storyclustering/alias_table.js\"></script> \n<script> var table = new AliasTable([0.4, 0.3, 0.2, 0.08, 0.02]); table.displayState(d3.select(\"#aliastable\"), 0); function randomizeAliasTable() { table.stop = true; table = new AliasTable([Math.random(), Math.random(), Math.random(), Math.random(), Math.random()]); table.displayState(d3.select(\"#aliastable\"), 0); } d3.select(\"#randomize_aliastable\").on(\"click\", randomizeAliasTable); </script> \n<p>From the above example, it can be seen that proposal functions control the convergence speed of the algorithm. In the ideal scenario, we would like a high-acceptance rate and the ability to move quickly around the space. For the toy example above, step-sizes of <script type=\"math/tex\">0.2 - 0.4</script> achieve the best results.</p> \n<h3 id=\"back-to-lda\">Back to LDA</h3> \n<p>We would like to have high proposal acceptance rates, good space coverage, and simple proposal generation complexity. LightLDA authors suggest using the expressions within the LDA’s update equation as proposal functions. These expressions match the function in certain regions. This has two further advantages: we don’t need to compute anything extra and simply use the statistics (count matrices) that we would have collected anyway, and we don’t actually need to create alias tables for one of the proposals.</p> \n<script type=\"math/tex; mode=display\">p (Z_{d,n} = k) \\propto (\\underbrace{C_k^d + \\alpha}_{doc-proposal}) \\times \\underbrace{(\\frac{C_k^w + \\beta}{C_k + V\\beta}}_{term-proposal})</script> \n<p><script type=\"math/tex\">Z_d</script> acts as a proxy alias table for the doc-proposal because it stores the number of times each topic has appeared in the document <script type=\"math/tex\">d</script>, and if we generate a random number between <script type=\"math/tex\">[0, len(d)]</script>, we get a sample from the doc-proposal. For the word-proposals, we generate alias tables for each word and use the afforementioned Alias Sampling trick.</p> \n<p>We cycle between these two proposals, and do the MH test on the fly, and accept/reject the proposals. We recompute an alias table for a word every time we have used <script type=\"math/tex\">K</script> proposals. The acceptance probabilities for doc-proposal (<script type=\"math/tex\">A_d</script>) and word-proposal (<script type=\"math/tex\">A_w</script>) given a proposal topic <script type=\"math/tex\">p</script> and the current topic <script type=\"math/tex\">k</script> can be calculated by using the LDA’s update equation as the objective function, and the doc and word proposals as the proposal functions.</p> \n<script type=\"math/tex; mode=display\">A_d = min\\{ 1, \\underbrace{\\frac{(C_p^d + \\alpha) (C_p^w + \\beta) (C_k + V\\beta)} {(C_k^d + \\alpha) (C_k^w + \\beta) (C_p + V\\beta)}}_{objective} \\times \\underbrace{\\frac{(C_k^d + \\alpha)} {(C_p^d + \\alpha)} \\}}_{doc-proposal}</script> \n<script type=\"math/tex; mode=display\">A_w = min \\{ 1, \\underbrace{\\frac{(C_p^d + \\alpha) (C_p^w + \\beta) (C_k + V\\beta)} {(C_k^d + \\alpha) (C_k^w + \\beta) (C_p + V\\beta)}}_{objective} \\times \\underbrace{\\frac{(C_k^w + \\beta) (C_p + V\\beta)} {(C_p^w + \\beta) (C_k + V\\beta)}}_{term-proposal} \\}</script> \n<p>Using Alias tables and MH tests, the algorithm looks like the following:</p> \n<div class=\"language-cpp highlighter-rouge\">\n <div class=\"highlight\">\n  <pre class=\"highlight\"><code>\n    <table class=\"rouge-table\">\n     <tbody>\n      <tr>\n       <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n</pre></td>\n       <td class=\"rouge-code\"><pre><span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">d</span> <span class=\"o\">&lt;</span> <span class=\"n\">D</span><span class=\"p\">;</span> <span class=\"n\">d</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n  <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"n\">n</span> <span class=\"o\">&lt;</span> <span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">]);</span> <span class=\"n\">n</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"n\">proposal</span> <span class=\"o\">=</span> <span class=\"n\">coinflip</span><span class=\"p\">();</span>\n    <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">n</span><span class=\"p\">];</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">Z</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">n</span><span class=\"p\">];</span>\n    <span class=\"n\">decrement_count_matrices</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">);</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">proposal</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"c1\">// doc-proposal\n</span>      <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"n\">randomInt</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">]));</span>\n      <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">Z</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">][</span><span class=\"n\">index</span><span class=\"p\">];</span>\n      <span class=\"n\">mh_acceptance</span> <span class=\"o\">=</span> <span class=\"n\">compute_doc_acceptance</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">);</span>\n    <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n      <span class=\"c1\">// term-proposal\n</span>      <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">alias_sample</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">);</span>\n      <span class=\"n\">mh_acceptance</span> <span class=\"o\">=</span> <span class=\"n\">compute_term_acceptance</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">);</span>\n    <span class=\"p\">}</span>\n    <span class=\"c1\">// MH-test\n</span>    <span class=\"n\">mh_sample</span> <span class=\"o\">=</span> <span class=\"n\">randomFloat</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">);</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">mh_sample</span> <span class=\"o\">&lt;</span> <span class=\"n\">mh_acceptance</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"n\">increment_count_matrices</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">);</span>  <span class=\"c1\">// reject proposal, revert to k\n</span>    <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n      <span class=\"n\">increment_count_matrices</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">);</span>  <span class=\"c1\">// accept proposal\n</span>    <span class=\"p\">}</span>\t\n  <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</pre></td>\n      </tr>\n     </tbody>\n    </table></code></pre>\n </div>\n</div> \n<p>By doing this, the algorithmic complexity of LDA comes down to an amortized <script type=\"math/tex\">O(DN)</script> which allows us to process documents an order of magnitude faster. The following table and the graph compare the runtime (in seconds) of LDA of 60,000 documents after 100 iterations (convergence) on a single process.</p> \n<center> \n <table align=\"center\"> \n  <col width=\"160\" /> \n  <col width=\"90\" /> \n  <col width=\"90\" /> \n  <col width=\"90\" /> \n  <col width=\"90\" /> \n  <col width=\"90\" /> \n  <tr> \n   <th> Num. of Topics </th> \n   <th> 100 </th> \n   <th> 200 </th> \n   <th> 300 </th> \n   <th> 500 </th> \n   <th> 1000 </th> \n  </tr> \n  <tr> \n   <td> CGS LDA </td> \n   <td> 3427.99 </td> \n   <td> 7605.02 </td> \n   <td> 12190.54 </td> \n   <td> 25274.20 </td> \n   <td> 57492.22 </td> \n  </tr> \n  <tr> \n   <td> MH/Alias LDA </td> \n   <td> 601.06 </td> \n   <td> 616.07 </td> \n   <td> 620.82 </td> \n   <td> 646.38 </td> \n   <td> 685.19 </td> \n  </tr> \n </table> \n</center> \n<div class=\"row\" style=\"text-align: center\"> \n <img src=\"http://engineering.flipboard.com/assets/storyclustering/comparison.png\" style=\"max-width:100%;\" /> \n</div> \n<h2 id=\"clustering\">Clustering</h2> \n<p>LDA provides us with a sparse and robust representation of texts that reduces term variability in much lower dimensions. In 1000 or lower dimensions, most simple algorithms work really well. When each document is represented in this space, we do a fast <a href=\"http://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor\">Approximate Nearest Neighbour search</a>, and cluster all documents that are within a certain distance from each other.</p> \n<p>Using a distance based metric has an added advantage of being able to capture near and exact duplicates. The documents that are mapped too close to each other (purple circle) are considered to be the same story. The documents that are a certain radius away from the exact duplicates (pink circle) make up the roundups for each story.</p> \n<div id=\"cluster_vis\" style=\"text-align:center;\"></div> \n<script src=\"/assets/storyclustering/clustering.js\"></script> \n<p>Removing exact duplicates helps in capturing different views on an event, and here is an example of one of our story-clusters where the roundups capture differing perspectives on the same news.</p> \n<div class=\"row\" style=\"text-align: center\"> \n <img src=\"http://engineering.flipboard.com/assets/storyclustering/griezmann.gif\" style=\"max-width:50%;\" /> \n</div> \n<h2 id=\"conclusion\">Conclusion</h2> \n<p>Story roundups directly help us in diversifying our users’ feeds while also providing users with multiple perspectives on important stories. We had a lot of fun implementing this cool new feature, not least because we came across several new tricks that can be applied in multiple domains.</p> \n<p>Alias Sampling and MH algorithm have been around for a long time but they are only now being used in tandem to optimize posterior and predictive inference problems.</p> \n<h3 id=\"key-takeaways\">Key Takeaways</h3> \n<ul> \n <li>LDA is awesome not just in context of qualitative topic analysis but also in terms of dimensionality reduction.</li> \n <li>Simple algorithms work really well in low dimensions; (almost) everything fails in very high dimensions.</li> \n <li><em>Bayesian Machine Learning doesn’t have to be slow or expensive.</em></li> \n <li>If you made it this far, I hope you learned a new way to optimize some posterior inference problems - Alias Tables and MH tests are an amazing combination.</li> \n</ul> \n<p><em>Extra special thanks to <a href=\"http://benfrederickson.com\">Ben Frederickson</a> for suggestions, edits and the Alias Table visualization. Thanks to <a href=\"http://www.linkedin.com/in/sizzler\">Dale Cieslak</a>, <a href=\"https://www.linkedin.com/in/mikecora\">Mike Vlad Cora</a> and <a href=\"https://twitter.com/miaq\">Mia Quagliarello</a> for proofreading.</em></p> \n<p>Enjoyed this post? <a href=\"https://about.flipboard.com/careers/\">We’re hiring!</a> <script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"> </script></p> \n<style type=\"text/css\"> .axis path, .axis line { fill: none; stroke: black; shape-rendering: crispEdges; } .axis text { font-family: sans-serif; font-size: 11px; } </style>","descriptionType":"text/html","publishedDate":"Wed, 08 Feb 2017 00:00:00 +0000","feedId":10903,"bgimg":"http://engineering.flipboard.com/assets/storyclustering/storycluster.gif","linkMd5":"c306108ac38b740531ec219afa98fa07","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn43@2020_2/2022/03/14/11-31-17-169_3fbb5fc25b93b2b9.webp","destWidth":640,"destHeight":1136,"sourceBytes":3657511,"destBytes":8131044,"author":"https://www.linkedin.com/in/arnab-bhadury-a6304768 (Arnab Bhadury)","articleImgCdnMap":{"http://engineering.flipboard.com/assets/storyclustering/storycluster.gif":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn43@2020_2/2022/03/14/11-31-17-169_3fbb5fc25b93b2b9.webp","http://engineering.flipboard.com/assets/storyclustering/docfactors.gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn55@2020_6/2022/03/14/11-31-20-194_8a0538af641a8d99.webp","http://engineering.flipboard.com/assets/storyclustering/lda.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn59@2020_1/2022/03/14/11-31-21-728_f0d92263213c3be4.webp","http://engineering.flipboard.com/assets/storyclustering/prob_vector.gif":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn68@2020_4/2022/03/14/11-31-19-475_18f17b567f620fc6.webp","http://engineering.flipboard.com/assets/storyclustering/comparison.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn63@2020_6/2022/03/14/11-31-19-493_ca02c744f14ce1b9.webp","http://engineering.flipboard.com/assets/storyclustering/griezmann.gif":null},"publishedOrCreatedDate":1647257449864}],"record":{"createdTime":"2022-03-14 19:30:49","updatedTime":"2022-03-14 19:30:49","feedId":10903,"fetchDate":"Mon, 14 Mar 2022 11:30:49 +0000","fetchMs":695,"handleMs":16,"totalMs":151600,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"4cf9ab886509ef3fd2ece0a16b714d9c","hostName":"europe-57*","requestId":"a1ed5c805e904ffd8525ec96875e10f6_10903","contentType":"application/xml","totalBytes":8215114,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":6,"articlesImgsGithubTotal":5,"successGithubMap":{"myreaderx10":1,"myreaderx12":1,"myreaderx13":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:36:17","updatedTime":"2020-09-05 01:22:59","id":10903,"name":"Flipboard Engineering","url":"http://engineering.flipboard.com/feed.xml","subscriber":282,"website":null,"icon":"http://engineering.flipboard.com/assets/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn23@2020_2/2020/09/04/17-22-58-219_3fa1299ecb41996f.ico","description":"The official Flipboard engineering blog","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2022-03-14 19:33:20","updatedTime":"2022-03-14 19:33:20","id":null,"feedId":10903,"linkMd5":"c306108ac38b740531ec219afa98fa07"}],"tmpCommonImgCdnBytes":8131044,"tmpBodyImgCdnBytes":84070,"tmpBgImgCdnBytes":0,"extra4":{"start":1647257449142,"total":0,"statList":[{"spend":707,"msg":"获取xml内容"},{"spend":16,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":121550,"msg":"正文链接上传到cdn"}]},"extra5":6,"extra6":5,"extra7ImgCdnFailResultVector":[null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-23.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe61.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-010.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-026.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://engineering.flipboard.com/assets/storyclustering/storycluster.gif","sourceStatusCode":200,"destWidth":640,"destHeight":1136,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn43@2020_2/2022/03/14/11-31-17-169_3fbb5fc25b93b2b9.webp","sourceBytes":3657511,"destBytes":8131044,"targetWebpQuality":75,"feedId":10903,"totalSpendMs":27669,"convertSpendMs":25162,"createdTime":"2022-03-14 19:30:51","host":"europe-56*","referer":"http://engineering.flipboard.com//2017/02/storyclustering","linkMd5ListStr":"c306108ac38b740531ec219afa98fa07,c306108ac38b740531ec219afa98fa07","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.5 MB","destSize":"7.8 MB","compressRate":"222.3%"},{"code":1,"isDone":false,"source":"http://engineering.flipboard.com/assets/storyclustering/prob_vector.gif","sourceStatusCode":200,"destWidth":600,"destHeight":180,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn68@2020_4/2022/03/14/11-31-19-475_18f17b567f620fc6.webp","sourceBytes":4869,"destBytes":4508,"targetWebpQuality":75,"feedId":10903,"totalSpendMs":443,"convertSpendMs":115,"createdTime":"2022-03-14 19:31:19","host":"us-038*","referer":"http://engineering.flipboard.com//2017/02/storyclustering","linkMd5ListStr":"c306108ac38b740531ec219afa98fa07","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.8 KB","destSize":"4.4 KB","compressRate":"92.6%"},{"code":1,"isDone":false,"source":"http://engineering.flipboard.com/assets/storyclustering/comparison.png","sourceStatusCode":200,"destWidth":600,"destHeight":371,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn63@2020_6/2022/03/14/11-31-19-493_ca02c744f14ce1b9.webp","sourceBytes":17456,"destBytes":9902,"targetWebpQuality":75,"feedId":10903,"totalSpendMs":685,"convertSpendMs":8,"createdTime":"2022-03-14 19:31:19","host":"europe61*","referer":"http://engineering.flipboard.com//2017/02/storyclustering","linkMd5ListStr":"c306108ac38b740531ec219afa98fa07","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"17 KB","destSize":"9.7 KB","compressRate":"56.7%"},{"code":1,"isDone":false,"source":"http://engineering.flipboard.com/assets/storyclustering/docfactors.gif","sourceStatusCode":200,"destWidth":600,"destHeight":280,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn55@2020_6/2022/03/14/11-31-20-194_8a0538af641a8d99.webp","sourceBytes":25068,"destBytes":25104,"targetWebpQuality":75,"feedId":10903,"totalSpendMs":1139,"convertSpendMs":147,"createdTime":"2022-03-14 19:31:19","host":"us-010*","referer":"http://engineering.flipboard.com//2017/02/storyclustering","linkMd5ListStr":"c306108ac38b740531ec219afa98fa07","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"24.5 KB","destSize":"24.5 KB","compressRate":"100.1%"},{"code":1,"isDone":false,"source":"http://engineering.flipboard.com/assets/storyclustering/lda.png","sourceStatusCode":200,"destWidth":630,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn59@2020_1/2022/03/14/11-31-21-728_f0d92263213c3be4.webp","sourceBytes":64928,"destBytes":44556,"targetWebpQuality":75,"feedId":10903,"totalSpendMs":913,"convertSpendMs":72,"createdTime":"2022-03-14 19:31:21","host":"europe-23*","referer":"http://engineering.flipboard.com//2017/02/storyclustering","linkMd5ListStr":"c306108ac38b740531ec219afa98fa07","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"63.4 KB","destSize":"43.5 KB","compressRate":"68.6%"}],"successGithubMap":{"myreaderx10":1,"myreaderx12":1,"myreaderx13":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{}}