{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-05-03 12:07:23","updatedTime":"2021-05-03 12:07:23","title":"Multi-Task Robotic Reinforcement Learning at Scale","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","description":"<span class=\"byline-author\">Posted by Karol Hausman, Senior Research Scientist and Yevgen Chebotar, Research Scientist, Robotics at Google</span> <p>For general-purpose robots to be most useful, they would need to be able to perform a range of tasks, such as cleaning, maintenance and delivery. But training even a single task (e.g., grasping) using <a href=\"https://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html\">offline reinforcement learning</a> (RL), a trial and error learning method where the agent uses training previously collected data, can take <a href=\"https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html\">thousands of robot-hours</a>, in addition to the significant engineering needed to enable autonomous operation of a large-scale robotic system. Thus, the computational costs of building general-purpose <a href=\"https://x.company/projects/everyday-robots/\">everyday robots</a> using current robot learning methods becomes prohibitive as the number of tasks grows.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-3zCzaQm-_Fo/YHn5zp0Iv_I/AAAAAAAAHZs/Fyg3TSZX28wlvIEuv5t1h1CiLH_YDIa6wCLcBGAsYHQ/s512/image1.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"384\" data-original-width=\"512\" height=\"300\" src=\"https://1.bp.blogspot.com/-3zCzaQm-_Fo/YHn5zp0Iv_I/AAAAAAAAHZs/Fyg3TSZX28wlvIEuv5t1h1CiLH_YDIa6wCLcBGAsYHQ/w400-h300/image1.gif\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Multi-task data collection across multiple robots where different robots collect data for different tasks.</td></tr></tbody></table><p>In other large-scale machine learning domains, such as <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">natural language processing</a> and <a href=\"https://en.wikipedia.org/wiki/Computer_vision\">computer vision</a>, a number of strategies have been applied to amortize the effort of learning over multiple skills. For example, <a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">pre-training</a> on large natural language datasets can enable few- or zero-shot learning of multiple tasks, such as <a href=\"https://en.wikipedia.org/wiki/Question_answering\">question answering</a> and <a href=\"https://en.wikipedia.org/wiki/Sentiment_analysis\">sentiment analysis</a>. However, because robots collect their own data, robotic skill learning presents a unique set of opportunities and challenges. Automating this process is a large engineering endeavour, and effectively reusing <a href=\"https://bair.berkeley.edu/blog/2019/11/26/robo-net/\">past robotic data collected by different robots</a> remains an open problem.  </p><p>Today we present two new advances for robotic RL at scale, <a href=\"https://arxiv.org/abs/2104.08212\">MT-Opt</a>, a new multi-task RL system for automated data collection and multi-task RL training, and <a href=\"https://arxiv.org/abs/2104.07749\">Actionable Models</a>, which leverages the acquired data for <a href=\"https://bair.berkeley.edu/blog/2018/09/06/rig/\">goal-conditioned</a> RL. MT-Opt introduces a scalable data-collection mechanism that is used to collect over 800,000 episodes of various tasks on real robots and demonstrates a successful application of multi-task RL that yields ~3x average improvement over baseline. Additionally, it enables robots to master new tasks quickly through use of its extensive multi-task dataset (new task fine-tuning in &lt;1 day of data collection). Actionable Models enables learning in the absence of specific tasks and rewards by training an implicit model of the world that is also an actionable robotic policy. This drastically increases the number of tasks the robot can perform (via visual goal specification) and enables more efficient learning of downstream tasks. </p><p><b>Large-Scale Multi-Task Data Collection System</b><br />The cornerstone for both MT-Opt and Actionable Models is the volume and quality of training data. To collect diverse, multi-task data at scale, users need a way to specify tasks, decide for which tasks to collect the data, and finally, manage and balance the resulting dataset. To that end, we create a scalable and intuitive multi-task success detector using data from all of the chosen tasks. The multi-task success is trained using supervised learning to detect the outcome of a given task and it allows users to quickly define new tasks and their rewards. When this success detector is being applied to collect data, it is periodically updated to accommodate distribution shifts caused by various real-world factors, such as varying lighting conditions, changing background surroundings, and novel states that the robots discover.  </p><p>Second, we simultaneously collect data for multiple distinct tasks across multiple robots by using solutions to easier tasks to effectively bootstrap learning of more complex tasks. This allows training of a policy for the harder tasks and improves the data collected for them. As such, the amount of per-task data and the number of successful episodes for each task grows over time. To further improve the performance, we focus data collection on underperforming tasks, rather than collecting data uniformly across tasks.  </p><p>This system collected 9600 robot hours of data (from 57 continuous data collection days on seven robots). However, while this data collection strategy was effective at collecting data for a large number of tasks, the success rate and data volume was imbalanced between tasks.  </p><p><b>Learning with MT-Opt</b><br />We address the data collection imbalance by transferring data across tasks and re-balancing the per-task data. The robots generate episodes that are labelled as success or failure for each task and are then copied and shared across other tasks. The balanced batch of episodes is then sent to our multi-task RL training pipeline to train the MT-Opt policy. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-ORHRr1O1TYc/YHn5-FZgWrI/AAAAAAAAHZw/_S5xTB7lVJUapExE3iix5l4NnM3SxxmvACLcBGAsYHQ/s816/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"816\" data-original-width=\"800\" height=\"400\" src=\"https://1.bp.blogspot.com/-ORHRr1O1TYc/YHn5-FZgWrI/AAAAAAAAHZw/_S5xTB7lVJUapExE3iix5l4NnM3SxxmvACLcBGAsYHQ/w393-h400/image2.gif\" width=\"393\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Data sharing and task re-balancing strategy used by MT-Opt. The robots generate episodes which then get labelled as success or failure for the current task and are then shared across other tasks.</td></tr></tbody></table><p>MT-Opt uses <a href=\"https://en.wikipedia.org/wiki/Q-learning\">Q-learning</a>, a popular RL method that learns a function that estimates the future sum of rewards, called the Q-function. The learned policy then picks the action that maximizes this learned Q-function. For multi-task policy training, we specify the task as an extra input to a large Q-learning network (inspired by our previous work on large-scale single-task learning with <a href=\"https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html\">QT-Opt</a>) and then train all of the tasks simultaneously with <a href=\"https://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html\">offline RL</a> using the entire multi-task dataset. In this way, MT-Opt is able to train on a wide variety of skills that include picking specific objects, placing them into various fixtures, aligning items on a rack, rearranging and covering objects with towels, etc.  </p><p>Compared to single-task baselines, MT-Opt performs similarly on the tasks that have the most data and significantly improves performance on underrepresented tasks. So, for a generic lifting task, which has the most supporting data, MT-Opt achieved an 89% success rate (compared to 88% for QT-Opt) and achieved a 50% average success rate across rare tasks, compared to 1% with a single-task QT-Opt baseline and 18% using a naïve, multi-task QT-Opt baseline. Using MT-Opt not only enables zero-shot generalization to new but similar tasks, but also can quickly (in about 1 day of data collection on seven robots) be fine-tuned to new, previously unseen tasks. For example, when applied to an unseen towel-covering task, the system achieved a zero-shot success rate of 92% for towel-picking and 79% for object-covering, which wasn’t present in the original dataset. </p> <video autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" style=\"margin-left: 10%;\" width=\"80%\"> <source src=\"https://karolhausman.github.io/mt-opt/img/mt-opt-grid.mp4\" type=\"video/mp4\"></source></video><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Example tasks that MT-Opt is able to learn, such as instance and indiscriminate grasping, chasing, placing, aligning and rearranging.</td></tr></tbody></table><!--<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-d5t1VaM4STQ/YHn6Lyq_Z4I/AAAAAAAAHZ4/9gDnROfIfa8zUjlcgXpMwuRAX8w8O8KWQCLcBGAsYHQ/s1600/image5.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"900\" data-original-width=\"1600\" height=\"360\" src=\"https://1.bp.blogspot.com/-d5t1VaM4STQ/YHn6Lyq_Z4I/AAAAAAAAHZ4/9gDnROfIfa8zUjlcgXpMwuRAX8w8O8KWQCLcBGAsYHQ/w640-h360/image5.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Example tasks that MT-Opt is able to learn, such as instance and indiscriminate grasping, chasing, placing, aligning and rearranging.</td></tr></tbody></table>--><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-jtZhMcye1DE/YHn6SkQzTkI/AAAAAAAAHZ8/qk6xAWKaQg400qOWNE4_gtmg67JKRpejgCLcBGAsYHQ/s480/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"360\" data-original-width=\"480\" height=\"300\" src=\"https://1.bp.blogspot.com/-jtZhMcye1DE/YHn6SkQzTkI/AAAAAAAAHZ8/qk6xAWKaQg400qOWNE4_gtmg67JKRpejgCLcBGAsYHQ/w400-h300/image3.gif\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Towel-covering task that was not present in the original dataset. We fine-tune MT-Opt on this novel task in 1 day to achieve a high (&gt;90%) success rate.</td></tr></tbody></table><p><b>Learning with Actionable Models</b><br />While supplying a rigid definition of tasks facilitates autonomous data collection for MT-Opt, it limits the number of learnable behaviors to a fixed set. To enable learning a wider range of tasks from the same data, we use <a href=\"https://bair.berkeley.edu/blog/2018/09/06/rig/\">goal-conditioned</a> learning, i.e., learning to reach given goal configurations of a scene in front of the robot, which we specify with goal images. In contrast to explicit <a href=\"https://bair.berkeley.edu/blog/2019/12/12/mbpo/\">model-based methods</a> that learn predictive models of future world observations, or approaches that employ <a href=\"https://arxiv.org/abs/1707.01495\">online data collection</a>, this approach learns goal-conditioned policies via offline model-free RL.  </p><p>To learn to reach any goal state, we perform <a href=\"https://arxiv.org/abs/1707.01495\">hindsight relabeling</a> of all trajectories and sub-sequences in our collected dataset and train a goal-conditioned Q-function in a <a href=\"https://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html\">fully offline</a> manner (in contrast to learning online using a fixed set of success examples as in <a href=\"https://ai.googleblog.com/2021/03/recursive-classification-replacing.html\">recursive classification</a>). One challenge in this setting is the distributional shift caused by learning only from “positive” hindsight relabeled examples. This we address by employing a <a href=\"https://arxiv.org/abs/2006.04779\">conservative strategy</a> to minimize Q-values of unseen actions using artificial negative actions. Furthermore, to enable reaching temporary-extended goals, we introduce a technique for chaining goals across multiple episodes.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-kpV2GPoLsso/YHn6ZuK2grI/AAAAAAAAHaA/2JkUk3LSRfE0KhB451FagVis-W_wSOoNwCLcBGAsYHQ/s800/image4.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"640\" data-original-width=\"800\" height=\"320\" src=\"https://1.bp.blogspot.com/-kpV2GPoLsso/YHn6ZuK2grI/AAAAAAAAHaA/2JkUk3LSRfE0KhB451FagVis-W_wSOoNwCLcBGAsYHQ/w400-h320/image4.gif\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Actionable Models relabel sub-sequences with all intermediate goals and regularize Q-values with artificial negative actions.</td></tr></tbody></table><p>Training with Actionable Models allows the system to learn a large repertoire of visually indicated skills, such as object grasping, container placing and object rearrangement. The model is also able to generalize to novel objects and visual objectives not seen in the training data, which demonstrates its ability to learn general functional knowledge about the world. We also show that downstream reinforcement learning tasks can be learned more efficiently by either fine-tuning a pre-trained goal-conditioned model or through a goal-reaching auxiliary objective during training. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HBzWWQ1ScEc/YHoPmj1aUgI/AAAAAAAAHaM/j814Lw21PTkhSR4O-aCo-ApM5DMLLqFDwCLcBGAsYHQ/s1100/actionable_models_grid.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"732\" data-original-width=\"1100\" height=\"426\" src=\"https://1.bp.blogspot.com/-HBzWWQ1ScEc/YHoPmj1aUgI/AAAAAAAAHaM/j814Lw21PTkhSR4O-aCo-ApM5DMLLqFDwCLcBGAsYHQ/w640-h426/actionable_models_grid.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Example tasks (specified by goal-images) that our Actionable Model is able to learn.</td></tr></tbody></table><p><b>Conclusion</b><br />The results of both MT-Opt and Actionable Models indicate that it is possible to collect and then learn many distinct tasks from large diverse real-robot datasets within a single model, effectively amortizing the cost of learning across many skills. We see this an important step towards general robot learning systems that can be further scaled up to perform many useful services and serve as a starting point for learning downstream tasks.  </p><p>This post is based on two papers, \"<a href=\"https://arxiv.org/abs/2104.08212\">MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale</a>\" and \"<a href=\"https://arxiv.org/abs/2104.07749\">Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills</a>,\" with additional information and videos on the project websites for <a href=\"https://karolhausman.github.io/mt-opt/\">MT-Opt</a> and <a href=\"https://actionable-models.github.io/\">Actionable Models</a>.</p><p><b>Acknowledgements</b><br /><em>This research was conducted by Dmitry Kalashnikov, Jake Varley, Karol Hausman, Yevgen Chebotar, Ben Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, Yao Lu, Alex Irpan, Ben Eysenbach, Ryan Julian and Ted Xiao. We’d like to give special thanks to Josh Weaver, Noah Brown, Khem Holden, Linda Luu and Brandon Kinman for their robot operation support; Anthony Brohan for help with distributed learning and testing infrastructure; Tom Small for help with videos and project media; Julian Ibarz, Kanishka Rao, Vikas Sindhwani and Vincent Vanhoucke for their support; Tuna Toksoz and Garrett Peake for improving the bin reset mechanisms; Satoshi Kataoka,  Michael Ahn, and Ken Oslund for help with the underlying control stack, and the rest of the Robotics at Google team for their overall support and encouragement. All the above contributions were incredibly enabling for this research.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=yojGTmYWICo:pqUgKFs0wY4:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/yojGTmYWICo\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 19 Apr 2021 16:57:00 +0000","feedId":4244,"bgimg":"https://1.bp.blogspot.com/-3zCzaQm-_Fo/YHn5zp0Iv_I/AAAAAAAAHZs/Fyg3TSZX28wlvIEuv5t1h1CiLH_YDIa6wCLcBGAsYHQ/w400-h300/image1.gif","linkMd5":"3e5d35daf49979fbc58d56b50deb21d9","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn18@2020_4/2021/05/03/04-08-41-452_76e36e8fcb73c4c7.webp","destWidth":400,"destHeight":300,"sourceBytes":5769592,"destBytes":2601314,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-3zCzaQm-_Fo/YHn5zp0Iv_I/AAAAAAAAHZs/Fyg3TSZX28wlvIEuv5t1h1CiLH_YDIa6wCLcBGAsYHQ/w400-h300/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn18@2020_4/2021/05/03/04-08-41-452_76e36e8fcb73c4c7.webp","https://1.bp.blogspot.com/-ORHRr1O1TYc/YHn5-FZgWrI/AAAAAAAAHZw/_S5xTB7lVJUapExE3iix5l4NnM3SxxmvACLcBGAsYHQ/w393-h400/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn76@2020_1/2021/05/03/04-08-45-049_ccfb317578390a78.webp","https://1.bp.blogspot.com/-jtZhMcye1DE/YHn6SkQzTkI/AAAAAAAAHZ8/qk6xAWKaQg400qOWNE4_gtmg67JKRpejgCLcBGAsYHQ/w400-h300/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn58@2020_5/2021/05/03/04-09-01-547_22e389c04e8a404c.webp","https://1.bp.blogspot.com/-kpV2GPoLsso/YHn6ZuK2grI/AAAAAAAAHaA/2JkUk3LSRfE0KhB451FagVis-W_wSOoNwCLcBGAsYHQ/w400-h320/image4.gif":null,"https://1.bp.blogspot.com/-HBzWWQ1ScEc/YHoPmj1aUgI/AAAAAAAAHaM/j814Lw21PTkhSR4O-aCo-ApM5DMLLqFDwCLcBGAsYHQ/w640-h426/actionable_models_grid.gif":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn66@2020_3/2021/05/03/04-08-49-793_eead50d0645bf8c6.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn61@2020_5/2021/05/03/04-08-42-159_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/yojGTmYWICo":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn70@2020_1/2021/05/03/04-08-41-982_78d5dffa8d284026.webp"},"publishedOrCreatedDate":1620014843769}],"record":{"createdTime":"2021-05-03 12:07:23","updatedTime":"2021-05-03 12:07:23","feedId":4244,"fetchDate":"Mon, 03 May 2021 04:07:23 +0000","fetchMs":369,"handleMs":47,"totalMs":105630,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"af0629e1ae74a27744b4cbd27b40a78e","hostName":"us-003*","requestId":"3198825f481844c38a23df94c98c91d3_4244","contentType":"text/xml; charset=UTF-8","totalBytes":8276274,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":7,"articlesImgsGithubTotal":6,"successGithubMap":{"myreaderx8":1,"myreaderx11":1,"myreaderx1":1,"myreaderx13":1,"myreaderx30":1,"myreaderx19":1},"failGithubMap":{"myreaderx14":1}},"feed":{"createdTime":"2020-08-25 04:29:40","updatedTime":"2020-09-01 10:46:06","id":4244,"name":"Google AI Blog","url":"http://googleresearch.blogspot.com/feeds/posts/default","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn78@2020_3/2020/09/01/02-46-06-599_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-05-03 12:09:08","updatedTime":"2021-05-03 12:09:08","id":null,"feedId":4244,"linkMd5":"3e5d35daf49979fbc58d56b50deb21d9"}],"tmpCommonImgCdnBytes":2601314,"tmpBodyImgCdnBytes":5674960,"tmpBgImgCdnBytes":0,"extra4":{"start":1620014843261,"total":0,"statList":[{"spend":462,"msg":"获取xml内容"},{"spend":47,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":26964,"msg":"正文链接上传到cdn"}]},"extra5":7,"extra6":7,"extra7ImgCdnFailResultVector":[null,{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-kpV2GPoLsso/YHn6ZuK2grI/AAAAAAAAHaA/2JkUk3LSRfE0KhB451FagVis-W_wSOoNwCLcBGAsYHQ/w400-h320/image4.gif","sourceStatusCode":200,"destWidth":400,"destHeight":320,"sourceBytes":2516895,"destBytes":1019948,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":1932,"convertSpendMs":1816,"createdTime":"2021-05-03 12:08:41","host":"us-001*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn71/contents/2021/05/03/04-08-43-817_76555fd2a4bc7345.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 03 May 2021 04:08:43 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["9C20:60AC:2BE0A24:6435154:608F774B"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1620016832"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn71/contents/2021/05/03/04-08-43-817_76555fd2a4bc7345.webp","historyStatusCode":[],"spendMs":64},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 MB","destSize":"996 KB","compressRate":"40.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-kpV2GPoLsso/YHn6ZuK2grI/AAAAAAAAHaA/2JkUk3LSRfE0KhB451FagVis-W_wSOoNwCLcBGAsYHQ/w400-h320/image4.gif","sourceStatusCode":200,"destWidth":400,"destHeight":320,"sourceBytes":2516895,"destBytes":1019948,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":2537,"convertSpendMs":1889,"createdTime":"2021-05-03 12:08:43","host":"europe-60*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn71/contents/2021/05/03/04-08-45-903_76555fd2a4bc7345.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 03 May 2021 04:08:46 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["805C:55B9:4F3C158:50A79D8:608F774D"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1620016832"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn71/contents/2021/05/03/04-08-45-903_76555fd2a4bc7345.webp","historyStatusCode":[],"spendMs":560},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 MB","destSize":"996 KB","compressRate":"40.5%"}],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-001.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-60.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-3zCzaQm-_Fo/YHn5zp0Iv_I/AAAAAAAAHZs/Fyg3TSZX28wlvIEuv5t1h1CiLH_YDIa6wCLcBGAsYHQ/w400-h300/image1.gif","sourceStatusCode":200,"destWidth":400,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn18@2020_4/2021/05/03/04-08-41-452_76e36e8fcb73c4c7.webp","sourceBytes":5769592,"destBytes":2601314,"targetWebpQuality":67,"feedId":4244,"totalSpendMs":1792,"convertSpendMs":1232,"createdTime":"2021-05-03 12:08:40","host":"us-010*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9,3e5d35daf49979fbc58d56b50deb21d9","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.5 MB","destSize":"2.5 MB","compressRate":"45.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/yojGTmYWICo","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn70@2020_1/2021/05/03/04-08-41-982_78d5dffa8d284026.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":376,"convertSpendMs":2,"createdTime":"2021-05-03 12:08:41","host":"us-013*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn61@2020_5/2021/05/03/04-08-42-159_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":647,"convertSpendMs":4,"createdTime":"2021-05-03 12:08:42","host":"europe-60*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-ORHRr1O1TYc/YHn5-FZgWrI/AAAAAAAAHZw/_S5xTB7lVJUapExE3iix5l4NnM3SxxmvACLcBGAsYHQ/w393-h400/image2.gif","sourceStatusCode":200,"destWidth":392,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn76@2020_1/2021/05/03/04-08-45-049_ccfb317578390a78.webp","sourceBytes":5842539,"destBytes":2941180,"targetWebpQuality":67,"feedId":4244,"totalSpendMs":8023,"convertSpendMs":2542,"createdTime":"2021-05-03 12:08:42","host":"europe-25*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.6 MB","destSize":"2.8 MB","compressRate":"50.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HBzWWQ1ScEc/YHoPmj1aUgI/AAAAAAAAHaM/j814Lw21PTkhSR4O-aCo-ApM5DMLLqFDwCLcBGAsYHQ/w640-h426/actionable_models_grid.gif","sourceStatusCode":200,"destWidth":640,"destHeight":426,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn66@2020_3/2021/05/03/04-08-49-793_eead50d0645bf8c6.webp","sourceBytes":15627603,"destBytes":2307000,"targetWebpQuality":4,"feedId":4244,"totalSpendMs":8303,"convertSpendMs":2978,"createdTime":"2021-05-03 12:08:41","host":"us-038*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"14.9 MB","destSize":"2.2 MB","compressRate":"14.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-jtZhMcye1DE/YHn6SkQzTkI/AAAAAAAAHZ8/qk6xAWKaQg400qOWNE4_gtmg67JKRpejgCLcBGAsYHQ/w400-h300/image3.gif","sourceStatusCode":200,"destWidth":400,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn58@2020_5/2021/05/03/04-09-01-547_22e389c04e8a404c.webp","sourceBytes":1610762,"destBytes":426398,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":25670,"convertSpendMs":524,"createdTime":"2021-05-03 12:08:42","host":"europe-22*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/yojGTmYWICo/multi-task-robotic-reinforcement.html","linkMd5ListStr":"3e5d35daf49979fbc58d56b50deb21d9","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.5 MB","destSize":"416.4 KB","compressRate":"26.5%"}],"successGithubMap":{"myreaderx8":1,"myreaderx11":1,"myreaderx1":1,"myreaderx13":1,"myreaderx30":1,"myreaderx19":1},"failGithubMap":{"myreaderx14":1}}