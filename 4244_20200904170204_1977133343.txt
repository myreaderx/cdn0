{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-09-05 01:01:27","updatedTime":"2020-09-05 01:01:27","title":"The Technology Behind our Recent Improvements in Flood Forecasting","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","description":"<span class=\"byline-author\">Posted by Sella Nevo, Senior Software Engineer, Google Research, Tel Aviv</span> <p>Flooding is the most common natural disaster on the planet, affecting the lives of hundreds of millions of people around the globe and causing around $10 billion in damages each year. Building on <a href=\"https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\">our work in previous years</a>, earlier this week we announced some of our recent efforts to <a href=\"https://blog.google/technology/ai/flood-forecasts-india-bangladesh/\">improve flood forecasting in India and Bangladesh</a>, expanding coverage to  more than 250 million people, and providing unprecedented lead time, accuracy and clarity.  </p><p>To enable these breakthroughs, we have devised a new approach for inundation modeling, called a <i>morphological inundation model</i>, which combines physics-based modeling with machine learning (ML) to create more accurate and scalable inundation models in real-world settings. Additionally, our new <i>alert-targeting model</i> allows identifying areas at risk of flooding at unprecedented scale using end-to-end machine learning models and data that is publicly available globally. In this post, we also describe developments for the next generation of flood forecasting systems, called&nbsp;<a href=\"https://ai4earthscience.github.io/iclr-2020-workshop/papers/ai4earth04.pdf\">HydroNets</a> (presented at <a href=\"https://ai4earthscience.github.io/iclr-2020-workshop/\">ICLR AI for Earth Sciences</a> and <a href=\"https://meetingorganizer.copernicus.org/EGU2020/EGU2020-4135.html\">EGU</a> this year), which is a new architecture specially built for hydrologic modeling across multiple basins, while still optimizing for accuracy at each location.  </p><p><b>Forecasting Water Levels</b><br />The first step in a flood forecasting system is to identify whether a river is expected to flood. <a href=\"https://en.wikipedia.org/wiki/Hydrological_model\">Hydrologic models</a> (or gauge-to-gauge models) have long been used by governments and disaster management agencies to improve the accuracy and extend the lead time of their forecasts. These models receive inputs like precipitation or upstream <a href=\"https://en.wikipedia.org/wiki/Stream_gauge\">gauge measurements</a> of water level (i.e., the absolute elevation of the water above sea level) and output a forecast for the water level (or discharge) in the river at some time in the future. </p><p>The hydrologic model component of the&nbsp;flood forecasting system described in this week’s <a href=\"https://blog.google/technology/ai/flood-forecasts-india-bangladesh/\">Keyword post</a> doubled the lead time of flood alerts for areas covering more than 75 million people. These models not only increase lead time, but also provide unprecedented accuracy, achieving an <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\">R<sup>2</sup> score</a> of more than 99% across all basins we cover, and predicting the water level within a 15 cm error bound more than 90% of the time.&nbsp;Once a river is predicted to reach flood level, the next step in generating actionable warnings is to convert the river level forecast into a prediction for how the floodplain will be affected.</p><p><b>Morphological Inundation Modeling</b><br /><a href=\"https://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html\">In prior work</a>, we developed high quality elevation maps based on satellite imagery, and ran physics-based models to simulate water flow across these digital terrains, which allowed  warnings with <a href=\"https://arxiv.org/abs/1910.05006\">unprecedented resolution and accuracy in data-scarce regions</a>. In collaboration with our satellite partners, <a href=\"https://www.airbus.com/\">Airbus</a>, <a href=\"https://www.maxar.com/\">Maxar</a> and <a href=\"https://www.planet.com/\">Planet</a>, we have now expanded the elevation maps to cover hundreds of millions of square kilometers. However, in order to scale up the coverage to such a large area while still retaining high accuracy, we had to re-invent how we develop inundation models. </p><p id=\"gdcalert1\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s960/Inundation-960px.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"406\" data-original-width=\"960\" src=\"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s640/Inundation-960px.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Inundation modeling estimates what areas will be flooded and how deep the water will be. This visualization conceptually shows how inundation could be simulated, how risk levels could be defined (represented by red and white colors), and how the model could be used to identify areas that should be warned (green dots).</td></tr></tbody></table><p></p><p>Inundation modeling at scale suffers from three significant challenges. Due to the large areas involved and the resolution required for such models, they necessarily have high computational complexity. In addition, most global elevation maps don’t include riverbed bathymetry, which is important for accurate modeling. Finally, the errors in existing data, which may include gauge measurement errors, missing features in the elevation maps, and the like, need to be understood and corrected. Correcting such problems may require collecting additional high-quality data or fixing erroneous data manually, neither of which scale well.  </p><p>Our new approach to inundation modeling, which we call a <em>morphological model</em>, addresses these issues by using several innovative tricks. Instead of modeling the complex behaviors of water flow in real time, we compute modifications to the morphology of the elevation map that allow one to simulate the inundation using simple physical principles, such as those describing <a href=\"https://en.wikipedia.org/wiki/Hydrostatics\">hydrostatic systems</a>.   </p><p>First, we train a pure-ML model (devoid of physics-based information) to estimate the one-dimensional river profile from gauge measurements. The model takes as input the water level at a specific point on the river (the stream gauge) and outputs the river profile, which is the water level at all points in the river. We assume that if the gauge increases, the water level increases monotonically, i.e., the water level at other points in the river increases as well. We also assume that the absolute elevation of the river profile decreases downstream (i.e., the river flows downhill).  </p><p>We then use this learned model and some heuristics to edit the elevation map to approximately “cancel out” the pressure gradient that would exist if that region were flooded. This new synthetic elevation map provides the foundation on which we model the flood behavior using a simple <a href=\"https://en.wikipedia.org/wiki/Flood_fill\">flood-fill algorithm</a>. Finally, we match the resulting flooded map to the satellite-based flood extent with the original stream gauge measurement. </p><p>This approach abandons some of the realistic constraints of classical <a href=\"https://en.wikipedia.org/wiki/Shallow_water_equations\">physics-based models</a>, but in data scarce regions where existing methods currently struggle, its flexibility allows the model to automatically learn the correct bathymetry and fix various errors to which physics-based models are sensitive. This morphological model improves accuracy by 3%, which can significantly improve forecasts for large areas, while also allowing for much more rapid model development by reducing the need for manual modeling and correction. </p><p><b>Alert targeting</b><br />Many people reside in areas that are not covered by the morphological inundation models, yet access to accurate predictions are still urgently needed. To reach this population and to increase the impact of our flood forecasting models, we designed an end-to-end ML-based approach, using almost exclusively data that is globally publicly available, such as stream gauge measurements, public satellite imagery, and low resolution elevation maps. We train the model to use the data it is receiving to directly infer the inundation map in real time.  </p><p id=\"gdcalert2\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-E7bwF9cbeU0/X1A0QyZ0DmI/AAAAAAAAGhU/YAfYBgxizXk_IQf2JJWAkcF2g3L0hwmKgCLcBGAsYHQ/s1250/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"650\" data-original-width=\"1250\" src=\"https://1.bp.blogspot.com/-E7bwF9cbeU0/X1A0QyZ0DmI/AAAAAAAAGhU/YAfYBgxizXk_IQf2JJWAkcF2g3L0hwmKgCLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A direct ML approach from real-time measurements to inundation.</td></tr></tbody></table><p></p><p>This approach works well “out of the box” when the model only needs to forecast an event that is within the range of events previously observed. Extrapolating to more extreme conditions is much more challenging. Nevertheless, proper use of existing elevation maps and real-time measurements can enable alerts that are more accurate than presently available for those in areas not covered by the more detailed morphological inundation models. Because this model is highly scalable, we were able to launch it across India after only a few months of work, and we hope to roll it out to many more countries soon.  </p><p><b>Improving Water Levels Forecasting</b><br />In an effort to continue improving flood forecasting, we have developed <a href=\"https://ai4earthscience.github.io/iclr-2020-workshop/papers/ai4earth04.pdf\">HydroNets</a> — a specialized deep neural network architecture built specifically for water levels forecasting — which allows us utilize some exciting <a href=\"https://hess.copernicus.org/articles/23/5089/2019/\">recent advances</a> in ML-based hydrology in a real-world operational setting. Two prominent features distinguish it from standard hydrologic models. First, it is able to differentiate between model components that generalize well between sites, such as the modeling of rainfall-runoff processes, and those that are specific to a given site, like the <a href=\"https://en.wikipedia.org/wiki/Rating_curve\">rating curve</a>, which converts a predicted discharge volume into an expected water level. This enables the model to generalize well to different sites, while still fine-tuning its performance to each location. Second, HydroNets takes into account the structure of the river network being modeled, by training a large architecture that is actually a web of smaller neural networks, each representing a different location along the river. This allows neural networks that are modeling upstream sites to pass information encoded in embeddings to models of downstream sites, so that every model can know everything it needs without a drastic increase in parameters.  </p><p>The animation below illustrates the structure and flow of information in HydroNets. The output from the modeling of upstream sub-basins is combined into a single representation of a given basin state. It is then processed by the shared model component, which is informed by all basins in the network, and passed on to the label prediction model, which calculates the water level (and the loss function). The output from this iteration of the network is then passed on to inform downstream models, and so on. </p><p id=\"gdcalert3\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-9lVIrd5gpNM/X1A0e1OjCmI/AAAAAAAAGhY/vQtm0gqG4q46oHnPpIG01FCe6rII24RmwCLcBGAsYHQ/s640/HydroNets%2BLOOP%2B06%2B640.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"640\" data-original-width=\"640\" src=\"https://1.bp.blogspot.com/-9lVIrd5gpNM/X1A0e1OjCmI/AAAAAAAAGhY/vQtm0gqG4q46oHnPpIG01FCe6rII24RmwCLcBGAsYHQ/s0/HydroNets%2BLOOP%2B06%2B640.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An illustration of the HydroNets architecture.</td></tr></tbody></table><p></p><p>We’re incredibly excited about this progress, and are working hard on improving our systems further. </p><p><b>Acknowledgements</b><br /><i>This work is a collaboration between the Google Flood Forecasting Initiative, the Google Geo and <a href=\"https://crisisresponse.google/\">Crisis Response</a> teams, <a href=\"http://Google.org\">Google.org</a> and many other research teams at Google, and is part of our <a href=\"https://ai.google/social-good/\">AI for Social Good</a> efforts. We would also like to thank the Partnerships and Policy teams.</i></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=y4cEEO1aTek:YOMx95JA-Qs:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/y4cEEO1aTek\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Thu, 03 Sep 2020 17:07:00 +0000","feedId":4244,"bgimg":"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s72-c/Inundation-960px.gif","linkMd5":"2f03995e1df19ad2a1e6ed194c273c60","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn52@2020_4/2020/09/04/17-01-30-493_49e2f078abc3561f.webp","destWidth":72,"destHeight":72,"sourceBytes":583062,"destBytes":238234,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s640/Inundation-960px.gif":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn13@2020_5/2020/09/04/17-01-36-594_4d88dce8ef215954.webp","https://1.bp.blogspot.com/-E7bwF9cbeU0/X1A0QyZ0DmI/AAAAAAAAGhU/YAfYBgxizXk_IQf2JJWAkcF2g3L0hwmKgCLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn20@2020_1/2020/09/04/17-01-30-673_23801682b9c50b9b.webp","https://1.bp.blogspot.com/-9lVIrd5gpNM/X1A0e1OjCmI/AAAAAAAAGhY/vQtm0gqG4q46oHnPpIG01FCe6rII24RmwCLcBGAsYHQ/s0/HydroNets%2BLOOP%2B06%2B640.gif":null,"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn82@2020_6/2020/09/04/17-01-27-676_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/y4cEEO1aTek":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn90@2020_3/2020/09/04/17-01-27-694_cd41e0a510b40d92.webp"},"publishedOrCreatedDate":1599238887602},{"createdTime":"2020-09-05 01:01:27","updatedTime":"2020-09-05 01:01:27","title":"KeyPose: Estimating the 3D Pose of Transparent Objects from Stereo","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","description":"<span class=\"byline-author\">Posted by Kurt Konolige, Software Engineer, Robotics at Google</span> <p>Estimating the position and orientation of 3D objects is one of the core problems in computer vision applications that involve object-level perception, such as <a href=\"https://en.wikipedia.org/wiki/Augmented_reality\">augmented reality</a> and <a href=\"https://en.wikipedia.org/wiki/Robotics#Manipulation\">robotic manipulation</a>. In these applications, it is important to know the 3D position of objects in the world, either to directly affect them, or to place simulated objects correctly around them. While there has been much research on this topic using machine learning (ML) techniques, especially <a href=\"https://arxiv.org/abs/1901.04780\">Deep Nets</a>, most have relied on the use of depth sensing devices, such as the <a href=\"https://en.wikipedia.org/wiki/Kinect\">Kinect</a>, which give direct measurements of the distance to an object. For objects that are shiny or transparent, direct depth sensing does not work well. For example, the figure below includes a number of objects (<i>left</i>), two of which are transparent stars. A depth device does not find good depth values for the stars, and gives a very poor reconstruction of the actual 3D points (<i>right</i>).</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s1999/image5.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"577\" data-original-width=\"1999\" src=\"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s640/image5.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left:</b> RGB image of transparent objects.&nbsp; <b>Right:</b> A four-panel image showing the reconstructed depth for the scene on the left.The top row includes depth images and the bottom row presents the 3D point cloud. The left panels were reconstructed using a depth camera and the right panels are output from the ClearGrasp model.&nbsp; Note that although ClearGrasp inpaints the depth of the stars, it mistakes the actual depth of the rightmost one.</td></tr></tbody></table><p>One solution to this problem, such as that proposed by <a href=\"http://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html\">ClearGrasp</a>, is to use a deep neural network to <a href=\"https://en.wikipedia.org/wiki/Inpainting\">inpaint</a> the corrupted depth map of the transparent objects. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries, which it uses to refine the initial depth estimates for all transparent surfaces in the scene (<i>far right</i>&nbsp;in the figure above). This approach is very promising, and allows scenes with transparent objects to be processed by pose-estimation methods that rely on depth.&nbsp; But inpainting can be tricky, especially when trained completely with synthetic images, and can still result in errors in depth.</p><p>In “<a href=\"https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_KeyPose_Multi-View_3D_Labeling_and_Keypoint_Estimation_for_Transparent_Objects_CVPR_2020_paper.html\">KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects</a>”, presented at <a href=\"http://cvpr2020.thecvf.com/\">CVPR 2020</a> in collaboration with the <a href=\"https://ai.stanford.edu/\">Stanford AI Lab</a>, we describe an ML system that estimates the depth of transparent objects by directly predicting 3D keypoints. To train the system we gather a large real-world dataset of images of transparent objects in a semi-automated way, and efficiently label their pose using 3D keypoints selected by hand. We then train deep models (called KeyPose) to estimate the 3D keypoints end-to-end from monocular or stereo images, without explicitly computing depth. The models work on objects both seen and unseen during training, for both individual objects and categories of objects. While KeyPose can work with monocular images, the extra information available from stereo images allows it to improve its results by a factor of two over monocular image input, with typical errors from 5 mm to 10 mm, depending on the objects. It substantially improves over state-of-the-art in pose estimation for these objects, even when competing methods are provided with ground truth depth. We are releasing the <a href=\"https://sites.google.com/corp/view/transparent-objects\">dataset of keypoint-labeled transparent objects</a> for use by the research community.</p><p><b>Real-World Transparent Object Dataset with 3D Keypoint Labels</b><br />To facilitate gathering large quantities of real-world images, we set up a robotic data-gathering system in which a robot arm moves through a trajectory while taking video with two devices, a stereo camera and the <a href=\"https://en.wikipedia.org/wiki/Azure_Kinect\">Kinect Azure</a> depth camera.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Sqc6oH8NKxY/X0_lofTN-8I/AAAAAAAAGfg/Xb_u6u4O84Ys_aJFWPaow7ekRSstoBMqwCLcBGAsYHQ/s386/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"217\" data-original-width=\"386\" src=\"https://1.bp.blogspot.com/-Sqc6oH8NKxY/X0_lofTN-8I/AAAAAAAAGfg/Xb_u6u4O84Ys_aJFWPaow7ekRSstoBMqwCLcBGAsYHQ/s0/image3.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Automated image sequence capture using a robot arm with a stereo camera and an <a href=\"https://en.wikipedia.org/wiki/Azure_Kinect\">Azure Kinect</a> device.</td></tr></tbody></table><p>The <a href=\"https://april.eecs.umich.edu/software/apriltag\">AprilTags</a> on the target enable accurate tracing of the pose of the cameras. By hand-labelling only a few images in each video with 2D keypoints, we can extract 3D keypoints for all frames of the video using multi-view geometry, thus increasing the labelling efficiency by a factor of 100.</p><p>We captured imagery for 15 different transparent objects in five categories, using 10 different background textures and four different poses for each object, yielding a total of 600 video sequences comprising 48k stereo and depth images. We also captured the same images with an opaque version of the object, to provide accurate ground truth depth images. All the images are labelled with 3D keypoints. We are releasing this <a href=\"https://sites.google.com/corp/view/transparent-objects\">dataset of real-world images</a> publicly, complementing the synthetic ClearGrasp dataset with which it shares similar objects.</p><p><b>KeyPose Algorithm Using Early Fusion Stereo</b><br />The idea of using stereo images directly for keypoint estimation was developed independently for this project; it has also <a href=\"https://bmvc2019.org/wp-content/uploads/papers/0219-paper.pdf\">appeared recently</a> in the context of hand-tracking. The diagram below shows the basic idea: the two images from a stereo camera are cropped around the object and fed to the KeyPose network, which predicts a sparse set of 3D keypoints that represent the 3D pose of the object. The network is trained using supervision from the labelled 3D keypoints.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-v52xY2GGnPo/X0_lwU_qNwI/AAAAAAAAGfk/ek5WT19WSDYOosGr0ibdySi8kMglGiqrACLcBGAsYHQ/s651/image4.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"276\" data-original-width=\"651\" src=\"https://1.bp.blogspot.com/-v52xY2GGnPo/X0_lwU_qNwI/AAAAAAAAGfk/ek5WT19WSDYOosGr0ibdySi8kMglGiqrACLcBGAsYHQ/s640/image4.png\" width=\"640\" /></a></div><p>One of the key aspects of stereo KeyPose is the use of early fusion to intermix the stereo images, and allow the network to implicitly compute disparity, in contrast to late fusion, in which keypoints are predicted for each image separately, and then combined. As shown in the diagram below, the output of KeyPose is a 2D keypoint heatmap in the image plane along with a disparity (i.e., inverse depth) heatmap for each keypoint. The combination of these two heatmaps yields the 3D coordinate of the keypoint, for each keypoint.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Ml5cWhhKCSs/X0_mU6X143I/AAAAAAAAGfw/FTEkz3R40D4UUtSBk24UEjRS77YBTiPWgCLcBGAsYHQ/s1586/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"472\" data-original-width=\"1586\" src=\"https://1.bp.blogspot.com/-Ml5cWhhKCSs/X0_mU6X143I/AAAAAAAAGfw/FTEkz3R40D4UUtSBk24UEjRS77YBTiPWgCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><span style=\"text-align: left;\">Keypose system diagram. Stereo images are passed to a CNN model to produce a probability heatmap for each keypoint.&nbsp; This heatmap yields 2D image coordinates U,V for the keypoint.&nbsp; The CNN model also produces a disparity (inverse depth) heatmap for each keypoint, which when combined with the U,V coordinates, gives a 3D position (X,Y,Z).</span></td></tr></tbody></table><p>When compared to late fusion or to monocular input, early fusion stereo typically is twice as accurate.</p><p><b>Results</b><br />The images below show qualitative results of KeyPose on individual objects. On the left is one of the original stereo images; in the middle are the predicted 3D keypoints projected onto the image. On the right, we visualize points from a 3D model of the bottle, placed at the pose determined by the predicted 3D keypoints. The network is efficient and accurate, predicting keypoints with an MAE of 5.2 mm for the bottle and 10.1 mm for the mug using just 5 ms on a standard GPU.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-pBSMt14zJ3k/X0_mob1XYAI/AAAAAAAAGf4/lfdQr2Ated4cms3LDNSxIVXy7cDJBWbhQCLcBGAsYHQ/s720/image6.gif\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"240\" data-original-width=\"720\" src=\"https://1.bp.blogspot.com/-pBSMt14zJ3k/X0_mob1XYAI/AAAAAAAAGf4/lfdQr2Ated4cms3LDNSxIVXy7cDJBWbhQCLcBGAsYHQ/s640/image6.gif\" width=\"640\" /></a></td></tr><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-DNHBONhX-Cc/X0_m8sIqWjI/AAAAAAAAGgA/8_uWFRwu3UYfA-sWyl-I36m2yIDnnK7WwCLcBGAsYHQ/s720/image2.gif\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"240\" data-original-width=\"720\" src=\"https://1.bp.blogspot.com/-DNHBONhX-Cc/X0_m8sIqWjI/AAAAAAAAGgA/8_uWFRwu3UYfA-sWyl-I36m2yIDnnK7WwCLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></td></tr></tbody></table><p>The following table shows results for KeyPose on category-level estimation. The test set used a background texture not seen by the training set. Note that the MAE varies from 5.8 mm to 9.9 mm, showing the accuracy of the method.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-sGr51cqYmCQ/X0_wfWbtq7I/AAAAAAAAGgM/N3fjM0dy_eQCh81NYRJ_xw4Z6kCeNkfqQCLcBGAsYHQ/s1050/KeyPoseComparison.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"418\" data-original-width=\"1050\" height=\"204\" src=\"https://1.bp.blogspot.com/-sGr51cqYmCQ/X0_wfWbtq7I/AAAAAAAAGgM/N3fjM0dy_eQCh81NYRJ_xw4Z6kCeNkfqQCLcBGAsYHQ/w512-h204/KeyPoseComparison.png\" width=\"512\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Quantitative comparison of KeyPose with the state-of-the-art <a href=\"https://arxiv.org/abs/1901.04780\">DenseFusion</a> system, on category-level data. We provide DenseFusion with two versions of depth, one from the transparent objects, and one from opaque objects. <b>&lt;2cm</b> is the percent of estimates with errors less than 2 cm. <b>MAE</b> is the mean absolute error of the keypoints, in mm.</td></tr></tbody></table><p>For a complete accounting of quantitative results, as well as, ablation studies, please see the <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_KeyPose_Multi-View_3D_Labeling_and_Keypoint_Estimation_for_Transparent_Objects_CVPR_2020_paper.html\">paper and supplementary materials</a> and the <a href=\"https://sites.google.com/corp/view/keypose/\">KeyPose website</a>.</p><p><b>Conclusion</b><br />This work shows that it is possible to accurately estimate the 3D pose of transparent objects from RGB images without reliance on depth images. It validates the use of stereo images as input to an early fusion deep net, where the network is trained to extract sparse 3D keypoints directly from the stereo pair. We hope the availability of an extensive, labelled dataset of transparent objects will help to advance the field. Finally, while we used semi-automatic methods to efficiently label the dataset, we hope to employ <a href=\"https://arxiv.org/abs/1807.03146\">self-supervision methods</a> in future work to do away with manual labelling.</p><p><b>Acknowledgements</b><br /><i>I want to thank my co-authors, Xingyu Liu of Stanford University, and Rico Jonschkowski and Anelia Angelova; as well the many who helped us through discussions during the project and paper writing, including Andy Zheng, Shuran Song, Vincent Vanhoucke, Pete Florence, and Jonathan Tompson.</i></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=K6pJMEaA3so:CsKPQeKS-vw:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/K6pJMEaA3so\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 02 Sep 2020 20:48:00 +0000","feedId":4244,"bgimg":"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s72-c/image5.png","linkMd5":"048e41449f66b74026b7d9acc4f4e314","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn49@2020_6/2020/09/04/17-01-29-767_b64e7b6d16d90bc0.webp","destWidth":72,"destHeight":72,"sourceBytes":7602,"destBytes":1440,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s640/image5.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn85@2020_2/2020/09/04/17-01-27-694_619a8b96a2044d50.webp","https://1.bp.blogspot.com/-Sqc6oH8NKxY/X0_lofTN-8I/AAAAAAAAGfg/Xb_u6u4O84Ys_aJFWPaow7ekRSstoBMqwCLcBGAsYHQ/s0/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn25@2020_5/2020/09/04/17-01-38-069_910c3773cf62205f.webp","https://1.bp.blogspot.com/-v52xY2GGnPo/X0_lwU_qNwI/AAAAAAAAGfk/ek5WT19WSDYOosGr0ibdySi8kMglGiqrACLcBGAsYHQ/s640/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn98@2020_3/2020/09/04/17-01-27-804_5d196a639c74e922.webp","https://1.bp.blogspot.com/-Ml5cWhhKCSs/X0_mU6X143I/AAAAAAAAGfw/FTEkz3R40D4UUtSBk24UEjRS77YBTiPWgCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn9@2020_5/2020/09/04/17-01-27-679_8bc37c8638038656.webp","https://1.bp.blogspot.com/-pBSMt14zJ3k/X0_mob1XYAI/AAAAAAAAGf4/lfdQr2Ated4cms3LDNSxIVXy7cDJBWbhQCLcBGAsYHQ/s640/image6.gif":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn1@2020_3/2020/09/04/17-01-29-406_5ff8e4c4dd08d108.webp","https://1.bp.blogspot.com/-DNHBONhX-Cc/X0_m8sIqWjI/AAAAAAAAGgA/8_uWFRwu3UYfA-sWyl-I36m2yIDnnK7WwCLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn6@2020_1/2020/09/04/17-01-30-708_9ffad500239aaabd.webp","https://1.bp.blogspot.com/-sGr51cqYmCQ/X0_wfWbtq7I/AAAAAAAAGgM/N3fjM0dy_eQCh81NYRJ_xw4Z6kCeNkfqQCLcBGAsYHQ/w512-h204/KeyPoseComparison.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn17@2020_2/2020/09/04/17-01-27-723_5253a4704a4b4a4c.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn82@2020_6/2020/09/04/17-01-27-676_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/K6pJMEaA3so":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn28@2020_1/2020/09/04/17-01-27-664_f9d2c899db69b0fe.webp"},"publishedOrCreatedDate":1599238887602}],"record":{"createdTime":"2020-09-05 01:01:27","updatedTime":"2020-09-05 01:01:27","feedId":4244,"fetchDate":"Fri, 04 Sep 2020 17:01:27 +0000","fetchMs":302,"handleMs":42,"totalMs":36755,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"13ceeb0da42989d3de6751ce913d735f","hostName":"us-021*","requestId":"f8e8503ef807436bb55f354ef487f5a9_4244","contentType":"text/xml; charset=UTF-8","totalBytes":7211636,"bgimgsTotal":2,"bgimgsGithubTotal":2,"articlesImgsTotal":13,"articlesImgsGithubTotal":12,"successGithubMap":{"myreaderx15":1,"myreaderx27":1,"myreaderx6":1,"myreaderx10":1,"myreaderx21":1,"myreaderx32":1,"myreaderx22":1,"myreaderx3":1,"myreaderx11":1,"myreaderx12":1,"myreaderx2":1,"myreaderx1":1,"myreaderx30":1,"myreaderx31":1,"myreaderx5oss":1,"myreaderx19":1},"failGithubMap":{"myreaderx14":1}},"feed":{"createdTime":"2020-08-25 04:29:40","updatedTime":"2020-09-01 10:46:06","id":4244,"name":"Google AI Blog","url":"http://googleresearch.blogspot.com/feeds/posts/default","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn78@2020_3/2020/09/01/02-46-06-599_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2020-09-05 01:02:03","updatedTime":"2020-09-05 01:02:03","id":null,"feedId":4244,"linkMd5":"2f03995e1df19ad2a1e6ed194c273c60"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":6971962,"tmpBgImgCdnBytes":239674,"extra4":{"start":1599238887241,"total":0,"statList":[{"spend":320,"msg":"获取xml内容"},{"spend":42,"msg":"解释文章"},{"spend":2059,"msg":"上传封面图到cdn"},{"spend":1925,"msg":"修正封面图上传失败重新上传"},{"spend":36390,"msg":"正文链接上传到cdn"}]},"extra5":13,"extra6":13,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-9lVIrd5gpNM/X1A0e1OjCmI/AAAAAAAAGhY/vQtm0gqG4q46oHnPpIG01FCe6rII24RmwCLcBGAsYHQ/s0/HydroNets%2BLOOP%2B06%2B640.gif","sourceStatusCode":200,"destWidth":640,"destHeight":640,"sourceBytes":3512733,"destBytes":1733726,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":20866,"convertSpendMs":20552,"createdTime":"2020-09-05 01:01:27","host":"us-001*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn94/contents/2020/09/04/17-01-48-367_713ec1be3a9b1e2f.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Fri, 04 Sep 2020 17:01:48 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A1EC:5B0D:577FE:E32D3:5F5272FC"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1599241040"],"x-ratelimit-used":["64"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn94/contents/2020/09/04/17-01-48-367_713ec1be3a9b1e2f.webp","historyStatusCode":[],"spendMs":111},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.4 MB","destSize":"1.7 MB","compressRate":"49.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-9lVIrd5gpNM/X1A0e1OjCmI/AAAAAAAAGhY/vQtm0gqG4q46oHnPpIG01FCe6rII24RmwCLcBGAsYHQ/s0/HydroNets%2BLOOP%2B06%2B640.gif","sourceStatusCode":200,"destWidth":640,"destHeight":640,"sourceBytes":3512733,"destBytes":1733726,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":15450,"convertSpendMs":15241,"createdTime":"2020-09-05 01:01:48","host":"us-55*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn94/contents/2020/09/04/17-02-03-890_713ec1be3a9b1e2f.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Fri, 04 Sep 2020 17:02:03 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["C042:5B0D:57A75:E38C6:5F52730B"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1599241040"],"x-ratelimit-used":["64"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn94/contents/2020/09/04/17-02-03-890_713ec1be3a9b1e2f.webp","historyStatusCode":[],"spendMs":83},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.4 MB","destSize":"1.7 MB","compressRate":"49.4%"}],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-56.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-55.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-022.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-030.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-52.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-026.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-001.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-017.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe64.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn82@2020_6/2020/09/04/17-01-27-676_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":819,"convertSpendMs":5,"createdTime":"2020-09-05 01:01:27","host":"us-038*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60,048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/y4cEEO1aTek","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn90@2020_3/2020/09/04/17-01-27-694_cd41e0a510b40d92.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":850,"convertSpendMs":15,"createdTime":"2020-09-05 01:01:27","host":"us-013*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/K6pJMEaA3so","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn28@2020_1/2020/09/04/17-01-27-664_f9d2c899db69b0fe.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":876,"convertSpendMs":4,"createdTime":"2020-09-05 01:01:27","host":"us-022*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s640/image5.png","sourceStatusCode":200,"destWidth":640,"destHeight":185,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn85@2020_2/2020/09/04/17-01-27-694_619a8b96a2044d50.webp","sourceBytes":144135,"destBytes":13958,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":898,"convertSpendMs":11,"createdTime":"2020-09-05 01:01:27","host":"us-026*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"140.8 KB","destSize":"13.6 KB","compressRate":"9.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-sGr51cqYmCQ/X0_wfWbtq7I/AAAAAAAAGgM/N3fjM0dy_eQCh81NYRJ_xw4Z6kCeNkfqQCLcBGAsYHQ/w512-h204/KeyPoseComparison.png","sourceStatusCode":200,"destWidth":512,"destHeight":204,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn17@2020_2/2020/09/04/17-01-27-723_5253a4704a4b4a4c.webp","sourceBytes":36932,"destBytes":15090,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":941,"convertSpendMs":30,"createdTime":"2020-09-05 01:01:27","host":"us-55*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"36.1 KB","destSize":"14.7 KB","compressRate":"40.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Ml5cWhhKCSs/X0_mU6X143I/AAAAAAAAGfw/FTEkz3R40D4UUtSBk24UEjRS77YBTiPWgCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":190,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn9@2020_5/2020/09/04/17-01-27-679_8bc37c8638038656.webp","sourceBytes":75954,"destBytes":14028,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":966,"convertSpendMs":15,"createdTime":"2020-09-05 01:01:27","host":"us-017*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"74.2 KB","destSize":"13.7 KB","compressRate":"18.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s72-c/image5.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn74@2020_4/2020/09/04/17-01-27-781_b64e7b6d16d90bc0.webp","sourceBytes":7602,"destBytes":1440,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":912,"convertSpendMs":3,"createdTime":"2020-09-05 01:01:27","host":"europe-22*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.4 KB","destSize":"1.4 KB","compressRate":"18.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-v52xY2GGnPo/X0_lwU_qNwI/AAAAAAAAGfk/ek5WT19WSDYOosGr0ibdySi8kMglGiqrACLcBGAsYHQ/s640/image4.png","sourceStatusCode":200,"destWidth":640,"destHeight":271,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn98@2020_3/2020/09/04/17-01-27-804_5d196a639c74e922.webp","sourceBytes":112167,"destBytes":13254,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":1055,"convertSpendMs":24,"createdTime":"2020-09-05 01:01:27","host":"europe64*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"109.5 KB","destSize":"12.9 KB","compressRate":"11.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s72-c/Inundation-960px.gif","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn78@2020_1/2020/09/04/17-01-28-050_49e2f078abc3561f.webp","sourceBytes":583062,"destBytes":238234,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":1907,"convertSpendMs":255,"createdTime":"2020-09-05 01:01:27","host":"europe-60*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"569.4 KB","destSize":"232.7 KB","compressRate":"40.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-7XquCYfjOQU/X0_ihgPuF_I/AAAAAAAAGfU/VyTajeIDrpQaBF-ZLtB8QCKuA9E0w1GmACLcBGAsYHQ/s72-c/image5.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn49@2020_6/2020/09/04/17-01-29-767_b64e7b6d16d90bc0.webp","sourceBytes":7602,"destBytes":1440,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":842,"convertSpendMs":16,"createdTime":"2020-09-05 01:01:29","host":"us-012*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.4 KB","destSize":"1.4 KB","compressRate":"18.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-pBSMt14zJ3k/X0_mob1XYAI/AAAAAAAAGf4/lfdQr2Ated4cms3LDNSxIVXy7cDJBWbhQCLcBGAsYHQ/s640/image6.gif","sourceStatusCode":200,"destWidth":640,"destHeight":213,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn1@2020_3/2020/09/04/17-01-29-406_5ff8e4c4dd08d108.webp","sourceBytes":3663670,"destBytes":489032,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":3024,"convertSpendMs":1664,"createdTime":"2020-09-05 01:01:27","host":"us-52*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.5 MB","destSize":"477.6 KB","compressRate":"13.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s72-c/Inundation-960px.gif","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn52@2020_4/2020/09/04/17-01-30-493_49e2f078abc3561f.webp","sourceBytes":583062,"destBytes":238234,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":1906,"convertSpendMs":741,"createdTime":"2020-09-05 01:01:29","host":"us-013*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"569.4 KB","destSize":"232.7 KB","compressRate":"40.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-DNHBONhX-Cc/X0_m8sIqWjI/AAAAAAAAGgA/8_uWFRwu3UYfA-sWyl-I36m2yIDnnK7WwCLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":640,"destHeight":213,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn6@2020_1/2020/09/04/17-01-30-708_9ffad500239aaabd.webp","sourceBytes":15784774,"destBytes":599658,"targetWebpQuality":4,"feedId":4244,"totalSpendMs":4589,"convertSpendMs":2572,"createdTime":"2020-09-05 01:01:27","host":"us-030*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"15.1 MB","destSize":"585.6 KB","compressRate":"3.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-E7bwF9cbeU0/X1A0QyZ0DmI/AAAAAAAAGhU/YAfYBgxizXk_IQf2JJWAkcF2g3L0hwmKgCLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":640,"destHeight":333,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn20@2020_1/2020/09/04/17-01-30-673_23801682b9c50b9b.webp","sourceBytes":5269619,"destBytes":3092784,"targetWebpQuality":67,"feedId":4244,"totalSpendMs":5448,"convertSpendMs":2830,"createdTime":"2020-09-05 01:01:27","host":"europe-56*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5 MB","destSize":"2.9 MB","compressRate":"58.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-L6moTCeZvmk/X1Az4lqzYdI/AAAAAAAAGhM/Jme7x-ErJwAHlBzkqop-MVSWUJzQ_TXYQCLcBGAsYHQ/s640/Inundation-960px.gif","sourceStatusCode":200,"destWidth":640,"destHeight":271,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn13@2020_5/2020/09/04/17-01-36-594_4d88dce8ef215954.webp","sourceBytes":16508460,"destBytes":725698,"targetWebpQuality":4,"feedId":4244,"totalSpendMs":10268,"convertSpendMs":3021,"createdTime":"2020-09-05 01:01:27","host":"us-005*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/y4cEEO1aTek/the-technology-behind-our-recent.html","linkMd5ListStr":"2f03995e1df19ad2a1e6ed194c273c60","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"15.7 MB","destSize":"708.7 KB","compressRate":"4.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Sqc6oH8NKxY/X0_lofTN-8I/AAAAAAAAGfg/Xb_u6u4O84Ys_aJFWPaow7ekRSstoBMqwCLcBGAsYHQ/s0/image3.gif","sourceStatusCode":200,"destWidth":386,"destHeight":217,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn25@2020_5/2020/09/04/17-01-38-069_910c3773cf62205f.webp","sourceBytes":32462207,"destBytes":2008006,"targetWebpQuality":4,"feedId":4244,"totalSpendMs":12092,"convertSpendMs":8674,"createdTime":"2020-09-05 01:01:27","host":"us-020*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/K6pJMEaA3so/keypose-estimating-3d-pose-of.html","linkMd5ListStr":"048e41449f66b74026b7d9acc4f4e314","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31 MB","destSize":"1.9 MB","compressRate":"6.2%"}],"successGithubMap":{"myreaderx15":1,"myreaderx27":1,"myreaderx6":1,"myreaderx10":1,"myreaderx21":1,"myreaderx32":1,"myreaderx22":1,"myreaderx3":1,"myreaderx11":1,"myreaderx12":1,"myreaderx2":1,"myreaderx1":1,"myreaderx30":1,"myreaderx31":1,"myreaderx5oss":1,"myreaderx19":1},"failGithubMap":{"myreaderx14":1}}