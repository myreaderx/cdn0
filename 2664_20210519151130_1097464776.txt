{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-05-19 23:10:59","updatedTime":"2021-05-19 23:10:59","title":"A Gentle Introduction to the BFGS Optimization Algorithm","link":"https://machinelearningmastery.com/?p=12194","description":"<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-12194 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=A+Gentle+Introduction+to+the+BFGS+Optimization+Algorithm&url=https://machinelearningmastery.com/bfgs-optimization-in-python/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/bfgs-optimization-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/cws/share?url=https://machinelearningmastery.com/bfgs-optimization-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The Broyden, Fletcher, Goldfarb, and Shanno, or <strong>BFGS Algorithm</strong>, is a local search optimization algorithm.</p>\n<p>It is a type of second-order optimization algorithm, meaning that it makes use of the second-order derivative of an objective function and belongs to a class of algorithms referred to as Quasi-Newton methods that approximate the second derivative (called the Hessian) for optimization problems where the second derivative cannot be calculated.</p>\n<p>The BFGS algorithm is perhaps one of the most widely used second-order algorithms for numerical optimization and is commonly used to fit machine learning algorithms such as the logistic regression algorithm.</p>\n<p>In this tutorial, you will discover the BFGS second-order optimization algorithm.</p>\n<p>After completing this tutorial, you will know:</p>\n<ul>\n<li>Second-order optimization algorithms are algorithms that make use of the second-order derivative, called the Hessian matrix for multivariate objective functions.</li>\n<li>The BFGS algorithm is perhaps the most popular second-order algorithm for numerical optimization and belongs to a group called Quasi-Newton methods.</li>\n<li>How to minimize objective functions using the BFGS and L-BFGS-B algorithms in Python.</li>\n</ul>\n<p>Let&#8217;s get started.</p>\n<div id=\"attachment_12200\" style=\"width: 810px\" class=\"wp-caption aligncenter\"><img aria-describedby=\"caption-attachment-12200\" loading=\"lazy\" class=\"size-full wp-image-12200\" src=\"https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm.jpg\" alt=\"A Gentle Introduction to the BFGS Optimization Algorithm\" width=\"800\" height=\"534\" srcset=\"https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm.jpg 800w, https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm-300x200.jpg 300w, https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm-768x513.jpg 768w\" sizes=\"(max-width: 800px) 100vw, 800px\" /><p id=\"caption-attachment-12200\" class=\"wp-caption-text\">A Gentle Introduction to the BFGS Optimization Algorithm<br />Photo by <a href=\"https://www.flickr.com/photos/timo_w2s/5902273583/\">Timo Newton-Syms</a>, some rights reserved.</p></div>\n<h2>Tutorial Overview</h2>\n<p>This tutorial is divided into three parts; they are:</p>\n<ol>\n<li>Second-Order Optimization Algorithms</li>\n<li>BFGS Optimization Algorithm</li>\n<li>Worked Example of BFGS</li>\n</ol>\n<h2>Second-Order Optimization Algorithms</h2>\n<p>Optimization involves finding values for input parameters that maximize or minimize an objective function.</p>\n<p>Newton-method optimization algorithms are those algorithms that make use of the second derivative of the objective function.</p>\n<p>You may recall from calculus that the <a href=\"https://en.wikipedia.org/wiki/Derivative\">first derivative</a> of a function is the rate of change or curvature of the function at a specific point. The derivative can be followed downhill (or uphill) by an optimization algorithm toward the minima of the function (the input values that result in the smallest output of the objective function).</p>\n<p>Algorithms that make use of the first derivative are called first-order optimization algorithms. An example of a first-order algorithm is the gradient descent optimization algorithm.</p>\n<ul>\n<li><strong>First-Order Methods</strong>: Optimization algorithms that make use of the first-order derivative to find the optima of an objective function.</li>\n</ul>\n<p>The <a href=\"https://en.wikipedia.org/wiki/Second_derivative\">second-order derivative</a> is the derivative of the derivative, or the rate of change of the rate of change.</p>\n<p>The second derivative can be followed to more efficiently locate the optima of the objective function. This makes sense more generally, as the more information we have about the objective function, the easier it may be to optimize it.</p>\n<p>The second-order derivative allows us to know both which direction to move (like the first-order) but also estimate how far to move in that direction, called the step size.</p>\n<blockquote><p>Second-order information, on the other hand, allows us to make a quadratic approximation of the objective function and approximate the right step size to reach a local minimum &#8230;</p></blockquote>\n<p>&#8212; Page 87, <a href=\"https://amzn.to/3i56bQZ\">Algorithms for Optimization</a>, 2019.</p>\n<p>Algorithms that make use of the second-order derivative are referred to as second-order optimization algorithms.</p>\n<ul>\n<li><strong>Second-Order Methods</strong>: Optimization algorithms that make use of the second-order derivative to find the optima of an objective function.</li>\n</ul>\n<p>An example of a second-order optimization algorithm is Newton&#8217;s method.</p>\n<p>When an objective function has more than one input variable, the input variables together may be thought of as a vector, which may be familiar from linear algebra.</p>\n<blockquote><p>The gradient is the generalization of the derivative to multivariate functions. It captures the local slope of the function, allowing us to predict the effect of taking a small step from a point in any direction.</p></blockquote>\n<p>&#8212; Page 21, <a href=\"https://amzn.to/3i56bQZ\">Algorithms for Optimization</a>, 2019.</p>\n<p>Similarly, the first derivative of multiple input variables may also be a vector, where each element is called a partial derivative. This vector of partial derivatives is referred to as the gradient.</p>\n<ul>\n<li><strong>Gradient</strong>: Vector of partial first derivatives for multiple input variables of an objective function.</li>\n</ul>\n<p>This idea generalizes to the second-order derivatives of the multivariate inputs, which is a matrix containing the second derivatives called the Hessian matrix.</p>\n<ul>\n<li><strong>Hessian</strong>: Matrix of partial second-order derivatives for multiple input variables of an objective function.</li>\n</ul>\n<p>The Hessian matrix is square and symmetric if the second derivatives are all continuous at the point where we are calculating the derivatives. This is often the case when solving real-valued optimization problems and an expectation when using many second-order methods.</p>\n<blockquote><p>The Hessian of a multivariate function is a matrix containing all of the second derivatives with respect to the input. The second derivatives capture information about the local curvature of the function.</p></blockquote>\n<p>&#8212; Page 21, <a href=\"https://amzn.to/3i56bQZ\">Algorithms for Optimization</a>, 2019.</p>\n<p>As such, it is common to describe second-order optimization algorithms making use of or following the Hessian to the optima of the objective function.</p>\n<p>Now that we have a high-level understanding of second-order optimization algorithms, let&#8217;s take a closer look at the BFGS algorithm.</p>\n<h2>BFGS Optimization Algorithm</h2>\n<p><strong>BFGS</strong> is a second-order optimization algorithm.</p>\n<p>It is an acronym, named for the four co-discovers of the algorithm: Broyden, Fletcher, Goldfarb, and Shanno.</p>\n<p>It is a local search algorithm, intended for convex optimization problems with a single optima.</p>\n<p>The BFGS algorithm is perhaps best understood as belonging to a group of algorithms that are an extension to Newton&#8217;s Method optimization algorithm, referred to as Quasi-Newton Methods.</p>\n<p>Newton&#8217;s method is a second-order optimization algorithm that makes use of the Hessian matrix.</p>\n<p>A limitation of Newton&#8217;s method is that it requires the calculation of the inverse of the Hessian matrix. This is a computationally expensive operation and may not be stable depending on the properties of the objective function.</p>\n<p>Quasi-Newton methods are second-order optimization algorithms that approximate the inverse of the Hessian matrix using the gradient, meaning that the Hessian and its inverse do not need to be available or calculated precisely for each step of the algorithm.</p>\n<blockquote><p>Quasi-Newton methods are among the most widely used methods for nonlinear optimization. They are incorporated in many software libraries, and they are effective in solving a wide variety of small to midsize problems, in particular when the Hessian is hard to compute.</p></blockquote>\n<p>&#8212; Page 411, <a href=\"https://amzn.to/39fWKtS\">Linear and Nonlinear Optimization</a>, 2009.</p>\n<p>The main difference between different Quasi-Newton optimization algorithms is the specific way in which the approximation of the inverse Hessian is calculated.</p>\n<p>The BFGS algorithm is one specific way for updating the calculation of the inverse Hessian, instead of recalculating it every iteration. It, or its extensions, may be one of the most popular Quasi-Newton or even second-order optimization algorithms used for numerical optimization.</p>\n<blockquote><p>The most popular quasi-Newton algorithm is the BFGS method, named for its discoverers Broyden, Fletcher, Goldfarb, and Shanno.</p></blockquote>\n<p>&#8212; Page 136, <a href=\"https://amzn.to/3sbjF2t\">Numerical Optimization</a>, 2006.</p>\n<p>A benefit of using the Hessian, when available, is that it can be used to determine both the direction and the step size to move in order to change the input parameters to minimize (or maximize) the objective function.</p>\n<p>Quasi-Newton methods like BFGS approximate the inverse Hessian, which can then be used to determine the direction to move, but we no longer have the step size.</p>\n<p>The BFGS algorithm addresses this by using a line search in the chosen direction to determine how far to move in that direction.</p>\n<p>For the derivation and calculations used by the BFGS algorithm, I recommend the resources in the further reading section at the end of this tutorial.</p>\n<p>The size of the Hessian and its inverse is proportional to the number of input parameters to the objective function. As such, the size of the matrix can become very large for hundreds, thousand, or millions of parameters.</p>\n<blockquote><p>&#8230; the BFGS algorithm must store the inverse Hessian matrix, M, that requires O(n2) memory, making BFGS impractical for most modern deep learning models that typically have millions of parameters.</p></blockquote>\n<p>&#8212; Page 317, <a href=\"https://amzn.to/3oEyDeU\">Deep Learning</a>, 2016.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Limited-memory_BFGS\">Limited Memory BFGS</a> (or L-BFGS) is an extension to the BFGS algorithm that addresses the cost of having a large number of parameters. It does this by not requiring that the entire approximation of the inverse matrix be stored, by assuming a simplification of the inverse Hessian in the previous iteration of the algorithm (used in the approximation).</p>\n<p>Now that we are familiar with the BFGS algorithm from a high-level, let&#8217;s look at how we might make use of it.</p>\n<h2>Worked Example of BFGS</h2>\n<p>In this section, we will look at some examples of using the BFGS optimization algorithm.</p>\n<p>We can implement the BFGS algorithm for optimizing arbitrary functions in Python using the <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\">minimize() SciPy function</a>.</p>\n<p>The function takes a number of arguments, but most importantly, we can specify the name of the objective function as the first argument, the starting point for the search as the second argument, and specify the &#8220;<em>method</em>&#8221; argument as &#8216;<em>BFGS</em>&#8216;. The name of the function used to calculate the derivative of the objective function can be specified via the &#8220;<em>jac</em>&#8221; argument.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# perform the bfgs algorithm search\nresult = minimize(objective, pt, method='BFGS', jac=derivative)</pre><p>Let&#8217;s look at an example.</p>\n<p>First, we can define a simple two-dimensional objective function, a bowl function, e.g. x^2. It is simple the sum of the squared input variables with an optima at f(0, 0) = 0.0.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># objective function\ndef objective(x):\n\treturn x[0]**2.0 + x[1]**2.0</pre><p>Next, let&#8217;s define a function for the derivative of the function, which is [x*2, y*2].</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># derivative of the objective function\ndef derivative(x):\n\treturn [x[0] * 2, x[1] * 2]</pre><p>We will define the bounds of the function as a box with the range -5 and 5 in each dimension.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define range for input\nr_min, r_max = -5.0, 5.0</pre><p>The starting point of the search will be a randomly generated position in the search domain.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# define the starting point as a random sample from the domain\npt = r_min + rand(2) * (r_max - r_min)</pre><p>We can then apply the BFGS algorithm to find the minima of the objective function by specifying the name of the objective function, the initial point, the method we want to use (BFGS), and the name of the derivative function.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# perform the bfgs algorithm search\nresult = minimize(objective, pt, method='BFGS', jac=derivative)</pre><p>We can then review the result reporting a message as to whether the algorithm finished successfully or not and the total number of evaluations of the objective function that were performed.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# summarize the result\nprint('Status : %s' % result['message'])\nprint('Total Evaluations: %d' % result['nfev'])</pre><p>Finally, we can report the input variables that were found and their evaluation against the objective function.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# evaluate solution\nsolution = result['x']\nevaluation = objective(solution)\nprint('Solution: f(%s) = %.5f' % (solution, evaluation))</pre><p>Tying this together, the complete example is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># bfgs algorithm local optimization of a convex function\nfrom scipy.optimize import minimize\nfrom numpy.random import rand\n\n# objective function\ndef objective(x):\n\treturn x[0]**2.0 + x[1]**2.0\n\n# derivative of the objective function\ndef derivative(x):\n\treturn [x[0] * 2, x[1] * 2]\n\n# define range for input\nr_min, r_max = -5.0, 5.0\n# define the starting point as a random sample from the domain\npt = r_min + rand(2) * (r_max - r_min)\n# perform the bfgs algorithm search\nresult = minimize(objective, pt, method='BFGS', jac=derivative)\n# summarize the result\nprint('Status : %s' % result['message'])\nprint('Total Evaluations: %d' % result['nfev'])\n# evaluate solution\nsolution = result['x']\nevaluation = objective(solution)\nprint('Solution: f(%s) = %.5f' % (solution, evaluation))</pre><p>Running the example applies the BFGS algorithm to our objective function and reports the results.</p>\n<p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p>\n\n<p>In this case, we can see that four iterations of the algorithm were performed and a solution very close to the optima f(0.0, 0.0) = 0.0 was discovered, at least to a useful level of precision.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Status: Optimization terminated successfully.\nTotal Evaluations: 4\nSolution: f([0.00000000e+00 1.11022302e-16]) = 0.00000</pre><p>The <em>minimize()</em> function also supports the L-BFGS algorithm that has lower memory requirements than BFGS.</p>\n<p>Specifically, the L-BFGS-B version of the algorithm where the -B suffix indicates a &#8220;<em>boxed</em>&#8221; version of the algorithm, where the bounds of the domain can be specified.</p>\n<p>This can be achieved by specifying the &#8220;<em>method</em>&#8221; argument as &#8220;<em>L-BFGS-B</em>&#8220;.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">...\n# perform the l-bfgs-b algorithm search\nresult = minimize(objective, pt, method='L-BFGS-B', jac=derivative)</pre><p>The complete example with this update is listed below.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\"># l-bfgs-b algorithm local optimization of a convex function\nfrom scipy.optimize import minimize\nfrom numpy.random import rand\n\n# objective function\ndef objective(x):\n\treturn x[0]**2.0 + x[1]**2.0\n\n# derivative of the objective function\ndef derivative(x):\n\treturn [x[0] * 2, x[1] * 2]\n\n# define range for input\nr_min, r_max = -5.0, 5.0\n# define the starting point as a random sample from the domain\npt = r_min + rand(2) * (r_max - r_min)\n# perform the l-bfgs-b algorithm search\nresult = minimize(objective, pt, method='L-BFGS-B', jac=derivative)\n# summarize the result\nprint('Status : %s' % result['message'])\nprint('Total Evaluations: %d' % result['nfev'])\n# evaluate solution\nsolution = result['x']\nevaluation = objective(solution)\nprint('Solution: f(%s) = %.5f' % (solution, evaluation))</pre><p>Running the example application applies the L-BFGS-B algorithm to our objective function and reports the results.</p>\n<p><strong>Note</strong>: Your <a href=\"https://machinelearningmastery.com/different-results-each-time-in-machine-learning/\">results may vary</a> given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p>\n\n<p>Again, we can see that the minima to the function is found in very few evaluations.</p><pre class=\"urvanov-syntax-highlighter-plain-tag\">Status : b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&#60;=_PGTOL'\nTotal Evaluations: 3\nSolution: f([-1.33226763e-15 1.33226763e-15]) = 0.00000</pre><p>It might be a fun exercise to increase the dimensions of the test problem to millions of parameters and compare the memory usage and run time of the two algorithms.</p>\n<h2>Further Reading</h2>\n<p>This section provides more resources on the topic if you are looking to go deeper.</p>\n<h3>Books</h3>\n<ul>\n<li><a href=\"https://amzn.to/3i56bQZ\">Algorithms for Optimization</a>, 2019.</li>\n<li><a href=\"https://amzn.to/3oEyDeU\">Deep Learning</a>, 2016.</li>\n<li><a href=\"https://amzn.to/3sbjF2t\">Numerical Optimization</a>, 2006.</li>\n<li><a href=\"https://amzn.to/39fWKtS\">Linear and Nonlinear Optimization</a>, 2009.</li>\n</ul>\n<h3>APIs</h3>\n<ul>\n<li><a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\">scipy.optimize.minimize API</a>.</li>\n</ul>\n<h3>Articles</h3>\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\">Broyden–Fletcher–Goldfarb–Shanno algorithm, Wikipedia</a>.</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Limited-memory_BFGS\">Limited-memory BFGS, Wikipedia</a>.</li>\n</ul>\n<h2>Summary</h2>\n<p>In this tutorial, you discovered the BFGS second-order optimization algorithm.</p>\n<p>Specifically, you learned:</p>\n<ul>\n<li>Second-order optimization algorithms are algorithms that make use of the second-order derivative, called the Hessian matrix for multivariate objective functions.</li>\n<li>The BFGS algorithm is perhaps the most popular second-order algorithm for numerical optimization and belongs to a group called Quasi-Newton methods.</li>\n<li>How to minimize objective functions using the BFGS and L-BFGS-B algorithms in Python.</li>\n</ul>\n<p><strong>Do you have any questions?</strong><br />\nAsk your questions in the comments below and I will do my best to answer.</p>\n<div class=\"simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-12194 post  simplesocialbuttons-inline-no-animation\">\n<button class=\"ssb_tweet-icon\"  data-href=\"https://twitter.com/share?text=A+Gentle+Introduction+to+the+BFGS+Optimization+Algorithm&url=https://machinelearningmastery.com/bfgs-optimization-in-python/\" rel=\"nofollow\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 72 72\"><path fill=\"none\" d=\"M0 0h72v72H0z\"/><path class=\"icon\" fill=\"#fff\" d=\"M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z\"/></svg></span><i class=\"simplesocialtxt\">Tweet </i></button>\n\t\t<button class=\"ssb_fbshare-icon\" target=\"_blank\" data-href=\"https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/bfgs-optimization-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\">\n\t\t\t\t\t\t<span class=\"icon\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" class=\"_1pbq\" color=\"#ffffff\"><path fill=\"#ffffff\" fill-rule=\"evenodd\" class=\"icon\" d=\"M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z\"></path></svg></span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share </span> </button>\n<button class=\"ssb_linkedin-icon\" data-href=\"https://www.linkedin.com/cws/share?url=https://machinelearningmastery.com/bfgs-optimization-in-python/\" onclick=\"javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;\" >\n\t\t\t\t\t\t<span class=\"icon\"> <svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" x=\"0px\" y=\"0px\" width=\"15px\" height=\"14.1px\" viewBox=\"-301.4 387.5 15 14.1\" enable-background=\"new -301.4 387.5 15 14.1\" xml:space=\"preserve\"> <g id=\"XMLID_398_\"> <path id=\"XMLID_399_\" fill=\"#FFFFFF\" d=\"M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z\"/> <path id=\"XMLID_400_\" fill=\"#FFFFFF\" d=\"M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z\"/> <path id=\"XMLID_401_\" fill=\"#FFFFFF\" d=\"M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z\"/> </g> </svg> </span>\n\t\t\t\t\t\t<span class=\"simplesocialtxt\">Share</span> </button>\n</div>\n<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/bfgs-optimization-in-python/\">A Gentle Introduction to the BFGS Optimization Algorithm</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n","descriptionType":"html","publishedDate":"Tue, 18 May 2021 19:00:13 +0000","feedId":2664,"bgimg":"https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm.jpg","linkMd5":"032c3d421636f8792700a24c12cc86fe","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn21@2020_5/2021/05/19/15-11-29-885_b68fe5b71c9cf497.webp","destWidth":800,"destHeight":534,"sourceBytes":64712,"destBytes":54734,"author":"Jason Brownlee","articleImgCdnMap":{"https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm.jpg":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn21@2020_5/2021/05/19/15-11-29-885_b68fe5b71c9cf497.webp"},"publishedOrCreatedDate":1621437059404}],"record":{"createdTime":"2021-05-19 23:10:59","updatedTime":"2021-05-19 23:10:59","feedId":2664,"fetchDate":"Wed, 19 May 2021 15:10:59 +0000","fetchMs":18,"handleMs":18,"totalMs":31011,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"245d88d7b2d8f26704713c23b090d029","hostName":"us-033*","requestId":"8ddd745b14c04eddbd4f403a8a0785ea_2664","contentType":"text/xml; charset=UTF-8","totalBytes":54734,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":1,"articlesImgsGithubTotal":1,"successGithubMap":{"myreaderx4":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-24 21:31:43","updatedTime":"2020-09-01 10:12:03","id":2664,"name":"Machine Learning Mastery","url":"http://feeds.feedburner.com/MachineLearningMastery","subscriber":null,"website":null,"icon":"https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-32x32.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn89@2020_6/2020/09/01/02-12-04-187_3ddce3e18398fd9b.png","description":"Making developers awesome at machine learning","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":54734,"tmpBodyImgCdnBytes":0,"tmpBgImgCdnBytes":0,"extra4":{"start":1621437059353,"total":0,"statList":[{"spend":34,"msg":"获取xml内容"},{"spend":18,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"正文链接上传到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"}]},"extra5":1,"extra6":1,"extra7ImgCdnFailResultVector":[null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://machinelearningmastery.com/wp-content/uploads/2021/05/A-Gentle-Introduction-to-the-BFGS-Optimization-Algorithm.jpg","sourceStatusCode":200,"destWidth":800,"destHeight":534,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn21@2020_5/2021/05/19/15-11-29-885_b68fe5b71c9cf497.webp","sourceBytes":64712,"destBytes":54734,"targetWebpQuality":75,"feedId":2664,"totalSpendMs":729,"convertSpendMs":17,"createdTime":"2021-05-19 23:11:29","host":"europe67*","referer":"https://machinelearningmastery.com/?p=12194","linkMd5ListStr":"032c3d421636f8792700a24c12cc86fe,032c3d421636f8792700a24c12cc86fe","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"63.2 KB","destSize":"53.5 KB","compressRate":"84.6%"}],"successGithubMap":{"myreaderx4":1},"failGithubMap":{}}