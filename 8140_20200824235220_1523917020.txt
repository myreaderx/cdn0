{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"OmniTact: A Multi-Directional High-Resolution Touch Sensor","link":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","description":"<!--\nTODO TODO TODO personal reminder for Daniel Seita :-)\nBe careful that these three lines are at the top,\nand that the title and image change for each blog post!\n-->\n<meta name=\"twitter:title\" content=\"OmniTact: A Multi-Directional High-Resolution Touch Sensor\" />\n\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/omnitact/Fig1.png\" />\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig1.svg\" width=\"500\" />\n<br />\n<i>\nHuman thumb next to our OmniTact sensor, and a US penny for scale.\n</i>\n</p>\n\n<p><a href=\"https://bair.berkeley.edu/blog/2019/03/21/tactile/\">Touch has been shown</a> to be important for dexterous <a href=\"https://www.researchgate.net/profile/Ravinder_Dahiya2/publication/221787495_Tactile_Sensing_for_Robotic_Applications/links/0fcfd50ff8e9185522000000.pdf\">manipulation</a> in\n<a href=\"http://www.biorobotics.harvard.edu/pubs/1994/tac-manip.pdf\">robotics</a>. Recently, the GelSight sensor has caught significant interest\nfor <em>learning-based robotics</em> due to its low cost and rich signal. For example,\nGelSight sensors have been used for learning inserting USB cables (<a href=\"https://dspace.mit.edu/bitstream/handle/1721.1/88136/GelSight_IROS%202014_final.pdf?sequence=1&amp;isAllowed=y\">Li et al,\n2014</a>), rolling a die (<a href=\"https://arxiv.org/abs/1903.04128\">Tian et al. 2019</a>) or grasping objects (<a href=\"https://arxiv.org/abs/1710.05512\">Calandra\net al.  2017</a>).</p>\n\n<p>The reason why learning-based methods work well with GelSight sensors is that\nthey output high-resolution tactile images from which a variety of features\nsuch as <a href=\"https://www.gelsight.com/applications/\">object geometry</a>, surface texture, normal and shear forces can be\n<a href=\"http://people.csail.mit.edu/yuan_wz/force-torque.htm\">estimated</a> that often prove critical to robotic control. The tactile images\ncan be fed into standard CNN-based computer vision pipelines allowing the use\nof a variety of different learning-based techniques: In <a href=\"https://arxiv.org/abs/1710.05512\">Calandra et al.\n2017</a> a grasp-success classifier is trained on GelSight data collected in\nself-supervised manner, in <a href=\"https://arxiv.org/abs/1903.04128\">Tian et al. 2019</a> <a href=\"https://bair.berkeley.edu/blog/2018/11/30/visual-rl/\">Visual Foresight</a>, a\nvideo-prediction-based control algorithm is used to make a robot roll a die\npurely based on tactile images, and in <a href=\"https://ieeexplore.ieee.org/abstract/document/9018215\">Lambeta et al. 2020</a> a model-based\nRL algorithm is applied to in-hand manipulation using GelSight images.</p>\n\n<p>Unfortunately applying GelSight sensors in practical real-world scenarios is\nstill challenging due to its large size and the fact that it is only sensitive\non one side. Here we introduce a new, more compact tactile sensor design based\non GelSight that allows for omnidirectional sensing, i.e. making the sensor\n<em>sensitive on all sides like a human finger</em>, and show how this opens up new\npossibilities for sensorimotor learning. We demonstrate this by teaching a\nrobot to pick up electrical plugs and insert them <em>purely based on tactile\nfeedback</em>.</p>\n\n<!--more-->\n\n<h1 id=\"gelsight-sensors\">GelSight Sensors</h1>\n\n<p>A standard GelSight sensor, shown in the figure below on the left, uses an\noff-the-shelf webcam to capture high-resolution images of deformations on the\nsilicone gel skin. The inside surface of the gel skin is illuminated with\ncolored LEDs, providing sufficient lighting for the tactile image.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig2.svg\" width=\"500\" />\n<br />\n<i>\nComparison of GelSight-style sensor (left side) to our OmniTact sensor (right\nside).\n</i>\n</p>\n\n<p>Existing GelSight designs are either flat, have small sensitive fields or only\nprovide low-resolution signals. For example, prior versions of the GelSight\nsensor, provide high resolution (400x400 pixel) images but are large and flat,\nproviding sensitivity on only one side, while the commercial <a href=\"https://onrobot.com/en/products/hex-6-axis-force-torque-sensor\">OptoForce</a>\nsensor (recently discontinued by OnRobot) is curved, but only provides force\nreadings as a single 3-dimensional force vector.</p>\n\n<h1 id=\"the-omnitact-sensor\">The OmniTact Sensor</h1>\n\n<p>Our OmniTact sensor design aims to address these limitations. It provides both\nmulti-directional and high-resolution sensing on its curved surface in a\ncompact form factor.  Similar to GelSight, OmniTact uses cameras embedded into\na silicone gel skin to capture deformation of the skin, providing a rich signal\nfrom which a wide range of features such as shear and normal forces, object\npose, geometry and material properties can be inferred. OmniTact uses multiple\ncameras giving it both high-resolution and multi-directional capabilities. The\nsensor itself can be used as a “finger” and can be integrated into a gripper or\nrobotic hand. It is more compact than previous GelSight sensors, which is\naccomplished by utilizing micro-cameras typically used in endoscopes, and by\ncasting the silicone gel directly onto the cameras. Tactile images from\nOmniTact are shown in the figures below.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig3.svg\" width=\"\" />\n<br />\n<i>\nTactile readings from OmniTact with various objects. From left to right: M3\nScrew Head, M3 Screw Threads, Combination Lock with numbers 4 3 9, Printed\nCircuit Board (PCB), Wireless Mouse USB. All images are taken from the\nupward-facing camera.\n</i>\n</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig4.svg\" width=\"\" />\n<br />\n<i>\nTactile readings from the OmniTact being rolled over a gear rack. The\nmulti-directional capabilities of OmniTact keep the gear rack in view as the\nsensor is rotated.\n</i>\n</p>\n\n<h1 id=\"design-highlights\">Design Highlights</h1>\n\n<p>One of our primary goals throughout the design process was to make OmniTact as\ncompact as possible. To accomplish this goal, we used micro-cameras with large\nviewing angles and a small focus distance. Specifically we picked cameras that\nare commonly used in medical endoscopes measuring just (1.35 x 1.35 x 5 mm) in\nsize with a focus distance of 5 mm. These cameras were arranged in a 3D printed\ncamera mount as shown in the figure below which allowed us to minimize blind\nspots on the surface of the sensor and reduce the diameter (D) of the sensor to\n30 mm.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig5.svg\" width=\"400\" />\n<br />\n<i>\nThis image shows the fields of view and arrangement of the 5 micro-cameras\ninside the sensor. Using this arrangement, most of the fingertip can be made\nsensitive effectively. In the vertical plane, shown in A, we obtain\n$\\alpha=270$ degrees of sensitivity. In the horizontal plane, shown in B, we\nobtain 360 degrees of sensitivity, except for small blind spots between the\nfields of view.\n</i>\n</p>\n\n<h1 id=\"electrical-connector-insertion-task\">Electrical Connector Insertion Task</h1>\n\n<p>We show that OmniTact’s multi-directional tactile sensing capabilities can be\nleveraged to solve a challenging robotic control problem: Inserting an\nelectrical connector blindly into a wall outlet purely based on information\nfrom the multi-directional touch sensor (shown in the figure below). This task\nis challenging since it requires localizing the electrical connector relative\nto the gripper and localizing the gripper relative to the wall outlet.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig6.svg\" width=\"\" />\n<br />\n</p>\n\n<p>To learn the insertion task, we used a simple imitation learning algorithm that\nestimates the end-effector displacement required for inserting the plug into\nthe outlet based on the tactile images from the OmniTact sensor. Our model was\ntrained with just 100 demonstrations of insertion by controlling the robot\nusing keyboard control. Successful insertions obtained by running the trained\npolicy are shown in the gifs below.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/gif1.gif\" height=\"150\" />\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/gif2.gif\" height=\"150\" />\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/gif3.gif\" height=\"150\" />\n<br />\n</p>\n\n<p>As shown in the table below, using the multi-directional capabilities (both the\ntop and side camera) of our sensor allowed for the highest success rate (80%)\nin comparison to using just one camera from the sensor, indicating that\nmulti-directional touch sensing is indeed crucial for solving this task. We\nadditionally compared performance with another multi-directional tactile\nsensor, the OptoForce sensor, which only had a success rate of 17%.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/omnitact/Fig7.png\" width=\"\" />\n<br />\n</p>\n\n<h1 id=\"whats-next\">What’s Next?</h1>\n\n<p>We believe that compact, high resolution and multi-directional touch sensing\nhas the potential to transform the capabilities of current robotic manipulation\nsystems. We suspect that multi-directional tactile sensing could be an\nessential element in general-purpose robotic manipulation in addition to\napplications such as robotic teleoperation in surgery, as well as in sea and\nspace missions. In the future, we plan to make OmniTact cheaper and more\ncompact, allowing it to be used in a wider range of tasks. Our team\nadditionally plans to conduct more robotic manipulation research that will\ninform future generations of tactile sensors.</p>\n\n<p>This blog post is based on the following paper which will be presented at the\nInternational Conference on Robotics and Automation 2020:</p>\n\n<ul>\n  <li><strong>OmniTact: A Multi-Directional High-Resolution Touch Sensor</strong><br />\nAkhil Padmanabha, Frederik Ebert, Stephen Tian, Roberto Calandra, Chelsea Finn, Sergey Levine<br />\nPaper Link: <a href=\"https://arxiv.org/abs/2003.06965\">https://arxiv.org/abs/2003.06965</a><br />\nResearch Website: <a href=\"https://sites.google.com/berkeley.edu/omnitact/home\">https://sites.google.com/berkeley.edu/omnitact/home</a></li>\n</ul>\n\n<p>We would like to thank Professor Sergey Levine, Professor Chelsea Finn, and\nStephen Tian for their valuable feedback when preparing this blog post.</p>\n\n","descriptionType":"text/html","publishedDate":"Thu, 14 May 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"cfd0bb55eedfe1f8759b2fb30581b5c9","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/omnitact/Fig1.svg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn45@2020_4/2020/08/24/23-52-03-397_d176521c493bd989.svg","https://bair.berkeley.edu/static/blog/omnitact/Fig2.svg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn74@2020_3/2020/08/24/23-52-04-520_8c14c6ecf93e478f.svg","https://bair.berkeley.edu/static/blog/omnitact/Fig3.svg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn1@2020_6/2020/08/24/23-52-03-850_899ae9224682b6e9.svg","https://bair.berkeley.edu/static/blog/omnitact/Fig4.svg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn98@2020_1/2020/08/24/23-52-07-015_f7370f60b4501be0.svg","https://bair.berkeley.edu/static/blog/omnitact/Fig5.svg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn33@2020_6/2020/08/24/23-52-05-368_8ba573b4563a53b7.svg","https://bair.berkeley.edu/static/blog/omnitact/Fig6.svg":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn17@2020_2/2020/08/24/23-52-05-438_2679ac501cbd37f9.svg","https://bair.berkeley.edu/static/blog/omnitact/gif1.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn1@2020_2/2020/08/24/23-52-12-547_d9c0dd27ad384252.webp","https://bair.berkeley.edu/static/blog/omnitact/gif2.gif":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn85@2020_2/2020/08/24/23-52-13-604_4bd1bec78874a444.webp","https://bair.berkeley.edu/static/blog/omnitact/gif3.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn58@2020_3/2020/08/24/23-52-11-541_1055c2e75c8f229a.webp","https://bair.berkeley.edu/static/blog/omnitact/Fig7.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn86@2020_5/2020/08/24/23-52-04-869_b2b0e53df6985c84.webp"},"publishedOrCreatedDate":1598313090352},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Open Compound Domain Adaptation","link":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","description":"<!--\nTODO TODO TODO personal reminder for Daniel Seita :-)\nBe careful that these three lines are at the top,\nand that the title and image change for each blog post!\n-->\n<meta name=\"twitter:title\" content=\"Open Compound Domain Adaptation\" />\n\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/ocda/figure_1.jpg\" />\n\n<h1 id=\"the-world-is-continuously-varying\">The World is Continuously Varying</h1>\n\n<p>Imagine we want to train a self-driving car in New York so that we can take it\nall the way to Seattle without tediously driving it for over 48 hours. We hope\nour car can handle all kinds of environments on the trip and send us safely to\nthe destination. We know that road conditions and views can be very different.\nIt is intuitive to simply collect road data of this trip, let the car learn\nfrom every possible condition, and hope it becomes the perfect self-driving car\nfor our New York to Seattle trip. It needs to understand the traffic and\nskyscrapers in big cities like New York and Chicago, more unpredictable weather\nin Seattle, mountains and forests in Montana, and all kinds of country views,\nfarmlands, animals, etc. However, how much data is enough? How many cities\nshould we collect data from? How many weather conditions should we consider? We\nnever know, and these questions never stop.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ocda/figure_1.jpg\" width=\"\" />\n<br />\n<i>\nFigure 1: Domains boundaries are rarely clear. Therefore, it is hard to set up\ndefinite domain descriptions for all possible domains.\n</i>\n</p>\n\n<!--more-->\n\n<p>As for the weather, the number of different conditions can be infinite, while\nthe words that can be used to describe weather conditions (such as sunny and\nrainy) are always limited. Some conditions have subtle differences that even\nhumans can not tell. For example, overcast v.s. cloudy, the transition state of\nrainy to non-rainy, light snow v.s. light rain, etc. We can always try to\ncollect data for every possible weather condition that we can imagine and use\ntraditional domain adaptation methods to help our car to adapt to these\nconditions. But our imagination is limited when it comes to weather, and the\nmain reason is that there are no clear boundaries between weather conditions.\nIn that case, none of the traditional domain adaptation approaches, where clear\nboundaries are assumed, can help us in real-world weather adaption. Therefore,\nwe start rethinking machine learning and domain adaptation systems, and try to\nintroduce a continuous learning protocol under domain adaptation scenario.</p>\n\n<h1 id=\"open-compound-domain-adaptation-ocda\">Open Compound Domain Adaptation (OCDA)</h1>\n\n<p>The goal of domain adaptation is to adapt the model learned on the training\ndata to the test data of a different distribution. Such a distributional gap is\noften formulated as a shift between discrete concepts of well defined data\ndomains, e.g., images collected in sunny weather versus those in rainy weather.\nThough domain generalization and latent domain adaptation have attempted to\ntackle complex target domains, most existing works usually assume that there is\na known clear distinction between domains. Such a known and clear distinction\nbetween domains is hard to define in practice, e.g., test images could be\ncollected in mixed, continually varying, and sometimes never seen weather\nconditions.  With numerous factors jointly contributing to data variance, it\nbecomes implausible to separate data into discrete domains.</p>\n\n<p>We propose to study <strong>Open Compound Domain Adaptation (OCDA)</strong>, a continuous\nand more realistic setting for domain adaptation (Figure 2). The task is to\nlearn a model from labeled source domain data and adapt it to unlabeled\ncompound target domain data which could differ from the source domain on\nvarious factors. Our target domain can be regarded as a combination of multiple\ntraditionally homogeneous domains where each is distinctive on one or two major\nfactors, and yet none of the domain labels are given. For example, the five\nwell-known datasets on digits recognition (SVHN, MNIST, MNIST-M, USPS, and\nSynNum) mainly differ from each other by the backgrounds and text fonts. It is\nnot necessarily the best practice, and not feasible under some scenarios, to\nconsider them as distinct domains. Instead, our compound target domain pools\nthem together. Furthermore, at the inference stage, OCDA tests the model not\nonly in the compound target domain but also in open domains that have\npreviously unseen during training.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ocda/figure_2.png\" width=\"700\" />\n<br />\n<i>\nFigure 2: <b>Open Compound Domain Adaptation</b> problem. Unlike existing\ndomain adaptation which assumes clear distinctions between discrete domains,\nour compound target domain is a combination of multiple traditionally\nhomogeneous domains without any domain labels. We also allow novel domains to\nshow up at the inference time.\n</i>\n</p>\n\n<p>While OCDA has not been defined in the literature, there are two closely\nrelated tasks which are often studied in isolation: single-target domain\nadaptation, and multi-target domain adaptation. Figure 3 summarizes their\ndifferences. The newly proposed Open Compound Domain Adaptation (OCDA) serves\nas a more comprehensive and more realistic touchstone for evaluating domain\nadaptation and transfer learning systems.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ocda/figure_3.png\" width=\"\" />\n<br />\n<i>\nFigure 3: The differences between single-target domain adaptation, multi-target\ndomain adaptation and our open compound domain adaptation (OCDA).\n</i>\n</p>\n\n<h1 id=\"the-importance-of-curriculum--memory\">The Importance of Curriculum &amp; Memory</h1>\n\n<p>In our OCDA setting, the target domain no longer has a predominantly uni-modal\ndistribution, posing challenges to existing domain adaptation methods. We\npropose a novel approach based on two technical insights into OCDA: 1) a\n<strong>curriculum domain adaptation</strong> strategy to bootstrap generalization across\ndomain distinction in a data-driven self-organizing fashion and 2) a <strong>memory\nmodule</strong> to increase the model’s agility towards novel domains, as illustrated\nin Figure 4.</p>\n\n<p>Firstly, unlike existing curriculum adaptation methods that rely on some\nholistic measure of instance difficulty, we schedule the learning of unlabeled\ninstances in the compound target domain according to their individual gaps to\nthe labeled source domain, so that we solve an incrementally harder domain\nadaptation problem till we cover the entire target domain.</p>\n\n<p>Our second technical insight is to prepare our model for open domains during\ninference with a memory module that effectively augments the representations of\nan input for classification. Intuitively, if the input is close enough to the\nsource domain, the feature extracted from itself can most likely already result\nin accurate classification. Otherwise, the input-activated memory features can\nstep in and play a more important role. Consequently, this memory-augmented\nnetwork is more agile at handling open domains than its vanilla counterpart.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ocda/figure_4.png\" width=\"\" />\n<br />\n<i>\nFigure 4: Overview of our approach. 1) We separate characteristics specific to\ndomains from those discriminative between classes. The teased out domain\nfeature is used to construct a curriculum for domain-robust learning. 2) We\nenhance our network with a memory module that facilitates knowledge transfer\nfrom the source domain to target domain instances, so that the network can\ndynamically balance the input information and the memory-transferred knowledge\nfor more agility towards previously unseen domains.\n</i>\n</p>\n\n<h1 id=\"robustness-to-compound-and-open-domains\">Robustness to Compound and Open Domains</h1>\n\n<p>We control the complexity of the compound and open target domain by varying the\nnumber of traditional target domains / datasets in it. Here we gradually\nincrease constituting domains from a single target domain to two, and\neventually three. As demonstrated in Figure 5, we observe that our approach is\nmore resilient to the various numbers of compound and open domains. 1) The\nlearned curriculum enables gradual knowledge transfer that is capable of coping\nwith complex  structures in the compound target domain. 2) Furthermore, the\ndomain indicator module in our framework helps dynamically calibrate the\nembedding, thus enhancing the robustness to open domains.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ocda/figure_5.png\" width=\"\" />\n<br />\n<i>\nFigure 5: The performance change w.r.t. the number of compound and open\ndomains.\n</i>\n</p>\n\n<h1 id=\"visualization-of-learning-dynamics\">Visualization of Learning Dynamics</h1>\n\n<p>Here is a visualization of disentangling domain characteristics. We separate\ncharacteristics specific to domains from those discriminative between classes.\nIt is achieved by a class-confusion algorithm in an unsupervised manner. Figure\n6 (a) and (b) visualize the examples embedded by the class encoder and domain\nencoder, respectively.  The class encoder places instances in the same class in\na cluster, while the domain encoder places instances according to their common\nappearances, regardless of their classes.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ocda/figure_6.png\" width=\"\" />\n<br />\n<i>\nFigure 6: t-SNE Visualization of our (a) class-discriminative features, and (b)\ndomain features. Our framework disentangles the mixed-domain data into\nclass-discriminative factors and domain-focused factors.\n</i>\n</p>\n\n<p><em>Acknowledgements: We thank all co-authors of the paper “Open Compound Domain\nAdaptation” for their contributions and discussions in preparing this blog. The\nviews and opinions expressed in this blog are solely of the authors of this\npaper.</em></p>\n\n<p>This blog post is based on the following paper which will be presented at IEEE\nConference on Computer Vision and Pattern Recognition (CVPR 2020) as an <strong>oral\npresentation</strong>:</p>\n\n<ul>\n  <li>Open Compound Domain Adaptation<br />\nZiwei Liu*, Zhongqi Miao*, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella X. Yu, Boqing Gong<br />\n<a href=\"https://arxiv.org/abs/1909.03403\">Paper</a>, <a href=\"https://liuziwei7.github.io/projects/CompoundDomain.html\">Project Page</a>, <a href=\"https://drive.google.com/drive/folders/1_uNTF8RdvhS_sqVTnYx17hEOQpefmE2r\">Dataset</a>, <a href=\"https://github.com/zhmiao/OpenCompoundDomainAdaptation-OCDA\">Code &amp; Model</a></li>\n</ul>\n\n","descriptionType":"text/html","publishedDate":"Sun, 14 Jun 2020 09:00:00 +0000","feedId":8140,"bgimg":"https://bair.berkeley.edu/static/blog/ocda/figure_1.jpg","linkMd5":"5385792a73444f69dd02146562fbbe85","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn27@2020_3/2020/08/24/23-51-38-323_4c4f2c5220e502dd.webp","destWidth":1020,"destHeight":409,"sourceBytes":178696,"destBytes":48684,"author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/ocda/figure_1.jpg":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn27@2020_3/2020/08/24/23-51-38-323_4c4f2c5220e502dd.webp","https://bair.berkeley.edu/static/blog/ocda/figure_2.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn17@2020_3/2020/08/24/23-52-08-961_390c0133597feed8.webp","https://bair.berkeley.edu/static/blog/ocda/figure_3.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn37@2020_1/2020/08/24/23-52-07-458_285b8b4d400be589.webp","https://bair.berkeley.edu/static/blog/ocda/figure_4.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn18@2020_2/2020/08/24/23-52-03-447_a7aae2632c30cbf1.webp","https://bair.berkeley.edu/static/blog/ocda/figure_5.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn26@2020_3/2020/08/24/23-52-03-518_a48ccaa913f617d3.webp","https://bair.berkeley.edu/static/blog/ocda/figure_6.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn53@2020_1/2020/08/24/23-52-04-974_f127ec37fbf79690.webp"},"publishedOrCreatedDate":1598313090351},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Unsupervised Meta-Learning: Learning to Learn without Supervision","link":"http://bair.berkeley.edu/blog/2020/05/01/umrl/","description":"<!--\nTODO TODO TODO personal reminder for Daniel Seita :-)\nBe careful that these three lines are at the top,\nand that the title and image change for each blog post!\n--> \n<meta name=\"twitter:title\" content=\"Unsupervised Meta-Learning: Learning to Learn without Supervision\" /> \n<meta name=\"twitter:card\" content=\"summary_image\" /> \n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/umrl/umrl_teaser.png\" /> \n<p><em>This post is cross-listed <a href=\"https://blog.ml.cmu.edu/2020/05/01/unsupervised-meta-learning-learning-to-learn-without-supervision/\">on the CMU ML blog</a>.</em></p> \n<p>The history of machine learning has largely been a story of increasing abstraction. In the dawn of ML, researchers spent considerable effort engineering features. As deep learning gained popularity, researchers then shifted towards tuning the update rules and learning rates for their optimizers. Recent research in meta-learning has climbed one level of abstraction higher: many researchers now spend their days manually constructing task distributions, from which they can automatically learn good optimizers. What might be the next rung on this ladder? In this post we introduce theory and algorithms for <strong>unsupervised meta-learning</strong>, where machine learning algorithms themselves propose their own task distributions. Unsupervised meta-learning further reduces the amount of human supervision required to solve tasks, potentially inserting a new rung on this ladder of abstraction.</p> \n<!--more--> \n<p>We start by discussing how machine learning algorithms use human supervision to find patterns and extract knowledge from observed data. The most common machine learning setting is <em>regression</em>, where a human provides labels $Y$ for a set of examples $X$. The aim is to return a predictor that correctly assigns labels to novel examples. Another common machine learning problem setting is <em>reinforcement learning (RL)</em>, where an agent takes actions in an environment. In RL, humans indicate the desired behavior through a reward function that the agent seeks to maximize. To draw a crude analogy to regression, the environment dynamics are the examples $X$, and the reward function gives the labels $Y$. Algorithms for regression and RL employ many tools, including tabular methods (e.g., value iteration), linear methods (e.g., linear regression) kernel-methods (e.g., RBF-SVMs), and deep neural networks. Broadly, we call these algorithms <em>learning procedures</em>: processes that take as input a dataset (examples with labels, or transitions with rewards) and output a function that performs well (achieves high accuracy or large reward) on the dataset.</p> \n<p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/umrl/control_room.jpg\" width=\"500\" /> <br /> <i> Machine learning research is similar to the control room for large physics experiments. Researchers have a number of knobs they can tune which affect the performance of the learning procedure. The right setting for the knobs depends on the particular experiment: some settings work well for high-energy experiments; others work well for ultracold atom experiments. <a href=\"https://www.flickr.com/photos/x-ray_delta_one/3941701730\">Figure Credit</a>. </i> </p> \n<p>Similar to lab procedures used in physics and biology, the learning procedures used in machine learning have many knobs<sup id=\"fnref:knob\"><a href=\"http://bair.berkeley.edu/blog/2020/05/01/umrl/#fn:knob\" class=\"footnote\">1</a></sup> that can be tuned. For example, the learning procedure for training a neural network might be defined by an optimizer (e.g., Nesterov, Adam) and a learning rate (e.g., 1e-5). Compared with regression, learning procedures specific to RL (e.g., DDPG) often have many more knobs, including the frequency of data collection and how frequently the policy is updated. Finding the right setting for the knobs can have a large effect on how quickly the learning procedure solves a task, and a good configuration of knobs for one learning procedure may be a bad configuration for another.</p> \n<h1 id=\"meta-learning-optimizes-knobs-of-the-learning-procedure\">Meta-Learning Optimizes Knobs of the Learning Procedure</h1> \n<p>While machine learning practitioners often carefully tune these knobs by hand, if we are going to solve many tasks, it may be useful to automatic this process. The process of setting the knobs of learning procedures via optimization is called <em>meta-learning</em> [<a href=\"https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1\">Thrun 1998</a>]. Algorithms that perform this optimization problem automatically are known as <em>meta-learning algorithms</em>. Explicitly tuning the knobs of learning procedures is an active area of research, with various researchers looking at tuning the update rules [<a href=\"https://arxiv.org/abs/1606.04474\">Andrychowicz 2016</a>, <a href=\"https://arxiv.org/abs/1611.02779\">Duan 2016</a>, <a href=\"https://arxiv.org/abs/1611.05763\">Wang 2016</a>], weight initialization [<a href=\"https://arxiv.org/abs/1703.03400\">Finn 2017</a>], network weights [<a href=\"https://arxiv.org/abs/1609.09106\">Ha 2016</a>], network architectures [<a href=\"https://arxiv.org/abs/1906.04358\">Gaier 2019</a>], and other facets of learning procedures.</p> \n<p>To evaluate a setting of knobs, meta-learning algorithms consider not one task but a distribution over many tasks. For example, a distribution over supervised learning tasks may include learning a dog detector, learning a cat detector, and learning a bird detector. In reinforcement learning, a task distribution could be defined as driving a car in a smooth, safe, and efficient manner, where tasks differ by the weights they place on smoothness, safety, and efficiency. Ideally, the task distribution is designed to mirror the distribution over tasks that we are likely to encounter in the real world. Since the tasks in a task distribution are typically related, information from one task may be useful in solving other tasks more efficiently. As you might expect, a knob setting that works best on one distribution of tasks may not be the best for another task distribution; the optimal knob setting depends on the task distribution.</p> \n<p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/umrl/Meta-Learning-v2.png\" width=\"\" /> <br /> <i> An illustration of meta-learning, where tasks correspond to arranging blocks into different types of towers. The human has a particular block tower in mind and rewards the robot when it builds the correct tower. The robot's aim is to build the correct tower as quickly as possible. </i> </p> \n<p>In many settings we want to do well on a task distribution to which we have only limited access. For example, in a self-driving car, tasks may correspond to finding the optimal balance of smoothness, safety, and efficiency for each rider, but querying riders to get rewards is expensive. A researcher can attempt to manually construct a task distribution that mimics the true task distribution, but this can be quite challenging and time consuming. Can we avoid having to manually design such task distributions?</p> \n<p>To answer this question, we must understand where the benefits of meta-learning come from. When we define task distributions for meta-learning, we do so with some prior knowledge in mind. Without this prior information, tuning the knobs of a learning procedure is often a zero-sum game: setting the knobs to any configuration will accelerate learning on some tasks while slowing learning on other tasks. Does this suggest there is no way to see the benefit of meta-learning without the manual construction of task distributions? Perhaps not! The next section presents an alternative.</p> \n<h1 id=\"optimizing-the-learning-procedure-with-self-proposed-tasks\">Optimizing the Learning Procedure with Self-Proposed Tasks</h1> \n<p>If designing task distributions is the bottleneck in applying meta-learning algorithms, why not have meta-learning algorithms propose their own tasks? At first glance this seems like a terrible idea, because the No Free Lunch Theorem suggests that this is impossible, <em>without additional knowledge</em>. However, many real-world settings do provide a bit of additional information, albeit disguised as unlabeled data. For example, in regression, we might have access to an unlabeled dataset and know that the downstream tasks will be labeled versions of this same image dataset. In a RL setting, a robot can interact with its environment without receiving any reward, knowing that downstream tasks will be constructed by defining reward functions for this very environment (i.e. the real world). Seen from this perspective, the recipe for <em>unsupervised meta-learning</em> (doing meta-learning without manually constructed tasks) becomes clear: given unlabeled data, construct task distributions from this unlabeled data or environment, and then meta-learn to quickly solve these self-proposed tasks.</p> \n<p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/umrl/Unsupervised-Meta-Learning-v2.png\" width=\"\" /> <br /> <i> In unsupervised meta-learning, the agent proposes its own tasks, rather than relying on tasks proposed by a human. </i> </p> \n<p>How can we use this unlabeled data to construct task distributions which will facilitate learning downstream tasks? In the case of regression, prior work on unsupervised meta-learning [<a href=\"https://arxiv.org/abs/1810.02334\">Hsu 2018</a>, <a href=\"https://arxiv.org/abs/1811.11819\">Khodadadeh 2019</a>] clusters an unlabeled dataset of images and then randomly chooses subsets of the clusters to define a distribution of classification tasks. Other work [<a href=\"https://arxiv.org/abs/1912.04226\">Jabri 2019</a>] look at an RL setting: after exploring an environment without a reward function to collect a set of behaviors that are feasible in this environment, these behaviors are clustered and used to define a distribution of reward functions. In both cases, even though the tasks constructed can be random, the resulting task distribution is not random, because all tasks share the underlying unlabeled data — the image dataset for regression and the environment dynamics for reinforcement learning. <em>The underlying unlabeled data are the inductive bias with which we pay for our free lunch.</em></p> \n<p>Let us take a deeper look into the RL case. Without knowing the downstream tasks or reward functions, what is the “best” task distribution for “practicing” to solve tasks quickly? Can we measure how effective a task distribution is for solving unknown, downstream tasks? Is there any sense in which one unsupervised task proposal mechanism is better than another? Understanding the answers to these questions may guide the principled development of meta-learning algorithms with little dependence on human supervision. Our work [<a href=\"https://arxiv.org/abs/1806.04640\">Gupta 2018</a>], takes a first step towards answering these questions. In particular, we examine the <em>worst-case</em> performance of learning procedures, and derive an optimal unsupervised meta-reinforcement learning procedure.</p> \n<h1 id=\"optimal-unsupervised-meta-learning\">Optimal Unsupervised Meta-Learning</h1> \n<p>To answer the questions posed above, our first step is to define an optimal meta-learner for the case where the distribution of tasks is known. We define an optimal meta-learner as the learning procedure that achieves the largest expected reward, averaged across the distribution of tasks. More precisely, we will compare the expected reward for a learning procedure $f$ to that of best learning procedure $f^*$, defining the <em>regret</em> of $f$ on a task distribution $p$ as follows:</p> \n<p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/umrl/labeled_regret_v2.png\" width=\"600\" /> <br /> </p> \n<p>Extending this definition to the case of unsupervised meta-learning, an optimal unsupervised meta-learner can be defined as a meta-learner that achieves the minimum <em>worst-case</em> regret across all possible task distributions that may be encountered in the environment. In the absence of any knowledge about the actual downstream task, we resort to a worst case formulation. An unsupervised meta-learning algorithm will find a single learning procedure $f$ that has the lowest regret against an <em>adversarially</em> chosen task distribution $p$:</p> \n<script type=\"math/tex; mode=display\">\\min_f \\max_p \\;\\; {\\rm Regret}(f,p).</script> \n<p>Our work analyzes how exactly we might obtain such an optimal unsupervised meta-learner, and provides bounds on the regret that it might incur in the worst case. Specifically, under some restrictions on the family of tasks that might be encountered at test-time, the optimal distribution for an unsupervised meta-learner to propose is <em>uniform</em> over all possible tasks.</p> \n<p>The intuition for this is straightforward: if the test time task distribution can be chosen adversarially, the algorithm must make sure it is uniformly good over <em>all</em> possible tasks that might be encountered. As a didactic example, if test-time reward functions were restricted to the class of goal-reaching tasks, the regret for reaching a goal at test-time is inverse related to the probability of sampling that goal during training-time. If any one of the goals $g$ has lower density than the others, an adversary can propose a task distribution solely consisting of reaching that goal $g$ causing the learning procedure to incur a higher regret. This example suggests that we can find an optimal unsupervised meta-learner using a uniform distribution over goals. Our paper formalizes this idea and extends it to broader classes task distributions.</p> \n<p>Now, actually sampling from a uniform distribution over all possible tasks is quite challenging. Several recent papers have proposed RL exploration methods based on maximizing mutual information [<a href=\"https://arxiv.org/abs/1807.10299\">Achiam 2018</a>, <a href=\"https://arxiv.org/abs/1802.06070\">Eysenbach 2018</a>, <a href=\"https://arxiv.org/abs/1611.07507\">Gregor 2016</a>, <a href=\"https://arxiv.org/abs/1906.05274\">Lee 2019</a>, <a href=\"https://arxiv.org/abs/1907.01657\">Sharma 2019</a>]. In this work, we show that these methods provide a tractable approximation to the uniform distribution over task distributions. To understand why this is, we can look at the form of a mutual information considered by [<a href=\"https://arxiv.org/abs/1802.06070\">Eysenbach 2018</a>], between states $s$ and latent variables $z$:</p> \n<script type=\"math/tex; mode=display\">\\mathcal{I}(s,z) = \\mathcal{H}(s) - \\mathcal{H}(s|z).</script> \n<p>In this objective, the first marginal entropy term is maximized when there is a uniform distribution over all possible tasks. The second conditional entropy term ensures consistency, by making sure that for each $z$, the resulting distribution of $s$ is narrow. This suggests constructing unsupervised task-distributions in an environment by optimizing mutual information gives us a provably optimal task distribution, according to our notion of min-max optimality.</p> \n<p>While the analysis makes some limiting assumptions about the forms of tasks encountered, we show how this analysis can be extended to provide a bound on the performance in the most general case of reinforcement learning. It also provides empirical gains on several simulated environments as compared to methods which train from scratch, as shown in the Figure below.</p> \n<p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/umrl/UMRL.png\" width=\"\" /> <br /> </p> \n<h1 id=\"summary--discussion\">Summary &amp; Discussion</h1> \n<p>In summary:</p> \n<ul> \n <li> <p>Learning procedures are recipes for converting datasets into function approximators. Learning procedures have many knobs, which can be tuned by optimizing the learning procedures to solve a distribution of tasks.</p> </li> \n <li> <p>Manually designing these task distributions is challenging, so a recent line of work suggests that the learning procedure can use unlabeled data to propose its own tasks for optimizing its knobs.</p> </li> \n <li> <p>These unsupervised meta-learning algorithms allow for learning in regimes previously impractical, and further expand that capability of machine learning methods.</p> </li> \n <li> <p>This work closely relates to other works on <a href=\"https://arxiv.org/abs/1907.01657\">unsupervised</a> <a href=\"https://arxiv.org/abs/1611.07507\">skill discovery</a>, <a href=\"https://arxiv.org/abs/1705.05363\">exploration</a> and <a href=\"https://arxiv.org/abs/1807.03748\">representation</a> <a href=\"https://arxiv.org/abs/1511.06434\">learning</a>, but explicitly optimizes for transferability of the representations and skills to downstream tasks.</p> </li> \n</ul> \n<p>A number of open questions remain about unsupervised meta-learning:</p> \n<ul> \n <li> <p>Unsupervised learning is closely connected to unsupervised meta-learning: the former uses unlabeled data to learn features, while the second uses unlabeled data to tune the learning procedure. Might there be some unifying treatment of both approaches?</p> </li> \n <li> <p>Our analysis only proves that task proposal based on mutual information is optimal for memoryless meta-learning algorithms. Meta-learning algorithms with memory, which we expect will perform better, may perform best with different task proposal mechanisms.</p> </li> \n <li> <p>Scaling unsupervised meta learning to leverage large-scale datasets and complex tasks holds the promise of acquiring learning procedures for solving real-world problems more efficiently than our current learning procedures.</p> </li> \n</ul> \n<p>Check out our paper for more experiments and proofs: <a href=\"https://arxiv.org/abs/1806.04640\">https://arxiv.org/abs/1806.04640</a></p> \n<h2 id=\"acknowledgments\">Acknowledgments</h2> \n<p>Thanks to Jake Tyo, Conor Igoe, Sergey Levine, Chelsea Finn, Misha Khodak, Daniel Seita, and Stefani Karp for their feedback.</p> \n<hr /> \n<div class=\"footnotes\"> \n <ol> \n  <li id=\"fn:knob\"> <p>These knobs are often known as hyperparameters, but we will stick with the colloquial “knob” to avoid having to draw a line between parameters and hyperparameters.&nbsp;<a href=\"http://bair.berkeley.edu/blog/2020/05/01/umrl/#fnref:knob\" class=\"reversefootnote\">↩</a></p> </li> \n </ol> \n</div>","descriptionType":"text/html","publishedDate":"Fri, 01 May 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"4923ec236785bf4c166ae57f30e71201","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/umrl/control_room.jpg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn4@2020_3/2020/08/24/23-52-03-489_d27bca5053769246.webp","https://bair.berkeley.edu/static/blog/umrl/Meta-Learning-v2.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn9@2020_4/2020/08/24/23-52-07-435_86ec97ee772bf3a2.webp","https://bair.berkeley.edu/static/blog/umrl/Unsupervised-Meta-Learning-v2.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn1@2020_2/2020/08/24/23-52-05-637_54298ecbf9b65c23.webp","https://bair.berkeley.edu/static/blog/umrl/labeled_regret_v2.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn73@2020_1/2020/08/24/23-52-04-665_1dd1fd6e3c88df87.webp","https://bair.berkeley.edu/static/blog/umrl/UMRL.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn61@2020_4/2020/08/24/23-52-05-953_ff1213976b69fa52.webp"},"publishedOrCreatedDate":1598313090352},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Exploring Exploration: Comparing Children with RL Agents in Unified Environments","link":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","description":"<meta name=\"twitter:title\" content=\"Exploring Exploration: Comparing Children with RL Agents in Unified Environments\" />\n\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/icm/image4.png\" />\n\n<p>Despite recent advances in artificial intelligence (AI) research, human\nchildren are still by far the best learners we know of, learning impressive\nskills like language and high-level reasoning from very little data. Children’s\nlearning is supported by highly efficient, hypothesis-driven exploration: in\nfact, they explore so well that many machine learning researchers have been\ninspired to put videos like the one below in their talks to motivate research\ninto exploration methods. However, because applying results from studies in\ndevelopmental psychology can be difficult, this video is often the extent to\nwhich such research actually connects with human cognition.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image0.gif\" width=\"\" />\n<br />\n<i>\nA time-lapse of a baby playing with toys. <a href=\"https://www.youtube.com/watch?v=8vNxjwt2AqY\">Source</a>.\n</i>\n</p>\n\n<!--more-->\n\n<p>Why is directly applying research from developmental psychology to problems in\nAI so hard? For one, taking inspiration from developmental studies can be\ndifficult because the environments that human children and artificial agents\nare typically studied in can be very different. Traditionally, reinforcement\nlearning (RL) research takes place in grid-world-like settings or other 2D\ngames, whereas children act in the real world which is rich and 3-dimensional.\nFurthermore, comparisons between children and AI agents are difficult to make\nbecause the experiments are not controlled and often have an objective\nmismatch; much of the developmental psychology research with children takes\nplace with children engaged in free exploration, whereas a majority of research\nin AI is goal-driven. Finally, it can be hard to ‘close the loop’, and not only\nbuild agents inspired by children, but learn about human cognition from\noutcomes in AI research. By studying children and artificial agents in the\nsame, controlled, 3D environment we can potentially alleviate many of these\nproblems above, and ultimately progress research in both AI and cognitive\nscience.</p>\n\n<p>In collaboration with Jessica Hamrick and Sandy Huang from DeepMind, and Deepak\nPathak, Pulkit Agrawal, John Canny, Alexei A. Efros, Jeffrey Liu, and Alison Gopnik from UC Berkeley, that’s\nexactly what this work aims to do. We have developed a platform and framework\nfor directly contrasting agent and child exploration based on DeepMind Lab – a\nfirst person 3D navigation and puzzle-solving environment originally built for\ntesting agents in mazes with rich visuals.</p>\n\n<h1 id=\"what-do-we-actually-know-about-how-children-explore\">What do we actually know about how children explore?</h1>\n\n<p><img src=\"https://bair.berkeley.edu/static/blog/icm/image1.png\" width=\"200\" hspace=\"40\" align=\"right\" /></p>\n\n<p>The main thing that we know about the child exploration is that children form\nhypotheses about how the world works, and they engage in exploration to test\nthose hypotheses. For example, studies such as the one from <a href=\"https://www.researchgate.net/publication/6231182_Serious_Fun_Preschoolers_Engage_in_More_Exploratory_Play_When_Evidence_Is_Confounded\">Liz Bonawitz et\nal., 2007</a> showed us that preschoolers’ exploratory play is affected by the\nevidence they observe. They conclude that if it seems like there are multiple\nways that a toy could work but it’s not clear which one is right (in other\nwords, the evidence is causally confounded) then children engage in\nhypothesis-driven exploration and will explore the toy for significantly longer\nthan when the dynamics and outcome are simple (in which case they would quickly\nmove on to a new toy).</p>\n\n<p><a href=\"https://science.sciencemag.org/content/348/6230/91\">Stahl and Feigneson et al., 2015</a> showed us that when babies as young as\n11-months are presented with objects that violate physical laws in their\nenvironments they will explore them more and even engage in hypothesis-testing\nbehaviors that reflect the particular kind of violation seen. For example, if\nthey see a car floating in the air (as in the video on the left), they find\nthis surprising; subsequently, children then bang the toy on the table to\nexplore how it works. In other words, these violations guide the children’s\nexploration in a meaningful way.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image2.gif\" height=\"200\" width=\"48%\" />\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image3.gif\" height=\"200\" width=\"48%\" />\n<br />\n</p>\n\n<h1 id=\"how-do-ai-agents-explore\">How do AI agents explore?</h1>\n\n<p>Classic work in computer science and AI focused on developing search methods\nthat try to seek out a goal. For example, a depth-first search strategy will\ncontinue exploring down a particular path until either the goal or a dead-end\nis reached. If a dead-end is reached, it will backtrack until the next\nunexplored path is found and then proceed down that path. However, unlike\nchildren’s exploration, methods like these don’t have a notion of exploring\nmore given surprising evidence, gathering information, or testing hypotheses.\nMore recent work in RL has seen the development of other types of exploration\nalgorithms. For example, <a href=\"https://arxiv.org/abs/1705.05363\">intrinsic motivation methods</a> provide a bonus for\nexploring interesting regions, such as those that have not been visited as much\npreviously or those which are surprising. While these seem in principle more\nsimilar to children’s exploration, they are typically used more to expose\nagents to a diverse set of experience during training, rather than to support\nrapid learning and exploration at decision time.</p>\n\n<h1 id=\"experimental-setup\">Experimental setup</h1>\n\n<p>To alleviate some of the difficulties mentioned before in regards to applying\nresults from developmental psychology to AI research, we develop a setup in\nwhich we have an exact comparison between children and agent behavior in the\nexact same environment with exactly the same observations and maps. We do this\nusing DeepMind Lab, an existing platform for training and evaluating RL agents.\nMoreover, we can restrict the action space in DeepMind Lab to four simple\nactions (forward, backward, turn left, and turn right) using custom controllers\nbuilt in the lab, which make it easier for children to navigate around the\nenvironment. Finally, in DeepMind lab we can procedurally generate a huge\namount of training data that we can use to bring agents up-to-speed on common\nconcepts like “wall” and “goal”.</p>\n\n<p>In the picture below you can see an overview of the parts of our experimental\nsetup. On the left, we see what it looks like when a child is exploring the\nmaze using the controller and the 4 possible actions they can take. In the\nmiddle, we see what the child is seeing while navigating through the maze, and\non the right there is an aerial view of the child’s overall trajectory in the\nmaze.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image4.png\" width=\"\" />\n<br />\n</p>\n\n<h1 id=\"experimental-results\">Experimental results</h1>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image5.png\" width=\"70%\" />\n<br />\n</p>\n\n<p>We first investigated whether or not children (ages 4-5) that are naturally\nmore curious/exploratory in a maze are better able to succeed at finding a goal\nlater introduced at a random location. To test this, we have children explore\nthe maze for 3 minutes without any specific instructions or goals, and then\nintroduce a goal (represented as a gummy) in the same maze and then ask the\nchildren to find the gummy. We measure both 1) the percentage of the maze\nexplored in the free exploration part of the task and 2) how long, in terms of\nnumber of steps, it takes the children to find the gummy after it is\nintroduced.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image6.png\" width=\"60%\" />\n<br />\n</p>\n\n<p>We breakdown the percentage of maze explored into 3 groups: low, medium and\nhigh explorers. The low explorers explored around 22% of the maze on average,\nthe medium explorers explored around 44% of the maze on average, and the high\nexplorers explored around 71% of the maze on average. What we find is that the\nless exploring the child did in the first part of the maze, the more steps it\ntook them to reach the gummy. This result can be visualized in the bar chart on\nthe left, where the Y-axis represents the number of steps it took them to find\nthe gummy, and the X-axis represents the explorer type. This data suggests a\ntrend that higher explorers are more efficient at finding a goal in this maze\nsetting.</p>\n\n<p>Now that we have data from the children in the mazes, How do we measure the\ndifference between an agent and a human trajectory?  One method is to measure\nif the child’s actions are “consistent” with that of a specific exploration\ntechnique. Given a specific state or observation, a human or agent has a set of\nactions that can be taken from this state. If the child takes the same action\nthat an agent would take, we call the action choice ‘consistent’. Otherwise the\naction would not be consistent. We measure the percentage of states in a\nchild’s trajectory where the action taken by the child is consistent with an\nalgorithmic approach. We call this percentage “choice-consistency.”</p>\n\n<p>One of the simplest algorithmic exploration techniques is to do a systematic\nsearch method called depth-first search (DFS). From our task, recall that\nchildren had a maze in which they first engaged in free exploration, and then a\ngoal-directed search. When we compare the consistency of the children’s\ntrajectories in those 2 settings with those of the DFS algorithm, we find that\nkids in the free exploration setting take actions that are consistent with DFS\nonly 90% of the time, whereas in the goal oriented setting they match DFS 96%\nof the time.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/icm/image7.png\" width=\"\" />\n<br />\n</p>\n\n<p>One way to interpret this result is that kids in the goal-oriented setting are\ntaking actions that are more systematic than in free exploration. In other\nwords, the children look more like a search algorithm when they are given a\ngoal.</p>\n\n<h1 id=\"conclusion-and-future-work\">Conclusion and future work</h1>\n\n<p>In conclusion, this work only begins to touch on a number of deep questions\nregarding how children and agents explore. The experiments presented here just\nbegin to address questions regarding how much children and agents are willing\nto explore; whether free versus goal-directed exploration strategies differ;\nand how reward shaping affects exploration. Yet, our setup allows us to ask so\nmany more questions, and we have concrete plans to do so.</p>\n\n<p>While DFS is a great first baseline for uninformed search, to enable a better\ncomparison, a next step is to compare the children’s trajectories to other\nclassical search algorithms and to that of RL agents from the literature.\nFurther, even the most sophisticated methods for exploration in RL tend to\nexplore only in the service of a specific goal, and are usually driven by error\nrather than seeking information. Properly aligning the objectives of RL\nalgorithms with those of an exploratory child is an open question.</p>\n\n<p>We believe that to truly build intelligent agents, they must do as children do:\nactively explore their environments, perform experiments, and gather\ninformation to weave together into a rich model of the world. In this\ndirection, we will be able to gain a deeper understanding of the way that\nchildren and agents explore novel environments, and how to close the gap\nbetween them.</p>\n\n<hr />\n\n<p>This blog post is based on the following paper:</p>\n\n<ul>\n  <li><strong>Exploring Exploration: Comparing Children with RL Agents in Unified Environments</strong>.<br />\nEliza Kosoy, Jasmine Collins, David M. Chan, Sandy Huang, Deepak Pathak, Pulkit Agrawal, John Canny, Alison Gopnik, and Jessica B. Hamrick<br />\n<a href=\"https://arxiv.org/abs/2005.02880\">arXiv preprint arXiv:2005.02880 (2020)</a></li>\n</ul>\n\n","descriptionType":"text/html","publishedDate":"Fri, 24 Jul 2020 09:00:00 +0000","feedId":8140,"bgimg":"https://bair.berkeley.edu/static/blog/icm/image0.gif","linkMd5":"cdcbf18cd5159cf20ae969d8792dfd9b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn23@2020_1/2020/08/24/23-51-40-109_c4a676d9408f38bd.webp","destWidth":600,"destHeight":423,"sourceBytes":6082991,"destBytes":1738802,"author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/icm/image0.gif":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn23@2020_1/2020/08/24/23-51-40-109_c4a676d9408f38bd.webp","https://bair.berkeley.edu/static/blog/icm/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn89@2020_4/2020/08/24/23-52-08-006_5b016bc6b6b35131.webp","https://bair.berkeley.edu/static/blog/icm/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn70@2020_2/2020/08/24/23-52-06-303_e1c36027967e8ac9.webp","https://bair.berkeley.edu/static/blog/icm/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn89@2020_4/2020/08/24/23-52-05-448_dfef929ece6a4399.webp","https://bair.berkeley.edu/static/blog/icm/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn22@2020_4/2020/08/24/23-52-07-110_15ed6fcfe1d1800f.webp","https://bair.berkeley.edu/static/blog/icm/image5.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn1@2020_2/2020/08/24/23-52-03-335_4249d6181fa7b571.webp","https://bair.berkeley.edu/static/blog/icm/image6.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn34@2020_4/2020/08/24/23-52-04-204_501411ce3b9695f3.webp","https://bair.berkeley.edu/static/blog/icm/image7.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn58@2020_2/2020/08/24/23-52-03-264_017dd2627ba25be9.webp"},"publishedOrCreatedDate":1598313090351},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Can RL From Pixels be as Efficient as RL From State?","link":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","description":"<meta name=\"twitter:title\" content=\"Can RL From Pixels be as Efficient as RL From State?\" />\n\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/curl/fig1.png\" />\n\n<p>A remarkable characteristic of human intelligence is our ability to learn tasks\nquickly. Most humans can learn reasonably complex skills like tool-use and\ngameplay within just a few hours, and understand the basics after only a few\nattempts. This suggests that data-efficient learning may be a meaningful part\nof developing broader intelligence.</p>\n\n<p>On the other hand, Deep Reinforcement Learning (RL) algorithms can achieve\nsuperhuman performance on games like Atari, Starcraft, Dota, and Go, but\nrequire large amounts of data to get there. Achieving superhuman performance on\nDota took over <em>10,000 human years</em> of gameplay. Unlike simulation, skill\nacquisition in the real-world is constrained to wall-clock time. In order to\nsee similar breakthroughs to AlphaGo in real-world settings, such as robotic\nmanipulation and autonomous vehicle navigation, RL algorithms need to be\ndata-efficient — they need to learn effective policies within a reasonable\namount of time.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig1.png\" width=\"80%\" />\n<br />\n<i>\n</i>\n</p>\n\n<p>To date, it has been commonly assumed that RL operating on coordinate state is\nsignificantly more data-efficient than pixel-based RL. However, coordinate\nstate is just a human crafted representation of visual information. In\nprinciple, if the environment is fully observable, we should also be able to\nlearn representations that capture the state.</p>\n\n<!--more-->\n\n<h1 id=\"recent-advances-in-data-efficient-rl\">Recent advances in data-efficient RL</h1>\n\n<p>Recently, there have been several algorithmic advances in Deep RL that have\nimproved learning policies from pixels. The methods fall into two categories:\n(i) model-free algorithms and (ii) model-based (MBRL) algorithms. The main\ndifference between the two is that model-based methods learn a forward\ntransition model $p(s_{t+1}|,s_t,a_t)$ while model-free ones do not. Learning a\nmodel has several distinct advantages. First, it is possible to use the model\nto plan through action sequences, generate fictitious rollouts as a form of\ndata augmentation, and temporally shape the latent space by learning a model.</p>\n\n<p>However, a distinct disadvantage of model-based RL is complexity. Model-based\nmethods operating on pixels require learning a model, an encoding scheme, a\npolicy, various auxiliary tasks such as reward prediction, and stitching these\nparts together to make a whole algorithm. Visual MBRL methods have a lot of\nmoving parts and tend to be less stable. On the other hand, model-free methods\nsuch as Deep Q Networks (DQN), Proximal Policy Optimization (PPO), and Soft\nActor-Critic (SAC) learn a policy in an end-to-end manner optimizing for one\nobjective. While traditionally, the simplicity of model-free RL has come at the\ncost of sample-efficiency, recent improvements have shown that model-free\nmethods can in fact be more data-efficient than MBRL and, more surprisingly,\nresult in policies that are as data efficient as policies trained on coordinate\nstate. In what follows we will focus on these recent advances in pixel-based\nmodel-free RL.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig2.png\" width=\"\" />\n<br />\n<i>\n</i>\n</p>\n\n<h1 id=\"why-now\">Why now?</h1>\n\n<p>Over the last few years, two trends have converged to make data-efficient\nvisual RL possible. First, end-to-end RL algorithms have become increasingly\nmore stable through algorithms like the Rainbow DQN, TD3, and SAC. Second,\nthere has been tremendous progress in label-efficient learning for image\nclassification using contrastive unsupervised representations (CPCv2, MoCo,\nSimCLR) and data augmentation (MixUp, AutoAugment, RandAugment). In recent work\nfrom our lab at BAIR (CURL, RAD), we combined contrastive learning and data\naugmentation techniques from computer vision with model-free RL to show\nsignificant data-efficiency gains on common RL benchmarks like Atari, DeepMind\ncontrol, ProcGen, and OpenAI gym.</p>\n\n<h1 id=\"contrastive-learning-in-rl-setting\">Contrastive Learning in RL Setting</h1>\n\n<p>CURL was inspired by recent advances in contrastive representation learning in\ncomputer vision (CPC, CPCv2, MoCo, SimCLR). Contrastive learning aims to\nmaximize / minimize similarity between two similar / dissimilar representations\nof an image. For example, in MoCo and SimCLR, the objective is to maximize\nagreement between two data-augmented versions of the same image and minimize it\nbetween all other images in the dataset, where optimization is performed with a\nNoise Contrastive Estimation loss. Through data augmentation, these\nrepresentations internalize powerful inductive biases about invariance in the\ndataset.</p>\n\n<p>In the RL setting, we opted for a similar approach and adopted the momentum\ncontrast (MoCo) mechanism, a popular contrastive learning method in computer\nvision that uses a moving average of the query encoder parameters (momentum) to\nencode the keys to stabilize training. There are two main differences in setup:\n(i) the RL dataset changes dynamically and (ii) visual RL is typically\nperformed on stacks of frames to access temporal information like velocities.\nRather than separating contrastive learning from the downstream task as done in\nvision, we learn contrastive representations jointly with the RL objective.\nInstead of discriminating across single images, we discriminate across the\nstack of frames.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig3.png\" width=\"\" />\n<br />\n<i>\n</i>\n</p>\n\n<p>By combining contrastive learning with Deep RL in the above manner <em>we found,\nfor the first time, that pixel-based RL can be nearly as data-efficient as\nstate-based RL</em> on the DeepMind control benchmark suite. In the figure below,\nwe show learning curves for DeepMind control tasks where contrastive learning\nis coupled with SAC (red) and compared to state-based SAC (gray).</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig4.png\" width=\"\" />\n<br />\n<i>\n</i>\n</p>\n\n<p>We also demonstrate data-efficiency gains on the Atari 100k step benchmark. In\nthis setting, we couple CURL with an Efficient Rainbow DQN (Eff. Rainbow) and\nshow that CURL outperforms the prior state-of-the-art (Eff. Rainbow, SimPLe) on\n20 out of 26 games tested.</p>\n\n<h1 id=\"rl-with-data-augmentation\">RL with Data Augmentation</h1>\n\n<p>Given that random cropping was a crucial component in CURL, it is natural to\nask — can we achieve the same results with data augmentation alone? In\nReinforcement Learning with Augmented Data (RAD), we performed the first\nextensive study of data augmentation in Deep RL and found that for the DeepMind\ncontrol benchmark the answer is yes. Data augmentation alone can outperform\nprior competing methods, match, and sometimes surpass the efficiency of\nstate-based RL. Similar results were also shown in concurrent work - DrQ.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig5.png\" width=\"\" />\n<br />\n<i>\n</i>\n</p>\n\n<p>We found that RAD also improves generalization on the ProcGen game suite,\nshowing that data augmentation is not limited to improving data-efficiency but\nalso helps RL methods generalize to test-time environments.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig6.png\" width=\"\" />\n<br />\n<i>\n</i>\n</p>\n\n<p>If data augmentation works for pixel-based RL, can it also improve state-based\nmethods? We introduced a new state-based augmentation — <em>random amplitude\nscaling</em> — and showed that simple RL with state-based data augmentation\nachieves state-of-the-art results on OpenAI gym environments and outperforms\nmore complex model-free and model-based RL algorithms.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/curl/fig7.png\" width=\"\" />\n<br />\n<i>\n</i>\n</p>\n\n<h1 id=\"contrastive-learning-vs-data-augmentation\">Contrastive Learning vs Data Augmentation</h1>\n\n<p>If data augmentation with RL performs so well, do we need unsupervised\nrepresentation learning? RAD outperforms CURL because it only optimizes for\nwhat we care about, which is the task reward. CURL, on the other hand, jointly\noptimizes the reinforcement and contrastive learning objectives. If the metric\nused to evaluate and compare these methods is the score attained on the task at\nhand, a method that purely focuses on reward optimization is expected to be\nbetter as long as it implicitly ensures similarity consistencies on the\naugmented views.</p>\n\n<p>However, many problems in RL cannot be solved with data augmentations alone.\nFor example, RAD would not be applicable to environments with sparse-rewards or\nno rewards at all, because it learns similarity consistency implicitly through\nthe observations coupled to a reward signal. On the other hand, the contrastive\nlearning objective in CURL internalizes invariances explicitly and is therefore\nable to learn semantic representations from high dimensional observations\ngathered from any rollout regardless of the reward signal. Unsupervised\nrepresentation learning may therefore be a better fit for real-world tasks,\nsuch as robotic manipulation, where the environment reward is more likely to be\nsparse or absent.</p>\n\n<hr />\n\n<p>This post is based on the following papers:</p>\n\n<ul>\n  <li>\n    <p><strong>CURL: Contrastive Unsupervised Representations for Reinforcement Learning</strong><br />\nMichael Laskin*, Aravind Srinivas*, Pieter Abbeel<br />\nThirty-seventh International Conference Machine Learning (ICML), 2020.<br />\n<a href=\"https://arxiv.org/abs/2004.04136\">arXiv</a>, <a href=\"https://mishalaskin.github.io/curl/\">Project Website</a></p>\n  </li>\n  <li>\n    <p><strong>Reinforcement Learning with Augmented Data</strong><br />\nMichael Laskin*, Kimin Lee*, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas<br />\n<a href=\"https://arxiv.org/abs/2004.14990\">arXiv</a>, <a href=\"https://mishalaskin.github.io/rad/\">Project Website</a></p>\n  </li>\n</ul>\n\n<p><strong>References</strong></p>\n<font size=\"-1\">\n</font>\n<ol><font size=\"-1\">\n    <li>Hafner et al. <a href=\"https://arxiv.org/abs/1811.04551\">Learning Latent Dynamics for Planning from Pixels</a>. ICML 2019.</li>\n    <li>Hafner et al. <a href=\"https://arxiv.org/abs/1912.01603\">Dream to Control: Learning Behaviors by Latent Imagination</a>. ICLR 2020.</li>\n    <li>Kaiser et al. <a href=\"https://arxiv.org/abs/1903.00374\">Model-Based Reinforcement Learning for Atari</a>. ICLR 2020.</li>\n    <li>Lee et al. <a href=\"https://arxiv.org/abs/1907.00953\">Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model</a>. arXiv 2019.</li>\n    <li>Henaff et al. <a href=\"https://arxiv.org/abs/1905.09272\">Data-Efficient Image Recognition with Contrastive Predictive Coding</a>. ICML 2020.</li>\n    <li>He et al. <a href=\"https://arxiv.org/abs/1911.05722\">Momentum Contrast for Unsupervised Visual Representation Learning</a>. CVPR 2020.</li>\n    <li>Chen et al. <a href=\"https://arxiv.org/abs/2002.05709\">A Simple Framework for Contrastive Learning of Visual Representations</a>. ICML 2020.</li>\n    <li>Kostrikov et al. <a href=\"https://arxiv.org/abs/2004.13649\">Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels</a>. arXiv 2020.</li>\n</font></ol>\n<font size=\"-1\">\n</font>\n","descriptionType":"text/html","publishedDate":"Sun, 19 Jul 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"d6cb67c944b5aeede514d99f8b6c5e9d","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/curl/fig1.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn81@2020_2/2020/08/24/23-52-03-302_4b3c4ba1afea1ee2.webp","https://bair.berkeley.edu/static/blog/curl/fig2.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn61@2020_4/2020/08/24/23-52-09-920_88d65322b4aeb8fe.webp","https://bair.berkeley.edu/static/blog/curl/fig3.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn25@2020_1/2020/08/24/23-52-03-703_5e59b30ace3fc8d8.webp","https://bair.berkeley.edu/static/blog/curl/fig4.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn5@2020_4/2020/08/24/23-52-05-842_6e265ed6c7d4f989.webp","https://bair.berkeley.edu/static/blog/curl/fig5.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn61@2020_1/2020/08/24/23-52-07-866_8561c15867f93bc4.webp","https://bair.berkeley.edu/static/blog/curl/fig6.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn82@2020_2/2020/08/24/23-52-05-976_09d401c72350188b.webp","https://bair.berkeley.edu/static/blog/curl/fig7.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn5@2020_5/2020/08/24/23-52-05-163_e5152df3c1e6d8e2.webp"},"publishedOrCreatedDate":1598313090351},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"AI Will Change the World.<br>Who Will Change AI?<br>We Will.","link":"http://bair.berkeley.edu/blog/2020/08/16/ai4all/","description":"<meta name=\"twitter:title\" content=\"AI Will Change the World. Who Will Change AI? We Will\" />\n\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/blog/assets/BAIR_Logo.png\" />\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/ai4all/ai4all-gif-overcooked.gif\" width=\"50%\" />\n<br />\n<i>\n</i>\n</p>\n\n<p><em>Editor’s Note: The following blog is a special guest post by a recent graduate\nof Berkeley BAIR’s AI4ALL summer program for high school students.</em></p>\n\n<p>AI4ALL is a nonprofit dedicated to increasing diversity and inclusion in AI\neducation, research, development, and policy.</p>\n\n<p><a href=\"https://ai-4-all.org/about/our-story/#:~:text=AI4ALL%20is%20a%20US%2Dbased,Dr.\">The idea for AI4ALL</a> began in early 2015 with Prof. Olga Russakovsky, then\na Stanford University Ph.D. student, AI researcher Prof. Fei-Fei Li, and Rick\nSommer – Executive Director of Stanford Pre-Collegiate Studies. They founded\nSAILORS as a summer outreach program for high school girls to learn about\nhuman-centered AI, which later became AI4ALL. In 2016, Prof. Anca Dragan\nstarted the Berkeley/BAIR AI4ALL camp, geared towards high school students from\nunderserved communities.</p>\n\n<!--more-->\n\n<h2 id=\"before-i-started-the-program\">Before I Started the Program</h2>\n\n<p>When I discovered AI4ALL during the spring semester, I was curious to learn\nmore. I knew that AI had the potential to change everything and that it was\nsomething I’d love to be a part of. To prepare for the program, I read up on\nthe <a href=\"https://bair.berkeley.edu/faculty.html\">BAIR faculty</a> and checked out the <a href=\"https://bair.berkeley.edu/students.html\">BAIR student</a> profiles. I watched\nStuart Russell’s TED talk “<a href=\"https://www.ted.com/talks/stuart_russell_3_principles_for_creating_safer_ai\">3 principles for creating safer AI</a>.” The people\nwere all so highly accomplished. And their ideas seemed either super\ntechnical, or at the other end of the spectrum, they sounded more like topics from\nthe philosophy department than the EECS department. I realized I had no idea\nwhat to expect but decided just to give it a try and get started.</p>\n\n<h2 id=\"the-first-day\">The First Day</h2>\n\n<p>After logging into my first day of AI4ALL on Zoom, I was pleasantly surprised\nby the number of eager and welcoming faces. Among them were Tim Hurt, Eva Chao,\nRachel Walsh, Ben Frazier, and Maya Maliviya. They were all there to help us\nfeel comfortable and succeed!</p>\n\n<p>We started off with a quick ice-breaker introduction activity. This\nparticularly resonated with me because it wasn’t like the typical type you’d\nhave on the first day of school. Instead, we were divided into virtual breakout\nrooms and asked to find as many similarities among our peers as possible.\nThe program was already off to a great start! Within just a few minutes, I\nlearned that five other people in the room have a sibling, have taken\nchemistry, like pizza, and had a quarantine haircut just like me! It was a\ngreat way to encourage collaboration and bonding.</p>\n\n<p>Next, we were joined by BAIR lab professor Anca Dragan for a talk about AI. The\npresentation was hard to forget because of her passion, her curiosity, and the\ndepth of her knowledge. Anca kickstarted the talk by explaining some examples\nof AI in real life. This was already so useful because it immediately cleared\nup the misconceptions about AI. In addition, it allowed everyone to have\ncommon, shared learning and not feel excluded if they didn’t know as much about\nAI before starting the program.</p>\n\n<p>Another element of Anca’s presentation that stood out was her description\nof an AI game. The game is simple: a robot is positioned in a grid and gains\npoints for reaching gems and loses points for falling in fire pits. Anca walked\nus through the AI “backstory” of the game. The robot’s goal is to maximize the\npoints earned. As the game’s allotted time decreases, the robot takes less\ncautious paths (ex: avoiding fire pits) and places its primary focus on gaining\npoints. We learned that this idea of optimization is a core part of all AI\nsystems.</p>\n\n<p>By the end of the day, we were immersed in a Python notebook while conversing\nwith peers in a Breakout Room. AI4ALL equipped us with Python notebooks through\n<a href=\"https://colab.research.google.com/notebooks/intro.ipynb\">Google Colab</a> so we would all be on the same page when talking about code.\nI really enjoyed this part of the program because it was open-ended and the\nmaterial was presented in such a clean and convenient fashion. As I read\nthrough the content and completed the coding exercises, I couldn’t help but\nalso notice the amusing GIFs embedded here and there! What a memorable way to\nbegin learning AI!</p>\n\n<h2 id=\"midway-through-the-program\">Midway Through the Program</h2>\n\n<p>Early on Day 3 of the 4 day AI4ALL program, I began to really understand the\nsignificance of AI. Through the eye-opening lecture presentations and\ndiscussions, I realized that AI really is everywhere! It’s in our <a href=\"https://research.google/pubs/pub45530/\">YouTube\nrecommendations</a>, <a href=\"https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/\">Spotify algorithms</a>, <a href=\"https://medium.com/swlh/ai-google-maps-79237f8946e3\">Google Maps</a>, and robotic\nsurgery equipment. That range of applications is part of what makes AI so\npromising. AI really can be for everyone, whether you’re a developer or a user —\nit’s not limited to people with mad coding skills. Once I got acquainted with\nthe basics of the subject, I began to see how almost any idea can be reshaped\nwith AI.</p>\n\n<p>I also learned that AI is often different from the way it’s presented in the\nmedia. Almost everyone is familiar with the idea of robots taking over jobs,\nbut that isn’t necessarily what will happen. AI still has a long way to go\nbefore it will truly “take over the world,” as hypothesized. AI is a work in\nprogress. Like its creators, it has biases. It can unintentionally\ndiscriminate. It has adversaries and struggles to find insights with incomplete\ndata. Still, AI has the power to change basic aspects of our world. This is why\nit is so important to have people from as many backgrounds as possible involved\nin AI. Introducing people from many different backgrounds into the field allows\nfor a better range of ideas and can help reduce the number of missed “red\nflags” that might later have a big impact on the lives of real people.</p>\n\n<h2 id=\"by-the-end-of-the-program\">By the End of the Program…</h2>\n\n<p>The last two days of AI4ALL sped by in a blur. I couldn’t help but notice how\nwell the program was organized. There was a balanced combination of lectures,\ndiscussion, and individual work time for coding and collaborating. I also loved\nhow the content at the end of the program reinforced the content from the\nstart. That aspect of the program’s structure made it so much easier to ask\nquestions, remember ideas, and apply to future activities.</p>\n\n<p>I particularly saw this idea of reinforcement demonstrated in Professor\nKamalika Chaudhuri’s presentation about AI adversaries. She explained how AI\nalgorithms could be manipulated so that an image correctly identified with 50%\nconfidence as a panda would then identify the same image with 90% confidence as\na gibbon. On the previous day, Professor Jacob Steinhardt explained how images\nthat appeared similar to the human eye can be tweaked to disrupt AI’s\nalgorithm. In another example, Kamalika described how image pixels could be\nstored as training data in the form of vectors. This idea built off of Tim\nHurt’s earlier point that training data is a result of an input being\ntranslated into computer language (e.g. a vector $x$), and then mapped to a label\noutput ($y$).</p>\n\n<p>After most of the lectures were done, we began working on our group projects.\nWe were divided into five groups, with each group under the instruction of a\nBerkeley Ph.D. student. I chose to be in the “Overcooked” group, which was with\nfirst-year EECS student Micah Carroll. Micah walked us through the game he’s been using in his research,\ncalled <em><a href=\"https://github.com/HumanCompatibleAI/overcooked_ai\">Overcooked-AI</a></em>. Simply put, <em>Overcooked-AI</em> is all about getting the most number\nof onion soups delivered while cooking in a cramped kitchen.</p>\n\n<p>Once again, we used Colab Notebooks to learn and experiment with the game’s\ncode. Micah patiently took us through the basics of imitation learning,\nreinforcement learning, decision trees, and graph fitting/displays. He was so\nopen to questions and never hesitated to help! The hours we spent together\nbreezed by, and soon enough I found myself crafting up a final presentation\nrecapping all that I learned. Time really passes when you’re enjoying and\nlearning.</p>\n\n<h2 id=\"final-thoughts\">Final Thoughts</h2>\n\n<p>In less than a week, the AI4ALL program has shaped my view of AI and my\nlearning process. The lectures, advice panels, and project groups came together\nto make an unforgettable experience. Beyond learning what AI is and how it\nworks, I now realize that everyone has the potential to explore AI. All you\nhave to do is start. And so, the next time you hear someone say “AI will change\nthe world, but who will change AI?”, you can say with confidence “we will!”</p>\n\n<p>Thank you so much to everyone who made AI4ALL possible!</p>\n\n","descriptionType":"text/html","publishedDate":"Sun, 16 Aug 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"dad24dc607eb2107cabc3dc273905bfc","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/ai4all/ai4all-gif-overcooked.gif":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn37@2020_4/2020/08/24/23-52-11-347_6116ec63f2a0601a.webp"},"publishedOrCreatedDate":1598313090351},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Estimating the fatality rate is difficult but doable with better data","link":"http://bair.berkeley.edu/blog/2020/08/03/covid-fatality/","description":"<meta name=\"twitter:title\" content=\"Estimating the fatality rate is difficult but doable with better data\" /> \n<meta name=\"twitter:card\" content=\"summary_image\" /> \n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/cfr/graphical-model.png\" /> \n<p>The case fatality rate quantifies how dangerous COVID-19 is, and how risk of death varies with strata like geography, age, and race. Current estimates of the COVID-19 case fatality rate (CFR) are biased for dozens of reasons, from under-testing of asymptomatic cases to government misreporting. We provide a careful and comprehensive overview of these biases and show how statistical thinking and modeling can combat such problems. Most importantly, data quality is key to unbiased CFR estimation. We show that a relatively small dataset collected via careful contact tracing would enable simple and potentially more accurate CFR estimation.</p> \n<!--more--> \n<h1 id=\"what-is-the-case-fatality-rate-and-why-do-we-need-to-estimate-it\">What is the case fatality rate, and why do we need to estimate it?</h1> \n<p>The case fatality rate (CFR) is the proportion of fatal COVID-19 cases. The term is ambiguous, since its value depends on the definition of a ‘case.’ No perfect definition of the case fatality rate exists, but in this article, I define it loosely as the proportion of deaths among all COVID-19-infected individuals.</p> \n<p>The CFR is a measure of disease severity. Furthermore, the relative CFR (the ratio of CFRs between two subpopulations) enables data-driven resource-allocation by quantifying relative risk. In other words, the CFR tells us how drastic our response needs to be; the relative CFR helps us allocate scarce resources to populations that have a higher risk of death.</p> \n<p>Although the CFR is defined as the number of fatal infections, we can not expect that dividing the number of deaths by the number of cases will give us a good estimate of the CFR. The problem is that both the numerator (#deaths) and the denominator (#infections) of this fraction are uncertain for systematic reasons due to the way data is collected. For this reason, we call that estimator “<strong>the naive estimator</strong>”, or simply <strong>deaths/cases</strong>, and denote it as $E_{\\rm naive}$.</p> \n<h1 id=\"why-are-all-cfr-estimates-biased\">Why are (all) CFR estimates biased?</h1> \n<figure> \n <img src=\"https://bair.berkeley.edu/static/blog/cfr/graphical-model.png\" alt=\"Graphical model.\" style=\"display: block; width=80%\" /> \n <figcaption> \n  <b>Fig. 1</b> Dozens of biases can corrupt the estimation of the CFR. Surveillance data gives partial information within the ‘sampling frame’ (light blue rectangle). Edges on the graph correspond roughly to conditional probabilities; e.g., the edge from D to DF is the probability a person dies if they are diagnosed with COVID-19. \n </figcaption> \n</figure> \n<p>In short, all CFR estimates are biased because the publicly available data is biased. We have reasson to believe that <a href=\"https://hdsr.mitpress.mit.edu/pub/l7a2t45s/release/1\">we are losing at least 99.8% of our sample efficiency due to this bias</a>. There is a “butterfly effect” caused by non-random sampling: a tiny correlation between the sampling method and the quantity of interest can have huge, destructive effects on an estimator. Even assuming a tiny 0.005 correlation between the population we test and the population infected, testing 10,000 people for SARS-CoV-2 is equivalent to testing 20 individuals randomly. For estimating the fatality rate, the situation is even worse, since we have ample evidence that severe cases are preferentially diagnosed and reported. In the words of Xiao-Li Meng, “<a href=\"https://statistics.fas.harvard.edu/files/statistics-2/files/statistical_paradises_and_paradoxes.pdf\">compensating for [data] quality with quantity is a doomed game</a>.” In our HDSR article, we show that in order for the naive estimator $E_{\\rm naive}$ to converge to the correct CFR, there must be no correlation between fatality and being tested — but severe cases are much more likely to be tested. Government and health organizations have been explicitly reserving tests for severe cases due to shortages, and severe cases are likely to go to the hospital and get tested, while asymptomatic ones are not.</p> \n<p>The primary source of COVID-19 data is population surveillance: county-level aggregate statistics reported by medical providers who diagnose patients on-site. Usually, somebody feels sick and goes to a hospital, where they get tested and diagnosed. The hospital reports the number of cases, deaths, and sometimes recoveries to local authorities, who release the data on a weekly basis. In reality, there are many differences in data collection between nations, local governments, and even hospitals.</p> \n<p>Dozens of biases are induced by this method of surveillance, falling coarsely into five categories: under-ascertainment of mild cases, time lags, interventions, group characteristics (e.g. age, sex, race), and imperfect reporting and attribution. An extensive (but not exhaustive) discussion of the magnitude and direction of these biases is in Section 2 of our article. Without mincing words, current data is extremely low quality. The vast majority of people who get COVID-19 go undiagnosed, there are misattributions of symptoms and deaths, data reported by governments is often (and perhaps purposefully) incorrect, cases are defined inconsistently across countries, and there are many time-lags. (For example, cases are counted as ‘diagnosed’ before they are ‘fatal’, leading to a downward bias in the CFR if the number of cases is growing over time.) <strong>Figure 1</strong> has a graphical model describing these many relationships; look to the paper for a detailed explanation of what biases occur across each edge.</p> \n<p>Correcting for biases is sometimes possible using outside data sources, but can result in a worse estimator overall due to partial bias cancellation. This is easier to see through example than it is to explain. Assume the true CFR is some value $p$ in the range 0 to 1 (i.e., deaths/infections is equal to $p$). Then, assume that because of under-ascertainment of mild cases, there are too many fatal cases being reported, which means $E_{\\rm naive}$ converges to $bp &gt; p$ (in other words, it is higher than it should be by a factor of $b$). Also assume the time-lag between diagnosis and death causes the proportion of deaths to diagnoses to decrease by the same factor $b$. Then, $E_{\\rm naive}$ converges to $b(p/b)=p$, the correct value. So, even though it might seem to be an objectively good idea to correct for time-lag between diagnosis and death, it would actually result in a worse estimator in this case, since time-lag is helping us out by cancelling out under-ascertainment.</p> \n<p>The mathematical form of the naive estimator $E_{\\rm naive}$ allows us to see easily what we need to do to make it unbiased. With $p$ being the true CFR, $q$ being the reporting rate, and $r$ being the covariance between death and diagnosis, the mean of $E_{\\rm naive}$ is:</p> \n<script type=\"math/tex; mode=display\">\\mu = (\\frac{r}{q} + p)(1 - (1-q)^N)</script> \n<p>This equation is pretty easy to understand. We wanted $\\mu$ to be equal to $p$. Instead, we got an expression that depends on $r$, $q$, and $N$. The $r/q$ term is the price we pay if people who are diagnosed are more likely to eventually die. We want $r/q=0$, but in practice, $r/q$ is probably much larger than $p$. (Actually, if we assume the CFR is around 0.5% and the measured CFR is 5.2% on June 22, 2020, then $r/q \\ge 0.047 \\gg 0.005$.) In other words, $r/q$ is the bias, and it can be large. The term $p$ is the true CFR, which we want. And the factor $(1−(1−q)^N)$ is what we pay because of non-response; however, it’s not a big deal, because it disappears quite fast as the number of samples $N$ grows. So really, our primary concern should be achieving $r=0$, because — and I cannot stress this enough — <b>$r/q$ does not decrease with more samples; it only decreases with higher quality samples</b>.</p> \n<h1 id=\"what-are-strategies-for-fixing-the-bias\">What are strategies for fixing the bias?</h1> \n<p>In our article, we outline a testing procedure that helps fix some of the above dataset biases. If we collect data properly, even the naive estimator $E_{\\rm naive}$ has good performance.</p> \n<p>In particular, data should be collected via a procedure like the following:</p> \n<ol> \n <li> <p>Diagnose person $P$ with COVID-19 by any means, like at a hospital.</p> </li> \n <li> <p>Reach out to contacts of $P$. If a contact has no symptoms, ask them to commit to getting a COVID-19 test.</p> </li> \n <li> <p>Test committed contacts after the virus has incubated.</p> </li> \n <li> <p>Keep data with maximum granularity while respecting ethics/law.</p> </li> \n <li> <p>Follow up after a few weeks to ascertain the severity of symptoms.</p> </li> \n <li> <p>For committed contacts who didn’t get tested, call and note if they are asymptomatic.</p> </li> \n</ol> \n<p>This protocol is meant to decrease the covariance between fatality and diagnosis. If patients commit to testing before they develop symptoms, this covariance simply cannot exist. However, there may still be issues with people dropping out of the study; if this is a problem in practice, it can be mitigated by a combination of incentives (payments) and consistent follow-ups.</p> \n<div class=\"halffigure\"> \n <img src=\"https://bair.berkeley.edu/static/blog/cfr/histograms.png\" alt=\"A figure.\" style=\"float:right; background-color:#fff; width:100%; height:50%; vertical-align:right; align: right\" /> \n <figcaption style=\"float:right ; width:100%; vertical-align:right; position:relative; margin-bottom: 1em\"> \n  <b>Fig. 2</b> Assuming data collection induces no correlation between disease severity and diagnosis, as the true CFR decreases, it requires more samples to estimate. The variable \n  <i>p</i> is the true CFR, and \n  <i>q</i> is the response rate. Each histogram represents the probability the naive estimator will take on a certain value, given \n  <i>N</i> samples of data (different colors correspond to different values of \n  <i>N</i>). The three stacked plots correspond to different values of \n  <i>p</i>; the smaller \n  <i>p</i> is, the harder it is to estimate, since death becomes an extremely rare event. \n </figcaption> \n</div> \n<p><strong>Figure 2</strong> represents an idealized version of this study. In the best case scenario, there is no covariance between death and diagnosis. Then, we only need $N=66$ samples for our estimator of the CFR to be approximately unbiased, even if $p=0.001$ ($1/1000$ cases die). Problems remain in the case that $p$ is small; namely, death is so rare that we need tons of samples to decrease the variance of our estimator. This will require lots of samples. But even if no deaths are observed, we get a lot of information about $p$; for example, if $N=1000$ and we have not observed a single death, then we can confidently say that $p&lt;0.01$ within the population we are sampling. This is simply because in the second panel of <strong>Figure 2</strong>, there is nearly zero mass in the $N=1000$ histogram at $E_{\\rm naive} = 0$. With this in mind, we could find the largest possible p that is consistent with our data — this would be a conservative upper bound on $p$, but it would be much closer to the true value than we can get with current data.</p> \n<p>This strategy mostly resolves what we believe is the largest set of biases in CFR estimation — under-ascertainment of mild cases and time-lags. However, there will still be lots of room for improvement, like understanding the dependency of CFR on age, sex, and race. (In other words, the CFR is a random quantity itself, depending on the population being sampled.) Distinctions between CFRs of these strata may be quite small, requiring a lot of high-quality data to analyze. If $p$ is extremely low, like 0.001, this may require collecting $N=100,000$ or $N=1,000,000$ samples <em>per group</em>. Perhaps there are ways to lower that number by pooling samples. Even though making correct inferences will require careful thought (as always), this data collection strategy will make it much simpler.</p> \n<p>I’d like to re-emphasize a point here: collecting data as above will make the naive estimator $E_{\\rm naive}$ <em>unbiased for the sampled population</em>. But the sampled population may not be the population we care about. However, there is a set of statistical techniques collectively called ‘post-stratification’ that can help deal with this problem effectively — see <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=08B82F2D1BB0D1C5EEDE33856D698AF5?doi=10.1.1.44.5270&amp;rep=rep1&amp;type=pdf\">Mr. P</a>.</p> \n<p>If you read our academic article, we provide some thoughts on how to use time-series data and outside information to correct time-lags and relative reporting rates. Our work was very heavily based on one of <a href=\"https://www.umass.edu/sphhs/person/faculty/nicholas-g-reich\">Nick Reich</a>’s papers. However, as I claimed earlier, even fancy estimators cannot overcome fundamental problems with data collection. I’ll defer discussion of that estimator, and the results we got from it, to the article. I’d love to hear your thoughts on it.</p> \n<p>CFR estimation is clearly a difficult problem — but with proper data collection and estimation guided by data scientists, I still believe that we can get a useful CFR estimate. This will help guide public policy decisions about this urgent and ongoing pandemic.</p> \n<hr /> \n<p>This blog post is based on the following paper:</p> \n<ul> \n <li><strong><a href=\"http://bair.berkeley.edu/blog/2020/08/03/covid-fatality/&quot;https://hdsr.mitpress.mit.edu/pub/y9vc2u36/release/6&quot;\">On Identifying and Mitigating Bias in the Estimation of the COVID-19 Case Fatality Rate</a></strong>.<br /> Anastasios Angelopoulos, Reese Pathak, Rohit Varma, Michael I. Jordan<br /> Harvard Data Science Review Special Issue 1 — COVID-19: Unprecedented Challenges and Chances. 2020.</li> \n</ul>","descriptionType":"text/html","publishedDate":"Mon, 03 Aug 2020 09:00:00 +0000","feedId":8140,"bgimg":"https://bair.berkeley.edu/static/blog/cfr/graphical-model.png","linkMd5":"966c30164ed85af0624eb0250bbb5312","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn32@2020_5/2020/08/24/23-51-39-014_f801ef3c4112c238.webp","destWidth":2981,"destHeight":2479,"sourceBytes":746473,"destBytes":413318,"author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/cfr/graphical-model.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn32@2020_5/2020/08/24/23-51-39-014_f801ef3c4112c238.webp","https://bair.berkeley.edu/static/blog/cfr/histograms.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn42@2020_3/2020/08/24/23-52-03-896_4ae3457cc0279593.webp"},"publishedOrCreatedDate":1598313090351},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"D4RL: Building Better Benchmarks for Offline Reinforcement Learning","link":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","description":"<!--\nTODO TODO TODO personal reminder for Daniel Seita :-)\nBe careful that these three lines are at the top,\nand that the title and image change for each blog post!\n-->\n<meta name=\"twitter:title\" content=\"D4RL: Building Better Benchmarks for Offline Reinforcement Learning\" />\n<p><!--TODO--></p>\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/d4rl/maze2d_medium.gif\" />\n\n<p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204105143_kitchen-mixed.gif\" height=\"250\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204097994_carla_lane.gif\" height=\"250\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204119188_ant_medium-250.gif\" height=\"250\" width=\"\" />\n</p>\n\n<!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204105143_kitchen-mixed.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204097994_carla_lane.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204119188_ant_medium-250.gif)\n-->\n\n<p>In the last decade, one of the biggest drivers for success in machine learning has arguably been the rise of high-capacity models such as <a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">neural networks</a> along with large datasets such as <a href=\"http://image-net.org\">ImageNet</a> to produce accurate models. While we have seen deep neural networks being applied to success in reinforcement learning (RL) in domains such as <a href=\"https://arxiv.org/abs/1806.10293\">robotics</a>, <a href=\"https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf\">poker</a>, <a href=\"https://arxiv.org/abs/1712.01815\">board games</a>, and <a href=\"https://openai.com/projects/five/\">team-based video games</a>, a significant barrier to getting these methods working on real-world problems is the difficulty of large-scale online data collection. Not only is online data collection time-consuming and expensive, it can also be dangerous in safety-critical domains such as driving or healthcare. For example, it would be unreasonable to allow reinforcement learning agents to explore, make mistakes, and learn while controlling an autonomous vehicle or treating patients in a hospital. This makes learning from pre-collected experience enticing, and we are fortunate in that many of these domains, there already exist large datasets for applications such as <a href=\"https://bdd-data.berkeley.edu\">self-driving cars</a>, <a href=\"https://mimic.physionet.org\">healthcare</a>, or <a href=\"https://www.robonet.wiki\">robotics</a>. Therefore, the ability for RL algorithms to learn <em>offline</em> from these datasets (a setting referred to as offline or batch RL) has an enormous potential impact in shaping the way we build machine learning systems for the future.</p>\n\n<!--more-->\n\n<p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341727777_ezgif.com-optimize-3.gif\" height=\"\" width=\"\" /><br />\n<i>\nIn off-policy RL, the algorithm learns from experience collected online from an\nexploration or behavior policy.\n</i>\n</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341504303_ezgif.com-optimize-2.gif\" height=\"\" width=\"\" /><br />\n<i>\nIn offline RL, we assume all experience is collected offline, fixed and no\nadditional data can be collected.\n</i>\n</p>\n\n<!--\n![In off-policy RL, the algorithm learns from experience collected online from an exploration or behavior policy.](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341727777_ezgif.com-optimize-3.gif)\n\n![In offline RL, we assume all experience is collected offline, fixed and no additional data can be collected.](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341504303_ezgif.com-optimize-2.gif)\n-->\n\n<p>The predominant method for benchmarking offline deep RL has been limited to a single scenario: the dataset is generated from some random or previously trained policy, and the goal of the algorithm is to improve in performance over the original policy [i.e., <a href=\"https://arxiv.org/abs/1812.02900\">1</a>,<a href=\"http://papers.nips.cc/paper/9349-stabilizing-off-policy-q-learning-via-bootstrapping-error-reduction\">2</a>,<a href=\"https://arxiv.org/abs/1911.11361\">3</a>,<a href=\"https://arxiv.org/abs/1910.00177\">4</a>,<a href=\"https://research.google/pubs/pub49020/\">5</a>,<a href=\"http://papers.nips.cc/paper/8503-dualdice-behavior-agnostic-estimation-of-discounted-stationary-distribution-corrections\">6</a>]. The problem with this approach is that real-world datasets are unlikely to be generated by a single RL-trained policy, and the many of the situations not covered by this evaluation method are unfortunately known to be problematic for RL algorithms. This makes it difficult to know how well our algorithms will perform when actually used outside of these benchmark tasks.</p>\n\n<p>In order to develop effective algorithms for offline RL, we need widely available benchmarks that are easy to use and can accurately measure progress on this problem. Using real-world data, such as in autonomous driving, would make a great indicator for progress, but evaluation of the algorithm becomes a challenge. Most research labs do not have the resources to deploy their algorithm on a real vehicle in order to test if their method really works. To fill the gap between realistic but infeasible real-world tasks, and the somewhat lacking but easy-to-use simulated tasks, we recently introduced the <a href=\"https://sites.google.com/view/d4rl/home\">D4RL benchmark</a> (Datasets for Deep Data-Driven Reinforcement Learning) for offline RL. The goal of D4RL is simple: we propose tasks that are designed to exercise dimensions of the offline RL problem which may make real-world application difficult, while keeping the entire benchmark in simulated domains that allow any researcher around the world to efficiently evaluate their method. In total, the D4RL benchmark includes over 40 tasks across 7 qualitatively distinct domains that cover application areas such as robotic manipulation, navigation, and autonomous driving.</p>\n\n<h2 id=\"what-properties-make-offline-rl-difficult\">What properties make offline RL difficult?</h2>\n\n<p>In <a href=\"https://bair.berkeley.edu/blog/2019/12/05/bear/\">a previous blog post</a>, we discussed that simply running an ordinary off-policy RL algorithm on the offline problem is typically insufficient, and in the worst case can cause the algorithm to diverge. There are in fact many factors which can cause poor performance for RL algorithms, which we use to guide the design of D4RL.</p>\n\n<p><strong>Narrow and biased data distributions</strong> are a common property in real-world datasets that can create problems for offline RL algorithms. By narrow data distributions, we mean that the dataset lacks significant coverage in the state-action space of the problem. A narrow data distribution does not by itself mean the task is unsolvable - for example, expert demonstrations often produce narrow distributions <a href=\"https://arxiv.org/pdf/1902.10250.pdf\">which can make learning difficult</a>. An intuitive reason for why narrow data distributions are difficult to learn from is that they often lack the mistakes that are necessary for an algorithm to learn. For example, in healthcare, datasets are often biased towards serious cases. We may only see very sick patients get treated with medicine (with a small percentage of them living), and mildly sick patients sent home without treatment (with nearly all of them living). A naive algorithm could learn that treatment causes death, but this is simply because we never see sick patients not treated, upon which we would learn that the survival rate for treatment is much higher.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591922248906_d4rl_medicine.png\" height=\"\" width=\"\" />\n</p>\n\n<p><strong>Data generated from non-representable policies</strong> can arise from several realistic situations. For example, human demonstrators can use cues not observable to an RL agent, leading to partial observability issues. Some controllers can utilize state or memory to create data that cannot be represented by any Markovian policy. These situations can cause several issues for offline RL algorithms. First, being unable to represent the policy class has been shown to <a href=\"https://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration.pdf\">create bias in the Q-learning algorithm</a>. Second, a crucial step in many offline RL algorithms, such as those based on importance weighting, is to estimate the action probabilities in the dataset. Being unable to represent the policy that generated the data can create an additional source of error.</p>\n\n<p><strong>Multitask and undirected data</strong> is a property we believe will be prevalent among large, cheaply collected datasets. Imagine if you were an RL practitioner and wished to procure a large dataset for training dialog agents and personal assistants. The simplest method to do so would be to simply log conversations between real humans, or scrape real conversations from the internet. In these situations, the recorded conversations may not have anything to do with a particular task you are trying to accomplish, such as booking a flight. However, many pieces of the conversation flow could be useful and and applicable to more than just the conversation it was observed in. One way to visualize this effect is with the following example. Imagine if an agent was trying to get from point A to C, but only observes paths from A to B and B to C. An agent can `stitch’ together the corresponding halves from the observed paths to form the shortest path from A to C.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591915803624_stitching-3.png\" height=\"\" width=\"\" />\n</p>\n\n<p><strong>Suboptimal data</strong> is another property which we expect to observe in realistic datasets because it may not always be feasible to have expert demonstrations for every task we may wish to solve. In many domains, such as in robotics, providing expert demonstrations is both tedious and time consuming. A key advantage of using RL instead of a method such as imitation learning is that RL gives us a clear signal for how to improve a policy, meaning we can learn and improve from a much broader scope of datasets.</p>\n\n<h2 id=\"d4rl-tasks\">D4RL Tasks</h2>\n\n<p>In order to capture the properties we outlined above, we introduce tasks spanning a wide variety of qualitatively different domains. All of the domains aside from Maze2D and AntMaze were originally proposed by other ML researchers, and we have adapted their work and generated datasets for use in the offline RL setting.</p>\n\n<ul>\n  <li>\n    <p>We begin with 3 navigation domains of increasing difficulty. The easiest of the domains is the Maze2D domain, which tries to navigate a ball along a 2D plane to a target goal location. There are 3 possible maze layouts (umaze, medium, and large). The AntMaze domain replaces the ball with the “Ant” quadraped robot, providing a more challenging low-level locomotion problem.</p>\n\n    <p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188887659_maze2d_umaze_300.gif\" height=\"240\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188894245_maze2d_medium_300.gif\" height=\"240\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188898461_maze2d_large_300.gif\" height=\"240\" width=\"\" />\n</p>\n\n    <p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188923068_ant-umaze-300.gif\" height=\"240\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188931174_ant_medium-250.gif\" height=\"240\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188940259_antmaze_large_300.gif\" height=\"240\" width=\"\" />\n</p>\n\n    <!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188887659_maze2d_umaze_300.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188894245_maze2d_medium_300.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188898461_maze2d_large_300.gif)\n\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188923068_ant-umaze-300.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188931174_ant_medium-250.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188940259_antmaze_large_300.gif)\n-->\n\n    <!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195697818_antmaze-large-play-v0.png)\n-->\n    <p>The navigation environments allow us to heavily test the “multitask and undirected” data property, since it is possible to “stitch” together existing trajectories to solve the task. The trajectories in the dataset of the large AntMaze task is shown below, with each trajectory plotted in a different color and the goal marked with a star. By learning how to repurpose existing trajectories, the agent can potentially solve the task without being forced to rely on extrapolating to unseen regions of the state space.</p>\n  </li>\n</ul>\n\n<p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195697818_antmaze-large-play-v0.png\" height=\"300\" width=\"\" />\n</p>\n\n<p>For a realistic, vision-based navigation task, we use the <a href=\"http://carla.org\">CARLA</a> simulator. This task adds a layer of perceptual challenge on top of the two aforementioned Maze2D and AntMaze tasks.</p>\n\n<p style=\"text-align:center;\">\n  <img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592192896126_carla-town-opt-200.gif\" width=\"\" height=\"250\" />\n  <img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195814701_carla_trajectories.png\" width=\"\" height=\"250\" />\n  </p>\n\n<p><!--\n  ![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592192896126_carla-town-opt-200.gif)\n  ![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195814701_carla_trajectories.png)\n  --></p>\n\n<ul>\n  <li>\n    <p>We also include 2 realistic robotic manipulation tasks, using the <a href=\"https://arxiv.org/abs/1709.10087\">Adroit</a> (based on the Shadow Hand robot) and the <a href=\"https://arxiv.org/pdf/1910.11956.pdf\">Franka</a> platforms. The Adroit domain contains 4 separate manipulation tasks, as well as human demonstrations recorded via motion capture. This provides a platform for studying the use of human-generated data within a simulated robotic platform.</p>\n\n    <p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174975902_hammer-250.gif\" width=\"\" height=\"180\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174981370_pen-250.gif\" width=\"\" height=\"180\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174985204_relocate-250.gif\" width=\"\" height=\"180\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174991459_door-250.gif\" width=\"\" height=\"180\" />\n</p>\n    <!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174975902_hammer-250.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174981370_pen-250.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174985204_relocate-250.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174991459_door-250.gif)\n-->\n\n    <p>The Franka kitchen environment places a robot in a realistic kitchen environment where objects can be freely interacted with. These include opening the microwave and various cabinets, moving the kettle, and turning on the lights and burners. The task is to reach a desired goal configuration of the objects in the scene.</p>\n\n    <p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188767696_kitchen-complete-v0.gif\" width=\"\" height=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520341217_kitchen-partial.gif\" width=\"\" height=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520372323_kitchen-mixed.gif\" width=\"\" height=\"\" />\n</p>\n    <!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188767696_kitchen-complete-v0.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520341217_kitchen-partial.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520372323_kitchen-mixed.gif)\n-->\n  </li>\n  <li>\n    <p>We include two tasks from the Flow benchmark (which has been covered in a <a href=\"https://bair.berkeley.edu/blog/2019/06/03/benchmarks/\">previous blog post</a>). The Flow project proposes to use autonomous vehicles for reducing traffic congestion, which we believe is a compelling use case for offline RL. We include a ring layout and a highway merge layout.</p>\n\n    <p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174922113_flow-ring.gif\" height=\"\" width=\"\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174938549_flow-merge.gif\" height=\"\" width=\"\" />\n</p>\n    <!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174922113_flow-ring.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174938549_flow-merge.gif)\n-->\n  </li>\n  <li>\n    <p>Finally, we also include datasets for HalfCheetah, Hopper, and Walker2D from the OpenAI Gym Mujoco benchmark. These tasks have been used extensively in prior work [<a href=\"https://arxiv.org/abs/1812.02900\">1</a>,<a href=\"http://papers.nips.cc/paper/9349-stabilizing-off-policy-q-learning-via-bootstrapping-error-reduction\">2</a>,<a href=\"https://arxiv.org/abs/1911.11361\">3</a>,<a href=\"https://arxiv.org/abs/1910.00177\">4</a>], and in order to ensure that evaluations are comparable across papers, we have standardized the datasets.</p>\n\n    <p style=\"text-align:center;\">\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188978363_mjc-200.gif\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188981020_mjc-200-2.gif\" />\n<img src=\"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188989101_mjc-200-3.gif\" />\n</p>\n    <!--\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188978363_mjc-200.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188981020_mjc-200-2.gif)\n![](https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188989101_mjc-200-3.gif)\n-->\n  </li>\n</ul>\n\n<h2 id=\"future-directions\">Future Directions</h2>\n\n<p>In the near future, we would be excited to see offline RL applications move from simulated domains to real-world domains where significant amounts of offline data is easily obtainable. There are many real-world problems in which offline RL could make a significant impact, including:</p>\n\n<ul>\n  <li>Autonomous Driving</li>\n  <li>Robotics</li>\n  <li>Healthcare and treatment plans</li>\n  <li>Education</li>\n  <li>Recommender Systems</li>\n  <li>Dialog agents</li>\n  <li>And many more!</li>\n</ul>\n\n<p>In many of these domains, there already exist large, pre-collected datasets of experience that are waiting to be used by an carefully designed offline RL algorithm. We believe that offline RL holds great promise as a potential paradigm to leverage vast amounts of existing sequential data within the flexible decision making framework of reinforcement learning.</p>\n\n<p>If you’re interested in this benchmark, the code is available <a href=\"https://github.com/rail-berkeley/d4rl\">open-source on Github</a>, and you can check out our <a href=\"https://sites.google.com/view/d4rl/home\">website</a> for more details.</p>\n\n<hr />\n\n<p>This blog post is based on the our recent paper:</p>\n\n<ul>\n  <li><strong>D4RL: Datasets for Deep Data-Driven Reinforcement Learning</strong><br />\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine<br />\n<a href=\"https://arxiv.org/abs/2004.07219\">arXiv:2004.07219</a></li>\n</ul>\n","descriptionType":"text/html","publishedDate":"Thu, 25 Jun 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"b97829498e5b7566a24b65576deb1a00","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204105143_kitchen-mixed.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn77@2020_5/2020/08/24/23-52-03-784_4ec075e66c31aac1.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204097994_carla_lane.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn29@2020_3/2020/08/24/23-52-07-353_f9e54a4eca43fef7.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204119188_ant_medium-250.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn22@2020_1/2020/08/24/23-52-03-991_9971ccb4db10f870.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341727777_ezgif.com-optimize-3.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn13@2020_5/2020/08/24/23-52-06-506_abba22d205466a45.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341504303_ezgif.com-optimize-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn93@2020_6/2020/08/24/23-52-04-318_a8540d769ea0266e.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591922248906_d4rl_medicine.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn13@2020_2/2020/08/24/23-52-03-560_4edfb6d299f30f07.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591915803624_stitching-3.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn85@2020_2/2020/08/24/23-52-02-780_daf8cae7d12aa9c4.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188887659_maze2d_umaze_300.gif":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn77@2020_3/2020/08/24/23-52-05-809_616603c7ef3692da.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188894245_maze2d_medium_300.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn42@2020_1/2020/08/24/23-52-03-503_0a72065c90f7c82e.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188898461_maze2d_large_300.gif":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn33@2020_2/2020/08/24/23-52-08-144_c90d57f361421ad2.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188923068_ant-umaze-300.gif":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn97@2020_6/2020/08/24/23-52-04-331_f8361882d0068262.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188931174_ant_medium-250.gif":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn9@2020_6/2020/08/24/23-52-03-545_b77da27892922a9e.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188940259_antmaze_large_300.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn57@2020_4/2020/08/24/23-52-04-608_7a8d27e612817a8b.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195697818_antmaze-large-play-v0.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn5@2020_2/2020/08/24/23-52-07-410_197a1782678e8875.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592192896126_carla-town-opt-200.gif":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn49@2020_3/2020/08/24/23-52-04-263_8a167ceed1511ee2.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195814701_carla_trajectories.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn18@2020_5/2020/08/24/23-52-04-067_8d58bcac6c23e87b.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174975902_hammer-250.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn66@2020_1/2020/08/24/23-52-06-856_8b598e1abcdda9d5.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174981370_pen-250.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn69@2020_6/2020/08/24/23-52-06-430_bca07b26c6dffbe9.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174985204_relocate-250.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn9@2020_4/2020/08/24/23-52-04-346_5dff390e712b0ee7.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174991459_door-250.gif":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn10@2020_2/2020/08/24/23-52-05-431_0b3b911768217d91.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188767696_kitchen-complete-v0.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn46@2020_5/2020/08/24/23-52-03-080_3168f810d9f157e3.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520341217_kitchen-partial.gif":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn21@2020_5/2020/08/24/23-52-06-635_f8a2a42ed0808695.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520372323_kitchen-mixed.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn82@2020_5/2020/08/24/23-52-06-419_1e5326835314f049.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174922113_flow-ring.gif":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_5/2020/08/24/23-52-03-727_f623c63869aac133.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174938549_flow-merge.gif":"https://cdn.jsdelivr.net/gh/myreaderx/cdn37@2020_6/2020/08/24/23-52-03-213_c171f2e9545b1453.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188978363_mjc-200.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn93@2020_6/2020/08/24/23-52-06-058_55a69dc7ddf9513d.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188981020_mjc-200-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_5/2020/08/24/23-52-10-058_c1c4a7c28548eff7.webp","https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188989101_mjc-200-3.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn34@2020_6/2020/08/24/23-52-03-302_74b243d90bec507d.webp"},"publishedOrCreatedDate":1598313090352},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Four Novel Approaches to Manipulating Fabric using Model-Free and Model-Based Deep Learning in Simulation","link":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","description":"<!--\nTODO TODO TODO personal reminder for Daniel Seita :-)\nBe careful that these three lines are at the top,\nand that the title and image change for each blog post!\n-->\n<meta name=\"twitter:title\" content=\"Four Novel Approaches to Manipulating Fabric using Model-Free and Model-Based Deep Learning in Simulation\" />\n\n<meta name=\"twitter:card\" content=\"summary_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_corners_analytic_v04.png\" />\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/daniel_IROS-2020-example-cloth-optim.gif\" height=\"250\" />\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/wilson_intro_gif.gif\" height=\"250\" />\n<br />\n</p>\n\n<p>Humans manipulate 2D deformable structures such as fabric on a daily basis,\nfrom putting on clothes to making beds. Can robots learn to perform similar\ntasks? Successful approaches can advance applications such as dressing\nassistance for senior care, folding of laundry, fabric upholstery, bed-making,\nmanufacturing, and other tasks. Fabric manipulation is challenging, however,\nbecause of the difficulty in modeling system states and dynamics, meaning that\nwhen a robot manipulates fabric, it is hard to predict the fabric’s resulting\nstate or visual appearance.</p>\n\n<p>In this blog post, we review four recent papers from two research labs (<a href=\"https://people.eecs.berkeley.edu/~pabbeel/\">Pieter\nAbbeel</a>’s and <a href=\"https://goldberg.berkeley.edu/\">Ken Goldberg</a>’s) at Berkeley AI Research (BAIR) that\ninvestigate the following hypothesis: is it possible to employ learning-based\napproaches to the problem of fabric manipulation?</p>\n\n<p>We demonstrate promising results in support of this hypothesis by using a\nvariety of learning-based methods with <em>fabric simulators</em> to train smoothing\n(and even folding) policies in simulation. We then perform sim-to-real transfer\nto deploy the policies on physical robots. Examples of the learned policies in\naction are shown in the GIFs above.</p>\n\n<p>We show that deep model-free methods trained from exploration or from\ndemonstrations work reasonably well for specific tasks like smoothing, but it\nis unclear how well they generalize to related tasks such as folding. On the\nother hand, we show that deep model-based methods have more potential for\ngeneralization to a variety of tasks, provided that the learned models are\nsufficiently accurate.  In the rest of this post, we summarize the papers,\nemphasizing the <em>techniques and tradeoffs</em> in each approach.</p>\n\n<!--more-->\n\n<h1 id=\"model-free-methods\">Model-Free Methods</h1>\n\n<h2 id=\"model-free-learning-without-demonstrations\">Model-Free Learning without Demonstrations</h2>\n\n<ul>\n  <li>Yilin Wu*, Wilson Yan*, Thanard Kurutach, Lerrel Pinto, Pieter Abbeel.<br />\n<strong><a href=\"https://arxiv.org/abs/1910.13439\">Learning to Manipulate Deformable Objects Without Demonstrations</a></strong><br />\nRobotics: Science and Systems, 2020. <a href=\"https://sites.google.com/view/alternating-pick-and-place\">Project Website with Code</a>.</li>\n</ul>\n\n<p><a href=\"https://arxiv.org/abs/1910.13439\">In this paper</a> we present a model-free deep reinforcement learning approach\nfor smoothing cloth. We use a <a href=\"https://arxiv.org/abs/1801.00690\">DM Control environment</a> with <a href=\"http://www.mujoco.org/\">MuJoCo</a>.\nWe emphasize two key innovations that help us accelerate training: a factorized\npick-and-place policy, along with learning the place policy conditioned on\nrandom pick points, and then choosing pick point by maximum value. The figure\nbelow shows a visualization.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper1_fig1.png\" />\n<br />\n<i>\nAs opposed to directly learning both the pick and place policy (a), our method\nlearns each component of a factorized pick-and-place model independently by\nfirst training with a place policy with random pick locations, and then\nlearning the pick policy.\n</i>\n</p>\n\n<p>Jointly training the pick and place policies may result in inefficient\nlearning. Consider the degenerate scenario when the pick policy collapses into\na suboptimal restrictive set of points. This would inhibit exploration of the\nplace policy since rewards come only after the pick and place actions are\nexecuted. In order to solve this problem, our method proposes to first use\n<a href=\"https://bair.berkeley.edu/blog/2018/12/14/sac/\">Soft Actor Critic (SAC)</a>, a state-of-the-art model-free deep reinforcement\nlearning algorithm, to learn a place policy conditioned on <em>pick points sampled\nuniformly from valid pick points on the cloth</em>.  Then, we characterize the pick\npolicy by selecting the point with the highest value from the approximated\nvalue estimator learned when training the place policy, thus Maximal Value\nunder Placing (MVP). We note that our approach is not tied to SAC, and can work\nwith any off-policy learning algorithm.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper1_fig2.png\" />\n<br />\n<i>\nAn example of real robot cloth smoothing experiments with varying starting\nstates and cloth colors. Each row shows a different episode from a start state\nto the achieved cloth smoothness. We observe that the robot can reach the goal\nstate from complex start states, and generalizes outside the training data\ndistribution.\n</i>\n</p>\n\n<p>The figure above shows different episodes on a real robot using a\npick-and-place policy learned with our method. The policy is trained in\nsimulation, and then transferred to a real robot using domain randomization on\ncloth physics, color, and lighting. We can see that the learned policy is able\nto successfully smooth cloth starting from many different complexities of\nstate, and for different cloth colors.</p>\n\n<p>The advantages of this paper’s model-free reinforcement learning approach is\nthat all training can be done in simulation without any demonstrations, and\nthat training can readily be applied using off-the-shelf algorithms and is\nfaster due to the pick-and-place structure we present which (as discussed\nearlier) can avoid mode collapse. The tradeoff is that it trains a policy that\ncan only do smoothing, and must be re-trained for other tasks. In addition, the\nactions may take relatively short pulls and might be inefficient when it comes\nto more difficult cloth tasks such as folding.</p>\n\n<h2 id=\"model-free-learning-with-simulated-demonstrations\">Model-Free Learning with Simulated Demonstrations</h2>\n\n<ul>\n  <li>Daniel Seita, Aditya Ganapathi, Ryan Hoque, Minho Hwang, Edward Cen, Ajay Kumar Tanwani, Ashwin Balakrishna, Brijen Thananjeyan, Jeffrey Ichnowski, Nawid Jamali, Katsu Yamane, Soshi Iba, John Canny, Ken Goldberg.<br />\n<strong><a href=\"https://arxiv.org/abs/1910.04854\">Deep Imitation Learning of Sequential Fabric Smoothing From an Algorithmic Supervisor</a></strong><br />\narXiv 2019. <a href=\"https://sites.google.com/view/fabric-smoothing\">Project Website with Code</a>.</li>\n</ul>\n\n<p>We now present <a href=\"https://arxiv.org/abs/1910.04854\">an alternative approach for smoothing fabrics</a>. Like the\nprior paper, we use a model-free method and we create an environment for fabric\nmanipulation using a simulator. Instead of MuJoCo, we use a custom-built\nsimulator that represents fabric as a $25 \\times 25$ grid of points. We’ve\n<a href=\"https://sites.google.com/view/fabric-smoothing\">open sourced this simulator</a> for other researchers to use.</p>\n\n<p>In this project, we consider a <em>fabric plane</em> as a white background square of\nthe same size as a fully smooth fabric. The performance metric is <em>coverage</em>,\nor how much of the background plane gets covered by the fabric, which\nencourages the robot to cover a specific location. We terminate an episode if\nthe robot attains at least 92% coverage.</p>\n\n<p>One way to smooth fabric is to pull at fabric corners. Since this policy is\neasy to define, we code an algorithmic supervisor in simulation and perform\nimitation learning using <a href=\"https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf\">Dataset Aggregation (DAgger)</a>. As briefly covered\n<a href=\"https://bair.berkeley.edu/blog/2017/10/26/dart/\">in a prior BAIR Blog post</a>, DAgger is an algorithm to correct for\ncovariate shift. It continually queries a <em>supervisor</em> agent to get corrective\nactions for states. This is normally a downside for DAgger, but is not a\nproblem in this case, as we have a simulator with full access to state\ninformation (i.e., the grid of $25\\times 25$ points) and can determine the\noptimal pull action efficiently.</p>\n\n<p>In addition to using color images, we use <em>depth</em> images, which provide a\n“height scale.” <a href=\"https://bair.berkeley.edu/blog/2018/10/23/depth-sensing/\">In a prior BAIR Blog post</a>, we discussed how depth was\nuseful for various robotics tasks. To obtain images, we use <a href=\"https://www.blender.org/\">Blender</a>, an\nopen-source computer graphics toolkit.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_corners_analytic_v04.png\" />\n<br />\n<i>\nAn example episode of our simulated corner-pulling supervisor policy. Each\ncolumn of images shows one action, represented by the overlaid white arrows.\nWhile we domain-randomize these images for training, for visualization purposes\nin this figure, we leave images at their \"default\" settings. The starting\nstate, represented to the left, is highly wrinkled and only covers 38.4% of the\nfabric plane. Through a sequence of five pick-and-pull actions, the policy\neventually gets 95.5% coverage.\n</i>\n</p>\n\n<p>The figure above visualizes the supervisor’s policy. The supervisor chooses the\nfabric corner to pull based on its distance from a known target on the\nbackground plane. Even though fabric corners are sometimes hidden by a top\nlayer, as in the second time step, the pick-and-pull actions are eventually\nable to get sufficient coverage.</p>\n\n<p>After training using DAgger on domain randomized data, we transfer the policy\nto a <a href=\"https://www.davincisurgery.com/\">da Vinci Surgical Robot</a> without any further training. The figure\nbelow represents an example episode of the da Vinci pulling and smoothing\nfabric.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_real_v02_color_correct.png\" />\n<br />\n<i>\nAn example seven-action episode taken by a policy trained only on simulated\nRGB-D images. The top row has screenshots of the video of the physical robot,\nwith overlaid black arrows to visualize the action. The second and third rows\nshow the color and depth images that are processed as input to be passed\nthrough the learned policy. Despite the highly wrinkled starting fabric, along\nwith hidden fabric corners, the da Vinci is able to adjust the fabric from\n40.1% to 92.2% coverage.\n</i>\n</p>\n\n<p>To summarize, we learn fabric smoothing policies using imitation learning with\na supervisor that has access to true state information of the fabrics. We\ndomain randomize the colors, brightness, and camera orientation on simulated\nimages to transfer policies to a physical da Vinci surgical robot. The\nadvantage of the approach is that the robot can efficiently smooth fabric in\nrelatively few actions and does not require a large workspace, as the training\ndata consists of long pulls constrained in the workspace. In addition,\nimplementing and debugging DAgger is relatively easy compared to model-free\nreinforcement learning methods as DAgger is similar to supervised learning and\none can inspect the output of the teacher. The primary limitations are that we\nneed to know how to implement the supervisor’s policy, which can be difficult\nfor tasks beyond smoothing, and that the learned policy is a smoothing\n“specialist” that must be re-trained for other tasks.</p>\n\n<h1 id=\"model-based-methods\">Model-Based Methods</h1>\n\n<h2 id=\"planning-over-image-states\">Planning Over Image States</h2>\n\n<ul>\n  <li>Ryan Hoque*, Daniel Seita*, Ashwin Balakrishna, Aditya Ganapathi, Ajay Kumar\nTanwani, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg.<br />\n<strong><a href=\"https://arxiv.org/abs/2003.09044\">VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation</a>.</strong><br />\nRobotics: Science and Systems, 2020. <a href=\"https://sites.google.com/view/fabric-vsf/home\">Project Website with Code</a>.</li>\n</ul>\n\n<p>While the previous two approaches give us solid performance on the smoothing\ntask on real robotic systems, the learned policies are “smoothing specialists”\nand must be re-trained from scratch for a new task, such as fabric folding. <a href=\"https://arxiv.org/abs/2003.09044\">In\nthis paper</a>, we consider the more general problem of goal-conditioned\nfabric manipulation: given a <em>single goal image observation</em> of a desired\nfabric state, we want a policy that can perform a sequence of pick-and-place\nactions to get from an arbitrary initial configuration to that state.</p>\n\n<p>To do so, we decouple the problem into first learning a model of fabric\ndynamics directly from image observations and then re-using that dynamics model\nfor different fabric manipulation tasks. For the former, we apply the <a href=\"https://bair.berkeley.edu/blog/2018/11/30/visual-rl/\">Visual\nForesight framework</a> proposed by our BAIR colleagues, a model-based\nreinforcement learning technique that trains a video prediction model to\npredict a sequence of images from the image observation of the current state as\nwell as an action sequence. With such a model, we can predict the results of\ntaking various action sequences, and can then use planning techniques such as\nthe cross-entropy method and model-predictive control to plan actions that\nminimize some cost function. We use Euclidean distance to the goal image for\nthe cost function in our experiments.</p>\n\n<p>We generate roughly 100,000 images from an <em>entirely random</em> policy, executed\nentirely in simulation. Using the same fabric simulator as in (<a href=\"https://arxiv.org/abs/1910.04854\">Seita et al.,\n2019</a>), we use <a href=\"https://arxiv.org/abs/1710.11252\">Stochastic Variational Video Prediction (SV2P)</a> as the\nvideo prediction model. We leverage both RGB and depth modalities, which we\nfind in our experiments to outperform either modality alone, and thus call the\nalgorithm <em>VisuoSpatial Foresight</em> (VSF).</p>\n\n<p>While <a href=\"https://bair.berkeley.edu/blog/2018/11/30/visual-rl/\">prior work on Visual Foresight</a> includes some fabric manipulation\nresults, the tasks considered are typically short horizon and have a wide range\nof goal states, such as covering a spoon with a pant leg. In contrast, we focus\non longer horizon tasks that require a sequence of <em>precise</em> pick points. See\nthe image below for typical test-time predictions from the visual dynamics\nmodel. The data is domain-randomized in color, camera angle, brightness, and\nnoise, to facilitate transfer to a da Vinci Surgical Robot.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/ryan_dr.jpeg\" width=\"500\" />\n<br />\n<i>\nWe show the ground truth images as a result of fabric manipulation, each paired\nwith predictions from the trained video prediction model. Given only a starting\nimage (not shown), along with the next four actions, the video prediction model\nmust predict the next four images, shown above.\n</i>\n</p>\n\n<p>The predictions are accurate enough for us to plan toward a variety of goal\nimages. Indeed, our resulting policy <em>rivals the performance of the smoothing\nspecialists</em>, despite only seeing random images at training time.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/gif6.jpg\" height=\"170\" />\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/gif4.jpg\" height=\"170\" />\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/gif2.jpg\" height=\"170\" /><br />\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/gif5.gif\" height=\"170\" />\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/gif3.gif\" height=\"170\" />\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/gif1.gif\" height=\"170\" />\n<br />\n<i>\nWe execute a sequence of pick-and-place actions to manipulate fabric toward\nsome goal image. The top row has three different goal images: smooth, folded,\nand doubly folded, which has three layers of fabric stacked in the center in a\nparticular order. In the bottom row, we show simulated rollouts (shown here as\ntime-lapses of image observations) of our VSF policy manipulating fabric toward\neach of the goal images. The bottom side of the fabric is a darker shade\n(slightly darker in the second and much darker in the third column), and the\nlight patches within the dark are due to self-collisions in the simulator that\nare difficult to model.\n</i>\n</p>\n\n<p>The main advantage of this approach is that we can train a <em>single</em> neural\nnetwork policy to be used for a variety of tasks, each of which are set by\nproviding a goal image of the target fabric configuration. For example, we can\ndo folding tasks, for which it may be challenging to hand-code an algorithmic\nsupervisor, <a href=\"https://arxiv.org/abs/1910.04854\">unlike the case of <em>smoothing</em></a>. The main downsides are that\ntraining a video prediction model is difficult due to the high dimensional\nnature of images, and that we typically require more actions than the imitation\nlearning agent to complete smoothing tasks as the data consists of\nshorter-magnitude actions.</p>\n\n<h2 id=\"planning-over-latent-states\">Planning Over Latent States</h2>\n\n<ul>\n  <li>Wilson Yan, Ashwin Vangipuram, Pieter Abbeel, Lerrel Pinto.<br />\n<strong><a href=\"https://arxiv.org/abs/2003.05436\">Learning Predictive Representations for Deformable Objects Using Contrastive Estimation</a></strong>.<br />\narXiv 2020. <a href=\"https://sites.google.com/view/contrastive-predictive-model\">Project Website with Code</a>.</li>\n</ul>\n\n<p><a href=\"https://arxiv.org/abs/2003.05436\">In this paper</a>, we similarly consider a model-based method, but instead of\ntraining a video prediction model to plan in pixel space, we instead <em>plan in a\nlearned lower-dimensional latent space</em> since learning a video prediction model\ncan be challenging, as the learned model must capture every detail of the\nenvironment. In addition, it is also difficult to learn proper pixel dynamics\nin the cases when we use frame-by-frame domain randomization to transfer to the\nreal world.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper2_fig1.png\" width=\"550\" />\n<br />\n<i>\nA visual depiction of the contrastive learning framework. Given positive\ncurrent-next state pairs and randomly sampled negative observations, we learn\nan encoder and forward model such that the estimated next states lie closer\nthan the negative observations in the latent space.\n</i>\n</p>\n\n<p>We jointly learn an <em>encoder</em> and a <em>latent forward model</em> using contrastive\nestimation methods. The encoder maps raw images into a lower dimensional latent\nspace. The latent forward model will take this latent variable, along with the\naction, and produce an estimate of the next state.</p>\n\n<p>We train our models by minimizing a variant of the <a href=\"https://arxiv.org/abs/1807.03748\">InfoNCE contrastive\nloss</a>, which encourages learning latents that maximize mutual information\nbetween encoded latents and their respective future observations. In practice,\nthis training method will bring current and subsequent latent encodings closer\n(in $L_2$ distance), while making other sampled non-next latent encodings to be\nfurther apart. As a result, we are able to use the learned encoder and learned\nforward model to effectively predict the future, similar to the image-based\napproach presented in the prior paper (<a href=\"https://arxiv.org/abs/2003.09044\">Hoque et al., 2020</a>), except we are\nnot predicting images but latent variables, which are potentially easier to\nwork with.</p>\n\n<p>In our cloth experiments, we apply random actions to collect 400,000 samples in\na <a href=\"https://arxiv.org/abs/1801.00690\">DM Control</a> simulator with added domain randomization on cloth physics,\nlighting, and cloth color. We use the learned encoder and forward model to\nperform model predictive control (MPC) with one-step prediction to plan towards\na desired goal state image. The figure below shows examples of smoothing out\ndifferent colored cloths on a real PR2 robot. Note that the same blue cloth is\nused as the goal image regardless of the actual cloth being manipulated. This\nindicates that the learned latents have learned to ignore unnecessary\nproperties of cloth such as color when performing manipulation tasks.</p>\n\n<p style=\"text-align:center;\">\n<img src=\"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper2_fig2.png\" />\n<br />\n<i>\nSeveral episodes of both manipulating rope and cloth using our method, with\ndifferent start and goal states. Note that the same blue cloth is used as the\ngoal state irrespective of the color of the cloth being manipulated\n</i>\n</p>\n\n<p>Similar to (Hoque et al., 2020), this method is able to solve multi-task goal\nstates, as shown in the example episodes above run on a real robot. Using\ncontrastive methods to learn a latent space, we also achieve better sample\ncomplexity in model learning compared to direct video prediction models,\nbecause the latter require more samples to predict high dimensional images. In\naddition, planning directly in latent spaces is easier compared to planning\nwith a visual model. In our paper, we show that using a simple one-step\nmodel-predictive control to plan in latent spaces works substantially better\nthan one-step planning with a learned visual forward model, perhaps because the\nlatents learn to ignore irrelevant aspects of the images. Although planning\nallows for cloth spreading and rope orientation manipulation, our models fail\nto perform long horizon manipulation since the models are trained on offline\nrandom actions.</p>\n\n<h1 id=\"discussion\">Discussion</h1>\n\n<p>To recap, we presented four related papers which present different approaches\nfor robot manipulation of fabrics. Two use model-free approaches (one with\nreinforcement learning and one with imitation learning) and two use model-based\nreinforcement learning approaches (with either images or latent variables).\nBased on what we’ve covered in this blog post, let’s consider possibilities for\nfuture work.</p>\n\n<p>One option is to combine these methods, as done in recent or concurrent work.\nFor example, <a href=\"https://arxiv.org/abs/1806.07851\">(Matas et al., 2018)</a> used model-free reinforcement learning\nwith imitation learning (through demonstrations) for cloth manipulation tasks.\nIt is also possible to add other tools from the robotics and computer vision\nliterature, such as <a href=\"https://arxiv.org/abs/1911.06283\"><em>state estimation strategies</em></a> to enable better\nplanning. Another potential tool might be <a href=\"https://arxiv.org/abs/1806.08756\"><em>dense object descriptors</em></a>\nwhich indicate <em>correspondence</em> among pixels in two different images. For\nexample, we have shown the utility of descriptors for a variety of <a href=\"https://arxiv.org/abs/2003.01835\">rope</a>\nand <a href=\"https://arxiv.org/abs/2003.12698\">fabric</a> manipulation tasks.</p>\n\n<p>Techniques such as imitation learning, reinforcement learning,\nself-supervision, visual foresight, depth sensing, dense object descriptors,\nand particularly the use of simulators, have been useful tools. We believe they\nwill continue to play an increasing role in robot manipulation of fabrics, and\ncould be used for more complex tasks such as wrapping items or fitting fabric\nto 3D objects.</p>\n\n<p>Reflecting back on our work, another direction to explore could be using these\nmethods to train six degree-of-freedom grasping. We restricted our setting to\nplanar pick-and-place policies, and noticed that the robots often had\ndifficulty with top-down grasps when fabric corners were not clearly exposed.\nIn these cases, more flexible grasps may be better for smoothing or folding.\nFinally, another direction for future work is to address the mismatches we\nobserved between simulated and physical policy performance. This may be due to\nimperfections in the fabric simulators, and it might be possible to use data\nfrom the physical robot to fine-tune the parameters of the fabric simulators to\nimprove performance.</p>\n\n<hr />\n\n<p>We thank Ajay Kumar Tanwani, Lerrel Pinto, Ken Goldberg, and Pieter Abbeel for\nproviding extensive feedback on this blog post.</p>\n\n<p>This research was performed in affiliation with the Berkeley AI Research (BAIR)\nLab, Berkeley Deep Drive (BDD), and the CITRIS “People and Robots” (CPAR)\nInitiative. The authors were supported in part by Honda, and by equipment\ngrants from Intuitive Surgical and Willow Garage.</p>\n\n","descriptionType":"text/html","publishedDate":"Tue, 05 May 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"4064027528e647e4c2f31474a205be17","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/fabrics/daniel_IROS-2020-example-cloth-optim.gif":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn94@2020_4/2020/08/24/23-52-11-052_10e201ac146b7445.webp","https://bair.berkeley.edu/static/blog/fabrics/wilson_intro_gif.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn30@2020_3/2020/08/24/23-52-11-889_d9193f9b76a1ed40.webp","https://bair.berkeley.edu/static/blog/fabrics/wilson_paper1_fig1.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn46@2020_1/2020/08/24/23-52-06-642_e36ed234bd8aeebf.webp","https://bair.berkeley.edu/static/blog/fabrics/wilson_paper1_fig2.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn73@2020_5/2020/08/24/23-52-06-424_ff27a726c0a9ac26.webp","https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_corners_analytic_v04.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn25@2020_2/2020/08/24/23-52-07-145_956d49f22234277e.webp","https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_real_v02_color_correct.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn97@2020_2/2020/08/24/23-52-05-111_de6c76e29f1a9976.webp","https://bair.berkeley.edu/static/blog/fabrics/ryan_dr.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn13@2020_4/2020/08/24/23-52-05-190_e2b01a67c7d30062.webp","https://bair.berkeley.edu/static/blog/fabrics/gif6.jpg":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn13@2020_2/2020/08/24/23-52-06-620_1ee4ab6ae8f0d471.webp","https://bair.berkeley.edu/static/blog/fabrics/gif4.jpg":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn65@2020_6/2020/08/24/23-52-03-159_4f952f854ee344a0.webp","https://bair.berkeley.edu/static/blog/fabrics/gif2.jpg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn77@2020_2/2020/08/24/23-52-06-139_462ddf2f83fbe722.webp","https://bair.berkeley.edu/static/blog/fabrics/gif5.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn89@2020_2/2020/08/24/23-52-04-604_c54656a68997b693.webp","https://bair.berkeley.edu/static/blog/fabrics/gif3.gif":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn42@2020_3/2020/08/24/23-52-06-196_5e3af93a0d047472.webp","https://bair.berkeley.edu/static/blog/fabrics/gif1.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn21@2020_3/2020/08/24/23-52-03-194_9605cfbe1bdb1c90.webp","https://bair.berkeley.edu/static/blog/fabrics/wilson_paper2_fig1.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn70@2020_1/2020/08/24/23-52-04-542_4a4d2e7fa1a905ad.webp","https://bair.berkeley.edu/static/blog/fabrics/wilson_paper2_fig2.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn49@2020_6/2020/08/24/23-52-03-713_36c212354a045d86.webp"},"publishedOrCreatedDate":1598313090352},{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","title":"Decentralized Reinforcement Learning:<br>Global Decision-Making via<br>Local Economic Transactions","link":"http://bair.berkeley.edu/blog/2020/07/11/auction/","description":"<article class=\"post-content\"> \n <meta name=\"twitter:title\" content=\"Global Decision-Making via Local Economic Transactions\" /> \n <meta name=\"twitter:card\" content=\"summary_large_image\" /> \n <meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/auction/mnist.png\" /> \n <!-- begin section I: introduction --> \n <p> Many neural network architectures that underlie various artificial intelligence systems today bear an interesting similarity to the early computers a century ago. Just as early computers were specialized circuits for specific purposes like <a href=\"http://jva.cs.iastate.edu/operation.php\">solving linear systems</a> or <a href=\"https://en.wikipedia.org/wiki/Colossus_computer\">cryptanalysis</a>, so too does the trained neural network generally function as a <a href=\"https://youtu.be/9EN_HoEk3KY?t=211\">specialized circuit</a> for performing a specific task, with all parameters coupled together in the same global scope. </p> \n <p> One might naturally wonder what it might take for <i>learning</i> systems to scale in complexity in the same way as <i>programmed</i> systems have. And if the history of <a href=\"https://www.youtube.com/watch?v=qAKrMdUycb8\">how abstraction enabled computer science to scale</a> gives any indication, one possible place to start would be to consider what it means to build complex learning systems at multiple levels of abstraction, where each level of learning is the emergent consequence of learning from the layer below. </p> \n <p> This post discusses <a href=\"https://arxiv.org/abs/2007.02382\">our recent paper</a> that introduces a framework for <b>societal decision-making</b>, a perspective on reinforcement learning through the lens of a self-organizing society of primitive agents. We prove the optimality of an incentive mechanism for engineering the society to optimize a collective objective. Our work also provides suggestive evidence that the local credit assignment scheme of the <b>decentralized reinforcement learning algorithms</b> we develop to train the society facilitates more efficient transfer to new tasks. </p> \n <!--more--> \n <h2 id=\"levels-of-abstraction-in-complex-learning-systems\">Levels of Abstraction in Complex Learning Systems</h2> \n <p> From corporations to organisms, <a href=\"https://youtu.be/uyUbGatPKpI?t=1419\">many large-scale systems in our world are composed of smaller individual autonomous components</a>, whose collective function serve a larger objective than the objective of any individual component alone. A corporation for example, optimizes for profits as if it were a single super-agent when in reality it is a society of self-interested human agents, each with concerns that may have little to do with profit. And every human is also simply an abstraction of organs, tissues, and cells individually adapting and making their own simpler decisions. </p> \n <blockquote cite=\"http://aurellem.org/society-of-mind/som-1.3.html\"> \n  <p>You know that everything you think and do is thought and done by you. But what's a \"you\"? What kinds of smaller entities cooperate inside your mind to do your work?</p> \n  <p align=\"right\">— Marvin Minsky, <cite>The Society of Mind</cite></p> \n </blockquote> \n <p> At the core of building complex learning systems at multiple levels of abstraction is to understand the mechanisms that bind consecutive levels together. In the context of learning for decision-making, this means to define three ingredients: </p> \n <ul id=\"ingredients\"> \n  <li>A <b>framework</b> for expressing the encapsulation of a society of primitive agents as a super-agent</li> \n  <li>An <b>incentive mechanism</b> that guarantees the optimal solution for the super-agent's decision problem emerges as a consequence of the primitives optimizing their individual decision problems</li> \n  <li>A <b>learning algorithm</b> for implicitly training the super-agent by directly training the primitives</li> \n </ul> \n <p> The incentive mechanism is the abstraction barrier that connects the optimization problems of the primitive agents from the optimization problem of the society as a super-agent. </p> \n <p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/auction/abstraction_barrier.png\" width=\"100%\" /> <br /> <i>Building complex learning systems at multiple levels of abstraction requires defining the incentive mechanism that connects the optimization problems at the level of primitive agent to the optimization problem at the level of the society. The incentive mechanism is the abstraction barrier that separates the society as a super-agent from its constituent primitive agents.</i> </p> \n <p> If it were possible to construct the incentive mechanism in a way that the <a href=\"https://en.wikipedia.org/wiki/Strategic_dominance#Dominance_and_Nash_equilibria\">dominant strategy equilibrium</a> of the primitive agents coincides with the optimal solution for the super-agent, then the society can in theory be faithfully abstracted as a super-agent, which could then serve as a primitive for the next level of abstraction, and so on, thereby constructing in a learning system the higher and higher levels of complexity that characterize the programmed systems of modern software infrastructure. </p> \n <!-- begin section II: model-based techniques --> \n <!-- <h2 id=\"global-decision-making-via-local-economic-transactions\">Global Decision-Making via Local Economic Transactions</h2>\n --> \n <h2 id=\"a-market-economy-perspective-on-reinforcement-learning\">A Market Economy Perspective on Reinforcement Learning</h2> \n <p> As a first step towards this goal, we can work backwards: start with an agent, imagine it were a super-agent, and study how to emulate optimal behavior of such an agent via a society of even more primitive agents. We consider a restricted scenario that builds upon existing frameworks familiar to us, <a href=\"https://en.wikipedia.org/wiki/Markov_decision_process\">Markov decision processes (MDP)</a>. Normally, the objective of the learner is to maximize the <a href=\"https://en.wikipedia.org/wiki/Markov_decision_process#Optimization_objective\">expected return</a> of the MDP. In deep reinforcement learning, the approach that directly optimizes this objective parameterizes the policy as a function that maps states to actions and adjusts the policy parameters according to the gradient of the MDP objective. </p> \n <p> We refer to this standard approach as the <strong>monolithic decision-making framework</strong> because all the learnable parameters are globally coupled together under a single objective. The monolithic decision-making framework views reinforcement learning from the perspective of a <strong><a href=\"https://en.wikipedia.org/wiki/Planned_economy\">command economy</a></strong>, in which all production — the transformation of past states $s_t$ into future states $s_{t+1}$ — and wealth distribution — the credit assignment of reward signals to parameters — derive directly from single central authority — the MDP objective. </p> \n <p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/auction/centralized_gif.gif\" width=\"100%\" /> <br /> <i>In the monolithic decision-making framework, actions are chosen passively by the agent.</i> </p> \n <p> But as suggested <a href=\"https://pdfs.semanticscholar.org/4373/526d6a46bac9c2de569557957d0b052a437a.pdf?_ga=2.198702767.1414080719.1594348633-1342068630.1594256465\">in</a> previous <a href=\"http://people.idsia.ch/~juergen/economy.html\">work</a> dating <a href=\"https://dl.acm.org/doi/10.5555/645511.657087\">back</a> at least two decades, we can also view reinforcement learning from the perspective of a <strong><a href=\"https://en.wikipedia.org/wiki/Market_economy\">market economy</a></strong>, in which production and wealth distribution are governed by the economic transactions between actions that <i>buy and sell states to each other.</i> Rather than being passively chosen by a global policy as in the monolithic framework, the actions are primitive agents that actively choose <i>themselves</i> when to activate in the environment by bidding in an auction to transform the state $s_t$ to the next state $s_{t+1}$. We call this the <strong>societal decision-making</strong> framework because these actions form a society of primitive agents that themselves seek to maximize their auction utility at each state. In other words, the society of primitive agents form a super-agent that solves the MDP as a consequence of the primitive agents' optimal auction strategies. </p> \n <p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/auction/decentralized_gif.gif\" width=\"100%\" /> <br /> <i>In the societal decision-making framework, actions actively choose themselves when to activate.</i> </p> \n <!-- <blockquote>\n<p>Wealth is distributed based on what future primitives decide to bid for the fruits of the labor of information processing carried out by past primitives transforming one state to another.</p>\n</blockquote> --> \n <p> In <a href=\"https://arxiv.org/abs/2007.02382\">our recent work</a>, we formalize the societal decision-making framework and develop a class of <b>decentralized reinforcement learning algorithms</b> for optimizing the super-agent as a by-product of optimizing the primitives' auction utilities. We show that adapting the <a href=\"https://en.wikipedia.org/wiki/Vickrey_auction\">Vickrey auction</a> as the auction mechanism and initializing redundant clones of each primitive yields a society, which we call the <strong>cloned Vickrey society</strong>, whose dominant strategy equilibrium of the primitives optimizing their auction utilities coincides with the optimal policy of the super-agent the society collectively represents. In particular, with the following specification of auction utility, we can leverage the <a href=\"https://en.wikipedia.org/wiki/Strategyproofness\">truthfulness</a> property of the Vickrey auction to incentivize the primitive agents, which we denote by $\\omega^{1:N}$, to bid the optimal Q-value of their corresponding action: </p> \n <p style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/auction/utility.png\" width=\"75%\" /> <br /> <i>The utility $\\hat{U}^i_{s_t}$ for the primitive with the highest bid, $\\hat{\\omega}^i$ is given by the revenue it receives from selling $s_{t+1}$ in the auction at the next time-step minus the price $\\max_{j \\neq i} \\mathbf{b}^j_{s_t}$ it pays for buying $s_t$ from the auction winner at the previous time-step. The revenue is given by the environment reward $r(s_t, \\hat{\\omega}^i)$ plus the discounted highest bid $\\max_k \\mathbf{b}^k_{s_{t+1}}$ at the next time-step. In accordance with the Vickrey auction, the price is given by the second highest bid at the current time-step. The utility of losing agents is $0$.</i> </p> \n <p> The revenue that the winning primitive receives for producing $s_{t+1}$ from $s_t$ depends on the price the winning primitive at $t+1$ is willing to bid for $s_{t+1}$. In turn, the winning primitive at $t+1$ sells $s_{t+2}$ to the winning primitive at $t+2$, and so on. Ultimately currency is grounded in the environment reward. Wealth is distributed based on what future primitives decide to bid for the fruits of the labor of information processing carried out by past primitives transforming one state to another. </p> \n <p> Under the Vickrey auction, the dominant strategy for each primitive is to truthfully bid exactly the revenue it would receive. With the above utility function, a primitive's truthful bid at equilibrium is the optimal Q-value of its corresponding action. And since the primitive with the maximum bid in the auction gets to take its associated action in the environment, overall the society at equilibrium activates the agent with the highest optimal Q-value — the optimal policy of the super agent. Thus in the restricted setting we consider, the societal decision-making framework, the cloned Vickrey society, and the decentralized reinforcement learning algorithms provide answers to the <a href=\"http://bair.berkeley.edu/blog/2020/07/11/auction/#ingredients\">three ingredients</a> outlined above for relating the learning problem of the primitive agent to the learning problem of the society. </p> \n <p> Societal decision-making frames standard reinforcement learning from the perspective of self-organizing primitive agents. As we discuss next, the primitive agents need not be restricted to literal actions. The agents can be any computation that transforms a state from one to another, including <a href=\"http://bair.berkeley.edu/blog/2020/07/11/auction/#hrl_fig\">options in semi-MDPs</a> or <a href=\"http://bair.berkeley.edu/blog/2020/07/11/auction/#mnist\">functions in dynamic computation graphs</a>. </p> \n <h3 id=\"local-credit-assignment-for-more-efficient-transfer\">Local Credit Assignment for More Efficient Learning and Transfer</h3> \n <p> Whereas learning in the command economy system of monolithic decision-making requires global credit assignment pathways because all learnable parameters are globally coupled, learning in the market economy system of societal decision-making requires only credit assignment that is local in space and time because the primitives only optimize for their immediate local auction utility without regard to the global learning objective of the society. Indeed, we find evidence that suggests that the inherent modularity in framing the learning problem of the society in this way offers advantages in transferring to new tasks. </p> \n <p id=\"hrl_fig\" style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/auction/hrl_weight.png\" width=\"90%\" /> <br /> <i>We consider transferring from the pre-training task of reaching the green goal to transfer task of reaching the blue goal in the <a href=\"https://github.com/maximecb/gym-minigrid\">MiniGrid gym environment</a>. $\\phi^0$ represents an option that opens the red door, $\\phi^1$ represents an option that reaches the blue goal, and $\\phi^2$ represents an option that reaches the green goal. The primitive associated with a particular option $\\phi^i$ activates by executing that option in the environment. \"Credit Conserving Vickrey Cloned\" refers to our society-based decentralized reinforcement learning algorithm, which learns much more efficiently than both a hierarchical monolithic baseline equipped to select the same options and a non-hierarchical monolithic baseline that only selects literal actions. In particular, we observe that a higher percentage of the hierarchical monolithic baseline's weights have shifted during transfer compared to our method, which suggests that the hierarchical monolithic baseline's weights are more globally coupled and perhaps thereby slower to transfer. </i> </p> \n <h3 id=\"problem-solving-via-analogy\">Problem Solving via Analogy</h3> \n <blockquote> \n  <p>Solving a problem simply means representing it so as to make the solution transparent.</p> \n  <p align=\"right\">— Herbert Simon, <cite>The Science of Design: Creating the Artificial.</cite></p> \n </blockquote> \n <p> <a href=\"https://mitpress.mit.edu/books/analogy-making-perception\">Re-representing an observation as an instance of what is more familiar</a> has been an important topic of study in human cognition from the perspective of <a href=\"http://bert.stuy.edu/pbrooks/ai/resources/Analogy%20as%20the%20Core%20of%20Cognition-2.pdf\">analogy-making</a>. One particularly intuitive example of this phenomenon are <a href=\"https://science.sciencemag.org/content/171/3972/701\">the mental rotations</a> studied by Roger Shepard that suggested that humans seemed to compose mental rotation operations in their mind for certain types of image recognition. Inspired by these above works, we considered an image recognition task based on <a href=\"https://arxiv.org/pdf/1807.04640.pdf\">earlier work</a> where we define each primitive agent as representing a different affine transformation. By using the classification accuracy of an MNIST digit classifier as the sole reward signal, the society of primitives learns to emulate the analogy-making process by iteratively re-representing unfamiliar images into more familiar ones that the classifier knows how to classify. </p> \n <p id=\"mnist\" style=\"text-align:center;\"> <img src=\"https://bair.berkeley.edu/static/blog/auction/mnist.png\" width=\"75%\" /> <br /> <i>The society learns to classify transformed digits by making analogies to the digit's canonical counterpart. Here $\\omega$ represents a primitive agent, $\\psi$ represents that agent's bidding policy, and $\\phi$ represents that agent's affine transformation. This figure shows a society with redundant primitives, where clones are indicated by an apostrophe. The benefits of redundancy for robustness are discussed in the paper.</i> </p> \n <h2 id=\"looking-forward\">Looking Forward</h2> \n <p> Modeling intelligence at various levels of abstraction has its roots in the <a href=\"https://en.wikipedia.org/wiki/Society_of_Mind\">early foundations of AI</a>, and modeling the mind as a society of agents goes back as far as <a href=\"https://en.wikipedia.org/wiki/Republic_(Plato)\">Plato's Republic</a>. In this restricted setting where the primitive agents seek to maximize utility in auctions and the society seeks to maximize return in the MDP, we now have a small piece of the puzzle towards building complex learning systems at multiple levels of abstraction. There are many more pieces left to go. </p> \n <p> In some sense these complex learning systems are grown rather than built because every component at every abstraction layer is learning. But in the same way that programming methodology emerged as a discipline for defining best practices for building complex programmed systems, so too will we need to specify, build, and test the scaffolding that guide the growth of complex learning systems. This type of deep learning is not only deep in levels of representation but deep in levels of learning. </p> \n <hr /> \n <p> This post is based on the following paper: </p> \n <ul> \n  <a href=\"https://arxiv.org/abs/2007.02382\"><strong>Decentralized Reinforcement Learning: <br />Global Decision-Making via Local Economic Transactions</strong></a> \n  <br /> \n  <a href=\"http://mbchang.github.io/\">Michael Chang</a>, \n  <a href=\"https://www.linkedin.com/in/sid-k-232763a6/\">Sidhant Kaushik</a>, \n  <a href=\"https://www.cs.princeton.edu/~smattw/\">S. Matthew Weinberg</a>, \n  <a href=\"http://cocosci.princeton.edu/tom/index.php\">Thomas Griffiths</a>, \n  <a href=\"http://people.eecs.berkeley.edu/~svlevine/\">Sergey Levine</a> \n  <br /> \n  <em>Thirty-seventh International Conference Machine Learning (ICML), 2020.</em> \n  <br /> \n  <a href=\"https://sites.google.com/view/clonedvickreysociety/home\">Webpage</a> \n </ul> \n <hr /> \n <p> <em>Michael Chang would like to thank Matt Weinberg, Tom Griffiths, and Sergey Levine for their guidance on this project, as well as Michael Janner, Anirudh Goyal, and Sam Toyer for discussions that inspired many of the ideas written here.</em> </p> \n <strong>References</strong> \n <font size=\"-1\"> </font>\n <ol>\n  <font size=\"-1\"> <li><a href=\"https://youtu.be/9EN_HoEk3KY?t=211\">Ilya Sutskever: OpenAI Meta-Learning and Self-Play | MIT Artificial General Intelligence (AGI)</a>. 2018.</li> <li><a href=\"https://www.youtube.com/watch?v=qAKrMdUycb8\">Barbara Liskov, 2007 ACM A.M. Turing Award Lecture \"The Power of Abstraction\"</a>. 2013.</li> <li><a href=\"https://www.youtube.com/watch?v=uyUbGatPKpI&amp;feature=youtu.be&amp;t=1419\">Social Intelligence | Blaise Aguera y Arcas | NeurIPS 2019</a>. 2020.</li> <li>Minsky, Marvin. <a href=\"http://aurellem.org/society-of-mind/som-1.3.html\">The Society of Mind</a>. 1988.</li> <li>Baum, Eric B. <a href=\"https://pdfs.semanticscholar.org/4373/526d6a46bac9c2de569557957d0b052a437a.pdf?_ga=2.198702767.1414080719.1594348633-1342068630.1594256465\">Toward a Model of Mind as a Laissez-Faire Economy of Idiots</a>. 1995. </li> <li>Schmidhuber, Juergen. <a href=\"http://people.idsia.ch/~juergen/economy.html\">Market Models for Machine Learning - Reinforcement Learning Economies</a>.</li> <li>Holland, John H. <a href=\"https://dl.acm.org/doi/10.5555/645511.657087\">Properties of the Bucket Brigade</a>. 1985.</li> <li>Simon, Herbert A. <a href=\"https://www.jstor.org/stable/1511391?casa_token=st5XoO1v6lAAAAAA%3AvHUEwBeEuaIqf6Dp-Aoc2Py8K42qUXeDtO0-ilJWmI1icYlKuevZ9utb2jaDdNcsuvoQjvpVHNrGa_beDtRKApW5UoOHYMIKg7bOmmPTnnG92nzkawlX&amp;seq=1#metadata_info_tab_contents\">The Science of Design: Creating the Artificial</a>. 1988.</li> <li>Mitchell, Melanie. <a href=\"https://mitpress.mit.edu/books/analogy-making-perception\">Analogy-Making as Perception: A Computer Model</a>. 1993.</li> <li>Hofstadter, Douglas R. <a href=\"http://bert.stuy.edu/pbrooks/ai/resources/Analogy%20as%20the%20Core%20of%20Cognition-2.pdf\">Analogy as the core of cognition</a>. 2001.</li> <li>Shepard, Roger N., and Jacqueline Metzler. <a href=\"https://science.sciencemag.org/content/171/3972/701\">Mental rotation of three-dimensional objects.</a> 1971.</li> <li>Chang, Michael B., et al. <a href=\"https://arxiv.org/abs/1807.04640\">Automatically composing representation transformations as a means for generalization</a>. 2019.</li> </font>\n </ol>\n <font size=\"-1\"> </font> \n</article>","descriptionType":"text/html","publishedDate":"Sat, 11 Jul 2020 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"408fef6277527bf1dc3742fe5e40e6fe","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/auction/abstraction_barrier.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn30@2020_1/2020/08/24/23-52-05-387_dca3580158e350e1.webp","https://bair.berkeley.edu/static/blog/auction/centralized_gif.gif":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn50@2020_1/2020/08/24/23-52-06-129_ff489a4423717278.webp","https://bair.berkeley.edu/static/blog/auction/decentralized_gif.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn25@2020_2/2020/08/24/23-52-07-685_ee86aa308cf9e5d1.webp","https://bair.berkeley.edu/static/blog/auction/utility.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn54@2020_2/2020/08/24/23-52-03-266_9061b7f06d2a57d3.webp","https://bair.berkeley.edu/static/blog/auction/hrl_weight.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn38@2020_5/2020/08/24/23-52-04-414_51073158d539dba0.webp","https://bair.berkeley.edu/static/blog/auction/mnist.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn65@2020_1/2020/08/24/23-52-04-805_3218f17caf31c402.webp"},"publishedOrCreatedDate":1598313090352}],"record":{"createdTime":"2020-08-25 07:51:30","updatedTime":"2020-08-25 07:51:30","feedId":8140,"fetchDate":"Mon, 24 Aug 2020 23:51:30 +0000","fetchMs":628,"handleMs":1983,"totalMs":52398,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"34.199.111.102","hostName":"us-012.herokuapp.com","requestId":"b22c1f1ba82b4a23a5c87d39c08393f7_8140","contentType":"application/xml","totalBytes":58670954,"bgimgsTotal":3,"bgimgsGithubTotal":3,"articlesImgsTotal":88,"articlesImgsGithubTotal":88,"successGithubMap":{"myreaderx14":3,"myreaderx8":2,"myreaderx15":3,"myreaderx7":3,"myreaderx6":3,"myreaderx16":3,"myreaderx32":4,"myreaderx10":3,"myreaderx4":3,"myreaderx33":4,"myreaderx3":2,"myreaderx11":3,"myreaderx2":3,"myreaderx12":3,"myreaderx13":3,"myreaderx1":3,"myreaderx30":4,"myreaderx31":3,"myreaderx18":3,"myreaderx19":3,"myreaderx":3,"myreaderx25":4,"myreaderx27":3,"myreaderx21":2,"myreaderx22":3,"myreaderx23":3,"myreaderx24":3,"myreaderx5oss":3,"myreaderx29":3},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:34:09","updatedTime":"2020-08-25 04:34:09","id":8140,"name":"The Berkeley Artificial Intelligence Research Blog","url":"http://bair.berkeley.edu/blog/feed.xml","subscriber":null,"website":null,"icon":"http://bair.berkeley.edu/favicon.ico","icon_jsdelivr":null,"description":"The BAIR Blog","weekly":null,"link":"http://bair.berkeley.edu"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":2200804,"tmpBodyImgCdnBytes":56470150,"tmpBgImgCdnBytes":0,"extra4":{"start":1598313087606,"total":0,"statList":[{"spend":763,"msg":"获取xml内容"},{"spend":1983,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":17312,"msg":"正文链接上传到cdn"}]},"extra5":88,"extra6":88,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{"http://bair.berkeley.edu/blog/2020/05/01/umrl/_#fnref:knob":"http://bair.berkeley.edu/blog/2020/05/01/umrl/#fnref:knob","http://bair.berkeley.edu/blog/2020/05/01/umrl/_#fn:knob":"http://bair.berkeley.edu/blog/2020/05/01/umrl/#fn:knob","http://bair.berkeley.edu/blog/2020/07/11/auction/_#ingredients":"http://bair.berkeley.edu/blog/2020/07/11/auction/#ingredients","http://bair.berkeley.edu/blog/2020/08/03/covid-fatality/_\"https://hdsr.mitpress.mit.edu/pub/y9vc2u36/release/6\"":"http://bair.berkeley.edu/blog/2020/08/03/covid-fatality/\"https://hdsr.mitpress.mit.edu/pub/y9vc2u36/release/6\"","http://bair.berkeley.edu/blog/2020/07/11/auction/_#hrl_fig":"http://bair.berkeley.edu/blog/2020/07/11/auction/#hrl_fig","http://bair.berkeley.edu/blog/2020/07/11/auction/_#mnist":"http://bair.berkeley.edu/blog/2020/07/11/auction/#mnist"},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe-24.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-002.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-007.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://europe70.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-011.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ocda/figure_1.jpg","sourceStatusCode":200,"destWidth":1020,"destHeight":409,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn27@2020_3/2020/08/24/23-51-38-323_4c4f2c5220e502dd.webp","sourceBytes":178696,"destBytes":48684,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1587,"convertSpendMs":42,"createdTime":"2020-08-25 07:51:37","host":"us-019*","referer":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","linkMd5ListStr":"5385792a73444f69dd02146562fbbe85,5385792a73444f69dd02146562fbbe85","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"174.5 KB","destSize":"47.5 KB","compressRate":"27.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/cfr/graphical-model.png","sourceStatusCode":200,"destWidth":2981,"destHeight":2479,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn32@2020_5/2020/08/24/23-51-39-014_f801ef3c4112c238.webp","sourceBytes":746473,"destBytes":413318,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2531,"convertSpendMs":573,"createdTime":"2020-08-25 07:51:37","host":"us-006*","referer":"http://bair.berkeley.edu/blog/2020/08/03/covid-fatality/","linkMd5ListStr":"966c30164ed85af0624eb0250bbb5312,966c30164ed85af0624eb0250bbb5312","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"729 KB","destSize":"403.6 KB","compressRate":"55.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image0.gif","sourceStatusCode":200,"destWidth":600,"destHeight":423,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn23@2020_1/2020/08/24/23-51-40-109_c4a676d9408f38bd.webp","sourceBytes":6082991,"destBytes":1738802,"targetWebpQuality":67,"feedId":8140,"totalSpendMs":3891,"convertSpendMs":1428,"createdTime":"2020-08-25 07:51:37","host":"us-031*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b,cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.8 MB","destSize":"1.7 MB","compressRate":"28.6%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591915803624_stitching-3.png","sourceStatusCode":200,"destWidth":888,"destHeight":303,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn85@2020_2/2020/08/24/23-52-02-780_daf8cae7d12aa9c4.webp","sourceBytes":12425,"destBytes":21346,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":993,"convertSpendMs":25,"createdTime":"2020-08-25 07:52:02","host":"us-020*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"12.1 KB","destSize":"20.8 KB","compressRate":"171.8%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/gif4.jpg","sourceStatusCode":200,"destWidth":48,"destHeight":48,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn65@2020_6/2020/08/24/23-52-03-159_4f952f854ee344a0.webp","sourceBytes":980,"destBytes":580,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1214,"convertSpendMs":3,"createdTime":"2020-08-25 07:52:02","host":"us-016*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"980 B","destSize":"580 B","compressRate":"59.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image7.png","sourceStatusCode":200,"destWidth":1318,"destHeight":426,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn58@2020_2/2020/08/24/23-52-03-264_017dd2627ba25be9.webp","sourceBytes":51523,"destBytes":17584,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1366,"convertSpendMs":26,"createdTime":"2020-08-25 07:52:02","host":"us-040*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"50.3 KB","destSize":"17.2 KB","compressRate":"34.1%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/auction/utility.png","sourceStatusCode":200,"destWidth":1403,"destHeight":249,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn54@2020_2/2020/08/24/23-52-03-266_9061b7f06d2a57d3.webp","sourceBytes":42192,"destBytes":20834,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1382,"convertSpendMs":16,"createdTime":"2020-08-25 07:52:02","host":"us-007*","referer":"http://bair.berkeley.edu/blog/2020/07/11/auction/","linkMd5ListStr":"408fef6277527bf1dc3742fe5e40e6fe","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.2 KB","destSize":"20.3 KB","compressRate":"49.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/gif1.gif","sourceStatusCode":200,"destWidth":48,"destHeight":48,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn21@2020_3/2020/08/24/23-52-03-194_9605cfbe1bdb1c90.webp","sourceBytes":7455,"destBytes":3318,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1395,"convertSpendMs":11,"createdTime":"2020-08-25 07:52:02","host":"us-040*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.3 KB","destSize":"3.2 KB","compressRate":"44.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig1.png","sourceStatusCode":200,"destWidth":873,"destHeight":583,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn81@2020_2/2020/08/24/23-52-03-302_4b3c4ba1afea1ee2.webp","sourceBytes":93811,"destBytes":35904,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1411,"convertSpendMs":25,"createdTime":"2020-08-25 07:52:02","host":"us-032*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"91.6 KB","destSize":"35.1 KB","compressRate":"38.3%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188767696_kitchen-complete-v0.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn46@2020_5/2020/08/24/23-52-03-080_3168f810d9f157e3.webp","sourceBytes":141836,"destBytes":170832,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1448,"convertSpendMs":353,"createdTime":"2020-08-25 07:52:02","host":"us-032*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"138.5 KB","destSize":"166.8 KB","compressRate":"120.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image5.png","sourceStatusCode":200,"destWidth":612,"destHeight":376,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn1@2020_2/2020/08/24/23-52-03-335_4249d6181fa7b571.webp","sourceBytes":80492,"destBytes":19634,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1458,"convertSpendMs":13,"createdTime":"2020-08-25 07:52:02","host":"us-002*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"78.6 KB","destSize":"19.2 KB","compressRate":"24.4%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174938549_flow-merge.gif","sourceStatusCode":200,"destWidth":300,"destHeight":233,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn37@2020_6/2020/08/24/23-52-03-213_c171f2e9545b1453.webp","sourceBytes":63391,"destBytes":99254,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1472,"convertSpendMs":454,"createdTime":"2020-08-25 07:52:02","host":"us-036*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"61.9 KB","destSize":"96.9 KB","compressRate":"156.6%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ocda/figure_5.png","sourceStatusCode":200,"destWidth":1899,"destHeight":739,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn26@2020_3/2020/08/24/23-52-03-518_a48ccaa913f617d3.webp","sourceBytes":201485,"destBytes":80896,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1740,"convertSpendMs":139,"createdTime":"2020-08-25 07:52:02","host":"us-007*","referer":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","linkMd5ListStr":"5385792a73444f69dd02146562fbbe85","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"196.8 KB","destSize":"79 KB","compressRate":"40.1%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188989101_mjc-200-3.gif","sourceStatusCode":200,"destWidth":200,"destHeight":200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn34@2020_6/2020/08/24/23-52-03-302_74b243d90bec507d.webp","sourceBytes":264713,"destBytes":207632,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1762,"convertSpendMs":515,"createdTime":"2020-08-25 07:52:02","host":"us-003*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"258.5 KB","destSize":"202.8 KB","compressRate":"78.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/umrl/control_room.jpg","sourceStatusCode":200,"destWidth":1439,"destHeight":752,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn4@2020_3/2020/08/24/23-52-03-489_d27bca5053769246.webp","sourceBytes":290982,"destBytes":135834,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1773,"convertSpendMs":75,"createdTime":"2020-08-25 07:52:02","host":"us-003*","referer":"http://bair.berkeley.edu/blog/2020/05/01/umrl/","linkMd5ListStr":"4923ec236785bf4c166ae57f30e71201","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"284.2 KB","destSize":"132.7 KB","compressRate":"46.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ocda/figure_4.png","sourceStatusCode":200,"destWidth":2000,"destHeight":609,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn18@2020_2/2020/08/24/23-52-03-447_a7aae2632c30cbf1.webp","sourceBytes":266729,"destBytes":99976,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1782,"convertSpendMs":52,"createdTime":"2020-08-25 07:52:02","host":"us-032*","referer":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","linkMd5ListStr":"5385792a73444f69dd02146562fbbe85","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"260.5 KB","destSize":"97.6 KB","compressRate":"37.5%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188894245_maze2d_medium_300.gif","sourceStatusCode":200,"destWidth":300,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn42@2020_1/2020/08/24/23-52-03-503_0a72065c90f7c82e.webp","sourceBytes":91942,"destBytes":144114,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1853,"convertSpendMs":746,"createdTime":"2020-08-25 07:52:02","host":"us-024*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"89.8 KB","destSize":"140.7 KB","compressRate":"156.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig1.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn45@2020_4/2020/08/24/23-52-03-397_d176521c493bd989.svg","sourceBytes":359578,"destBytes":359578,"feedId":8140,"totalSpendMs":1853,"convertSpendMs":0,"createdTime":"2020-08-25 07:52:02","host":"us-011*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"351.2 KB","destSize":"351.2 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188931174_ant_medium-250.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn9@2020_6/2020/08/24/23-52-03-545_b77da27892922a9e.webp","sourceBytes":59552,"destBytes":140274,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1888,"convertSpendMs":747,"createdTime":"2020-08-25 07:52:02","host":"us-011*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"58.2 KB","destSize":"137 KB","compressRate":"235.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper2_fig2.png","sourceStatusCode":200,"destWidth":921,"destHeight":662,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn49@2020_6/2020/08/24/23-52-03-713_36c212354a045d86.webp","sourceBytes":915825,"destBytes":43846,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1898,"convertSpendMs":55,"createdTime":"2020-08-25 07:52:02","host":"us-020*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"894.4 KB","destSize":"42.8 KB","compressRate":"4.8%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1591922248906_d4rl_medicine.png","sourceStatusCode":200,"destWidth":1036,"destHeight":287,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn13@2020_2/2020/08/24/23-52-03-560_4edfb6d299f30f07.webp","sourceBytes":19821,"destBytes":19846,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1952,"convertSpendMs":19,"createdTime":"2020-08-25 07:52:02","host":"europe-24*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"19.4 KB","destSize":"19.4 KB","compressRate":"100.1%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig3.png","sourceStatusCode":200,"destWidth":2400,"destHeight":1133,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn25@2020_1/2020/08/24/23-52-03-703_5e59b30ace3fc8d8.webp","sourceBytes":600778,"destBytes":99602,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2105,"convertSpendMs":165,"createdTime":"2020-08-25 07:52:02","host":"us-028*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"586.7 KB","destSize":"97.3 KB","compressRate":"16.6%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/cfr/histograms.png","sourceStatusCode":200,"destWidth":2000,"destHeight":3000,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn42@2020_3/2020/08/24/23-52-03-896_4ae3457cc0279593.webp","sourceBytes":316397,"destBytes":143612,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2252,"convertSpendMs":429,"createdTime":"2020-08-25 07:52:02","host":"us-54*","referer":"http://bair.berkeley.edu/blog/2020/08/03/covid-fatality/","linkMd5ListStr":"966c30164ed85af0624eb0250bbb5312","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"309 KB","destSize":"140.2 KB","compressRate":"45.4%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204105143_kitchen-mixed.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn77@2020_5/2020/08/24/23-52-03-784_4ec075e66c31aac1.webp","sourceBytes":200238,"destBytes":270360,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2398,"convertSpendMs":1012,"createdTime":"2020-08-25 07:52:02","host":"us-54*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"195.5 KB","destSize":"264 KB","compressRate":"135%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204119188_ant_medium-250.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn22@2020_1/2020/08/24/23-52-03-991_9971ccb4db10f870.webp","sourceBytes":59552,"destBytes":140274,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2408,"convertSpendMs":823,"createdTime":"2020-08-25 07:52:02","host":"us-020*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"58.2 KB","destSize":"137 KB","compressRate":"235.5%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195814701_carla_trajectories.png","sourceStatusCode":200,"destWidth":649,"destHeight":498,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn18@2020_5/2020/08/24/23-52-04-067_8d58bcac6c23e87b.webp","sourceBytes":558188,"destBytes":29560,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2340,"convertSpendMs":21,"createdTime":"2020-08-25 07:52:02","host":"europe62*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"545.1 KB","destSize":"28.9 KB","compressRate":"5.3%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174922113_flow-ring.gif","sourceStatusCode":200,"destWidth":300,"destHeight":233,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_5/2020/08/24/23-52-03-727_f623c63869aac133.webp","sourceBytes":332616,"destBytes":545004,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2502,"convertSpendMs":991,"createdTime":"2020-08-25 07:52:02","host":"us-016*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"324.8 KB","destSize":"532.2 KB","compressRate":"163.9%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image6.png","sourceStatusCode":200,"destWidth":738,"destHeight":390,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn34@2020_4/2020/08/24/23-52-04-204_501411ce3b9695f3.webp","sourceBytes":55101,"destBytes":11836,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2380,"convertSpendMs":12,"createdTime":"2020-08-25 07:52:02","host":"europe-58*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"53.8 KB","destSize":"11.6 KB","compressRate":"21.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper2_fig1.png","sourceStatusCode":200,"destWidth":633,"destHeight":529,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn70@2020_1/2020/08/24/23-52-04-542_4a4d2e7fa1a905ad.webp","sourceBytes":50388,"destBytes":19084,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1307,"convertSpendMs":16,"createdTime":"2020-08-25 07:52:04","host":"us-024*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"49.2 KB","destSize":"18.6 KB","compressRate":"37.9%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/gif5.gif","sourceStatusCode":200,"destWidth":48,"destHeight":48,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn89@2020_2/2020/08/24/23-52-04-604_c54656a68997b693.webp","sourceBytes":10812,"destBytes":4620,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1219,"convertSpendMs":23,"createdTime":"2020-08-25 07:52:04","host":"us-028*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"10.6 KB","destSize":"4.5 KB","compressRate":"42.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/umrl/labeled_regret_v2.png","sourceStatusCode":200,"destWidth":947,"destHeight":392,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn73@2020_1/2020/08/24/23-52-04-665_1dd1fd6e3c88df87.webp","sourceBytes":52382,"destBytes":55110,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1476,"convertSpendMs":49,"createdTime":"2020-08-25 07:52:04","host":"us-011*","referer":"http://bair.berkeley.edu/blog/2020/05/01/umrl/","linkMd5ListStr":"4923ec236785bf4c166ae57f30e71201","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.2 KB","destSize":"53.8 KB","compressRate":"105.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/auction/hrl_weight.png","sourceStatusCode":200,"destWidth":1333,"destHeight":861,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn38@2020_5/2020/08/24/23-52-04-414_51073158d539dba0.webp","sourceBytes":182525,"destBytes":73878,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2857,"convertSpendMs":56,"createdTime":"2020-08-25 07:52:02","host":"europe66*","referer":"http://bair.berkeley.edu/blog/2020/07/11/auction/","linkMd5ListStr":"408fef6277527bf1dc3742fe5e40e6fe","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"178.2 KB","destSize":"72.1 KB","compressRate":"40.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/auction/mnist.png","sourceStatusCode":200,"destWidth":2256,"destHeight":642,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn65@2020_1/2020/08/24/23-52-04-805_3218f17caf31c402.webp","sourceBytes":104604,"destBytes":44536,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1813,"convertSpendMs":358,"createdTime":"2020-08-25 07:52:03","host":"us-036*","referer":"http://bair.berkeley.edu/blog/2020/07/11/auction/","linkMd5ListStr":"408fef6277527bf1dc3742fe5e40e6fe","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"102.2 KB","destSize":"43.5 KB","compressRate":"42.6%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig7.png","sourceStatusCode":200,"destWidth":2280,"destHeight":574,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn86@2020_5/2020/08/24/23-52-04-869_b2b0e53df6985c84.webp","sourceBytes":414639,"destBytes":62744,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1598,"convertSpendMs":70,"createdTime":"2020-08-25 07:52:04","host":"us-040*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"404.9 KB","destSize":"61.3 KB","compressRate":"15.1%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174985204_relocate-250.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn9@2020_4/2020/08/24/23-52-04-346_5dff390e712b0ee7.webp","sourceBytes":171793,"destBytes":238726,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3139,"convertSpendMs":492,"createdTime":"2020-08-25 07:52:02","host":"europe66*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"167.8 KB","destSize":"233.1 KB","compressRate":"139%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188923068_ant-umaze-300.gif","sourceStatusCode":200,"destWidth":300,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn97@2020_6/2020/08/24/23-52-04-331_f8361882d0068262.webp","sourceBytes":99907,"destBytes":138214,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3138,"convertSpendMs":574,"createdTime":"2020-08-25 07:52:02","host":"europe-58*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"97.6 KB","destSize":"135 KB","compressRate":"138.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_real_v02_color_correct.png","sourceStatusCode":200,"destWidth":1000,"destHeight":401,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn97@2020_2/2020/08/24/23-52-05-111_de6c76e29f1a9976.webp","sourceBytes":513981,"destBytes":49194,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1577,"convertSpendMs":38,"createdTime":"2020-08-25 07:52:04","host":"us-003*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"501.9 KB","destSize":"48 KB","compressRate":"9.6%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/ryan_dr.jpeg","sourceStatusCode":200,"destWidth":960,"destHeight":1166,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn13@2020_4/2020/08/24/23-52-05-190_e2b01a67c7d30062.webp","sourceBytes":208697,"destBytes":55092,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1511,"convertSpendMs":62,"createdTime":"2020-08-25 07:52:04","host":"us-020*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"203.8 KB","destSize":"53.8 KB","compressRate":"26.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig7.png","sourceStatusCode":200,"destWidth":1872,"destHeight":502,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn5@2020_5/2020/08/24/23-52-05-163_e5152df3c1e6d8e2.webp","sourceBytes":131700,"destBytes":104446,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1636,"convertSpendMs":69,"createdTime":"2020-08-25 07:52:04","host":"us-54*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"128.6 KB","destSize":"102 KB","compressRate":"79.3%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341504303_ezgif.com-optimize-2.gif","sourceStatusCode":200,"destWidth":600,"destHeight":134,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn93@2020_6/2020/08/24/23-52-04-318_a8540d769ea0266e.webp","sourceBytes":106693,"destBytes":242652,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3365,"convertSpendMs":558,"createdTime":"2020-08-25 07:52:02","host":"europe70*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"104.2 KB","destSize":"237 KB","compressRate":"227.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/auction/abstraction_barrier.png","sourceStatusCode":200,"destWidth":1540,"destHeight":622,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn30@2020_1/2020/08/24/23-52-05-387_dca3580158e350e1.webp","sourceBytes":76041,"destBytes":46696,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1433,"convertSpendMs":84,"createdTime":"2020-08-25 07:52:04","host":"us-036*","referer":"http://bair.berkeley.edu/blog/2020/07/11/auction/","linkMd5ListStr":"408fef6277527bf1dc3742fe5e40e6fe","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"74.3 KB","destSize":"45.6 KB","compressRate":"61.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig3.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn1@2020_6/2020/08/24/23-52-03-850_899ae9224682b6e9.svg","sourceBytes":7593832,"destBytes":7593832,"feedId":8140,"totalSpendMs":3633,"convertSpendMs":0,"createdTime":"2020-08-25 07:52:02","host":"us-036*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.2 MB","destSize":"7.2 MB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592192896126_carla-town-opt-200.gif","sourceStatusCode":200,"destWidth":200,"destHeight":200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn49@2020_3/2020/08/24/23-52-04-263_8a167ceed1511ee2.webp","sourceBytes":426094,"destBytes":175648,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3593,"convertSpendMs":298,"createdTime":"2020-08-25 07:52:02","host":"europe-24*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"416.1 KB","destSize":"171.5 KB","compressRate":"41.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig2.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn74@2020_3/2020/08/24/23-52-04-520_8c14c6ecf93e478f.svg","sourceBytes":316389,"destBytes":316389,"feedId":8140,"totalSpendMs":3586,"convertSpendMs":0,"createdTime":"2020-08-25 07:52:02","host":"europe66*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"309 KB","destSize":"309 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188940259_antmaze_large_300.gif","sourceStatusCode":200,"destWidth":300,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn57@2020_4/2020/08/24/23-52-04-608_7a8d27e612817a8b.webp","sourceBytes":88141,"destBytes":200422,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3601,"convertSpendMs":883,"createdTime":"2020-08-25 07:52:02","host":"europe70*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"86.1 KB","destSize":"195.7 KB","compressRate":"227.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig5.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn33@2020_6/2020/08/24/23-52-05-368_8ba573b4563a53b7.svg","sourceBytes":239781,"destBytes":239781,"feedId":8140,"totalSpendMs":1660,"convertSpendMs":0,"createdTime":"2020-08-25 07:52:04","host":"us-024*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"234.2 KB","destSize":"234.2 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ocda/figure_6.png","sourceStatusCode":200,"destWidth":2656,"destHeight":1154,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn53@2020_1/2020/08/24/23-52-04-974_f127ec37fbf79690.webp","sourceBytes":1206081,"destBytes":260386,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3731,"convertSpendMs":122,"createdTime":"2020-08-25 07:52:02","host":"europe62*","referer":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","linkMd5ListStr":"5385792a73444f69dd02146562fbbe85","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"254.3 KB","compressRate":"21.6%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174991459_door-250.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn10@2020_2/2020/08/24/23-52-05-431_0b3b911768217d91.webp","sourceBytes":325994,"destBytes":413312,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2171,"convertSpendMs":901,"createdTime":"2020-08-25 07:52:04","host":"us-032*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"318.4 KB","destSize":"403.6 KB","compressRate":"126.8%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image3.gif","sourceStatusCode":200,"destWidth":600,"destHeight":311,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn89@2020_4/2020/08/24/23-52-05-448_dfef929ece6a4399.webp","sourceBytes":4193316,"destBytes":893408,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":4040,"convertSpendMs":1726,"createdTime":"2020-08-25 07:52:02","host":"us-007*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4 MB","destSize":"872.5 KB","compressRate":"21.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/umrl/Unsupervised-Meta-Learning-v2.png","sourceStatusCode":200,"destWidth":1334,"destHeight":330,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn1@2020_2/2020/08/24/23-52-05-637_54298ecbf9b65c23.webp","sourceBytes":42138,"destBytes":34308,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2259,"convertSpendMs":28,"createdTime":"2020-08-25 07:52:04","host":"europe66*","referer":"http://bair.berkeley.edu/blog/2020/05/01/umrl/","linkMd5ListStr":"4923ec236785bf4c166ae57f30e71201","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.2 KB","destSize":"33.5 KB","compressRate":"81.4%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig6.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn17@2020_2/2020/08/24/23-52-05-438_2679ac501cbd37f9.svg","sourceBytes":2506988,"destBytes":2506988,"feedId":8140,"totalSpendMs":2327,"convertSpendMs":0,"createdTime":"2020-08-25 07:52:04","host":"us-007*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 MB","destSize":"2.4 MB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/gif2.jpg","sourceStatusCode":200,"destWidth":48,"destHeight":48,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn77@2020_2/2020/08/24/23-52-06-139_462ddf2f83fbe722.webp","sourceBytes":1127,"destBytes":750,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1148,"convertSpendMs":4,"createdTime":"2020-08-25 07:52:05","host":"us-020*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.1 KB","destSize":"750 B","compressRate":"66.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/auction/centralized_gif.gif","sourceStatusCode":200,"destWidth":853,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn50@2020_1/2020/08/24/23-52-06-129_ff489a4423717278.webp","sourceBytes":171675,"destBytes":46918,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1885,"convertSpendMs":393,"createdTime":"2020-08-25 07:52:05","host":"us-040*","referer":"http://bair.berkeley.edu/blog/2020/07/11/auction/","linkMd5ListStr":"408fef6277527bf1dc3742fe5e40e6fe","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"167.7 KB","destSize":"45.8 KB","compressRate":"27.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/umrl/UMRL.png","sourceStatusCode":200,"destWidth":901,"destHeight":294,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn61@2020_4/2020/08/24/23-52-05-953_ff1213976b69fa52.webp","sourceBytes":93596,"destBytes":51340,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1438,"convertSpendMs":37,"createdTime":"2020-08-25 07:52:05","host":"us-003*","referer":"http://bair.berkeley.edu/blog/2020/05/01/umrl/","linkMd5ListStr":"4923ec236785bf4c166ae57f30e71201","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"91.4 KB","destSize":"50.1 KB","compressRate":"54.9%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig4.png","sourceStatusCode":200,"destWidth":8000,"destHeight":4500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn5@2020_4/2020/08/24/23-52-05-842_6e265ed6c7d4f989.webp","sourceBytes":3216385,"destBytes":513886,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":4458,"convertSpendMs":2180,"createdTime":"2020-08-25 07:52:02","host":"us-024*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.1 MB","destSize":"501.8 KB","compressRate":"16%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/gif3.gif","sourceStatusCode":200,"destWidth":49,"destHeight":49,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn42@2020_3/2020/08/24/23-52-06-196_5e3af93a0d047472.webp","sourceBytes":2922,"destBytes":1164,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2038,"convertSpendMs":5,"createdTime":"2020-08-25 07:52:05","host":"europe-24*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 KB","destSize":"1.1 KB","compressRate":"39.8%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188887659_maze2d_umaze_300.gif","sourceStatusCode":200,"destWidth":300,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn77@2020_3/2020/08/24/23-52-05-809_616603c7ef3692da.webp","sourceBytes":171236,"destBytes":106172,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3021,"convertSpendMs":563,"createdTime":"2020-08-25 07:52:04","host":"europe-24*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"167.2 KB","destSize":"103.7 KB","compressRate":"62%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper1_fig2.png","sourceStatusCode":200,"destWidth":1271,"destHeight":603,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn73@2020_5/2020/08/24/23-52-06-424_ff27a726c0a9ac26.webp","sourceBytes":676720,"destBytes":44386,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1593,"convertSpendMs":54,"createdTime":"2020-08-25 07:52:05","host":"us-032*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"660.9 KB","destSize":"43.3 KB","compressRate":"6.6%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig6.png","sourceStatusCode":200,"destWidth":1542,"destHeight":694,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn82@2020_2/2020/08/24/23-52-05-976_09d401c72350188b.webp","sourceBytes":671431,"destBytes":106764,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3134,"convertSpendMs":53,"createdTime":"2020-08-25 07:52:04","host":"europe62*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"655.7 KB","destSize":"104.3 KB","compressRate":"15.9%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/gif6.jpg","sourceStatusCode":200,"destWidth":48,"destHeight":48,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn13@2020_2/2020/08/24/23-52-06-620_1ee4ab6ae8f0d471.webp","sourceBytes":661,"destBytes":264,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1152,"convertSpendMs":9,"createdTime":"2020-08-25 07:52:06","host":"us-54*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"661 B","destSize":"264 B","compressRate":"39.9%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592341727777_ezgif.com-optimize-3.gif","sourceStatusCode":200,"destWidth":600,"destHeight":133,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn13@2020_5/2020/08/24/23-52-06-506_abba22d205466a45.webp","sourceBytes":37504,"destBytes":51774,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1109,"convertSpendMs":133,"createdTime":"2020-08-25 07:52:06","host":"us-040*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"36.6 KB","destSize":"50.6 KB","compressRate":"138%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image2.gif","sourceStatusCode":200,"destWidth":600,"destHeight":338,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn70@2020_2/2020/08/24/23-52-06-303_e1c36027967e8ac9.webp","sourceBytes":8901499,"destBytes":542264,"targetWebpQuality":45,"feedId":8140,"totalSpendMs":4764,"convertSpendMs":2306,"createdTime":"2020-08-25 07:52:02","host":"us-003*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"8.5 MB","destSize":"529.6 KB","compressRate":"6.1%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188978363_mjc-200.gif","sourceStatusCode":200,"destWidth":200,"destHeight":200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn93@2020_6/2020/08/24/23-52-06-058_55a69dc7ddf9513d.webp","sourceBytes":757959,"destBytes":859556,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3296,"convertSpendMs":1210,"createdTime":"2020-08-25 07:52:04","host":"us-016*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"740.2 KB","destSize":"839.4 KB","compressRate":"113.4%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520372323_kitchen-mixed.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn82@2020_5/2020/08/24/23-52-06-419_1e5326835314f049.webp","sourceBytes":200238,"destBytes":270360,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1721,"convertSpendMs":585,"createdTime":"2020-08-25 07:52:05","host":"us-007*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"195.5 KB","destSize":"264 KB","compressRate":"135%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/wilson_paper1_fig1.png","sourceStatusCode":200,"destWidth":1442,"destHeight":376,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn46@2020_1/2020/08/24/23-52-06-642_e36ed234bd8aeebf.webp","sourceBytes":185638,"destBytes":35356,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2475,"convertSpendMs":25,"createdTime":"2020-08-25 07:52:05","host":"europe62*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"181.3 KB","destSize":"34.5 KB","compressRate":"19%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174981370_pen-250.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn69@2020_6/2020/08/24/23-52-06-430_bca07b26c6dffbe9.webp","sourceBytes":197664,"destBytes":294132,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2156,"convertSpendMs":827,"createdTime":"2020-08-25 07:52:05","host":"us-54*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"193 KB","destSize":"287.2 KB","compressRate":"148.8%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image4.png","sourceStatusCode":200,"destWidth":972,"destHeight":336,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn22@2020_4/2020/08/24/23-52-07-110_15ed6fcfe1d1800f.webp","sourceBytes":420125,"destBytes":27888,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1606,"convertSpendMs":22,"createdTime":"2020-08-25 07:52:06","host":"us-016*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"410.3 KB","destSize":"27.2 KB","compressRate":"6.6%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/daniel_traj_corners_analytic_v04.png","sourceStatusCode":200,"destWidth":1300,"destHeight":488,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn25@2020_2/2020/08/24/23-52-07-145_956d49f22234277e.webp","sourceBytes":308330,"destBytes":34430,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1651,"convertSpendMs":38,"createdTime":"2020-08-25 07:52:06","host":"us-003*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"301.1 KB","destSize":"33.6 KB","compressRate":"11.2%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592520341217_kitchen-partial.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn21@2020_5/2020/08/24/23-52-06-635_f8a2a42ed0808695.webp","sourceBytes":223361,"destBytes":315820,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3500,"convertSpendMs":864,"createdTime":"2020-08-25 07:52:04","host":"europe70*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"218.1 KB","destSize":"308.4 KB","compressRate":"141.4%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592174975902_hammer-250.gif","sourceStatusCode":200,"destWidth":250,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn66@2020_1/2020/08/24/23-52-06-856_8b598e1abcdda9d5.webp","sourceBytes":84923,"destBytes":190416,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2888,"convertSpendMs":401,"createdTime":"2020-08-25 07:52:05","host":"europe66*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"82.9 KB","destSize":"186 KB","compressRate":"224.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ocda/figure_3.png","sourceStatusCode":200,"destWidth":2370,"destHeight":674,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn37@2020_1/2020/08/24/23-52-07-458_285b8b4d400be589.webp","sourceBytes":826311,"destBytes":158690,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1779,"convertSpendMs":71,"createdTime":"2020-08-25 07:52:06","host":"us-032*","referer":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","linkMd5ListStr":"5385792a73444f69dd02146562fbbe85","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"806.9 KB","destSize":"155 KB","compressRate":"19.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/umrl/Meta-Learning-v2.png","sourceStatusCode":200,"destWidth":1338,"destHeight":332,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn9@2020_4/2020/08/24/23-52-07-435_86ec97ee772bf3a2.webp","sourceBytes":36747,"destBytes":29064,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2113,"convertSpendMs":36,"createdTime":"2020-08-25 07:52:06","host":"europe62*","referer":"http://bair.berkeley.edu/blog/2020/05/01/umrl/","linkMd5ListStr":"4923ec236785bf4c166ae57f30e71201","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"35.9 KB","destSize":"28.4 KB","compressRate":"79.1%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592195697818_antmaze-large-play-v0.png","sourceStatusCode":200,"destWidth":557,"destHeight":677,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn5@2020_2/2020/08/24/23-52-07-410_197a1782678e8875.webp","sourceBytes":279777,"destBytes":22542,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2147,"convertSpendMs":18,"createdTime":"2020-08-25 07:52:06","host":"europe-24*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"273.2 KB","destSize":"22 KB","compressRate":"8.1%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/Fig4.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn98@2020_1/2020/08/24/23-52-07-015_f7370f60b4501be0.svg","sourceBytes":5021674,"destBytes":5021674,"feedId":8140,"totalSpendMs":2954,"convertSpendMs":0,"createdTime":"2020-08-25 07:52:06","host":"us-024*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.8 MB","destSize":"4.8 MB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/icm/image1.png","sourceStatusCode":200,"destWidth":408,"destHeight":360,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn89@2020_4/2020/08/24/23-52-08-006_5b016bc6b6b35131.webp","sourceBytes":277774,"destBytes":21618,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3046,"convertSpendMs":16,"createdTime":"2020-08-25 07:52:06","host":"europe-58*","referer":"http://bair.berkeley.edu/blog/2020/07/24/icm-kids/","linkMd5ListStr":"cdcbf18cd5159cf20ae969d8792dfd9b","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"271.3 KB","destSize":"21.1 KB","compressRate":"7.8%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188898461_maze2d_large_300.gif","sourceStatusCode":200,"destWidth":300,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn33@2020_2/2020/08/24/23-52-08-144_c90d57f361421ad2.webp","sourceBytes":159627,"destBytes":182620,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2564,"convertSpendMs":1528,"createdTime":"2020-08-25 07:52:06","host":"us-54*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"155.9 KB","destSize":"178.3 KB","compressRate":"114.4%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592204097994_carla_lane.gif","sourceStatusCode":200,"destWidth":256,"destHeight":256,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn29@2020_3/2020/08/24/23-52-07-353_f9e54a4eca43fef7.webp","sourceBytes":6399751,"destBytes":373312,"targetWebpQuality":60,"feedId":8140,"totalSpendMs":2645,"convertSpendMs":565,"createdTime":"2020-08-25 07:52:06","host":"europe66*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.1 MB","destSize":"364.6 KB","compressRate":"5.8%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/auction/decentralized_gif.gif","sourceStatusCode":200,"destWidth":853,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn25@2020_2/2020/08/24/23-52-07-685_ee86aa308cf9e5d1.webp","sourceBytes":211173,"destBytes":63354,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":4480,"convertSpendMs":1560,"createdTime":"2020-08-25 07:52:04","host":"europe-58*","referer":"http://bair.berkeley.edu/blog/2020/07/11/auction/","linkMd5ListStr":"408fef6277527bf1dc3742fe5e40e6fe","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"206.2 KB","destSize":"61.9 KB","compressRate":"30%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig5.png","sourceStatusCode":200,"destWidth":8000,"destHeight":4500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn61@2020_1/2020/08/24/23-52-07-866_8561c15867f93bc4.webp","sourceBytes":805686,"destBytes":191214,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":5775,"convertSpendMs":2343,"createdTime":"2020-08-25 07:52:03","host":"europe-58*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"786.8 KB","destSize":"186.7 KB","compressRate":"23.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ocda/figure_2.png","sourceStatusCode":200,"destWidth":1592,"destHeight":1131,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn17@2020_3/2020/08/24/23-52-08-961_390c0133597feed8.webp","sourceBytes":501487,"destBytes":140906,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3672,"convertSpendMs":1405,"createdTime":"2020-08-25 07:52:06","host":"us-028*","referer":"http://bair.berkeley.edu/blog/2020/06/14/ocda/","linkMd5ListStr":"5385792a73444f69dd02146562fbbe85","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"489.7 KB","destSize":"137.6 KB","compressRate":"28.1%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/curl/fig2.png","sourceStatusCode":200,"destWidth":8000,"destHeight":4500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn61@2020_4/2020/08/24/23-52-09-920_88d65322b4aeb8fe.webp","sourceBytes":3011092,"destBytes":442644,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":8652,"convertSpendMs":6286,"createdTime":"2020-08-25 07:52:02","host":"us-028*","referer":"http://bair.berkeley.edu/blog/2020/07/19/curl-rad/","linkMd5ListStr":"d6cb67c944b5aeede514d99f8b6c5e9d","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 MB","destSize":"432.3 KB","compressRate":"14.7%"},{"code":1,"isDone":false,"source":"https://paper-attachments.dropbox.com/s_4F9162F3A4AABEDE031F0146AB6FE1EDDF0BCCBF5DDA0C2D5C72EFD92EC6E5C3_1592188981020_mjc-200-2.gif","sourceStatusCode":200,"destWidth":200,"destHeight":200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_5/2020/08/24/23-52-10-058_c1c4a7c28548eff7.webp","sourceBytes":811293,"destBytes":782818,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":6408,"convertSpendMs":4752,"createdTime":"2020-08-25 07:52:05","host":"us-028*","referer":"http://bair.berkeley.edu/blog/2020/06/25/D4RL/","linkMd5ListStr":"b97829498e5b7566a24b65576deb1a00","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"792.3 KB","destSize":"764.5 KB","compressRate":"96.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/ai4all/ai4all-gif-overcooked.gif","sourceStatusCode":200,"destWidth":796,"destHeight":634,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn37@2020_4/2020/08/24/23-52-11-347_6116ec63f2a0601a.webp","sourceBytes":849750,"destBytes":376774,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":7506,"convertSpendMs":5643,"createdTime":"2020-08-25 07:52:04","host":"us-011*","referer":"http://bair.berkeley.edu/blog/2020/08/16/ai4all/","linkMd5ListStr":"dad24dc607eb2107cabc3dc273905bfc","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"829.8 KB","destSize":"367.9 KB","compressRate":"44.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/daniel_IROS-2020-example-cloth-optim.gif","sourceStatusCode":200,"destWidth":450,"destHeight":334,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn94@2020_4/2020/08/24/23-52-11-052_10e201ac146b7445.webp","sourceBytes":7123304,"destBytes":2401008,"targetWebpQuality":60,"feedId":8140,"totalSpendMs":6491,"convertSpendMs":3956,"createdTime":"2020-08-25 07:52:06","host":"us-036*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.8 MB","destSize":"2.3 MB","compressRate":"33.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/fabrics/wilson_intro_gif.gif","sourceStatusCode":200,"destWidth":600,"destHeight":338,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn30@2020_3/2020/08/24/23-52-11-889_d9193f9b76a1ed40.webp","sourceBytes":28791303,"destBytes":1791196,"targetWebpQuality":4,"feedId":8140,"totalSpendMs":11477,"convertSpendMs":5550,"createdTime":"2020-08-25 07:52:02","host":"europe70*","referer":"http://bair.berkeley.edu/blog/2020/05/05/fabrics/","linkMd5ListStr":"4064027528e647e4c2f31474a205be17","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"27.5 MB","destSize":"1.7 MB","compressRate":"6.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/gif1.gif","sourceStatusCode":200,"destWidth":320,"destHeight":180,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn1@2020_2/2020/08/24/23-52-12-547_d9c0dd27ad384252.webp","sourceBytes":7431750,"destBytes":7918276,"targetWebpQuality":52,"feedId":8140,"totalSpendMs":8769,"convertSpendMs":5502,"createdTime":"2020-08-25 07:52:06","host":"us-011*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.1 MB","destSize":"7.6 MB","compressRate":"106.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/gif3.gif","sourceStatusCode":200,"destWidth":320,"destHeight":180,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn58@2020_3/2020/08/24/23-52-11-541_1055c2e75c8f229a.webp","sourceBytes":6864423,"destBytes":8390718,"targetWebpQuality":60,"feedId":8140,"totalSpendMs":10816,"convertSpendMs":5291,"createdTime":"2020-08-25 07:52:05","host":"us-016*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.5 MB","destSize":"8 MB","compressRate":"122.2%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/omnitact/gif2.gif","sourceStatusCode":200,"destWidth":320,"destHeight":180,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn85@2020_2/2020/08/24/23-52-13-604_4bd1bec78874a444.webp","sourceBytes":6247675,"destBytes":8037086,"targetWebpQuality":67,"feedId":8140,"totalSpendMs":13875,"convertSpendMs":5271,"createdTime":"2020-08-25 07:52:06","host":"europe70*","referer":"http://bair.berkeley.edu/blog/2020/05/14/omnitact/","linkMd5ListStr":"cfd0bb55eedfe1f8759b2fb30581b5c9","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6 MB","destSize":"7.7 MB","compressRate":"128.6%"}],"successGithubMap":{"myreaderx14":3,"myreaderx8":2,"myreaderx15":3,"myreaderx7":3,"myreaderx6":3,"myreaderx16":3,"myreaderx32":4,"myreaderx10":3,"myreaderx4":3,"myreaderx33":4,"myreaderx3":2,"myreaderx11":3,"myreaderx2":3,"myreaderx12":3,"myreaderx13":3,"myreaderx1":3,"myreaderx30":4,"myreaderx31":3,"myreaderx18":3,"myreaderx19":3,"myreaderx":3,"myreaderx25":4,"myreaderx27":3,"myreaderx21":2,"myreaderx22":3,"myreaderx23":3,"myreaderx24":3,"myreaderx5oss":3,"myreaderx29":3},"failGithubMap":{}}