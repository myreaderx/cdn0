{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-10-17 01:43:29","updatedTime":"2020-10-17 01:43:29","title":"Kafka设计解析（七）- Kafka Stream","link":"http://www.jasongj.com/kafka/kafka_stream/","description":"<blockquote> \n <p>原创文章，转载请务必将下面这段话置于文章开头处。<br>本文转发自<a href=\"http://www.jasongj.com\"><strong>技术世界</strong></a>，<a href=\"http://www.jasongj.com/kafka/kafka_stream/\">原文链接</a>　<a href=\"http://www.jasongj.com/kafka/kafka_stream/\">http://www.jasongj.com/kafka/kafka_stream/</a></br></p> \n</blockquote> \n<h1 id=\"Kafka-Stream背景\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream背景\" class=\"headerlink\" title=\"Kafka Stream背景\"></a>Kafka Stream背景</h1> \n<h2 id=\"Kafka-Stream是什么\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream是什么\" class=\"headerlink\" title=\"Kafka Stream是什么\"></a>Kafka Stream是什么</h2> \n<p>Kafka Stream是Apache Kafka从0.10版本引入的一个新Feature。它是提供了对存储于Kafka内的数据进行流式处理和分析的功能。</p> \n<p>Kafka Stream的特点如下：</p> \n<ul> \n <li>Kafka Stream提供了一个非常简单而轻量的Library，它可以非常方便地嵌入任意Java应用中，也可以任意方式打包和部署</li> \n <li>除了Kafka外，无任何外部依赖</li> \n <li>充分利用Kafka分区机制实现水平扩展和顺序性保证</li> \n <li>通过可容错的state store实现高效的状态操作（如windowed join和aggregation）</li> \n <li>支持正好一次处理语义</li> \n <li>提供记录级的处理能力，从而实现毫秒级的低延迟</li> \n <li>支持基于事件时间的窗口操作，并且可处理晚到的数据（late arrival of records）</li> \n <li>同时提供底层的处理原语Processor（类似于Storm的spout和bolt），以及高层抽象的DSL（类似于Spark的map/group/reduce）</li> \n</ul> \n<h2 id=\"什么是流式计算\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#什么是流式计算\" class=\"headerlink\" title=\"什么是流式计算\"></a>什么是流式计算</h2> \n<p>一般流式计算会与批量计算相比较。在流式计算模型中，输入是持续的，可以认为在时间上是无界的，也就意味着，永远拿不到全量数据去做计算。同时，计算结果是持续输出的，也即计算结果在时间上也是无界的。流式计算一般对实时性要求较高，同时一般是先定义目标计算，然后数据到来之后将计算逻辑应用于数据。同时为了提高计算效率，往往尽可能采用增量计算代替全量计算。</p> \n<div align=\"center\"> \n <br><img width=\"70%\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/stream_procissing.png\" alt=\"Stream Processing\"><br /></img></br> \n</div> \n<p>批量处理模型中，一般先有全量数据集，然后定义计算逻辑，并将计算应用于全量数据。特点是全量计算，并且计算结果一次性全量输出。</p> \n<div align=\"center\"> \n <br><img width=\"70%\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/batch_procissing.png\" alt=\"Batch Processing\"><br /></img></br> \n</div> \n<h2 id=\"为什么要有Kafka-Stream\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#为什么要有Kafka-Stream\" class=\"headerlink\" title=\"为什么要有Kafka Stream\"></a>为什么要有Kafka Stream</h2> \n<p>当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如MapR，Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。</p> \n<p>既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？笔者认为主要有如下原因。</p> \n<p>第一，Spark和Storm都是流式处理<strong>框架</strong>，而Kafka Stream提供的是一个基于Kafka的流式处理<strong>类库</strong>。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理<strong>类库</strong>，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。</p> \n<div align=\"center\"> \n <br><img width=\"60%\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/library.png\" alt=\"Library vs. Framework\"><br /></img></br> \n</div> \n<p>第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。更为重要的是，Kafka Stream充分利用了<a href=\"http://www.jasongj.com/2015/03/10/KafkaColumn1/#Topic-amp-Partition\">Kafka的分区机制</a>和<a href=\"http://www.jasongj.com/2015/08/09/KafkaColumn4/#High-Level-Consumer-Rebalance\">Consumer的Rebalance机制</a>，使得Kafka Stream可以非常方便的水平扩展，并且各个实例可以使用不同的部署方式。具体来说，每个运行Kafka Stream的应用程序实例都包含了Kafka Consumer实例，多个同一应用的实例之间并行处理数据集。而不同实例之间的部署方式并不要求一致，比如部分实例可以运行在Web容器中，部分实例可运行在Docker或Kubernetes中。</p> \n<p>第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。</p> \n<p>第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。</p> \n<p>第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。</p> \n<p>第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。</p> \n<h1 id=\"Kafka-Stream架构\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream架构\" class=\"headerlink\" title=\"Kafka Stream架构\"></a>Kafka Stream架构</h1> \n<h2 id=\"Kafka-Stream整体架构\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream整体架构\" class=\"headerlink\" title=\"Kafka Stream整体架构\"></a>Kafka Stream整体架构</h2> \n<p>Kafka Stream的整体架构图如下所示。</p> \n<div align=\"center\"> \n <br><img width=\"80%\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/Kafka%20Stream%20Architecture.png\" alt=\"Kafka Stream Architecture\"><br /></img></br> \n</div> \n<p>目前（Kafka 0.11.0.0）Kafka Stream的数据源只能如上图所示是Kafka。但是处理结果并不一定要如上图所示输出到Kafka。实际上KStream和Ktable的实例化都需要指定Topic。<br> \n  <figure class=\"highlight java\"> \n   <table> \n    <tr> \n     <td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br /></br></br></pre></td> \n     <td class=\"code\"><pre><span class=\"line\">KStream&lt;String, String&gt; stream = builder.stream(<span class=\"string\">\"words-stream\"</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">KTable&lt;String, String&gt; table = builder.table(<span class=\"string\">\"words-table\"</span>, <span class=\"string\">\"words-store\"</span>);</span><br /></br></br></pre></td> \n    </tr> \n   </table> \n  </figure></br></p> \n<p>另外，上图中的Consumer和Producer并不需要开发者在应用中显示实例化，而是由Kafka Stream根据参数隐式实例化和管理，从而降低了使用门槛。开发者只需要专注于开发核心业务逻辑，也即上图中Task内的部分。</p> \n<h2 id=\"Processor-Topology\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Processor-Topology\" class=\"headerlink\" title=\"Processor Topology\"></a>Processor Topology</h2> \n<p>基于Kafka Stream的流式应用的业务逻辑全部通过一个被称为Processor Topology的地方执行。它与Storm的Topology和Spark的DAG类似，都定义了数据在各个处理单元（在Kafka Stream中被称作Processor）间的流动方式，或者说定义了数据的处理逻辑。</p> \n<p>下面是一个Processor的示例，它实现了Word Count功能，并且每秒输出一次结果。<br> \n  <figure class=\"highlight java\"> \n   <table> \n    <tr> \n     <td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br /></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></pre></td> \n     <td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordCountProcessor</span> <span class=\"keyword\">implements</span> <span class=\"title\">Processor</span>&lt;<span class=\"title\">String</span>, <span class=\"title\">String</span>&gt; </span>{</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> ProcessorContext context;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> KeyValueStore&lt;String, Integer&gt; kvStore;</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"meta\">@SuppressWarnings</span>(<span class=\"string\">\"unchecked\"</span>)</span><br><span class=\"line\"> <span class=\"meta\">@Override</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">init</span><span class=\"params\">(ProcessorContext context)</span> </span>{</span><br><span class=\"line\"> <span class=\"keyword\">this</span>.context = context;</span><br><span class=\"line\"> <span class=\"keyword\">this</span>.context.schedule(<span class=\"number\">1000</span>);</span><br><span class=\"line\"> <span class=\"keyword\">this</span>.kvStore = (KeyValueStore&lt;String, Integer&gt;) context.getStateStore(<span class=\"string\">\"Counts\"</span>);</span><br><span class=\"line\"> }</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"meta\">@Override</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">process</span><span class=\"params\">(String key, String value)</span> </span>{</span><br><span class=\"line\"> Stream.of(value.toLowerCase().split(<span class=\"string\">\" \"</span>)).forEach((String word) -&gt; {</span><br><span class=\"line\"> Optional&lt;Integer&gt; counts = Optional.ofNullable(kvStore.get(word));</span><br><span class=\"line\"> <span class=\"keyword\">int</span> count = counts.map(wordcount -&gt; wordcount + <span class=\"number\">1</span>).orElse(<span class=\"number\">1</span>);</span><br><span class=\"line\"> kvStore.put(word, count);</span><br><span class=\"line\"> });</span><br><span class=\"line\"> }</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"meta\">@Override</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">punctuate</span><span class=\"params\">(<span class=\"keyword\">long</span> timestamp)</span> </span>{</span><br><span class=\"line\"> KeyValueIterator&lt;String, Integer&gt; iterator = <span class=\"keyword\">this</span>.kvStore.all();</span><br><span class=\"line\"> iterator.forEachRemaining(entry -&gt; {</span><br><span class=\"line\"> context.forward(entry.key, entry.value);</span><br><span class=\"line\"> <span class=\"keyword\">this</span>.kvStore.delete(entry.key);</span><br><span class=\"line\"> });</span><br><span class=\"line\"> context.commit();</span><br><span class=\"line\"> }</span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"meta\">@Override</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">close</span><span class=\"params\">()</span> </span>{</span><br><span class=\"line\"> <span class=\"keyword\">this</span>.kvStore.close();</span><br><span class=\"line\"> }</span><br><span class=\"line\"></span><br><span class=\"line\">}</span><br /></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></br></pre></td> \n    </tr> \n   </table> \n  </figure></br></p> \n<p>从上述代码中可见</p> \n<ul> \n <li><code>process</code>定义了对每条记录的处理逻辑，也印证了Kafka可具有记录级的数据处理能力。</li> \n <li>context.scheduler定义了punctuate被执行的周期，从而提供了实现窗口操作的能力。</li> \n <li>context.getStateStore提供的状态存储为有状态计算（如窗口，聚合）提供了可能。</li> \n</ul> \n<h2 id=\"Kafka-Stream并行模型\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream并行模型\" class=\"headerlink\" title=\"Kafka Stream并行模型\"></a>Kafka Stream并行模型</h2> \n<p>Kafka Stream的并行模型中，最小粒度为Task，而每个Task包含一个特定子Topology的所有Processor。因此每个Task所执行的代码完全一样，唯一的不同在于所处理的数据集互补。这一点跟Storm的Topology完全不一样。Storm的Topology的每一个Task只包含一个Spout或Bolt的实例。因此Storm的一个Topology内的不同Task之间需要通过网络通信传递数据，而Kafka Stream的Task包含了完整的子Topology，所以Task之间不需要传递数据，也就不需要网络通信。这一点降低了系统复杂度，也提高了处理效率。</p> \n<p>如果某个Stream的输入Topic有多个(比如2个Topic，1个Partition数为4，另一个Partition数为3)，则总的Task数等于Partition数最多的那个Topic的Partition数（max(4,3)=4）。这是因为Kafka Stream使用了Consumer的Rebalance机制，每个Partition对应一个Task。</p> \n<p>下图展示了在一个进程（Instance）中以2个Topic（Partition数均为4）为数据源的Kafka Stream应用的并行模型。从图中可以看到，由于Kafka Stream应用的默认线程数为1，所以4个Task全部在一个线程中运行。</p> \n<div align=\"center\"> \n <br><img width=\"80%\" alt=\"1 thread\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/1%20thread.png\"><br /></img></br> \n</div> \n<p>为了充分利用多线程的优势，可以设置Kafka Stream的线程数。下图展示了线程数为2时的并行模型。</p> \n<div align=\"center\"> \n <br><img width=\"80%\" alt=\"2 threads\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20threads.png\"><br /></img></br> \n</div> \n<p>前文有提到，Kafka Stream可被嵌入任意Java应用（理论上基于JVM的应用都可以）中，下图展示了在同一台机器的不同进程中同时启动同一Kafka Stream应用时的并行模型。注意，这里要保证两个进程的<code>StreamsConfig.APPLICATION_ID_CONFIG</code>完全一样。因为Kafka Stream将APPLICATION_ID_CONFIG作为隐式启动的Consumer的Group ID。只有保证APPLICATION_ID_CONFIG相同，才能保证这两个进程的Consumer属于同一个Group，从而可以通过Consumer Rebalance机制拿到互补的数据集。</p> \n<div align=\"center\"> \n <br><img width=\"80%\" alt=\"2 instances\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20instances.png\"><br /></img></br> \n</div> \n<p>既然实现了多进程部署，可以以同样的方式实现多机器部署。该部署方式也要求所有进程的APPLICATION_ID_CONFIG完全一样。从图上也可以看到，每个实例中的线程数并不要求一样。但是无论如何部署，Task总数总会保证一致。</p> \n<div align=\"center\"> \n <br><img width=\"80%\" alt=\"2 servers\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20servers.png\"><br /></img></br> \n</div> \n<p>注意：Kafka Stream的并行模型，非常依赖于《<a href=\"http://www.jasongj.com/2015/03/10/KafkaColumn1\">Kafka设计解析（一）- Kafka背景及架构介绍</a>》一文中介绍的<a href=\"http://www.jasongj.com/2015/03/10/KafkaColumn1/#Topic-amp-Partition\">Kafka分区机制</a>和《<a href=\"http://www.jasongj.com/2015/08/09/KafkaColumn4/\">Kafka设计解析（四）- Kafka Consumer设计解析</a>》中介绍的<a href=\"http://www.jasongj.com/2015/08/09/KafkaColumn4/#High-Level-Consumer-Rebalance\">Consumer的Rebalance机制</a>。强烈建议不太熟悉这两种机制的朋友，先行阅读这两篇文章。</p> \n<p>这里对比一下Kafka Stream的Processor Topology与Storm的Topology。</p> \n<ul> \n <li>Storm的Topology由Spout和Bolt组成，Spout提供数据源，而Bolt提供计算和数据导出。Kafka Stream的Processor Topology完全由Processor组成，因为它的数据固定由Kafka的Topic提供。</li> \n <li>Storm的不同Bolt运行在不同的Executor中，很可能位于不同的机器，需要通过网络通信传输数据。而Kafka Stream的Processor Topology的不同Processor完全运行于同一个Task中，也就完全处于同一个线程，无需网络通信。</li> \n <li>Storm的Topology可以同时包含Shuffle部分和非Shuffle部分，并且往往一个Topology就是一个完整的应用。而Kafka Stream的一个物理Topology只包含非Shuffle部分，而Shuffle部分需要通过<code>through</code>操作显示完成，该操作将一个大的Topology分成了2个子Topology。</li> \n <li>Storm的Topology内，不同Bolt/Spout的并行度可以不一样，而Kafka Stream的子Topology内，所有Processor的并行度完全一样。</li> \n <li>Storm的一个Task只包含一个Spout或者Bolt的实例，而Kafka Stream的一个Task包含了一个子Topology的所有Processor。</li> \n</ul> \n<h2 id=\"KTable-vs-KStream\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#KTable-vs-KStream\" class=\"headerlink\" title=\"KTable vs. KStream\"></a>KTable vs. KStream</h2> \n<p>KTable和KStream是Kafka Stream中非常重要的两个概念，它们是Kafka实现各种语义的基础。因此这里有必要分析下二者的区别。</p> \n<p>KStream是一个数据流，可以认为所有记录都通过Insert only的方式插入进这个数据流里。而KTable代表一个完整的数据集，可以理解为数据库中的表。由于每条记录都是Key-Value对，这里可以将Key理解为数据库中的Primary Key，而Value可以理解为一行记录。可以认为KTable中的数据都是通过Update only的方式进入的。也就意味着，如果KTable对应的Topic中新进入的数据的Key已经存在，那么从KTable只会取出同一Key对应的最后一条数据，相当于新的数据更新了旧的数据。</p> \n<p>以下图为例，假设有一个KStream和KTable，基于同一个Topic创建，并且该Topic中包含如下图所示5条数据。此时遍历KStream将得到与Topic内数据完全一样的所有5条数据，且顺序不变。而此时遍历KTable时，因为这5条记录中有3个不同的Key，所以将得到3条记录，每个Key对应最新的值，并且这三条数据之间的顺序与原来在Topic中的顺序保持一致。这一点与Kafka的日志compact相同。</p> \n<div align=\"center\"> \n <br><img width=\"60%\" alt=\"KStream vs. KTable\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/ktable_kstream.png\"><br /></img></br> \n</div> \n<p>此时如果对该KStream和KTable分别基于key做Group，对Value进行Sum，得到的结果将会不同。对KStream的计算结果是<code>&lt;Jack，4&gt;</code>，<code>&lt;Lily，7&gt;</code>，<code>&lt;Mike，4&gt;</code>。而对Ktable的计算结果是<code>&lt;Mike，4&gt;</code>，<code>&lt;Jack，3&gt;</code>，<code>&lt;Lily，5&gt;</code>。</p> \n<h2 id=\"State-store\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#State-store\" class=\"headerlink\" title=\"State store\"></a>State store</h2> \n<p>流式处理中，部分操作是无状态的，例如过滤操作（Kafka Stream DSL中用<code>filer</code>方法实现）。而部分操作是有状态的，需要记录中间状态，如Window操作和聚合计算。State store被用来存储中间状态。它可以是一个持久化的Key-Value存储，也可以是内存中的HashMap，或者是数据库。Kafka提供了基于Topic的状态存储。</p> \n<p>Topic中存储的数据记录本身是Key-Value形式的，同时Kafka的log compaction机制可对历史数据做compact操作，保留每个Key对应的最后一个Value，从而在保证Key不丢失的前提下，减少总数据量，从而提高查询效率。</p> \n<p>构造KTable时，需要指定其state store name。默认情况下，该名字也即用于存储该KTable的状态的Topic的名字，遍历KTable的过程，实际就是遍历它对应的state store，或者说遍历Topic的所有key，并取每个Key最新值的过程。为了使得该过程更加高效，默认情况下会对该Topic进行compact操作。</p> \n<p>另外，除了KTable，所有状态计算，都需要指定state store name，从而记录中间状态。</p> \n<h1 id=\"Kafka-Stream如何解决流式系统中关键问题\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream如何解决流式系统中关键问题\" class=\"headerlink\" title=\"Kafka Stream如何解决流式系统中关键问题\"></a>Kafka Stream如何解决流式系统中关键问题</h1> \n<h2 id=\"时间\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#时间\" class=\"headerlink\" title=\"时间\"></a>时间</h2> \n<p>在流式数据处理中，时间是数据的一个非常重要的属性。从Kafka 0.10开始，每条记录除了Key和Value外，还增加了<code>timestamp</code>属性。目前Kafka Stream支持三种时间</p> \n<ul> \n <li>事件发生时间。事件发生的时间，包含在数据记录中。发生时间由Producer在构造ProducerRecord时指定。并且需要Broker或者Topic将<code>message.timestamp.type</code>设置为<code>CreateTime</code>（默认值）才能生效。</li> \n <li>消息接收时间，也即消息存入Broker的时间。当Broker或Topic将<code>message.timestamp.type</code>设置为<code>LogAppendTime</code>时生效。此时Broker会在接收到消息后，存入磁盘前，将其<code>timestamp</code>属性值设置为当前机器时间。一般消息接收时间比较接近于事件发生时间，部分场景下可代替事件发生时间。</li> \n <li>消息处理时间，也即Kafka Stream处理消息时的时间。</li> \n</ul> \n<p>注：Kafka Stream允许通过实现<code>org.apache.kafka.streams.processor.TimestampExtractor</code>接口自定义记录时间。</p> \n<h2 id=\"窗口\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#窗口\" class=\"headerlink\" title=\"窗口\"></a>窗口</h2> \n<p>前文提到，流式数据是在时间上无界的数据。而聚合操作只能作用在特定的数据集，也即有界的数据集上。因此需要通过某种方式从无界的数据集上按特定的语义选取出有界的数据。窗口是一种非常常用的设定计算边界的方式。不同的流式处理系统支持的窗口类似，但不尽相同。</p> \n<p>Kafka Stream支持的窗口如下。</p> \n<ol> \n <li><p><code>Hopping Time Window</code> 该窗口定义如下图所示。它有两个属性，一个是Window size，一个是Advance interval。Window size指定了窗口的大小，也即每次计算的数据集的大小。而Advance interval定义输出的时间间隔。一个典型的应用场景是，每隔5秒钟输出一次过去1个小时内网站的PV或者UV。</p> \n  <div align=\"center\"> \n   <br><img width=\"60%\" alt=\"Hopping Time Window\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/Hopping%20Time%20Window.gif\"><br /></img></br> \n  </div></li> \n <li><p><code>Tumbling Time Window</code>该窗口定义如下图所示。可以认为它是Hopping Time Window的一种特例，也即Window size和Advance interval相等。它的特点是各个Window之间完全不相交。</p> \n  <div align=\"center\"> \n   <br><img width=\"60%\" alt=\"Tumbling Time Window\" src=\"http://www.jasongj.com/img/kafka/KafkaColumn7/Tumbling Time Window.gif\"><br /></img></br> \n  </div></li> \n <li><p><code>Sliding Window</code>该窗口只用于2个KStream进行Join计算时。该窗口的大小定义了Join两侧KStream的数据记录被认为在同一个窗口的最大时间差。假设该窗口的大小为5秒，则参与Join的2个KStream中，记录时间差小于5的记录被认为在同一个窗口中，可以进行Join计算。</p></li> \n <li><p><code>Session Window</code>该窗口用于对Key做Group后的聚合操作中。它需要对Key做分组，然后对组内的数据根据业务需求定义一个窗口的起始点和结束点。一个典型的案例是，希望通过Session Window计算某个用户访问网站的时间。对于一个特定的用户（用Key表示）而言，当发生登录操作时，该用户（Key）的窗口即开始，当发生退出操作或者超时时，该用户（Key）的窗口即结束。窗口结束时，可计算该用户的访问时间或者点击次数等。</p></li> \n</ol> \n<h2 id=\"Join\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Join\" class=\"headerlink\" title=\"Join\"></a>Join</h2> \n<p>Kafka Stream由于包含KStream和Ktable两种数据集，因此提供如下Join计算</p> \n<ul> \n <li><code>KTable Join KTable</code> 结果仍为KTable。任意一边有更新，结果KTable都会更新。</li> \n <li><code>KStream Join KStream</code> 结果为KStream。必须带窗口操作，否则会造成Join操作一直不结束。</li> \n <li><code>KStream Join KTable / GlobalKTable</code> 结果为KStream。只有当KStream中有新数据时，才会触发Join计算并输出结果。KStream无新数据时，KTable的更新并不会触发Join计算，也不会输出数据。并且该更新只对下次Join生效。一个典型的使用场景是，KStream中的订单信息与KTable中的用户信息做关联计算。</li> \n</ul> \n<p>对于Join操作，如果要得到正确的计算结果，需要保证参与Join的KTable或KStream中Key相同的数据被分配到同一个Task。具体方法是</p> \n<ul> \n <li>参与Join的KTable或KStream的Key类型相同（实际上，业务含意也应该相同）</li> \n <li>参与Join的KTable或KStream对应的Topic的Partition数相同</li> \n <li>Partitioner策略的最终结果等效（实现不需要完全一样，只要效果一样即可），也即Key相同的情况下，被分配到ID相同的Partition内</li> \n</ul> \n<p>如果上述条件不满足，可通过调用如下方法使得它满足上述条件。<br> \n  <figure class=\"highlight java\"> \n   <table> \n    <tr> \n     <td class=\"gutter\"><pre><span class=\"line\">1</span><br /></pre></td> \n     <td class=\"code\"><pre><span class=\"line\"><span class=\"function\">KStream&lt;K, V&gt; <span class=\"title\">through</span><span class=\"params\">(Serde&lt;K&gt; keySerde, Serde&lt;V&gt; valSerde, StreamPartitioner&lt;K, V&gt; partitioner, String topic)</span></span></span><br /></pre></td> \n    </tr> \n   </table> \n  </figure></br></p> \n<h2 id=\"聚合与乱序处理\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#聚合与乱序处理\" class=\"headerlink\" title=\"聚合与乱序处理\"></a>聚合与乱序处理</h2> \n<p>聚合操作可应用于KStream和KTable。当聚合发生在KStream上时必须指定窗口，从而限定计算的目标数据集。</p> \n<p>需要说明的是，聚合操作的结果肯定是KTable。因为KTable是可更新的，可以在晚到的数据到来时（也即发生数据乱序时）更新结果KTable。</p> \n<p>这里举例说明。假设对KStream以5秒为窗口大小，进行Tumbling Time Window上的Count操作。并且KStream先后出现时间为1秒, 3秒, 5秒的数据，此时5秒的窗口已达上限，Kafka Stream关闭该窗口，触发Count操作并将结果3输出到KTable中（假设该结果表示为&lt;1-5,3&gt;）。若1秒后，又收到了时间为2秒的记录，由于1-5秒的窗口已关闭，若直接抛弃该数据，则可认为之前的结果&lt;1-5,3&gt;不准确。而如果直接将完整的结果&lt;1-5,4&gt;输出到KStream中，则KStream中将会包含该窗口的2条记录，&lt;1-5,3&gt;, &lt;1-5,4&gt;，也会存在肮数据。因此Kafka Stream选择将聚合结果存于KTable中，此时新的结果&lt;1-5,4&gt;会替代旧的结果&lt;1-5,3&gt;。用户可得到完整的正确的结果。</p> \n<p>这种方式保证了数据准确性，同时也提高了容错性。</p> \n<p>但需要说明的是，Kafka Stream并不会对所有晚到的数据都重新计算并更新结果集，而是让用户设置一个<code>retention period</code>，将每个窗口的结果集在内存中保留一定时间，该窗口内的数据晚到时，直接合并计算，并更新结果KTable。超过<code>retention period</code>后，该窗口结果将从内存中删除，并且晚到的数据即使落入窗口，也会被直接丢弃。</p> \n<h2 id=\"容错\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#容错\" class=\"headerlink\" title=\"容错\"></a>容错</h2> \n<p>Kafka Stream从如下几个方面进行容错</p> \n<ul> \n <li>高可用的Partition保证无数据丢失。每个Task计算一个Partition，而Kafka数据复制机制保证了Partition内数据的高可用性，故无数据丢失风险。同时由于数据是持久化的，即使任务失败，依然可以重新计算。</li> \n <li>状态存储实现快速故障恢复和从故障点继续处理。对于Join和聚合及窗口等有状态计算，状态存储可保存中间状态。即使发生Failover或Consumer Rebalance，仍然可以通过状态存储恢复中间状态，从而可以继续从Failover或Consumer Rebalance前的点继续计算。</li> \n <li>KTable与<code>retention period</code>提供了对乱序数据的处理能力。</li> \n</ul> \n<h1 id=\"Kafka-Stream应用示例\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream应用示例\" class=\"headerlink\" title=\"Kafka Stream应用示例\"></a>Kafka Stream应用示例</h1> \n<p>下面结合一个案例来讲解如何开发Kafka Stream应用。本例完整代码可从<a href=\"https://github.com/habren/KafkaExample\" target=\"_blank\" rel=\"noopener\">作者Github</a>获取。</p> \n<p>订单KStream（名为orderStream），底层Topic的Partition数为3，Key为用户名，Value包含用户名，商品名，订单时间，数量。用户KTable（名为userTable），底层Topic的Partition数为3，Key为用户名，Value包含性别，地址和年龄。商品KTable（名为itemTable），底层Topic的Partition数为6，Key为商品名，价格，种类和产地。现在希望计算每小时购买产地与自己所在地相同的用户总数。</p> \n<p>首先由于希望使用订单时间，而它包含在orderStream的Value中，需要通过提供一个实现TimestampExtractor接口的类从orderStream对应的Topic中抽取出订单时间。<br> \n  <figure class=\"highlight java\"> \n   <table> \n    <tr> \n     <td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br /></br></br></br></br></br></br></br></br></br></br></pre></td> \n     <td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">OrderTimestampExtractor</span> <span class=\"keyword\">implements</span> <span class=\"title\">TimestampExtractor</span> </span>{</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"meta\">@Override</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">long</span> <span class=\"title\">extract</span><span class=\"params\">(ConsumerRecord&lt;Object, Object&gt; record)</span> </span>{</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(record <span class=\"keyword\">instanceof</span> Order) {</span><br><span class=\"line\"> <span class=\"keyword\">return</span> ((Order)record).getTS();</span><br><span class=\"line\"> } <span class=\"keyword\">else</span> {</span><br><span class=\"line\"> <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"> }</span><br><span class=\"line\"> }</span><br><span class=\"line\">}</span><br /></br></br></br></br></br></br></br></br></br></br></pre></td> \n    </tr> \n   </table> \n  </figure></br></p> \n<p>接着通过将orderStream与userTable进行Join，来获取订单用户所在地。由于二者对应的Topic的Partition数相同，且Key都为用户名，再假设Producer往这两个Topic写数据时所用的Partitioner实现相同，则此时上文所述Join条件满足，可直接进行Join。<br> \n  <figure class=\"highlight java\"> \n   <table> \n    <tr> \n     <td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br /></br></br></br></br></br></br></br></br></pre></td> \n     <td class=\"code\"><pre><span class=\"line\">orderUserStream = orderStream</span><br><span class=\"line\">    .leftJoin(userTable, </span><br><span class=\"line\">         <span class=\"comment\">// 该lamda表达式定义了如何从orderStream与userTable生成结果集的Value</span></span><br><span class=\"line\">        (Order order, User user) -&gt; OrderUser.fromOrderUser(order, user), </span><br><span class=\"line\">         <span class=\"comment\">// 结果集Key序列化方式</span></span><br><span class=\"line\"> Serdes.String(),</span><br><span class=\"line\"> <span class=\"comment\">// 结果集Value序列化方式</span></span><br><span class=\"line\"> SerdesFactory.serdFrom(Order.class))</span><br><span class=\"line\"> .filter((String userName, OrderUser orderUser) -&gt; orderUser.userAddress != <span class=\"keyword\">null</span>)</span><br /></br></br></br></br></br></br></br></br></pre></td> \n    </tr> \n   </table> \n  </figure></br></p> \n<p>从上述代码中，可以看到，Join时需要指定如何从参与Join双方的记录生成结果记录的Value。Key不需要指定，因为结果记录的Key与Join Key相同，故无须指定。Join结果存于名为orderUserStream的KStream中。</p> \n<p>接下来需要将orderUserStream与itemTable进行Join，从而获取商品产地。此时orderUserStream的Key仍为用户名，而itemTable对应的Topic的Key为产品名，并且二者的Partition数不一样，因此无法直接Join。此时需要通过through方法，对其中一方或双方进行重新分区，使得二者满足Join条件。这一过程相当于Spark的Shuffle过程和Storm的FieldGrouping。<br> \n  <figure class=\"highlight java\"> \n   <table> \n    <tr> \n     <td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br /></br></br></br></br></br></br></br></br></br></pre></td> \n     <td class=\"code\"><pre><span class=\"line\">orderUserStrea</span><br><span class=\"line\">    .through(</span><br><span class=\"line\">        <span class=\"comment\">// Key的序列化方式</span></span><br><span class=\"line\">        Serdes.String(),</span><br><span class=\"line\">        <span class=\"comment\">// Value的序列化方式 </span></span><br><span class=\"line\"> SerdesFactory.serdFrom(OrderUser.class), </span><br><span class=\"line\"> <span class=\"comment\">// 重新按照商品名进行分区，具体取商品名的哈希值，然后对分区数取模</span></span><br><span class=\"line\"> (String key, OrderUser orderUser, <span class=\"keyword\">int</span> numPartitions) -&gt; (orderUser.getItemName().hashCode() &amp; <span class=\"number\">0x7FFFFFFF</span>) % numPartitions, </span><br><span class=\"line\"> <span class=\"string\">\"orderuser-repartition-by-item\"</span>)</span><br><span class=\"line\"> .leftJoin(itemTable, (OrderUser orderUser, Item item) -&gt; OrderUserItem.fromOrderUser(orderUser, item), Serdes.String(), SerdesFactory.serdFrom(OrderUser.class))</span><br /></br></br></br></br></br></br></br></br></br></pre></td> \n    </tr> \n   </table> \n  </figure></br></p> \n<p>从上述代码可见，through时需要指定Key的序列化器，Value的序列化器，以及分区方式和结果集所在的Topic。这里要注意，该Topic（orderuser-repartition-by-item）的Partition数必须与itemTable对应Topic的Partition数相同，并且through使用的分区方法必须与iteamTable对应Topic的分区方式一样。经过这种<code>through</code>操作，orderUserStream与itemTable满足了Join条件，可直接进行Join。</p> \n<h1 id=\"总结\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1> \n<ul> \n <li>Kafka Stream的并行模型完全基于Kafka的分区机制和Rebalance机制，实现了在线动态调整并行度</li> \n <li>同一Task包含了一个子Topology的所有Processor，使得所有处理逻辑都在同一线程内完成，避免了不必的网络通信开销，从而提高了效率。</li> \n <li><code>through</code>方法提供了类似Spark的Shuffle机制，为使用不同分区策略的数据提供了Join的可能</li> \n <li>log compact提高了基于Kafka的state store的加载效率</li> \n <li>state store为状态计算提供了可能</li> \n <li>基于offset的计算进度管理以及基于state store的中间状态管理为发生Consumer rebalance或Failover时从断点处继续处理提供了可能，并为系统容错性提供了保障</li> \n <li>KTable的引入，使得聚合计算拥用了处理乱序问题的能力</li> \n</ul> \n<h1 id=\"Kafka系列文章\"><a href=\"http://www.jasongj.com/kafka/kafka_stream/#Kafka系列文章\" class=\"headerlink\" title=\"Kafka系列文章\"></a>Kafka系列文章</h1> \n<ul> \n <li><a href=\"http://www.jasongj.com/2015/03/10/KafkaColumn1/\">Kafka设计解析（一）- Kafka背景及架构介绍</a></li> \n <li><a href=\"http://www.jasongj.com/2015/04/24/KafkaColumn2/\">Kafka设计解析（二）- Kafka High Availability （上）</a></li> \n <li><a href=\"http://www.jasongj.com/2015/06/08/KafkaColumn3/\">Kafka设计解析（三）- Kafka High Availability （下）</a></li> \n <li><a href=\"http://www.jasongj.com/2015/08/09/KafkaColumn4/\">Kafka设计解析（四）- Kafka Consumer设计解析</a></li> \n <li><a href=\"http://www.jasongj.com/2015/12/31/KafkaColumn5_kafka_benchmark/\">Kafka设计解析（五）- Kafka性能测试方法及Benchmark报告</a></li> \n <li><a href=\"http://www.jasongj.com/kafka/high_throughput/\">Kafka设计解析（六）- Kafka高性能架构之道</a></li> \n <li><a href=\"http://www.jasongj.com/kafka/kafka_stream/\">Kafka设计解析（七）- Kafka Stream</a></li> \n</ul>","descriptionType":"html","publishedDate":"Mon, 07 Aug 2017 00:01:01 +0000","feedId":27658,"bgimg":"","linkMd5":"9dcc61c2aa069a3b94ef5b475d9ca297","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"http://www.jasongj.com/img/kafka/KafkaColumn7/stream_procissing.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn85@2020_3/2020/10/16/17-43-45-175_a4cae49ad6eaf4ad.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/batch_procissing.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn9@2020_2/2020/10/16/17-43-45-627_3b4356201229f785.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/library.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn14@2020_2/2020/10/16/17-43-43-833_23d35646a23ea780.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/Kafka%20Stream%20Architecture.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn93@2020_5/2020/10/16/17-43-44-804_e69392aa75fc0c41.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/1%20thread.png":null,"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20threads.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn81@2020_5/2020/10/16/17-44-45-787_c12d96d2b9e1bcf5.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/2%20instances.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn21@2020_6/2020/10/16/17-43-45-698_5feb22949879cc72.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/2%20servers.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn5@2020_5/2020/10/16/17-43-46-595_0c7bd08e3b207baa.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/ktable_kstream.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn2@2020_3/2020/10/16/17-43-46-112_fafd94928f637136.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/Hopping%20Time%20Window.gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn89@2020_6/2020/10/16/17-43-45-872_ff1b20fa2a57f147.webp","http://www.jasongj.com/img/kafka/KafkaColumn7/Tumbling Time Window.gif":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn18@2020_2/2020/10/16/17-43-46-997_010388be408160a3.webp"},"publishedOrCreatedDate":1602870209618}],"record":{"createdTime":"2020-10-17 01:43:29","updatedTime":"2020-10-17 01:43:29","feedId":27658,"fetchDate":"Fri, 16 Oct 2020 17:43:29 +0000","fetchMs":967,"handleMs":58,"totalMs":78306,"newArticles":0,"totalArticles":49,"status":1,"type":0,"ip":"d29e7b16565154882990b6eed02f7029","hostName":"us-033*","requestId":"5a939f6f6f214a02a45c06e49ca944f4_27658","contentType":"application/xml","totalBytes":890424,"bgimgsTotal":0,"bgimgsGithubTotal":0,"articlesImgsTotal":11,"articlesImgsGithubTotal":10,"successGithubMap":{"myreaderx16":1,"myreaderx10":1,"myreaderx4":1,"myreaderx11":1,"myreaderx12":1,"myreaderx13":1,"myreaderx1":1,"myreaderx5oss":1,"myreaderx31":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1}},"feed":{"createdTime":"2020-09-07 02:41:57","updatedTime":"2020-09-07 04:44:09","id":27658,"name":"技术世界","url":"http://www.jasongj.com/atom.xml","subscriber":118,"website":null,"icon":"https://www.gravatar.com/avatar/b7c3335ef8378cf904c036c9f55912c9","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn56@2020_3/2020/09/06/20-44-01-919_23594f0cd775e7e9.jpg","description":"分享交流大数据领域技术，包括但不限于Storm、Spark、Hadoop等流行分布式计算系统，Kafka、MetaQ等分布式消息系统，MongoDB、Cassandra等NoSQL，PostgreSQL、MySQL等RDBMS以及其它前沿技术","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2020-10-17 01:44:46","updatedTime":"2020-10-17 01:44:46","id":null,"feedId":27658,"linkMd5":"9dcc61c2aa069a3b94ef5b475d9ca297"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":890424,"tmpBgImgCdnBytes":0,"extra4":{"start":1602870208474,"total":0,"statList":[{"spend":1086,"msg":"获取xml内容"},{"spend":58,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":64632,"msg":"正文链接上传到cdn"}]},"extra5":11,"extra6":11,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/1%20thread.png","sourceStatusCode":200,"destWidth":1544,"destHeight":1256,"sourceBytes":169883,"destBytes":97970,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":3472,"convertSpendMs":77,"createdTime":"2020-10-17 01:43:42","host":"europe-22*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn98/contents/2020/10/16/17-43-45-348_27371e8232a2ff3a.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Fri, 16 Oct 2020 17:43:45 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["AAE4:662A:BC844B3:DA64B1C:5F89DBBF"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1602870249"],"x-ratelimit-used":["60"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn98/contents/2020/10/16/17-43-45-348_27371e8232a2ff3a.webp","historyStatusCode":[],"spendMs":378},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"95.7 KB","compressRate":"57.7%","sourceSize":"165.9 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/1%20thread.png","sourceStatusCode":200,"destWidth":1544,"destHeight":1256,"sourceBytes":169883,"destBytes":97970,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":2503,"convertSpendMs":118,"createdTime":"2020-10-17 01:43:45","host":"us-013*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn98/contents/2020/10/16/17-43-48-220_27371e8232a2ff3a.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Fri, 16 Oct 2020 17:43:48 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["E700:6CDD:E1FAA0:2193F5F:5F89DBD3"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1602870249"],"x-ratelimit-used":["60"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn98/contents/2020/10/16/17-43-48-220_27371e8232a2ff3a.webp","historyStatusCode":[],"spendMs":53},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"95.7 KB","compressRate":"57.7%","sourceSize":"165.9 KB"},null],"extra10_invalidATagHrefValue":{"http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/04/24/KafkaColumn2/":"http://www.jasongj.com/2015/04/24/KafkaColumn2/","http://www.jasongj.com/kafka/kafka_stream/_#KTable-vs-KStream":"http://www.jasongj.com/kafka/kafka_stream/#KTable-vs-KStream","http://www.jasongj.com/kafka/kafka_stream/_#聚合与乱序处理":"http://www.jasongj.com/kafka/kafka_stream/#聚合与乱序处理","http://www.jasongj.com/kafka/kafka_stream/_#Kafka系列文章":"http://www.jasongj.com/kafka/kafka_stream/#Kafka系列文章","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream整体架构":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream整体架构","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/08/09/KafkaColumn4/#High-Level-Consumer-Rebalance":"http://www.jasongj.com/2015/08/09/KafkaColumn4/#High-Level-Consumer-Rebalance","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream架构":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream架构","http://www.jasongj.com/kafka/kafka_stream/_#为什么要有Kafka-Stream":"http://www.jasongj.com/kafka/kafka_stream/#为什么要有Kafka-Stream","http://www.jasongj.com/kafka/kafka_stream/_#总结":"http://www.jasongj.com/kafka/kafka_stream/#总结","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream如何解决流式系统中关键问题":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream如何解决流式系统中关键问题","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream应用示例":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream应用示例","http://www.jasongj.com/kafka/kafka_stream/_#什么是流式计算":"http://www.jasongj.com/kafka/kafka_stream/#什么是流式计算","http://www.jasongj.com/kafka/kafka_stream/_#时间":"http://www.jasongj.com/kafka/kafka_stream/#时间","http://www.jasongj.com/kafka/kafka_stream/_#容错":"http://www.jasongj.com/kafka/kafka_stream/#容错","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/08/09/KafkaColumn4/":"http://www.jasongj.com/2015/08/09/KafkaColumn4/","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream背景":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream背景","http://www.jasongj.com/kafka/kafka_stream/_#窗口":"http://www.jasongj.com/kafka/kafka_stream/#窗口","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream并行模型":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream并行模型","http://www.jasongj.com/kafka/kafka_stream/_#Kafka-Stream是什么":"http://www.jasongj.com/kafka/kafka_stream/#Kafka-Stream是什么","http://www.jasongj.com/kafka/kafka_stream/_#State-store":"http://www.jasongj.com/kafka/kafka_stream/#State-store","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/03/10/KafkaColumn1":"http://www.jasongj.com/2015/03/10/KafkaColumn1","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/kafka/high_throughput/":"http://www.jasongj.com/kafka/high_throughput/","http://www.jasongj.com/kafka/kafka_stream/_#Processor-Topology":"http://www.jasongj.com/kafka/kafka_stream/#Processor-Topology","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/kafka/kafka_stream/":"http://www.jasongj.com/kafka/kafka_stream/","http://www.jasongj.com/kafka/kafka_stream/_#Join":"http://www.jasongj.com/kafka/kafka_stream/#Join","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/06/08/KafkaColumn3/":"http://www.jasongj.com/2015/06/08/KafkaColumn3/","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/12/31/KafkaColumn5_kafka_benchmark/":"http://www.jasongj.com/2015/12/31/KafkaColumn5_kafka_benchmark/","http://www.jasongj.com/kafka/kafka_stream/_//www.jasongj.com/2015/03/10/KafkaColumn1/":"http://www.jasongj.com/2015/03/10/KafkaColumn1/"},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-56.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-001.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-034.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe64.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-60.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/library.png","sourceStatusCode":200,"destWidth":1024,"destHeight":590,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn14@2020_2/2020/10/16/17-43-43-833_23d35646a23ea780.webp","sourceBytes":68794,"destBytes":37262,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":2579,"convertSpendMs":52,"createdTime":"2020-10-17 01:43:42","host":"us-013*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"36.4 KB","compressRate":"54.2%","sourceSize":"67.2 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/Kafka%20Stream%20Architecture.png","sourceStatusCode":200,"destWidth":1808,"destHeight":1492,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn93@2020_5/2020/10/16/17-43-44-804_e69392aa75fc0c41.webp","sourceBytes":273980,"destBytes":131990,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":3725,"convertSpendMs":239,"createdTime":"2020-10-17 01:43:42","host":"us-009*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"128.9 KB","compressRate":"48.2%","sourceSize":"267.6 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/stream_procissing.png","sourceStatusCode":200,"destWidth":1534,"destHeight":604,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn85@2020_3/2020/10/16/17-43-45-175_a4cae49ad6eaf4ad.webp","sourceBytes":95208,"destBytes":67452,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":3943,"convertSpendMs":81,"createdTime":"2020-10-17 01:43:42","host":"us-034*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"65.9 KB","compressRate":"70.8%","sourceSize":"93 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/batch_procissing.png","sourceStatusCode":200,"destWidth":1530,"destHeight":604,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn9@2020_2/2020/10/16/17-43-45-627_3b4356201229f785.webp","sourceBytes":82716,"destBytes":63896,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":2406,"convertSpendMs":117,"createdTime":"2020-10-17 01:43:44","host":"us-025*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"62.4 KB","compressRate":"77.2%","sourceSize":"80.8 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/Hopping%20Time%20Window.gif","sourceStatusCode":200,"destWidth":1000,"destHeight":145,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn89@2020_6/2020/10/16/17-43-45-872_ff1b20fa2a57f147.webp","sourceBytes":506789,"destBytes":138562,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":4769,"convertSpendMs":699,"createdTime":"2020-10-17 01:43:42","host":"us-021*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"135.3 KB","compressRate":"27.3%","sourceSize":"494.9 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20instances.png","sourceStatusCode":200,"destWidth":1686,"destHeight":1222,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn21@2020_6/2020/10/16/17-43-45-698_5feb22949879cc72.webp","sourceBytes":188352,"destBytes":112210,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":4932,"convertSpendMs":80,"createdTime":"2020-10-17 01:43:42","host":"europe64*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"109.6 KB","compressRate":"59.6%","sourceSize":"183.9 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/ktable_kstream.png","sourceStatusCode":200,"destWidth":1086,"destHeight":624,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn2@2020_3/2020/10/16/17-43-46-112_fafd94928f637136.webp","sourceBytes":67953,"destBytes":47480,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":5066,"convertSpendMs":64,"createdTime":"2020-10-17 01:43:42","host":"europe-60*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"46.4 KB","compressRate":"69.9%","sourceSize":"66.4 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20servers.png","sourceStatusCode":200,"destWidth":1786,"destHeight":1234,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn5@2020_5/2020/10/16/17-43-46-595_0c7bd08e3b207baa.webp","sourceBytes":200105,"destBytes":123990,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":5485,"convertSpendMs":97,"createdTime":"2020-10-17 01:43:42","host":"us-038*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"121.1 KB","compressRate":"62%","sourceSize":"195.4 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/Tumbling Time Window.gif","sourceStatusCode":200,"destWidth":1006,"destHeight":163,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn18@2020_2/2020/10/16/17-43-46-997_010388be408160a3.webp","sourceBytes":285359,"destBytes":66138,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":4225,"convertSpendMs":275,"createdTime":"2020-10-17 01:43:43","host":"us-001*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"64.6 KB","compressRate":"23.2%","sourceSize":"278.7 KB"},{"code":1,"isDone":false,"source":"http://www.jasongj.com/img/kafka/KafkaColumn7/2%20threads.png","sourceStatusCode":200,"destWidth":1548,"destHeight":1236,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn81@2020_5/2020/10/16/17-44-45-787_c12d96d2b9e1bcf5.webp","sourceBytes":172344,"destBytes":101444,"targetWebpQuality":75,"feedId":27658,"totalSpendMs":1724,"convertSpendMs":260,"createdTime":"2020-10-17 01:44:45","host":"us-001*","referer":"http://www.jasongj.com/kafka/kafka_stream/","linkMd5ListStr":"9dcc61c2aa069a3b94ef5b475d9ca297","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"99.1 KB","compressRate":"58.9%","sourceSize":"168.3 KB"}],"successGithubMap":{"myreaderx16":1,"myreaderx10":1,"myreaderx4":1,"myreaderx11":1,"myreaderx12":1,"myreaderx13":1,"myreaderx1":1,"myreaderx5oss":1,"myreaderx31":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1}}