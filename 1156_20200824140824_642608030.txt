{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Google at ACL 2020","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/pliU9YmOakA/google-at-acl-2020.html","description":"<span class=\"byline-author\">Posted by Cat Armato and Emily Knapp, Program Managers</span><br /><br />This week, the 58th Annual Meeting of the Association for Computational Linguistics (<a href=\"https://acl2020.org/\">ACL 2020</a>), a premier conference covering a broad spectrum of research areas that are concerned with computational approaches to natural language, takes place online.<br /><br />As a leader in natural language processing and understanding, and a <a href=\"https://acl2020.org/sponsors/list/\">Diamond Level sponsor of ACL 2020</a>, Google will showcase the latest research in the field with over 30 publications, and the organization of and participation in a variety of workshops and tutorials.<br /><br />If you’re registered for ACL 2020, we hope that you’ll visit the <a href=\"https://goo.gle/ACLhomepage\">Google virtual booth</a> to learn more about the projects and opportunities at Google that go into solving interesting problems for billions of people. You can also learn more about the Google research being presented at ACL 2020 below (Google affiliations <b>bolded</b>).<br /><br /><b><u>Committees</u></b><br />Diversity &amp; Inclusion (D&amp;I) Chair: <b><i>Vinodkumar Prabhakaran</i></b><br />Accessibility Chair:<b> <i>Sushant Kafle</i></b><br />Local Sponsorship Chair:<b> <i>Kristina Toutanova</i></b><br />Virtual Infrastructure Committee: <b><i>Yi Luan</i></b><br />Area Chairs:<b><i>  Anders Søgaard, Ankur Parikh, Annie Louis, Bhuvana Ramabhadran, Christo Kirov, Daniel Cer, Dipanjan Das, Diyi Yang, Emily Pitler, Eunsol Choi, George Foster, Idan Szpektor, Jacob Eisenstein, Jason Baldridge, Jun Suzuki, Kenton Lee, Luheng He, Marius Pasca, Ming-Wei Chang, Sebastian Gehrmann, Shashi Narayan, Slav Petrov, Vinodkumar Prabhakaran, Waleed Ammar, William Cohen</i></b><br /><br /><b><u>Long Papers</u></b><br /><a href=\"https://arxiv.org/pdf/2005.00246.pdf\">Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage</a><br /><b><i>Ashish V. Thapliyal, Radu Soricut </i></b><br /><br /><a href=\"https://arxiv.org/pdf/1911.00650.pdf\">Automatic Detection of Generated Text is Easiest when Humans are Fooled</a><br /><b><i>Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2005.00661.pdf\">On Faithfulness and Factuality in Abstractive Summarization</a><br /><b><i>Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.02984.pdf\">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a><br /><b><i>Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2005.04625.pdf\">BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps</a><br /><i>Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, <b>Vihan Jain</b>, <b>Eugene Ie</b>, <b>Fei Sha</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2005.06606.pdf\">Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation</a><br /><i>Xuanli He, Gholamreza Haffari,<b> Mohammad Norouzi </b></i><br /><br /><a href=\"https://arxiv.org/pdf/2005.00547v1.pdf\">GoEmotions: A Dataset of Fine-Grained Emotions</a><br /><b><i>Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, Sujith Ravi </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.02349.pdf\">TaPas: Weakly Supervised Table Parsing via Pre-training</a> (<i>see <a href=\"https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html\">blog post</a>)</i><br /><b><i>Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, Julian Eisenschlos</i></b><br /><br /><a href=\"https://www.aclweb.org/anthology/2020.acl-main.396.pdf\">Toxicity Detection: Does Context Really Matter?</a><br /><i>John Pavlopoulos, Jeffrey Sorensen, <b>Lucas Dixon</b>, <b>Nithum Thain</b>, Ion Androutsopoulos </i><br /><br /><a href=\"https://arxiv.org/pdf/2005.09099.pdf\">(Re)construing Meaning in NLP</a><br /><i>Sean Trott, Tiago Timponi Torrent, <b>Nancy Chang</b>, Nathan Schneider</i><br /><br /><a href=\"https://arxiv.org/pdf/2005.10389.pdf\">Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models</a><br /><b><i>Dan Iter, Kelvin Guu, Larry Lansing, Dan Jurafsky </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2005.01898.pdf\">Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering</a><br /><i>Hao Cheng, <b>Ming-Wei Chang</b>,<b> Kenton Lee</b>, <b>Kristina Toutanova </b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.11834.pdf\">AdvAug: Robust Adversarial Augmentation for Neural Machine Translation</a><br /><b><i>Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2005.07150.pdf\">Named Entity Recognition as Dependency Parsing</a><br /><i>Juntao Yu,  <b>Bernd Bohnet</b>, Massimo Poesio </i><br /><br /><a href=\"https://arxiv.org/pdf/2005.00908.pdf\">Cross-modal Coherence Modeling for Caption Generation</a><br /><i>Malihe Alikhani, <b>Piyush Sharma</b>, Shengjie Li, <b>Radu Soricut</b>, Matthew Stone </i><br /><br /><a href=\"https://www.aclweb.org/anthology/2020.acl-main.580.pdf\">Representation Learning for Information Extraction from Form-like Documents</a> <i>(see <a href=\"https://ai.googleblog.com/2020/06/extracting-structured-data-from.html\">blog post</a>)</i><br /><i>Bodhisattwa Prasad Majumder, <b>Navneet Potti</b>, <b>Sandeep Tata</b>, <b>James Bradley Wendt</b>, <b>Qi Zhao, Marc Najork</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2005.00545.pdf\">Low-Dimensional Hyperbolic Knowledge Graph Embeddings</a><br /><b><i>Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, Christopher Ré </i></b><br /><br /><a href=\"https://arxiv.org/pdf/1910.14464.pdf\">What Question Answering can Learn from Trivia Nerds</a><br /><b><i>Jordan Boyd-Graber, Benjamin Börschinger </i></b><br /><br /><a href=\"https://arxiv.org/pdf/1908.10940.pdf\">Learning a Multi-Domain Curriculum for Neural Machine Translation</a> <i>(see <a href=\"https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html\">blog post</a>)</i><br /><b><i>Wei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac Caswell, Zarana Parekh </i></b><br /><br /><a href=\"https://arxiv.org/pdf/1911.03823.pdf\">Translationese as a Language in \"Multilingual\" NMT</a><br /><b><i>Parker Riley, Isaac Caswell, Markus Freitag, David Grangier </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2005.03776.pdf\">Mapping Natural Language Instructions to Mobile UI Action Sequences</a> <i>(see <a href=\"https://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html\">blog post</a>)</i><br /><b><i>Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge </i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.04696.pdf\">BLEURT: Learning Robust Metrics for Text Generation</a> <i>(see <a href=\"https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html\">blog post</a>)</i><br /><b><i>Thibault Sellam, Dipanjan Das, Ankur Parikh </i></b><br /><br /><a href=\"https://www.aclweb.org/anthology/2020.acl-main.742.pdf\">Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing</a><br /><i>Alane Suhr, <b>Ming-Wei Chang</b>, <b>Peter Shaw</b>, <b>Kenton Lee </b></i><br /><br /><a href=\"https://www.aclweb.org/anthology/2020.acl-main.733.pdf\">Frugal Paradigm Completion</a><br /><i>Alexander Erdmann, <b>Tom Kenter</b>, <b>Markus Becker</b>, <b>Christian Schallhart </b></i><br /><br /><b><u>Short Papers</u></b><br /><a href=\"https://arxiv.org/pdf/2004.06201.pdf\">Reverse Engineering Configurations of Neural Text Generation Models</a><br /><b><i>Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, Andrew Tomkins</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.11999.pdf\">Syntactic Data Augmentation Increases Robustness to Inference Heuristics</a><br /><i>Junghyun Min, R. Thomas McCoy, <b>Dipanjan Das</b>, <b>Emily Pitler</b>, Tal Linzen </i><br /><br /><a href=\"https://arxiv.org/pdf/2005.04816.pdf\">Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation</a><br /><b><i>Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan, Yonghui Wu</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2005.00813.pdf\">Social Biases in NLP Models as Barriers for Persons with Disabilities</a><br /><b><i>Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, Stephen Denuyl </i></b><br /><br /><a href=\"https://www.cis.upenn.edu/~ccb/publications/toward-better-storylines-with-sentence-level-language-models.pdf\">Toward Better Storylines with Sentence-Level Language Models</a><br /><b><i>Daphne Ippolito, David Grangier, Douglas Eck, Chris Callison-Burch </i></b><br /><br /><b><u>TACL Papers</u></b><br /><a href=\"https://arxiv.org/pdf/2003.05002.pdf\">TYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages</a> <i>(see <a href=\"https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html\">blog post)</a></i><br /><b><i>Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, Jennimaria Palomaki</i></b><br /><br /><a href=\"https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00296\">Phonotactic Complexity and Its Trade-offs</a><br /><i>Tiago Pimentel,<b> Brian Roark</b>, Ryan Cotterell</i><br /><br /><b><u>Demos</u></b><br /><a href=\"https://arxiv.org/pdf/1907.04307.pdf\">Multilingual Universal Sentence Encoder for Semantic Retrieval</a> (<i>see <a href=\"https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html\">blog post</a></i>)<br /><b><i>Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil</i></b><br /><br /><b><u>Workshops</u></b><br /><a href=\"https://iwpt20.sigparse.org/\">IWPT - The 16th International Conference on Parsing Technologies</a><br /><i>Yuji Matsumoto, Stephan Oepen, Kenji Sagae, <b>Anders Søgaard</b>, Weiwei Sun and Reut Tsarfaty</i><br /><br /><a href=\"https://alvr-workshop.github.io/#overview\">ALVR - Workshop on Advances in Language and Vision Research</a><br /><i>Xin Wang, Jesse Thomason, Ronghang Hu, Xinlei Chen, Peter Anderson, Qi Wu, Asli Celikyilmaz, <b>Jason Baldridge</b> and William Yang Wang</i><br /><br /><a href=\"https://sites.google.com/corp/view/wngt20/home?authuser=0\">WNGT - The 4th Workshop on Neural Generation and Translation </a><br /><i>Alexandra Birch, Graham Neubig, Andrew Finch, Hiroaki Hayashi, Kenneth Heafield, Ioannis Konstas, <b>Yusuke Oda</b> and Xian Li</i><br /><br /><a href=\"https://sites.google.com/corp/view/nlp4medicalconversations/home?authuser=0\">NLPMC - NLP for Medical Conversations</a><br /><i>Parminder Bhatia, Chaitanya Shivade, Mona Diab, Byron Wallace, Rashmi Gangadharaiah, <b>Nan Du</b>, <b>Izhak Shafran</b> and Steven Lin</i><br /><br /><a href=\"https://autosimtrans.github.io/\">AutoSimTrans - The 1st Workshop on Automatic Simultaneous Translation</a><br /><i>Hua Wu, <b>Colin Cherry</b>, James Cross, Liang Huang, Zhongjun He, Mark Liberman and Yang Liu</i><br /><br /><b><u>Tutorials</u></b><br /><a href=\"https://acl2020.org/program/tutorials/#t1-interpretability-and-analysis-in-neural-nlp-cutting-edge-\">Interpretability and Analysis in Neural NLP (cutting-edge)</a><br /><i>Yonatan Belinkov, <b>Sebastian Gehrmann</b>, <b>Ellie Pavlick</b></i><br /><br /><a href=\"https://homes.cs.washington.edu/~msap/acl2020-commonsense/\">Commonsense Reasoning for Natural Language Processing (Introductory)</a><br /><i>Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, <b>Dan Roth</b></i><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=pliU9YmOakA:TCIdMhqSw2k:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/pliU9YmOakA\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 06 Jul 2020 16:20:00 +0000","feedId":1156,"bgimg":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","linkMd5":"b09a356862e25dc3f311f71d606af524","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","destWidth":62,"destHeight":24,"sourceBytes":997,"destBytes":310,"author":"Google AI","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/pliU9YmOakA":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn22@2020_1/2020/08/24/14-07-56-344_343b7f3261e59ba1.webp"},"publishedOrCreatedDate":1598278048133},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Announcing ScaNN: Efficient Vector Similarity Search","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html","description":"<span class=\"byline-author\">Posted by Philip Sun, Software Engineer, Google Research</span> \n<br />\n<br />Suppose one wants to search through a large dataset of literary works using queries that require an exact match of title, author, or other easily machine-indexable criteria. Such a task would be well suited for a relational database using a language such as SQL. However, if one wants to support more abstract queries, such as “Civil War poem,” it is no longer possible to rely on naive similarity metrics such as the number of words in common between two phrases. For example, the query “science fiction” is more related to “future” than it is to “earth science” despite the former having zero, and the latter having one, word in common with the query.\n<br />\n<br />Machine learning (ML) has greatly improved computers’ abilities to understand language semantics and therefore answer these abstract queries. Modern ML models can transform inputs such as text and images into embeddings, high dimensional vectors trained such that more similar inputs cluster closer together. For a given query, we can therefore compute its embedding, and find the literary works whose embeddings are closest to the query’s. In this manner, ML has transformed an abstract and previously difficult-to-specify task into a rigorous mathematical one. However, a computational challenge remains: for a given query embedding, how does one quickly find the nearest dataset embeddings? The set of embeddings is often too large for exhaustive search and its high dimensionality makes pruning difficult. \n<br />\n<br />In our \n<a href=\"https://icml.cc/Conferences/2020\">ICML 2020</a> paper, “\n<a href=\"https://arxiv.org/abs/1908.10396\">Accelerating Large-Scale Inference with Anisotropic Vector Quantization,”</a> we address this problem by focusing on how to compress the dataset vectors to enable fast approximate distance computations, and propose a new compression technique that significantly boosts accuracy compared to prior works. This technique is utilized in our recently open-sourced \n<a href=\"https://github.com/google-research/google-research/tree/master/scann\">vector similarity search library</a> (ScaNN), and enables us to outperform other vector similarity search libraries by a factor of two, as measured on \n<a href=\"http://ann-benchmarks.com\">ann-benchmarks.com</a>.\n<br />\n<br />\n<b>The Importance of Vector Similarity Search</b>\n<br />Embedding-based search is a technique that is effective at answering queries that rely on semantic understanding rather than simple indexable properties. In this technique, machine learning models are trained to map the queries and database items to a common vector embedding space, such that the distance between embeddings carries semantic meaning, i.e., similar items are closer together.\n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Rsymb9XvPOE/Xx8rfRnmTHI/AAAAAAAAGRQ/U2n_bBNXS4IBstYrx2IalrFXufLUvmn2gCLcBGAsYHQ/s1600/ScaNN%2Btom%2Bexport.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"245\" data-original-width=\"512\" height=\"306\" src=\"https://1.bp.blogspot.com/-Rsymb9XvPOE/Xx8rfRnmTHI/AAAAAAAAGRQ/U2n_bBNXS4IBstYrx2IalrFXufLUvmn2gCLcBGAsYHQ/s640/ScaNN%2Btom%2Bexport.gif\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">The two-tower neural network model, illustrated above, is a specific type of embedding-based search where queries and database items are mapped to the embedding space by two respective neural networks. In this example the model responds to natural-language queries for a hypothetical literary database.</td>\n  </tr>\n </tbody>\n</table>To answer a query with this approach, the system must first map the query to the embedding space. It then must find, among all database embeddings, the ones closest to the query; this is the \n<a href=\"https://en.wikipedia.org/wiki/Nearest_neighbor_search\">nearest neighbor search</a> problem. One of the most common ways to define the query-database embedding similarity is by their \n<a href=\"https://en.wikipedia.org/wiki/Dot_product\">inner product</a>; this type of nearest neighbor search is known as \n<a href=\"https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf\">maximum inner-product search</a> (MIPS).\n<br />\n<br />Because the database size can easily be in the millions or even billions, MIPS is often the computational bottleneck to inference speed, and exhaustive search is impractical. This necessitates the use of approximate MIPS algorithms that exchange some accuracy for a significant speedup over brute-force search.\n<br />\n<br />\n<b>A New Quantization Approach for MIPS</b>\n<br />\n<a href=\"https://github.com/erikbern/ann-benchmarks#evaluated\">Several state-of-the-art solutions</a> for MIPS are based on compressing the database items so that an approximation of their inner product can be computed in a fraction of the time taken by brute-force. This compression is commonly done with \n<a href=\"https://en.wikipedia.org/wiki/Vector_quantization\">learned quantization</a>, where a \n<em>codebook</em> of vectors is trained from the database and is used to approximately represent the database elements.\n<br />\n<br />Previous vector quantization schemes quantized database elements with the aim of minimizing the average distance between each vector \n<em>x</em> and its quantized form \n<em>x̃</em>. While this is a useful metric, optimizing for this is not equivalent to optimizing nearest-neighbor search accuracy. The key idea behind our paper is that encodings with \n<em>higher</em> average distance may actually result in superior MIPS accuracy.\n<br />\n<br />The intuition for our result is illustrated below. Suppose we have two database embeddings \n<em>x</em>\n<sub>1</sub> and \n<em>x</em>\n<sub>2</sub>, and must quantize each to one of two centers: \n<em>c</em>\n<sub>1</sub> or \n<em>c</em>\n<sub>2</sub>. Our goal is to quantize each \n<em>x<sub>i</sub></em> to \n<em>x̃<sub>i</sub></em> such that the inner product &lt;\n<em>q</em>, \n<em>x̃<sub>i</sub></em>&gt; is as similar to the original inner product &lt;\n<em>q</em>, \n<em>x<sub>i</sub></em>&gt; as possible. This can be visualized as making the magnitude of the projection of \n<em>x̃<sub>i</sub></em> onto \n<em>q</em> as similar as possible to the projection of \n<em>x<sub>i</sub></em> onto \n<em>q</em>. In the traditional approach to quantization (left), we would pick the closest center for each \n<em>x<sub>i</sub></em>, which leads to an incorrect relative ranking of the two points: &lt;\n<em>q</em>, \n<em>x̃<sub>1</sub></em>&gt; is \n<em>greater</em> than &lt;\n<em>q</em>, \n<em>x̃</em>\n<sub>2</sub>&gt;, even though &lt;\n<em>q</em>, \n<em>x</em>\n<sub>1</sub>&gt; is \n<em>less</em> than &lt;\n<em>q</em>, \n<em>x</em>\n<sub>2</sub>&gt;! If we instead assign \n<em>x</em>\n<sub>1</sub> to \n<em>c</em>\n<sub>1</sub> and \n<em>x</em>\n<sub>2</sub> to \n<em>c</em>\n<sub>2</sub>, we get the correct ranking. This is illustrated in the figure below.\n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-0HxtiXvnyTU/Xx8xNOgfUSI/AAAAAAAAGRc/Vgf0gK50N9cIG1aA9TWFLx7nqAYwuP5TQCLcBGAsYHQ/s1600/image2.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"557\" data-original-width=\"1600\" height=\"222\" src=\"https://1.bp.blogspot.com/-0HxtiXvnyTU/Xx8xNOgfUSI/AAAAAAAAGRc/Vgf0gK50N9cIG1aA9TWFLx7nqAYwuP5TQCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">The goal is to quantize each <i>x<sub>i</sub></i> to <i>x̃<sub>i</sub></i> = <i>c</i><sub>1</sub> or <i>x̃<sub>i</sub></i> = <i>c</i><sub>2</sub>. Traditional quantization (left) results in the incorrect ordering of <em>x</em><sub>1</sub> and <em>x</em><sub>2</sub> for this query. Even though our approach (right) chooses centers farther away from the data points, this in fact leads to lower inner product error and higher accuracy.</td>\n  </tr>\n </tbody>\n</table>It turns out that direction matters as well as magnitude--even though \n<em>c</em>\n<sub>1</sub> is farther from \n<em>x</em>\n<sub>1</sub> than \n<em>c</em>\n<sub>2</sub>, \n<em>c</em>\n<sub>1</sub> is offset from \n<em>x</em>\n<sub>1</sub> in a direction almost entirely orthogonal to \n<em>x</em>\n<sub>1</sub>, while \n<em>c</em>\n<sub>2</sub>’s offset is parallel (for \n<em>x</em>\n<sub>2</sub>, the same situation applies but flipped). Error in the parallel direction is much more harmful in the MIPS problem because it disproportionately impacts high inner products, which by definition are the ones that MIPS is trying to estimate accurately.\n<br />\n<br />Based on this intuition, we more heavily penalize quantization error that is parallel to the original vector. We refer to our novel quantization technique as \n<em>anisotropic vector quantization</em> due to the directional dependence of its loss function. The ability of this technique to trade increased quantization error of lower inner products in exchange for superior accuracy for high inner products is the key innovation and the source of its performance gains.\n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-l4VY-q1YcEE/XxsvREuIEvI/AAAAAAAAGQs/zzJNUHTZ9SU8LtKzm2rgl0oQCuiJ9fhIwCLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"513\" data-original-width=\"1600\" height=\"204\" src=\"https://1.bp.blogspot.com/-l4VY-q1YcEE/XxsvREuIEvI/AAAAAAAAGQs/zzJNUHTZ9SU8LtKzm2rgl0oQCuiJ9fhIwCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">In the above diagrams, ellipses denote contours of equal loss. In anisotropic vector quantization, error parallel to the original data point x is penalized more.</td>\n  </tr>\n </tbody>\n</table>\n<b>Anisotropic Vector Quantization in ScaNN</b>\n<br />Anisotropic vector quantization allows ScaNN to better estimate inner products that are likely to be in the top-\n<em>k</em> MIPS results and therefore achieve higher accuracy. On the \n<a href=\"http://ann-benchmarks.com/glove-100-angular_10_angular.html\">glove-100-angular benchmark</a> from \n<a href=\"http://ann-benchmarks.com\">ann-benchmarks.com</a>, ScaNN outperformed eleven other carefully tuned vector similarity search libraries, handling roughly twice as many queries per second for a given accuracy as the next-fastest library.\n<a href=\"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html#1\" name=\"top1\"><sup>*</sup></a>\n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/--mbMV8fQY28/XxsvbGL_l-I/AAAAAAAAGQ0/Br9B3XGnBa07barUxC4XTi8hSDxYzwAEgCLcBGAsYHQ/s1600/image5.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"943\" data-original-width=\"1600\" height=\"376\" src=\"https://1.bp.blogspot.com/--mbMV8fQY28/XxsvbGL_l-I/AAAAAAAAGQ0/Br9B3XGnBa07barUxC4XTi8hSDxYzwAEgCLcBGAsYHQ/s640/image5.png\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">Recall@k is a commonly used metric for nearest neighbor search accuracy, which measures the proportion of the true nearest k neighbors that are present in an algorithm’s returned k neighbors. ScaNN (upper purple line) consistently achieves superior performance across various points of the speed-accuracy trade-off.</td>\n  </tr>\n </tbody>\n</table>ScaNN is open-source software and you can try it yourself at \n<a href=\"https://github.com/google-research/google-research/tree/master/scann\">GitHub</a>. The library can be directly installed via Pip and has interfaces for both TensorFlow and Numpy inputs. Please see the GitHub repository for further instructions on installing and configuring ScaNN.\n<br />\n<br />\n<b>Conclusion</b>\n<br />By modifying the vector quantization objective to align with the goals of MIPS, we achieve \n<a href=\"http://ann-benchmarks.com/\">state-of-the-art performance</a> on nearest neighbor search benchmarks, a key indicator of embedding-based search performance. Although anisotropic vector quantization is an important technique, we believe it is just one example of the performance gains achievable by optimizing algorithms for the end goal of improving search accuracy rather than an intermediate goal such as compression distortion.\n<br />\n<br />\n<b>Acknowledgements</b>\n<br />\n<em>This post reflects the work of the entire ScaNN team: David Simcha, Erik Lindgren, Felix Chern, Nathan Cordeiro, Ruiqi Guo, Sanjiv Kumar, and Zonglin Li. We’d also like to thank Dan Holtmann-Rice, Dave Dopson, and Felix Yu.</em>\n<br />\n<br />\n<hr width=\"100%\">\n <p><span class=\"Apple-style-span\" style=\"font-size: x-small;\"><br /><a name=\"1\"><b>* </b></a>ScaNN performs similarly well on the other datasets of <a href=\"http://ann-benchmarks.com\">ann-benchmarks.com</a>, but the website currently shows outdated, lower numbers. See this <a href=\"https://github.com/erikbern/ann-benchmarks/pull/172\">pull request</a> for more representative performance figures on other datasets. <a href=\"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html#top1\">↩</a><br /></span>\n  <div class=\"feedflare\"> \n   <a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=sKILqR_zvQI:hLKHaLB-804:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\" /></a> \n  </div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/sKILqR_zvQI\" height=\"1\" width=\"1\" alt=\"\" /></p>\n</hr>","descriptionType":"html","publishedDate":"Tue, 28 Jul 2020 17:03:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-Rsymb9XvPOE/Xx8rfRnmTHI/AAAAAAAAGRQ/U2n_bBNXS4IBstYrx2IalrFXufLUvmn2gCLcBGAsYHQ/s640/ScaNN%2Btom%2Bexport.gif","linkMd5":"6fa5004149ccce453a3e5645aa8b9477","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx/cdn78@2020_5/2020/08/24/14-07-35-843_d3214e2bba50cc30.webp","destWidth":512,"destHeight":245,"sourceBytes":827250,"destBytes":728984,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-Rsymb9XvPOE/Xx8rfRnmTHI/AAAAAAAAGRQ/U2n_bBNXS4IBstYrx2IalrFXufLUvmn2gCLcBGAsYHQ/s640/ScaNN%2Btom%2Bexport.gif":"https://cdn.jsdelivr.net/gh/myreaderx/cdn78@2020_5/2020/08/24/14-07-35-843_d3214e2bba50cc30.webp","https://1.bp.blogspot.com/-0HxtiXvnyTU/Xx8xNOgfUSI/AAAAAAAAGRc/Vgf0gK50N9cIG1aA9TWFLx7nqAYwuP5TQCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn18@2020_4/2020/08/24/14-07-56-416_c53290c1c77806fa.webp","https://1.bp.blogspot.com/-l4VY-q1YcEE/XxsvREuIEvI/AAAAAAAAGQs/zzJNUHTZ9SU8LtKzm2rgl0oQCuiJ9fhIwCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn98@2020_4/2020/08/24/14-07-55-408_cb4f9bcf2189e939.webp","https://1.bp.blogspot.com/--mbMV8fQY28/XxsvbGL_l-I/AAAAAAAAGQ0/Br9B3XGnBa07barUxC4XTi8hSDxYzwAEgCLcBGAsYHQ/s640/image5.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn21@2020_5/2020/08/24/14-08-04-796_df28e85253ab9cd7.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/sKILqR_zvQI":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn97@2020_1/2020/08/24/14-07-56-223_d463e7644372228c.webp"},"publishedOrCreatedDate":1598278048093},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Improving Holistic Scene Understanding with Panoptic-DeepLab","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/A_hvdqM_OXc/improving-holistic-scene-understanding.html","description":"<span class=\"byline-author\">Posted by Bowen Cheng, Student Researcher, and Liang-Chieh Chen, Research Scientist, Google Research</span>  <br /><br />Real-world computer vision applications, such as self-driving cars and robotics, rely on two core tasks —  <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/whats-in-an-image-fast-accurate-image-segmentation-with-cloud-tpus\">instance segmentation</a> and <a href=\"https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html\">semantic segmentation</a>. Instance segmentation identifies the class and extent of individual “things” in an image (i.e., countable objects such as people, animals, cars, etc.) and assigns unique identifiers to each (e.g., car_1 and car_2). This is complemented by <a href=\"https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html\">semantic segmentation</a>, which labels all pixels in an image, including the “things” that are present as well as the surrounding “stuff” (e.g., amorphous regions of similar texture or material, such as grass, sky or road). This latter task, however, does not differentiate between pixels of the same class that belong to different <em>instances</em> of that class.<br /><br /><a href=\"https://arxiv.org/abs/1801.00868\">Panoptic segmentation</a> represents the unification of these two approaches with the goal of assigning a unique value to every pixel in an image that encodes both semantic label and instance ID. Most existing panoptic segmentation algorithms are based on <a href=\"https://arxiv.org/abs/1703.06870\">Mask R-CNN</a>, which treats semantic and instance segmentation separately. The instance segmentation step identifies objects in an image, but it often produces object instance masks that overlap one another. To settle the conflict between overlapping instance masks, one commonly employs an heuristic that resolves the discrepancy either based on the mask with a higher confidence score or by use of a pre-defined pairwise relationship between categories (e.g., a tie should always be worn on a person’s front). Additionally, the discrepancies between semantic and instance segmentation results are sorted out by favoring the instance predictions. While these methods generally produce good results, they also introduce heavy latency, which makes it challenging to apply them in real-time applications. <br /><br />Driven by the need of a real-time panoptic segmentation model, we propose “<a href=\"https://arxiv.org/abs/1911.10194\">Panoptic-DeepLab: a simple, fast and strong system for panoptic segmentation</a>”, accepted to <a href=\"http://cvpr2020.thecvf.com/\">CVPR 2020</a>. In this work, we extend the commonly used modern semantic segmentation model, <a href=\"https://arxiv.org/abs/1606.00915\">DeepLab</a>, to perform panoptic segmentation using only a small number of additional parameters with the addition of marginal computation overhead. The resulting model, Panoptic-DeepLab, produces semantic and instance segmentation in parallel and without overlap, avoiding the need for the manually designed heuristics adopted by other methods. Additionally, we develop a computationally efficient operation that merges the semantic and instance segmentation results, enabling near real-time end-to-end panoptic segmentation prediction. Unlike methods based on Mask R-CNN, Panoptic-DeepLab does not generate <a href=\"https://ai.googleblog.com/2017/06/supercharge-your-computer-vision-models.html\">bounding box predictions</a> and requires only three loss functions during training, significantly fewer than current state-of-the-art methods, such as <a href=\"https://arxiv.org/abs/1901.03784\">UPSNet</a>, which can have up to eight. Finally, Panoptic-DeepLab has demonstrated state-of-the-art performance on several academic datasets.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-oSVo5zrtc_Y/Xxc5yMjPndI/AAAAAAAAGP8/yLbgy8VsfeoSY_5TTgJSjupW5QGRtjjewCLcBGAsYHQ/s1600/image2.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"173\" data-original-width=\"683\" height=\"162\" src=\"https://1.bp.blogspot.com/-oSVo5zrtc_Y/Xxc5yMjPndI/AAAAAAAAGP8/yLbgy8VsfeoSY_5TTgJSjupW5QGRtjjewCLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Panoptic segmentation results obtained by Panoptic-DeepLab. <b>Left:</b> Video frames used as input to the panoptic segmentation model. <b>Right:</b> Results overlaid on video frames. Each object instance has a unique label, e.g., car_1, car_2, etc.</td></tr></tbody></table><b>Overview</b><br />Panoptic-DeepLab is simple both conceptually and architecturally. At a high-level, it predicts three outputs. The first is semantic segmentation, in which it assigns a semantic class (e.g., car or grass) to each pixel. However, it does not differentiate between multiple instances of the same class. So, for example, if one car is partly behind another, the pixels associated with both would have the same associated class and would be indistinguishable from one another. This can be addressed by the second two outputs from the model: a center-of-mass prediction for each instance and instance center regression, where the model learns to regress each instance pixel to its center of mass. This latter step ensures that the model associates pixels of a given class to the appropriate instance. The class-agnostic instance segmentation, obtained by grouping predicted foreground pixels to their closest predicted instance centers, is then fused with semantic segmentation by majority-vote rule to generate the final panoptic segmentation.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-gerljyblrUs/Xxc56h-1AuI/AAAAAAAAGQA/YB6id3ki1P87w3ZzYdwt6n6SuOGQR07awCLcBGAsYHQ/s1600/image3.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1096\" data-original-width=\"914\" height=\"640\" src=\"https://1.bp.blogspot.com/-gerljyblrUs/Xxc56h-1AuI/AAAAAAAAGQA/YB6id3ki1P87w3ZzYdwt6n6SuOGQR07awCLcBGAsYHQ/s640/image3.png\" width=\"532\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Overview of Panoptic-DeepLab. Semantic segmentation associates pixels in the image with general classes, while the class-agnostic instance segmentation step identifies the pixels associated with an individual object, regardless of the class. Taken together one gets the final panoptic segmentation image.</td></tr></tbody></table><b>Neural Network Design</b><br />Panoptic-DeepLab consists of four components: (1) an encoder backbone pre-trained on <a href=\"http://www.image-net.org/\">ImageNet</a>, shared by both the semantic segmentation and instance segmentation branches of the architecture; (2) <a href=\"https://arxiv.org/abs/1606.00915\">atrous spatial pyramid pooling</a> (ASPP) modules, similar to that used by <a href=\"https://arxiv.org/abs/1606.00915\">DeepLab</a>, which are deployed independently in each branch in order to perform segmentation at a range of spatial scales; (3) similarly decoupled <a href=\"https://arxiv.org/abs/1802.02611\">decoder modules</a> specific to each segmentation task; and (4) task-specific prediction heads.<br /><br />The encoder backbone (1), which has been pre-trained on ImageNet, extracts feature maps that are shared by both the semantic segmentation and instance segmentation branches of the architecture. Typically, the feature map is generated by the backbone model using a standard convolution, which reduces the resolution of the output map to 1/32nd that of the input image and is too coarse for accurate image segmentation. In order to preserve the details of object boundaries, we instead employ <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d\">atrous convolution</a>, which better retains important features like edges, to generate a feature map with a resolution of 1/16th the original. This is then followed by two  ASPP modules (2), one for each branch, which captures multi-scale information for segmentation.<br /><br />The light-weight decoder modules (3) follow those used in the most recent DeepLab version (<a href=\"https://arxiv.org/abs/1802.02611\">DeepLabV3+</a>), but with two modifications. First, we reintroduce an additional low-level feature map (1/8th scale) to the decoder, which helps to preserve spatial information from the original image (e.g., object boundaries) that can be significantly degraded in the final feature map output by the backbone. Second, instead of using the typical 3 × 3 kernel, the decoder employs a 5 × 5 depthwise-separable convolution, which yields somewhat better performance at only a minimal cost in additional overhead.<br /><br />The two prediction heads (4) are tailored to their task. The semantic segmentation head employs a weighted version of the standard bootstrapped cross entropy loss function, which weights each pixel differently and has proven to be more effective for segmentation of small-scale objects. The instance segmentation head is trained to predict the offsets between the center of mass of an object instance and the surrounding pixels, without knowledge of the object class, forming the class-agnostic instance masks.<br /><br /><b>Results</b><br />To demonstrate the effectiveness of Panoptic-DeepLab, we conduct experiments on three popular academic datasets, <a href=\"https://www.cityscapes-dataset.com/\">Cityscapes</a>, <a href=\"https://www.mapillary.com/dataset/vistas?pKey=cc5dEAyQECBFF9MN3MbdZA\">Mapillary Vistas</a>, and <a href=\"http://cocodataset.org/#panoptic-2019\">COCO</a> datasets. With a simple architecture, Panoptic-DeepLab ranks first in <a href=\"https://www.cityscapes-dataset.com/\">Cityscapes</a> for all three tasks (semantic, instance and panoptic segmentation) without any task-specific fine-tuning. Additionally, Panoptic-DeepLab won the <em>Best Result</em>, <em>Best Paper</em>, and <em>Most Innovative</em> awards on the <a href=\"http://cocodataset.org/workshop/coco-mapillary-iccv-2019.html#mapillary-panoptic\">Mapillary Panoptic Segmentation track</a> at <a href=\"http://cocodataset.org/workshop/coco-mapillary-iccv-2019.html\">ICCV 2019 Joint COCO and Mapillary Recognition Challenge Workshop</a>. It outperforms the winner of 2018 by a healthy margin of 1.5%. Finally, Panoptic-DeepLab sets new state-of-the-art bottom-up (i.e., box-free) panoptic segmentation results on the <a href=\"http://cocodataset.org/#panoptic-2019\">COCO</a> dataset, and is also comparable to other methods based on Mask R-CNN.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HNNrEpGoZyY/Xxc6AlgpRxI/AAAAAAAAGQE/QqqLwHukUuQxiIFeJkT_sBw5HPZ_avvVwCLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1065\" data-original-width=\"1600\" height=\"424\" src=\"https://1.bp.blogspot.com/-HNNrEpGoZyY/Xxc6AlgpRxI/AAAAAAAAGQE/QqqLwHukUuQxiIFeJkT_sBw5HPZ_avvVwCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Accuracy (PQ) vs. Speed (GPU inference time) across three datasets.</td></tr></tbody></table><b>Conclusion</b><br />With a simple architecture and only three training loss functions, Panoptic-DeepLab achieves state-of-the-art performance while being faster than other methods based on <a href=\"https://arxiv.org/abs/1703.06870\">Mask R-CNN</a>. To summarize, we develop the first single-shot panoptic segmentation model that attains state-of-the-art performance on several public benchmarks, and delivers near real time end-to-end inference speed. We hope our simple and effective Panoptic-DeepLab could establish a solid baseline and further benefit the research community.<br /><br /><b>Acknowledgements</b><br /><em>We would like to thank the support and valuable discussions with Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, Florian Schroff as well as the Google Mobile Vision team.</em> <div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=A_hvdqM_OXc:oPBVy-UvrCw:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/A_hvdqM_OXc\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 21 Jul 2020 20:34:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-oSVo5zrtc_Y/Xxc5yMjPndI/AAAAAAAAGP8/yLbgy8VsfeoSY_5TTgJSjupW5QGRtjjewCLcBGAsYHQ/s640/image2.gif","linkMd5":"120a22df18c057ba75ed58782d9abeaf","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn73@2020_4/2020/08/24/14-07-28-896_283470d036354fef.webp","destWidth":640,"destHeight":162,"sourceBytes":1203387,"destBytes":313796,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-oSVo5zrtc_Y/Xxc5yMjPndI/AAAAAAAAGP8/yLbgy8VsfeoSY_5TTgJSjupW5QGRtjjewCLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn73@2020_4/2020/08/24/14-07-28-896_283470d036354fef.webp","https://1.bp.blogspot.com/-gerljyblrUs/Xxc56h-1AuI/AAAAAAAAGQA/YB6id3ki1P87w3ZzYdwt6n6SuOGQR07awCLcBGAsYHQ/s640/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn71@2020_6/2020/08/24/14-07-55-615_07e460ae2b3ef04c.webp","https://1.bp.blogspot.com/-HNNrEpGoZyY/Xxc6AlgpRxI/AAAAAAAAGQE/QqqLwHukUuQxiIFeJkT_sBw5HPZ_avvVwCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn25@2020_5/2020/08/24/14-07-55-538_615660cbc8bd0abb.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/A_hvdqM_OXc":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn90@2020_5/2020/08/24/14-07-56-312_97d2e8923dd09c96.webp"},"publishedOrCreatedDate":1598278048093},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Grounding Natural Language Instructions to Mobile UI Actions","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/wmN0OhKV4I4/grounding-natural-language-instructions.html","description":"<span class=\"byline-author\">Posted by Yang Li, Research Scientist, Google Research</span>  <br /><br />Mobile devices offer a myriad of functionalities that can assist in everyday activities. However, many of these functionalities are not easily discoverable or accessible to users, forcing users to look up how to perform a specific task -- how to turn on the traffic mode in Maps or change notification settings in YouTube, for example. While searching the web for detailed instructions for these questions is an option, it is still up to the user to follow these instructions step-by-step and navigate UI details through a small touchscreen, which can be tedious and time consuming, and results in reduced accessibility. What if one could design a computational agent to turn these language instructions into actions and automatically execute them on the user’s behalf?<br /><br />In “<a href=\"https://arxiv.org/abs/2005.03776\">Mapping Natural Language Instructions to Mobile UI Action Sequences</a>”, published at <a href=\"https://acl2020.org/\">ACL 2020</a>, we present the first step towards addressing the problem of <em>automatic action sequence mapping</em>, creating three new datasets used to train deep learning models that ground natural language instructions to executable mobile UI actions. This work lays the technical foundation for task automation on mobile devices that would alleviate the need to maneuver through UI details, which may be especially valuable for users who are visually or situationally impaired. We have also open-sourced our model code and data pipelines through our <a href=\"https://github.com/google-research/google-research/tree/master/seq2act\">GitHub repository</a>, in order to spur further developments among the research community.<br /><br /><b>Constructing Language Grounding Models</b><br />People often provide one another with instructions in order to coordinate joint efforts and accomplish tasks involving complex sequences of actions, for example, following a recipe to bake a cake, or having a friend walk you through setting up a home network. Building computational agents able to help with similar interactions is an important goal that requires true <a href=\"https://en.wikipedia.org/wiki/Symbol_grounding_problem\">language grounding</a> in the environments in which the actions take place. <br /><br />The learning task addressed here is to predict a sequence of actions for a mobile platform given a set of instructions, a sequence of screens produced as the system transitions from one screen to another, as well as the set of interactive elements on those screens. Training such a model end-to-end would require paired language-action data, which is difficult to acquire at a large scale. <br /><br />Instead, we deconstruct the problem into two sequential steps: an <em>action phrase-extraction</em> step and a <em>grounding</em> step. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-CemiVojz7C0/XwekwEp2UxI/AAAAAAAAGOM/xIAJa9i8DXA7Ev3B6qcx6JcQ7MJ9n0xqgCLcBGAsYHQ/s1600/image2%2B%25281%2529.jpg\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1600\" data-original-width=\"1314\" height=\"640\" src=\"https://1.bp.blogspot.com/-CemiVojz7C0/XwekwEp2UxI/AAAAAAAAGOM/xIAJa9i8DXA7Ev3B6qcx6JcQ7MJ9n0xqgCLcBGAsYHQ/s640/image2%2B%25281%2529.jpg\" width=\"523\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The workflow of grounding language instructions to executable actions.</td></tr></tbody></table>The action phrase-extraction step identifies the operation, object and argument descriptions from multi-step instructions using a <a href=\"https://arxiv.org/abs/1706.03762\">Transformer</a> model with <a href=\"https://arxiv.org/abs/1810.10126\">area attention</a> for representing each description phrase. Area attention allows the model to attend to a group of adjacent words in the instruction (a span) as a whole for decoding a description.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-74GDeejuQSg/XwiZLA3EmVI/AAAAAAAAGOs/EekF8fpsUKMGHreoMDBbQQPFWWrMge4eACLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"665\" data-original-width=\"1600\" height=\"264\" src=\"https://1.bp.blogspot.com/-74GDeejuQSg/XwiZLA3EmVI/AAAAAAAAGOs/EekF8fpsUKMGHreoMDBbQQPFWWrMge4eACLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The action phrase extraction model takes a word sequence of a natural language instruction and outputs a sequence of spans (denoted in red boxes) that indicate the phrases describing the operation, the object and the argument of each action in the task.</td></tr></tbody></table>Next, the grounding step matches the extracted operation and object descriptions with a UI object on the screen. Again, we use a Transformer model, but in this case, it contextually represents UI objects and grounds object descriptions to them.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-TXjVAPwTxks/XwelMUqNFBI/AAAAAAAAGOc/9bFKQnVg4ZYF8ne_qr2st5LxxBb7iY6QwCLcBGAsYHQ/s1600/image3%2B%25281%2529.jpg\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"638\" data-original-width=\"1377\" height=\"296\" src=\"https://1.bp.blogspot.com/-TXjVAPwTxks/XwelMUqNFBI/AAAAAAAAGOc/9bFKQnVg4ZYF8ne_qr2st5LxxBb7iY6QwCLcBGAsYHQ/s640/image3%2B%25281%2529.jpg\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The grounding model takes the extracted spans as input and grounds them to executable actions, including the object an action is applied to, given the UI screen at each step during execution.</td></tr></tbody></table><b>Results</b><br />To investigate the feasibility of this task and the effectiveness of our approach, we construct three new datasets to train and evaluate our model. The first dataset includes 187 multi-step English instructions for operating Pixel phones along their corresponding action-screen sequences and enables assessment of full task performance on naturally occurring instructions, which is used for testing end-to-end grounding quality. For action phrase extraction training and evaluation, we obtain English “how-to” instructions that can be found abundantly from the web and annotate phrases that describe each action. To train the grounding model, we synthetically generate 295K single-step commands to UI actions, covering 178K different UI objects across 25K mobile UI screens from a <a href=\"https://dl.acm.org/doi/10.1145/3126594.3126651\">public android UI corpus</a>.<br /><br />A Transformer with area attention obtains 85.56% accuracy for predicting span sequences that completely match the ground truth. The phrase extractor and grounding model together obtain 89.21% partial and 70.59% complete accuracy for matching ground-truth action sequences on the more challenging task of mapping language instructions to executable actions end-to-end. We also evaluated alternative methods and representations of UI objects, such as using a <a href=\"http://proceedings.mlr.press/v48/niepert16.html\">graph convolutional network</a> (GCN) or a <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\">feedforward network</a>, and found those that can represent an object contextually in the screen lead to better grounding accuracy. The new datasets, models and results provide an important first step on the challenging problem of grounding natural language instructions to mobile UI actions. <br /><br /><b>Conclusion</b><br />This research, and language grounding in general, is an important step for translating multi-stage instructions into actions on a graphical user interface. Successful application of task automation to the UI domain has the potential to significantly improve accessibility, where language interfaces might help individuals who are visually impaired perform tasks with interfaces that are predicated on sight. This also matters for situational impairment when one cannot access a device easily while encumbered by tasks at hand.<br /><br />By deconstructing the problem into action phrase extraction and language grounding, progress on either can improve full task performance and it alleviates the need to have language-action paired datasets, which are difficult to collect at scale. For example, action span extraction is related to both semantic role labeling and extraction of multiple facts from text and could benefit from innovations in span identification and multitask learning. Reinforcement learning that has been applied in previous grounding work may help improve out-of-sample prediction for grounding in UIs and improve direct grounding from hidden state representations. Although our datasets were based on Android UIs, our approach can be applied generally to instruction grounding on other user interface platforms. Lastly, our work provides a technical foundation for investigating user experiences in language-based human computer interaction. <br /><br /><b>Acknowledgements </b><br /><i>Many thanks to my co-authors Jiacong He, Xin Zhou, Yuan Zhang and Jason Baldridge on this work at Google Research. I would also like to thank Gang Li who provided generous help for creating open-source datasets, and Ashwin Kakarla, Muqthar Mohammad and Mohd Majeed for their help with the annotations.</i><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=wmN0OhKV4I4:VlcN57_2mkI:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/wmN0OhKV4I4\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Fri, 10 Jul 2020 17:01:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-CemiVojz7C0/XwekwEp2UxI/AAAAAAAAGOM/xIAJa9i8DXA7Ev3B6qcx6JcQ7MJ9n0xqgCLcBGAsYHQ/s640/image2%2B%25281%2529.jpg","linkMd5":"82c16e7daa20dcc8b6c6188bc66eba60","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn66@2020_6/2020/08/24/14-07-28-409_6902584c7a1e23fc.webp","destWidth":525,"destHeight":640,"sourceBytes":92161,"destBytes":41896,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-CemiVojz7C0/XwekwEp2UxI/AAAAAAAAGOM/xIAJa9i8DXA7Ev3B6qcx6JcQ7MJ9n0xqgCLcBGAsYHQ/s640/image2%2B%25281%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn66@2020_6/2020/08/24/14-07-28-409_6902584c7a1e23fc.webp","https://1.bp.blogspot.com/-74GDeejuQSg/XwiZLA3EmVI/AAAAAAAAGOs/EekF8fpsUKMGHreoMDBbQQPFWWrMge4eACLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn58@2020_4/2020/08/24/14-07-56-743_afb98d70ce3e2007.webp","https://1.bp.blogspot.com/-TXjVAPwTxks/XwelMUqNFBI/AAAAAAAAGOc/9bFKQnVg4ZYF8ne_qr2st5LxxBb7iY6QwCLcBGAsYHQ/s640/image3%2B%25281%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn9@2020_5/2020/08/24/14-07-56-399_b2e0a7ccad17f532.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/wmN0OhKV4I4":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn13@2020_4/2020/08/24/14-07-55-325_c664d740dde00de9.webp"},"publishedOrCreatedDate":1598278048099},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Tackling Open Challenges in Offline Reinforcement Learning","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","description":"<span class=\"byline-author\">Posted by George Tucker, Research Scientist and Sergey Levine, Faculty Advisor, Google Research</span> <div><span class=\"byline-author\"><br /></span></div><p>Over the past several years, there has been a surge of interest in <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">reinforcement learning</a> (RL) driven by its high-profile successes in <a href=\"https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html\">game playing</a> and <a href=\"https://ai.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html\">robotic control</a>. However, unlike <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\">supervised learning</a> methods, which learn from massive datasets that are collected once and then reused, RL algorithms use a trial-and-error feedback loop that requires active interaction during learning, collecting data every time a new policy is learned. This approach is prohibitive in many real-world settings, such as healthcare, autonomous driving, and dialogue systems, where trial-and-error data collection can be costly, time consuming, or even irresponsible. Even for problems where some active data collection <em>can</em> be used, the requirement for interactive collection limits dataset size and diversity.</p><p>Offline RL (also called <em>batch</em> RL or <em>fully off-policy</em> RL) relies solely on a previously collected dataset without further interaction. It provides a way to utilize previously collected datasets — from previous RL experiments, from human demonstrations, and from hand-engineered exploration strategies — in order to automatically learn decision-making strategies. In principle, while <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">off-policy RL algorithms can be used in the offline setting</a> (<em>fully off-policy</em>), they are generally only successful when used with active environment interaction — without receiving this direct feedback, they often exhibit <a href=\"https://arxiv.org/abs/1812.02900\">undesirable performance in practice</a>. Consequently, while offline RL has enormous potential, that potential cannot be reached without resolving significant algorithmic challenges.  </p><p>In “<a href=\"https://arxiv.org/abs/2005.01643\">Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems</a>”, we provide a comprehensive tutorial on approaches for tackling the challenges of offline RL and discuss the many issues that remain. To address these issues, we have designed and released an open-source benchmarking framework, <a href=\"https://sites.google.com/corp/view/d4rl/home\">Datasets for Deep Data-Driven Reinforcement Learning</a> (D4RL), as well as a new, simple, and highly effective offline RL algorithm, called <a href=\"https://sites.google.com/view/cql-offline-rl\">conservative Q-learning</a> (CQL). </p><p><b>Benchmarks for Offline RL</b><br />In order to understand the capabilities of current approaches and to guide future progress, it is first necessary to have effective benchmarks. A common choice in prior work was to simply use <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">data generated by a successful online RL run</a>. However, while simple, this data collection approach is artificial because it involves training an online RL agent which is prohibitive in many real-world settings as we discussed previously. One wishes to learn a policy that is <em>better</em> than the current best from diverse data sources that provides good <em>coverage</em> of the task. For example, one might have data collected from a hand-designed controller of a robot arm, and use offline RL to train an improved controller. To enable progress in this field under realistic settings, one needs a benchmark suite that accurately reflects these settings, while being simple and accessible enough to enable rapid experimentation.  </p><p><a href=\"https://sites.google.com/corp/view/d4rl/home\">D4RL</a> provides standardized environments, datasets and evaluation protocols, as well as reference scores for recent algorithms to help accomplish this. This is a “batteries-included” resource, making it ideal for anyone to jump in and get started with minimal fuss.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s1386/Environments.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1023\" data-original-width=\"1386\" src=\"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s640/Environments.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Environments in D4RL</td></tr></tbody></table><p>The key design goal for D4RL was to develop tasks that reflect both real-world dataset challenges as well as real-world applications. Previous datasets used data collected either from random agents or agents <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">trained with RL</a>. Instead, by thinking through potential applications in autonomous driving, robotics, and other domains, we considered how real-world applications of offline RL might require handling of data generated from human demonstrations or hard-coded controllers, data collected from heterogeneous sources, and data collected by agents with a variety of different goals.</p><p></p><p>Aside from the widely used <a href=\"http://www.mujoco.org/\">MuJoCo</a> locomotion tasks, D4RL includes datasets for more complex tasks. The <a href=\"https://sites.google.com/corp/view/deeprl-dexterous-manipulation\">Adroit domain</a>, which requires manipulating a realistic robotic hand to use a hammer, for example, illustrates the challenges of working with limited human demonstrations, without which these tasks are extremely challenging. <a href=\"https://arxiv.org/abs/1911.11361\">Previous work</a> found that existing datasets could not distinguish between competing methods, whereas the Adroit domain reveals clear deficiencies between them. </p><p>Another common scenario for real-world tasks is one in which the dataset used for training is collected from agents performing a wide range of other activities that are related to, but not specifically targeted towards, the task of interest. For example, data from human drivers may illustrate how to drive a car well, but do not necessarily show how to reach a specific desired destination. In this case, one might like offline RL methods to “stitch” together parts of routes in the driving dataset to accomplish a task that was not actually seen in the data (i.e., navigation). As an illustrative example, given paths labeled “A” and “B” in the picture below, offline RL should be able to “remix” them to produce path C. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-59MnhMRiRQs/Xz2rScdINAI/AAAAAAAAGbc/O6nSOZaF-FsWJ6DZ8FwNRHRTZreK5tN0ACLcBGAsYHQ/s1278/image6.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"480\" data-original-width=\"1278\" height=\"154\" src=\"https://1.bp.blogspot.com/-59MnhMRiRQs/Xz2rScdINAI/AAAAAAAAGbc/O6nSOZaF-FsWJ6DZ8FwNRHRTZreK5tN0ACLcBGAsYHQ/w410-h154/image6.png\" width=\"410\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Having only observed paths A and B, they can be combined to form a shortest path (C).</td></tr></tbody></table><p>We constructed a series of increasingly difficult tasks to exercise this “stitching” ability. The maze environments, shown below, require two robots (a simple ball or an “Ant” robot) to navigate to locations in a series of mazes.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-OLiDcAzEtCg/Xz2rbvae8KI/AAAAAAAAGbk/bQvUDWg-l60yVhBUJXmZv2NFHXFWaENFQCLcBGAsYHQ/s610/image1.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"300\" data-original-width=\"610\" src=\"https://1.bp.blogspot.com/-OLiDcAzEtCg/Xz2rbvae8KI/AAAAAAAAGbk/bQvUDWg-l60yVhBUJXmZv2NFHXFWaENFQCLcBGAsYHQ/s0/image1.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Maze navigation environments in D4RL, which require “stitching” parts of paths to accomplish new navigational goals that were not seen in the dataset.</td></tr></tbody></table><p>A more complex “stitching” scenario is provided by the Franka kitchen domain (based on the <a href=\"https://github.com/google-research/relay-policy-learning\">Adept environment</a>), where demonstrations from humans using a VR interface comprise a multi-task dataset, and offline RL methods must again “remix” this data.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Yg5us32h5YE/Xz2rogG6KHI/AAAAAAAAGbo/Ay4oOI1bQncrCt-RbE0kI6icoFUDG9oXQCLcBGAsYHQ/s770/image5.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"250\" data-original-width=\"770\" src=\"https://1.bp.blogspot.com/-Yg5us32h5YE/Xz2rogG6KHI/AAAAAAAAGbo/Ay4oOI1bQncrCt-RbE0kI6icoFUDG9oXQCLcBGAsYHQ/s640/image5.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The “Franka kitchen” domain requires using data from human demonstrators performing a variety of different tasks in a simulated kitchen.</td></tr></tbody></table><p>Finally, D4RL includes two tasks that are meant to more accurately reflect potential realistic applications of offline RL, both based on existing driving simulators. One is a first-person driving dataset that utilizes the widely used <a href=\"http://carla.org/\">CARLA</a> simulator developed at Intel, which provides photo-realistic images in realistic driving domains, and the other is a dataset from the <a href=\"https://flow-project.github.io/\">Flow</a> traffic control simulator (from UC Berkeley), which requires controlling autonomous vehicles to facilitate effective traffic flow.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-aMeUM4tOpxM/Xz2sHEc0WEI/AAAAAAAAGb0/xAeSG6rn0q0Y2SnIHxGZwY1_fRssvPfPwCLcBGAsYHQ/s590/image4.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"256\" data-original-width=\"590\" src=\"https://1.bp.blogspot.com/-aMeUM4tOpxM/Xz2sHEc0WEI/AAAAAAAAGb0/xAeSG6rn0q0Y2SnIHxGZwY1_fRssvPfPwCLcBGAsYHQ/s0/image4.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">D4RL includes datasets based on existing realistic simulators for driving with CARLA (<b>left</b>) and traffic management with Flow (<b>right</b>).</td></tr></tbody></table><p>We have packaged these tasks and standardized datasets into an easy-to-use <a href=\"https://github.com/rail-berkeley/d4rl\">Python package</a> to accelerate research. Furthermore, we provide <a href=\"https://sites.google.com/corp/view/d4rl/home\">benchmark numbers</a> for all tasks using relevant prior methods (<a href=\"https://www.mitpressjournals.org/doi/abs/10.1162/neco.1991.3.1.88\">BC</a>, <a href=\"https://arxiv.org/abs/1812.05905\">SAC</a>, <a href=\"https://arxiv.org/abs/1906.00949\">BEAR</a>, <a href=\"https://arxiv.org/abs/1911.11361\">BRAC</a>, <a href=\"https://arxiv.org/abs/1910.00177\">AWR</a>, <a href=\"https://arxiv.org/abs/1812.02900\">BCQ</a>), in order to  baseline new approaches. We are not the first to propose a benchmark for offline RL: a number of <a href=\"https://arxiv.org/abs/1812.02900\">prior</a> <a href=\"https://github.com/aviralkumar2907/BEAR\">works</a> have proposed simple datasets based on running RL algorithms, and several <a href=\"https://arxiv.org/abs/2006.13888\">more recent</a> works have proposed datasets with image observations and other features. However, we believe that the more realistic dataset composition in D4RL makes it an effective way to drive progress in the field. </p><p><b>An Improved Algorithm for Offline RL</b><br />As we developed the benchmark tasks, we found that existing methods could not solve the more challenging tasks. The central challenge arises from a <em>distributional shift</em>: in order to improve over the historical data, offline RL algorithms must learn to make decisions that differ from the decisions taken in the dataset. However, this can lead to problems when the consequences of a seemingly good decision cannot be deduced from the data — if no agent has taken this particular turn in the maze, how does one know if it leads to the goal or not? Without handling this distributional shift problem, offline RL methods can extrapolate erroneously, making over-optimistic conclusions about the outcomes of rarely seen actions. Contrast this with the online setting, where reward bonuses modeled after <a href=\"https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html\">curiosity and surprise</a> optimistically bias the agent to explore all potentially rewarding paths. Because the agent receives interactive feedback, if the action turns out to be unrewarding, then it can simply avoid the path in the future. </p><p>To address this, we developed <a href=\"https://sites.google.com/view/cql-offline-rl\">conservative Q-learning</a> (CQL), an offline RL algorithm designed to guard against overestimation while avoiding explicit construction of a separate behavior model and without using importance weights. While standard <a href=\"https://en.wikipedia.org/wiki/Q-learning\">Q-learning</a> (and actor-critic) methods <a href=\"https://en.wikipedia.org/wiki/Temporal_difference_learning\">bootstrap from previous estimates</a>, CQL is unique in that it is fundamentally a pessimistic algorithm: it assumes that if a good outcome was not seen for a given action, that action is likely to not be a good one. The central idea of CQL is to learn a lower bound on the policy’s expected return (called the <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning#Value_function\">Q-function</a>), instead of learning to approximate the expected return. If we then optimize our policy under this conservative Q-function, we can be confident that its value is no lower than this estimate, preventing errors from overestimation. </p><p>We found that CQL attains state-of-the-art results on many of the harder D4RL tasks: CQL outperformed other approaches on the AntMaze, Kitchen tasks, and 6 out of 8 Adroit tasks. In particular, on the AntMaze tasks, which require navigating through a maze with an “Ant” robot, CQL is often the only algorithm that is able to learn non-trivial policies. CQL also performs well on other tasks, including Atari games. On the Atari tasks from <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">Agarwal et al.</a>, CQL outperforms prior methods when data is limited (“1%” dataset). Moreover, CQL is simple to implement on top of existing algorithms (e.g., <a href=\"https://arxiv.org/abs/1710.10044\">QR-DQN</a> and <a href=\"https://arxiv.org/abs/1812.05905\">SAC</a>), without training additional neural networks. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-uEkUMJ-m8Gw/Xz2zWt4CGzI/AAAAAAAAGcI/CXOmIimUTDcD4wFBB4fN8bfgyhXVpV4yQCLcBGAsYHQ/s1164/image3.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"342\" data-original-width=\"1164\" height=\"96\" src=\"https://1.bp.blogspot.com/-uEkUMJ-m8Gw/Xz2zWt4CGzI/AAAAAAAAGcI/CXOmIimUTDcD4wFBB4fN8bfgyhXVpV4yQCLcBGAsYHQ/w328-h96/image3.png\" width=\"328\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Performance of CQL on Atari games with the 1% dataset from <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">Agarwal et al.</a></td></tr></tbody></table><p><b>Future Thoughts</b><br />We are excited about the fast-moving field of offline RL. While we took a first step towards a standard benchmark, there is clearly still room for improvement. We expect that as algorithms improve, we will need to reevaluate the tasks in the benchmark and develop more challenging tasks. We look forward to working with the community to evolve the benchmark and evaluation protocols. Together, we can bring the rich promises of offline RL to real-world applications. </p><p><b>Acknowledgements</b><br /><em>This work was carried out in collaboration with UC Berkeley PhD students Aviral Kumar, Justin Fu, and Aurick Zhou, with contributions from Ofir Nachum from Google Research.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=TQbqJYtduXE:tmhMJ5VV1bQ:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/TQbqJYtduXE\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Thu, 20 Aug 2020 17:02:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s72-c/Environments.png","linkMd5":"a6859779ff2bda767c63a2b9c5a2c0a9","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn10@2020_5/2020/08/24/14-07-59-404_3d283dac124096aa.webp","destWidth":72,"destHeight":72,"sourceBytes":8468,"destBytes":2168,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s640/Environments.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn85@2020_1/2020/08/24/14-07-55-793_712572a622809d76.webp","https://1.bp.blogspot.com/-59MnhMRiRQs/Xz2rScdINAI/AAAAAAAAGbc/O6nSOZaF-FsWJ6DZ8FwNRHRTZreK5tN0ACLcBGAsYHQ/w410-h154/image6.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn30@2020_5/2020/08/24/14-07-58-363_1c437a54de8d29b6.webp","https://1.bp.blogspot.com/-OLiDcAzEtCg/Xz2rbvae8KI/AAAAAAAAGbk/bQvUDWg-l60yVhBUJXmZv2NFHXFWaENFQCLcBGAsYHQ/s0/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn66@2020_1/2020/08/24/14-07-59-448_e9f25e0788533521.webp","https://1.bp.blogspot.com/-Yg5us32h5YE/Xz2rogG6KHI/AAAAAAAAGbo/Ay4oOI1bQncrCt-RbE0kI6icoFUDG9oXQCLcBGAsYHQ/s640/image5.gif":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn13@2020_1/2020/08/24/14-08-02-206_679438de95c6e5d4.webp","https://1.bp.blogspot.com/-aMeUM4tOpxM/Xz2sHEc0WEI/AAAAAAAAGb0/xAeSG6rn0q0Y2SnIHxGZwY1_fRssvPfPwCLcBGAsYHQ/s0/image4.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn18@2020_6/2020/08/24/14-08-01-930_a309715d45d891d9.webp","https://1.bp.blogspot.com/-uEkUMJ-m8Gw/Xz2zWt4CGzI/AAAAAAAAGcI/CXOmIimUTDcD4wFBB4fN8bfgyhXVpV4yQCLcBGAsYHQ/w328-h96/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn93@2020_2/2020/08/24/14-07-58-028_c9d67371e06d509b.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/TQbqJYtduXE":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn42@2020_6/2020/08/24/14-07-57-585_c8d48371b012b000.webp"},"publishedOrCreatedDate":1598278048079},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Introducing the Model Card Toolkit for Easier Model Transparency Reporting","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Ojg1wuVgxWs/introducing-model-card-toolkit-for.html","description":"<span class=\"byline-author\">Posted by Huanming Fang and Hui Miao, Software Engineers, Google Research</span> \n<br />\n<br />Machine learning (ML) model transparency is important across a wide variety of domains that impact peoples’ lives, from healthcare to personal finance to employment. The information needed by downstream users will vary, as will the details that developers need in order to decide whether or not a model is appropriate for their use case. This desire for transparency led us to develop a new tool for model transparency, \n<a href=\"https://arxiv.org/pdf/1810.03993.pdf\">Model Cards</a>, which provide a structured framework for reporting on ML model provenance, usage, and ethics-informed evaluation and give a detailed overview of a model’s suggested uses and limitations that can benefit developers, regulators, and downstream users alike. \n<br />\n<br />Over the past year, we’ve \n<a href=\"https://modelcards.withgoogle.com/about\">launched Model Cards publicly </a>and worked to create Model Cards for open-source models released by teams across Google. For example, the \n<a href=\"https://mediapipe.dev/\">MediaPipe</a> team creates state-of-the-art computer vision models for a number of common tasks, and has included Model Cards for each of their open-source models in \n<a href=\"https://github.com/google/mediapipe/blob/master/mediapipe/models/README.md\">their GitHub repository</a>. Creating Model Cards like these takes substantial time and effort, often requiring a detailed evaluation and analysis of both data and model performance. In many cases, one needs to additionally evaluate how a model performs on different subsets of data, noting any areas where the model underperforms. Further, Model Card creators may want to report on the model’s intended uses and limitations, as well as any ethical considerations potential users might find useful, compiling and presenting the information in a format that’s accessible and understandable. \n<br />\n<br />To streamline the creation of Model Cards for all ML practitioners, we are sharing the \n<a href=\"http://github.com/tensorflow/model-card-toolkit\">Model Card Toolkit</a> (MCT), a collection of tools that support developers in compiling the information that goes into a Model Card and that aid in the creation of interfaces that will be useful for different audiences. To demonstrate how the MCT can be used in practice, we have also released a \n<a href=\"https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/documentation/examples/MLMD_Model_Card_Toolkit_Demo.ipynb\">Colab tutorial</a> that builds a Model Card for a simple classification model trained on the \n<a href=\"http://archive.ics.uci.edu/ml/datasets/Census+Income\">UCI Census Income dataset</a>.\n<br />\n<br />\n<strong>Introducing the MCT</strong>\n<br />To guide the Model Card creator to organize model information, we provide a \n<a href=\"https://json-schema.org/\">JSON schema</a>, which specifies the fields to include in the Model Card. Using the model provenance information stored with \n<a href=\"https://www.tensorflow.org/tfx/guide/mlmd\">ML Metadata</a> (MLMD), the MCT automatically populates the JSON with relevant information, such as class distributions in the data and model performance statistics. We also provide a ModelCard data API to represent an instance of the JSON schema and visualize it as a Model Card. The Model Card creator can choose which metrics and graphs to display in the final Model Card, including metrics that highlight areas where the model’s performance might deviate from its overall performance.\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n <a href=\"https://1.bp.blogspot.com/-Lb1GXOp7Ilk/XyC54ZsgAeI/AAAAAAAAGRo/Z7ZScvrPVZ4YXepnkU2eM4Vpelt2OmsPwCLcBGAsYHQ/s1600/image2%2B%25283%2529.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"352\" data-original-width=\"957\" height=\"234\" src=\"https://1.bp.blogspot.com/-Lb1GXOp7Ilk/XyC54ZsgAeI/AAAAAAAAGRo/Z7ZScvrPVZ4YXepnkU2eM4Vpelt2OmsPwCLcBGAsYHQ/s640/image2%2B%25283%2529.jpg\" width=\"640\" /></a>\n</div>Once the MCT has populated the Model Card with key metrics and graphs, the Model Card creator can supplement this with information regarding the model’s intended usage, limitations, trade-offs, and any other ethical considerations that would otherwise be unknown to people using the model. If a model underperforms for certain slices of data, the limitations section would be another place to acknowledge this, along with suggested mitigation strategies to help developers address these issues. This type of information is critical in helping developers decide whether or not a model is suitable for their use case, and helps Model Card creators provide context so that their models are used appropriately. Right now, we’re providing \n<a href=\"https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/template/html/default_template.html.jinja\">one UI template</a> to visualize the Model Card, but you can create different templates in HTML should you want to visualize the information in other formats.\n<br />\n<br />Currently, the MCT is available to anyone using \n<a href=\"https://www.tensorflow.org/tfx\">TensorFlow Extended</a> (TFX) in open source or on \n<a href=\"https://cloud.google.com/\">Google Cloud Platform</a>. Users who are not serving their ML models via TFX can still leverage the JSON schema and the methods to visualize via the HTML template. \n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Ks99sDT1Rv4/XyC5-OoGSWI/AAAAAAAAGRs/76W7kZPo0_wgsVKNE8veNk9DxwZ2VqCgACLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1600\" data-original-width=\"1067\" src=\"https://1.bp.blogspot.com/-Ks99sDT1Rv4/XyC5-OoGSWI/AAAAAAAAGRs/76W7kZPo0_wgsVKNE8veNk9DxwZ2VqCgACLcBGAsYHQ/s1600/image1.png\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">Here is an example of the completed Model Card from the Colab tutorial, which leverages the MCT and the provided UI template.</td>\n  </tr>\n </tbody>\n</table>\n<strong>Conclusion</strong>\n<br />Currently, the MCT includes a standard template for reporting on ML models broadly, but we’re continuing to create UI templates for more specific applications of ML. If you’d like to join the conversation about what fields are important and how best to leverage the MCT for different use cases, you can \n<a href=\"https://github.com/tensorflow/model-card-toolkit\">get started here</a> or with \n<a href=\"https://github.com/tensorflow/model-card-toolkit/blob/master/model_card_toolkit/documentation/examples/MLMD_Model_Card_Toolkit_Demo.ipynb\">the Colab tutorial</a>. Let us know how you’ve leveraged the MCT for your use case by emailing us at \n<a href=\"mailto:model-cards@google.com\">model-cards@google.com</a>. You can learn more about Google’s efforts to promote responsible AI in the TensorFlow ecosystem on our \n<a href=\"https://www.tensorflow.org/resources/responsible-ai\">TensorFlow Responsible AI page</a>.\n<br />\n<br />\n<strong>Acknowledgements</strong>\n<br />\n<em>Huanming Fang, Hui Miao, Karan Shukla, Dan Nanas, Catherina Xu, Christina Greer, Neoklis Polyzotis, Tulsee Doshi, Tiffany Deng, Margaret Mitchell, Timnit Gebru, Andrew Zaldivar, Mahima Pushkarna, Meena Natarajan, Roy Kim, Parker Barnes, Tom Murray, Susanna Ricco, Lucy Vasserman, and Simone Wu</em>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=Ojg1wuVgxWs:51XASHSOx3w:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/Ojg1wuVgxWs\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 29 Jul 2020 22:15:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-Lb1GXOp7Ilk/XyC54ZsgAeI/AAAAAAAAGRo/Z7ZScvrPVZ4YXepnkU2eM4Vpelt2OmsPwCLcBGAsYHQ/s640/image2%2B%25283%2529.jpg","linkMd5":"a43cbc46c792f96b6fd8e05e6a83fd1c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn89@2020_6/2020/08/24/14-07-28-322_290fee323372c895.webp","destWidth":640,"destHeight":235,"sourceBytes":29538,"destBytes":11064,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-Lb1GXOp7Ilk/XyC54ZsgAeI/AAAAAAAAGRo/Z7ZScvrPVZ4YXepnkU2eM4Vpelt2OmsPwCLcBGAsYHQ/s640/image2%2B%25283%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn89@2020_6/2020/08/24/14-07-28-322_290fee323372c895.webp","https://1.bp.blogspot.com/-Ks99sDT1Rv4/XyC5-OoGSWI/AAAAAAAAGRs/76W7kZPo0_wgsVKNE8veNk9DxwZ2VqCgACLcBGAsYHQ/s1600/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn81@2020_3/2020/08/24/14-07-57-050_e4b2185d748452ca.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/Ojg1wuVgxWs":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn78@2020_2/2020/08/24/14-07-55-362_281eb60395c68e9a.webp"},"publishedOrCreatedDate":1598278048091},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"SpineNet: A Novel Architecture for Object Detection Discovered with Neural Architecture Search","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","description":"<span class=\"byline-author\">Posted by Xianzhi Du, Software Engineer and Jaeyoun Kim, Technical Program Manager, Google Research</span>  <br /><img src=\"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s1600/image3.png\" style=\"display: none;\" /><br /><span style=\"font-weight: normal;\"><a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">Convolutional neural networks</a> created for image tasks typically encode an input image into a sequence of intermediate features that capture the semantics of an image (from local to global), where each subsequent layer has a lower spatial dimension. However, this scale-decreased model may not be able to deliver strong features for multi-scale visual recognition tasks where recognition and localization are both important (e.g., <a href=\"https://en.wikipedia.org/wiki/Object_detection\">object detection</a> and <a href=\"https://en.wikipedia.org/wiki/Image_segmentation\">segmentation</a>). Several works including <a href=\"https://arxiv.org/abs/1612.03144\">FPN</a> and <a href=\"https://arxiv.org/abs/1802.02611\">DeepLabv3+</a> propose multi-scale encoder-decoder architectures to address this issue, where a scale-decreased network (e.g., a <a href=\"https://arxiv.org/abs/1512.03385\">ResNet</a>) is taken as the encoder (commonly referred to as a backbone model). A decoder network is then applied to the backbone to recover the spatial information.</span><span style=\"font-weight: normal;\"><br /></span><br /><span style=\"font-weight: normal;\">While this architecture has yielded improved success for image recognition and localization tasks, it still relies on a scale-decreased backbone that throws away spatial information by down-sampling, which the decoder then must attempt to recover. What if one were to design an alternate backbone model that avoids this loss of spatial information, and is thus inherently well-suited for simultaneous image recognition and localization?</span><br /><br />In our recent <a href=\"http://cvpr2020.thecvf.com/\">CVPR 2020</a> paper “<a href=\"https://arxiv.org/abs/1912.05027\">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization</a>”, we propose a meta architecture called a <em>scale-permuted model</em> that enables two major improvements on backbone architecture design. First, the spatial resolution of intermediate feature maps should be able to increase or decrease anytime so that the model can retain spatial information as it grows deeper. Second, the connections between feature maps should be able to go across feature scales to facilitate multi-scale feature fusion. We then use <a href=\"https://arxiv.org/abs/1611.01578\">neural architecture search</a> (NAS) with a novel search space design that includes these features to discover an effective scale-permuted model. We demonstrate that this model is successful in multi-scale visual recognition tasks, outperforming networks with standard, scale-reduced backbones. To facilitate continued work in this space, we have open sourced the SpineNet code to the <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/detection\">Tensorflow TPU GitHub repository</a> in Tensorflow 1 and <a href=\"https://github.com/tensorflow/models/tree/master/official/vision/detection\">TensorFlow Model Garden GitHub repository</a> in Tensorflow 2.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-rOS64hhFqaY/XvpdnFjiZbI/AAAAAAAAGL0/I2oGEgdWWsoLzTxKetvdinIMGARJIRTBACLcBGAsYHQ/s1600/image2.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"479\" data-original-width=\"1364\" height=\"224\" src=\"https://1.bp.blogspot.com/-rOS64hhFqaY/XvpdnFjiZbI/AAAAAAAAGL0/I2oGEgdWWsoLzTxKetvdinIMGARJIRTBACLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A scale-decreased backbone is shown on the left and a scale-permuted backbone is shown on the right. Each rectangle represents a building block. Colors and shapes represent different spatial resolutions and feature dimensions. Arrows represent connections among building blocks.</td></tr></tbody></table><b>Design of SpineNet Architecture</b><br />In order to efficiently design the architecture for SpineNet, and avoid a time-intensive manual search of what is optimal, we leverage NAS to determine an optimal architecture. The backbone model is learned on the object detection task using the <a href=\"http://cocodataset.org/#home\">COCO dataset</a>, which requires simultaneous recognition and localization. During architecture search, we learn three things:<br /><ul><li><b>Scale permutations:</b> The orderings of network building blocks are important because each block can only be built from those that already exist (i.e., with a “lower ordering”). We define the search space of scale permutations by rearranging intermediate and output blocks, respectively.<br /></li><li><b>Cross-scale connections:</b> We define two input connections for each block in the search space. The parent blocks can be any block with a lower ordering or a block from the stem network.<br /></li><li><b>Block adjustments (optional):</b> We allow the block to adjust its scale level and type.</li></ul><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s1600/image3.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"412\" data-original-width=\"1600\" height=\"164\" src=\"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s640/image3.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The architecture search process from a scale-decreased backbone to a scale-permuted backbone.</td></tr></tbody></table>Taking the <a href=\"https://arxiv.org/abs/1512.03385\">ResNet-50</a> backbone as the seed for the NAS search, we first learn scale-permutation and cross-scale connections. All candidate models in the search space have roughly the same computation as ResNet-50 since we just permute the ordering of feature blocks to obtain candidate models. The learned scale-permuted model outperforms <a href=\"https://arxiv.org/abs/1612.03144\">ResNet-50-FPN</a> by +2.9% <a href=\"https://developers.google.com/machine-learning/glossary#average-precision\">average precision</a> (AP) in the object detection task. The efficiency can be further improved (-10% <a href=\"https://en.wikipedia.org/wiki/FLOPS\">FLOPs</a>) by adding search options to adjust scale and type (e.g., <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\">residual block</a> or <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\">bottleneck block</a>, used in the <a href=\"https://arxiv.org/pdf/1512.03385.pdf\">ResNet</a> model family) of each candidate feature block.  <br /><br />We name the learned 49-layer scale-permuted backbone architecture SpineNet-49. SpineNet-49 can be further scaled up to SpineNet-96/143/190 by repeating blocks two, three, or four times and increasing the feature dimension. An architecture comparison between ResNet-50-FPN and the final SpineNet-49 is shown below. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-oULUcK5QgtE/Xvpd-UHfDkI/AAAAAAAAGMA/UP9pK20VOlAUcU394ElECpqu2poxuDfOgCLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"811\" data-original-width=\"1295\" height=\"250\" src=\"https://1.bp.blogspot.com/-oULUcK5QgtE/Xvpd-UHfDkI/AAAAAAAAGMA/UP9pK20VOlAUcU394ElECpqu2poxuDfOgCLcBGAsYHQ/s400/image1.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The architecture comparison between a ResNet backbone (<b>left</b>) and the SpineNet backbone (<b>right</b>) derived from it using NAS.</td></tr></tbody></table><b>Performance</b> <br />We demonstrate the performance of SpineNet models through comparison with ResNet-FPN. Using similar building blocks, SpineNet models outperform their ResNet-FPN counterparts by ~3% AP at various scales while using 10-20% fewer FLOPs. In particular, our largest model, SpineNet-190, achieves 52.1% AP on <a href=\"http://cocodataset.org/#home\">COCO</a> for a single model without multi-scale testing during inference, significantly outperforming prior detectors. SpineNet also transfers to classification tasks, achieving 5% top-1 accuracy improvement on the challenging <a href=\"https://arxiv.org/abs/1707.06642\">iNaturalist</a> fine-grained dataset. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-xHxGJ9xuFWY/XvpgD6BE_fI/AAAAAAAAGMM/yM8aF2fv8U4kqvWOmAEBv26kWB_p9DgaQCLcBGAsYHQ/s1600/COCOPerformance.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"653\" data-original-width=\"1244\" height=\"334\" src=\"https://1.bp.blogspot.com/-xHxGJ9xuFWY/XvpgD6BE_fI/AAAAAAAAGMM/yM8aF2fv8U4kqvWOmAEBv26kWB_p9DgaQCLcBGAsYHQ/s640/COCOPerformance.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Performance comparisons of SpineNet models and ResNet-FPN models adopting the RetinaNet detection framework on COCO bounding box detection.</td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-TyWiSe5e8hM/XvpgQjxMlnI/AAAAAAAAGMQ/gg0yBASTjP0IfwXz-HJYkm3vnLTwAVYRQCLcBGAsYHQ/s1600/iNaturalistPerformance.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"654\" data-original-width=\"1235\" height=\"338\" src=\"https://1.bp.blogspot.com/-TyWiSe5e8hM/XvpgQjxMlnI/AAAAAAAAGMQ/gg0yBASTjP0IfwXz-HJYkm3vnLTwAVYRQCLcBGAsYHQ/s640/iNaturalistPerformance.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Performance comparisons of SpineNet models and ResNet models on ImageNet classification and iNaturalist fine-grained image classification.</td></tr></tbody></table><b>Conclusion</b> <br />In this work, we identify that the conventional scale-decreased model, even with a decoder network, is not effective for simultaneous recognition and localization. We propose the scale-permuted model, a new meta-architecture, to address the issue. To prove the effectiveness of scale-permuted models, we learn SpineNet by Neural Architecture Search in object detection and demonstrate it can be used directly in image classification. In the future, we hope the scale-permuted model will become the meta-architecture design of backbones across many visual tasks beyond detection and classification. <br /><br /><b><em>Acknowledgements</em></b> <br /><em>Special thanks to the co-authors of the paper: Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, and Xiaodan Song. We also would like to acknowledge Yeqing Li, Youlong Cheng, Jing Li, Jianwei Xie, Russell Power, Hongkun Yu, Chad Richards, Liang-Chieh Chen, Anelia Angelova, and the larger Google Brain Team for their help.</em> <div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=fqWE2c6TI9o:g6d-MnYu2zo:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fqWE2c6TI9o\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 30 Jun 2020 17:01:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s1600/image3.png","linkMd5":"6473b353c163127f73885d9cb5187135","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn49@2020_6/2020/08/24/14-07-28-535_dc187b158ce2fd24.webp","destWidth":1600,"destHeight":412,"sourceBytes":55785,"destBytes":18714,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s1600/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn49@2020_6/2020/08/24/14-07-28-535_dc187b158ce2fd24.webp","https://1.bp.blogspot.com/-rOS64hhFqaY/XvpdnFjiZbI/AAAAAAAAGL0/I2oGEgdWWsoLzTxKetvdinIMGARJIRTBACLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn10@2020_2/2020/08/24/14-07-55-450_cfaa7a0cdc525a07.webp","https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s640/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_1/2020/08/24/14-07-57-482_a7cb0ed5cd7c385d.webp","https://1.bp.blogspot.com/-oULUcK5QgtE/Xvpd-UHfDkI/AAAAAAAAGMA/UP9pK20VOlAUcU394ElECpqu2poxuDfOgCLcBGAsYHQ/s400/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn6@2020_4/2020/08/24/14-07-55-478_0bd29865a4eafd86.webp","https://1.bp.blogspot.com/-xHxGJ9xuFWY/XvpgD6BE_fI/AAAAAAAAGMM/yM8aF2fv8U4kqvWOmAEBv26kWB_p9DgaQCLcBGAsYHQ/s640/COCOPerformance.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn57@2020_4/2020/08/24/14-07-55-382_4042c46e2fa06811.webp","https://1.bp.blogspot.com/-TyWiSe5e8hM/XvpgQjxMlnI/AAAAAAAAGMQ/gg0yBASTjP0IfwXz-HJYkm3vnLTwAVYRQCLcBGAsYHQ/s640/iNaturalistPerformance.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn22@2020_2/2020/08/24/14-07-58-258_73c14236e5173644.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fqWE2c6TI9o":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn77@2020_5/2020/08/24/14-07-58-945_d08657f8cc5c378e.webp"},"publishedOrCreatedDate":1598278048112},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Understanding Deep Learning on Controlled Noisy Labels","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","description":"<span class=\"byline-author\">Posted by Lu Jiang, Senior Research Scientist and Weilong Yang, Senior Staff Software Engineer, Google Research</span> <p>The success of deep neural networks depends on access to high-quality labeled training data, as the presence of label errors (label noise) in training data <a href=\"https://openreview.net/forum?id=Sy8gdB9xx&amp;noteId=Sy8gdB9xx\">can greatly reduce the accuracy</a> of models on clean test data. Unfortunately, large training datasets almost always contain examples with inaccurate or incorrect labels. This leads to a paradox: on one hand, large datasets are necessary to train better deep networks, while on the other hand, deep networks tend to memorize training label noise, resulting in poorer model performance in practice. </p><p>The research community has recognized the importance of this problem, introducing works attempting to understand noisy training labels, e.g., by <a href=\"https://arxiv.org/abs/1706.05394\">Arpit et al.</a>, as well as mitigation strategies, such as <a href=\"https://arxiv.org/abs/1712.05055\">MentorNet</a> or <a href=\"https://arxiv.org/abs/1804.06872\">co-teaching</a>, to overcome them. <a href=\"https://en.wikipedia.org/wiki/Scientific_control\">Controlled experiments</a> play a crucial role in understanding noisy labels by studying the impact of the <em>noise level — </em>the percentage of examples with incorrect labels in the dataset — on model performance. However, current experiments have only been performed on <em>synthetic</em> labels, in which noisy examples have randomly assigned labels, not <em>real-world</em> label noise, which follows a different noise distribution. Such studies may then result in very different or even contradictory findings about noisy labels compared to practical experience. In addition, methods that perform well on synthetic noise may not work as well on real-world noisy labels. </p><p>In “<a href=\"https://arxiv.org/abs/1911.09781\">Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</a>”, published at <a href=\"https://icml.cc/Conferences/2020/\">ICML 2020</a>, we make three contributions towards better understanding deep learning on non-synthetic noisy labels. First, we establish the first controlled dataset and benchmark of realistic, real-world label noise sourced from the web (i.e., <em>web label noise</em>). Second, we propose a simple but highly effective method to overcome both synthetic and real-world noisy labels. Finally, we conduct the largest study to date that compares synthetic and web label noise across a wide variety of settings. </p><p><b>Properties of Synthetic vs Real-World (Web) Label Noise</b><br />There are a number of differences between the distribution of images with synthetic versus real-world (web) label noise. First, images with web label noise tend to be more consistent, visually or semantically, with the true positive images. Second, synthetic label noise is at class-level (all examples in the same class are equally noisy), whereas real-world label noise is at instance-level (certain images are more likely to be mislabelled than others, regardless of the associated class). For example, images of “Honda Civic” and “Honda Accord” are more often confused when the images are taken from the side than when the vehicles are imaged from the front. Third, images with real-world label noise come from an open class vocabulary that may not overlap with the class vocabulary of a specific dataset. For example, the web noisy images of “ladybug” include classes such as “fly” and other bugs that are not included in the class list of the dataset being used. The benchmark for controlled label noise will help provide better quantitative understanding of the differences between synthetic and real-world web label noise. </p><p><b>Benchmark for Controlled Label Noise from the Web</b><br />The benchmark in this work is built on two public datasets: <a href=\"https://github.com/yaoyao-liu/mini-imagenet-tools\">Mini-ImageNet</a>, for coarse-grained image classification, and <a href=\"https://ai.stanford.edu/~jkrause/cars/car_dataset.html\">Stanford Cars</a>, for fine-grained image classification. We gradually replace clean images in these datasets with incorrectly labeled images gathered from the web, following <a href=\"https://openreview.net/forum?id=Sy8gdB9xx&amp;noteId=Sy8gdB9xx\">standard methods</a> for the construction of <em>synthetic</em> datasets. </p><p>To do this, we collect images from the web using the class name (e.g., “ladybug”) as a keyword — an automatic approach to collect noisy labeled images from the web without manual annotations. Each retrieved image is then examined by 3-5 annotators using <a href=\"https://cloud.google.com/ai-platform/data-labeling/docs\">Google Cloud Labeling Service</a> who identify whether or not the web label given is correct, yielding nearly 213k annotated images. We use these web images with incorrect labels to replace a percentage of the clean training images in the original Mini-ImageNet and Stanford Cars datasets. We create 10 different datasets with progressively higher levels of label noise (from 0% clean data to 80% data with erroneous labels). The datasets have been open-sourced at our <a href=\"https://google.github.io/controlled-noisy-web-labels/index.html\">Controlled Noisy Web Labels website</a>.</p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s1176/image3.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"580\" data-original-width=\"1176\" src=\"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s640/image3.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Comparison of synthetic label noise and web label noise. From left to right, columns are true positive images in the Mini-ImageNet or Stanford Cars dataset, images with incorrect synthetic labels, and images with incorrect web labels (collected in the present work).</td></tr></tbody></table><p><b>MentorMix:  A Simple Robust Learning Method</b><br />Given a dataset of some unknown noise level, our goal is to train a robust model that can generalize well on the clean test data. We introduce a simple yet effective method for dealing with both synthetic and real-world noisy labels, called MentorMix, which we developed on the Controlled Noisy Web Labels dataset.</p><p>MentorMix is an iterative approach built on two existing techniques, <a href=\"https://arxiv.org/abs/1712.05055\">MentorNet</a> and <a href=\"https://github.com/hongyi-zhang/mixup\">Mixup</a>, that comprises four steps: weight, sample, mixup, and weight again. In the first step, a weight is computed for every example in a mini-batch by a MentorNet network, which can be tailored to the task at hand, and the weights are normalized into a distribution. In practice, the goal is to assign high weights for correctly labeled examples and zero weights for incorrectly labeled examples. In reality, we don't know which are correct and which are incorrect, so MentorNet weights are based on approximations. In the example here, MentorNet uses the StudentNet training loss to determine the weights in the distribution.&nbsp;</p><p>Next, for each example, we use <a href=\"https://en.wikipedia.org/wiki/Importance_sampling#:~:text=In%20statistics%2C%20importance%20sampling%20is,umbrella%20sampling%20in%20computational%20physics\">importance sampling</a> to select another example in the same mini-batch according to the distribution. As examples with higher weights tend to have the correct label, they are favored in the sampling procedure. We then use Mixup to mix the original and sampled examples so that the model interpolates between the two and avoids over-fitting the noisy training examples. Finally, we may compute another weight for the mixed example to scale the final loss. The impact of this second weighting strategy becomes more pronounced for high noise levels.&nbsp;</p><p>Conceptually, the above steps implement a new robust loss, which turns out to be more resilient to noisy training labels. More discussion on this topic can be found in <a href=\"https://arxiv.org/abs/1911.09781\">our paper</a>. The animation below illustrates the four key steps in MentorMix, where StudentNet is the model to be trained on noisy labeled data. We employ a very simple version of MentorNet, as described by <a href=\"https://arxiv.org/abs/1712.05055\">Jiang et al.</a>, to compute the weight for each example.</p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-FZa1l-V1xUk/Xz2Mus9zzUI/AAAAAAAAGas/pcMTYi0RCf41dzn9evv4D3mxgy-R9ih_ACLcBGAsYHQ/s830/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"324\" data-original-width=\"830\" src=\"https://1.bp.blogspot.com/-FZa1l-V1xUk/Xz2Mus9zzUI/AAAAAAAAGas/pcMTYi0RCf41dzn9evv4D3mxgy-R9ih_ACLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Illustration of four steps in the MentorMix method: weight, sample, mixup, and weight again.</td></tr></tbody></table><p><b>Evaluation</b><br />We evaluate MentorMix on five datasets including <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR 10/100</a> with synthetic label noise, and <a href=\"https://data.vision.ee.ethz.ch/cvl/webvision/dataset2017.html\">WebVision 1.0</a>, a large dataset of 2.2 million images with real-world noisy labels. MentorMix consistently yields improved results on the CIFAR 10/100 datasets and achieves the best published result on the WebVision dataset, improving the previous best method by a significant ~3% in terms of the top-1 classification accuracy on the <a href=\"http://www.image-net.org/challenges/LSVRC/\">ImageNet ILSVRC12</a> validation set.</p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XJMFuqkmy_w/Xz2M2UH1I0I/AAAAAAAAGaw/KkXuhBMdiSYSG36OECfxpF8GYUOmRFmYwCLcBGAsYHQ/s1312/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"742\" data-original-width=\"1312\" src=\"https://1.bp.blogspot.com/-XJMFuqkmy_w/Xz2M2UH1I0I/AAAAAAAAGaw/KkXuhBMdiSYSG36OECfxpF8GYUOmRFmYwCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Our model is trained only on the WebVision 2.2 million noisy training sample and is tested on the ImageNet ILSVRC12 validation set. The baseline models reported are (<a href=\"https://arxiv.org/abs/1711.07131\">Lee et al. 2018</a>), (<a href=\"https://arxiv.org/abs/1712.05055\">MentorNet 2018</a>), and (<a href=\"https://arxiv.org/abs/1808.01097\">Guo et al. 2018</a>).</td></tr></tbody></table><p><b>New Findings on Noisy Labels from the Web</b><br />This work represents the largest study to date into understanding deep neural networks trained on noisy labels. We propose three new findings on web label noise: </p><ul><li><em>Deep neural networks generalize much better on web label noise</em><p>    While it is well known that deep neural networks <a href=\"https://arxiv.org/abs/1611.03530\">generalize poorly on synthetic label noise</a>, our results suggest that deep neural networks generalize much better on web label noise. For example, the classification accuracy of a network trained on the Stanford Cars dataset using the 60% web label noise level is 0.66, much higher than that for the same network trained at the same 60% level of synthetic noise, which achieves only 0.09. This pattern is consistent across our two datasets using both fine-tuning and training from scratch. </p></li></ul><ul> <li><em>Deep neural networks may NOT learn patterns first when trained on web label noise</em><p>    Our common understanding is that <a href=\"https://arxiv.org/abs/1706.05394\">deep neural networks learn patterns first</a> — an interesting property in which DNNs are able to automatically capture generalizable “patterns” in the early training stage before memorizing noisy training labels. Because of this, <a href=\"https://en.wikipedia.org/wiki/Early_stopping\">early stopping</a> is commonly used for training on noisy data. However, our results suggest deep neural networks may not learn patterns first when trained using datasets with web label noise, at least for the fine-grained classification task, suggesting that early stopping may not be effective on real-world label noise from the web. </p></li></ul><ul> <li><em>ImageNet architectures generalize on noisy training labels when the networks are fine-tuned</em><p>    <a href=\"https://arxiv.org/abs/1805.08974\">Kornblith et al. (2019)</a> found that fine-tuning more advanced architectures trained on ImageNet tend to perform better on downstream tasks that have clean training labels. Our results extend this finding to noisy training data, showing that a better pre-trained architecture that exhibits better performance when pre-trained on ImageNet is likely to perform better even when it is fine-tuned on noisy training labels. </p></li></ul><p><b>Summary</b><br />Based on <a href=\"https://arxiv.org/abs/1911.09781\">our findings</a>, we have the following practical recommendations for training deep neural networks on noisy data. </p><ol><li>A simple way to deal with noisy labels is to fine-tune a model that is pre-trained on clean datasets, like ImageNet. The better the pre-trained model is, the better it may generalize on downstream noisy training tasks.  </li><li><a href=\"https://en.wikipedia.org/wiki/Early_stopping\">Early stopping</a> may not be effective on the real-world label noise from the web.  </li><li>Methods that perform well on synthetic noise may not work as well on the real-world noisy labels from the web.  </li><li>The label noise from the web appears to be less harmful, yet it is more difficult for our current robust learning methods to tackle. This encourages more future research to be carried out on controlled real-world label noise.  </li><li>The proposed MentorMix can better overcome both synthetic and real-world noisy labels. </li></ol><p>The code of MentorMix is available on <a href=\"https://github.com/google-research/google-research/tree/master/mentormix\">GitHub</a>, the datasets are on our <a href=\"https://google.github.io/controlled-noisy-web-labels/index.html\">Dataset website</a>. </p><p><b>Aknowledgements</b><br /><em>This research was conducted by Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. We'd like to thank Boqing Gong and Fei Sha for constructive feedback. Additional thanks go to the leadership Andrew Moore for supporting our data labeling effort, along with Tomas Izo and Rahul Sukthankar for help in releasing the dataset.</em></p><p></p><p></p><p></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=sokv3SkAWtI:WDfdmsJNnLE:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/sokv3SkAWtI\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 19 Aug 2020 20:41:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s72-c/image3.png","linkMd5":"f368ca441fd668170d34ab61787b6561","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn13@2020_4/2020/08/24/14-07-59-515_730a903ab0b285f6.webp","destWidth":72,"destHeight":72,"sourceBytes":11638,"destBytes":2780,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s640/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn61@2020_5/2020/08/24/14-07-55-448_51f7346668ccac4d.webp","https://1.bp.blogspot.com/-FZa1l-V1xUk/Xz2Mus9zzUI/AAAAAAAAGas/pcMTYi0RCf41dzn9evv4D3mxgy-R9ih_ACLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn87@2020_6/2020/08/24/14-08-02-712_e641fe3b7ebe5395.webp","https://1.bp.blogspot.com/-XJMFuqkmy_w/Xz2M2UH1I0I/AAAAAAAAGaw/KkXuhBMdiSYSG36OECfxpF8GYUOmRFmYwCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn97@2020_5/2020/08/24/14-07-57-031_da466c188dac83fd.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/sokv3SkAWtI":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn42@2020_3/2020/08/24/14-07-58-510_0d21a9d9b92b6723.webp"},"publishedOrCreatedDate":1598278048124},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Google at ICML 2020","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fMpogH0lgys/google-at-icml-2020.html","description":"<span class=\"byline-author\">Posted by Jaqui Herman and Cat Armato, Program Managers</span><br /><br />Machine learning is a key strategic focus at Google, with highly active groups pursuing research in virtually all aspects of the field, including deep learning and more classical algorithms, exploring theory as well as application. We utilize scalable tools and architectures to build machine learning systems that enable us to solve deep scientific and engineering challenges in areas of language, speech, translation, music, visual processing and more.<br /><br />As a leader in machine learning research, Google is proud to be a <a href=\"https://icml.cc/Conferences/2020/Sponsors\">Platinum Sponsor</a> of the thirty-seventh <a href=\"https://icml.cc/Conferences/2020\">International Conference on Machine Learning</a> (ICML 2020), a premier annual event taking place virtually this week. With over 100 accepted publications and Googlers participating in workshops, we look forward to our continued collaboration with the larger machine learning research community.<br /><br />If you're registered for ICML 2020, we hope you'll visit the Google virtual booth to learn more about the exciting work, creativity and fun that goes into solving some of the field's most interesting challenges. You can also learn more about the Google research being presented at ICML 2020 in the list below (Google affiliations <b>bolded</b>).<br /><br /><b><u>ICML Expo</u></b><br /><a href=\"https://icml.cc/ExpoConferences/2020/Expo#\">Google Dataset Search: Building an Open Ecosystem for Dataset Discovery</a> <br /><b><i>Natasha Noy</i></b><br /><br /><a href=\"https://icml.cc/ExpoConferences/2020/Expo#\">End-to-end Bayesian inference workflows in TensorFlow Probability</a> <br /><b><i>Colin Carroll </i></b><br /><br /><b><u>Publications</u></b><br /><a href=\"https://arxiv.org/pdf/2006.03227.pdf\">Population-Based Black-Box Optimization for Biological Sequence Design</a><br /><b><i>Christof Angermueller, David Belanger, Andreea Gane, Zelda Mariet, David Dohan, Kevin Murphy, Lucy Colwell, D Sculley</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2003.01086.pdf\">Predictive Coding for Locally-Linear Control</a><br /><i>Rui Shu, Tung Nguyen, <b>Yinlam Chow</b>, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano Ermon, Hung Bui</i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/5967-Paper.pdf\">FedBoost: A Communication-Efficient Algorithm for Federated Learning</a><br /><b><i>Jenny Hamer, Mehryar Mohri, Ananda Theertha Suresh</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2007.02817.pdf\">Faster Graph Embeddings via Coarsening</a><br /><b><i>Matthew Fahrbach, Gramoz Goranci, Richard Peng, Sushant Sachdeva, Chi Wang</i></b><br /><br /><a href=\"http://acsweb.ucsd.edu/~wfedus/pdf/replay.pdf\">Revisiting Fundamentals of Experience Replay</a><br /><i><b>William Fedus, Prajit Ramachandran, Rishabh Agarwal, </b>Yoshua Bengio<b>, Hugo Larochelle, </b>Mark Rowland, Will Dabney</i><br /><br /><a href=\"https://arxiv.org/pdf/1906.08720.pdf\">Boosting for Control of Dynamical Systems</a><br /><b><i>Naman Agarwal, Nataly Brukhim, Elad Hazan, Zhou Lu</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1901.00409.pdf\">Neural Clustering Processes</a><br /><i>Ari Pakman, <b>Yueqi Wang</b>, Catalin Mitelut, JinHyung Lee, Liam Paninski</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.07772.pdf\">The Tree Ensemble Layer: Differentiability Meets Conditional Computation</a><br /><i>Hussein Hazimeh, <b>Natalia Ponomareva</b>,<b> Petros Mol, Zhenyu Tan</b>, Rahul Mazumder</i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/5540-Paper.pdf\">Representations for Stable Off-Policy Reinforcement Learning</a><br /><b><i>Dibya Ghosh, Marc Bellemare</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.08909.pdf\">REALM: Retrieval-Augmented Language Model Pre-Training</a><br /><b><i>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1911.00038.pdf\">Context Aware Local Differential Privacy</a><br /><i>Jayadev Acharya, <b>Keith Bonawitz</b>,<b> Peter Kairouz</b>,<b> Daniel Ramage</b>,<b> Ziteng Sun</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.15502.pdf\">Scalable Deep Generative Modeling for Sparse Graphs</a><br /><i><b>Hanjun Dai, Azade Nazi, </b>Yujia Li<b>, Bo Dai, Dale Schuurmans</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2004.12289.pdf\">Deep k-NN for Noisy Labels</a><br /><b><i>Dara Bahri, Heinrich Jiang, Maya Gupta<sup>†</sup></i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.02959.pdf\">Revisiting Spatial Invariance with Low-Rank Local Connectivity</a><br /><b><i>Gamaleldin F. Elsayed, Prajit Ramachandran, Jonathon Shlens, Simon Kornblith</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1910.06378.pdf\">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning</a><br /><b><i>Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, Ananda Theertha Suresh</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.09067.pdf\">Incremental Sampling Without Replacement for Sequence Models</a><br /><b><i>Kensen Shi, David Bieber, Charles Sutton</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2006.16038.pdf\">SoftSort: A Continuous Relaxation for the argsort Operator</a><br /><i>Sebastian Prillo, <b>Julian Martin Eisenschlos</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2003.11080.pdf\">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation</a> <i>(see <a href=\"https://ai.googleblog.com/2020/04/xtreme-massively-multilingual-multi.html\">blog post</a>)</i><br /><i>Junjie Hu, Sebastian Ruder, <b>Aditya Siddhant</b>, Graham Neubig, <b>Orhan Firat</b>,<b> Melvin Johnson</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.05082.pdf\">Learning to Stop While Learning to Predict</a><br /><i>Xinshi Chen, <b>Hanjun Dai</b>, Yu Li, Xin Gao, Le Song</i><br /><br /><a href=\"https://arxiv.org/pdf/2003.02287.pdf\">Bandits with Adversarial Scaling</a><br /><i>Thodoris Lykouris, <b>Vahab Mirrokni</b>,<b> Renato Paes Leme</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.15353.pdf\">SimGANs: Simulator-Based Generative Adversarial Networks for ECG Synthesis to Improve Deep ECG Classification</a><br /><i>Tomer Golany, <b>Daniel Freedman</b>, Kira Radinsky</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.11860.pdf\">Stochastic Frank-Wolfe for Constrained Finite-Sum Minimization</a><br /><i>Geoffrey Negiar, Gideon Dresdner, Alicia Yi-Ting Tsai, Laurent El Ghaoui, Francesco Locatello, Robert M. Freund, <b>Fabian Pedregosa</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2002.08943.pdf\">Implicit differentiation of Lasso-type models for hyperparameter optimization</a><br /><i>Quentin Bertrand, Quentin Klopfenstein,<b> Mathieu Blondel</b>, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon</i><br /><br /><a href=\"https://arxiv.org/pdf/2006.10540.pdf\">Infinite attention: NNGP and NTK for deep attention networks</a><br /><b><i>Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, Roman Novak</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.08095.pdf\">Logarithmic Regret for Learning Linear Quadratic Regulators Efficiently</a><br /><i>Asaf Cassel, <b>Alon Cohen</b>, Tomer Koren</i><br /><br /><a href=\"https://arxiv.org/pdf/2004.13617.pdf\">Adversarial Learning Guarantees for Linear Hypotheses and Neural Networks</a><br /><i><b>Pranjal Awasthi, </b>Natalie Frank<b>, Mehryar Mohri</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.04655.pdf\">Random Hypervolume Scalarizations for Provable Multi-Objective Black Box Optimization</a><br /><b><i>Daniel Golovin, Qiuyi (Richard) Zhang</i></b><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/1158-Paper.pdf\">Generating Programmatic Referring Expressions via Program Synthesis</a><br /><i>Jiani Huang, Calvin Smith, Osbert Bastani, <b>Rishabh Singh</b>, Aws Albarghouthi, Mayur Naik</i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/3138-Paper.pdf\">Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach</a><br /><i><b>Martin Mladenov, Elliot Creager, Omer Ben-Porat, Kevin Swersky, </b>Richard Zemel<b>, Craig Boutilier</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2003.03384.pdf\">AutoML-Zero: Evolving Machine Learning Algorithms From Scratch</a> <i>(see <a href=\"https://ai.googleblog.com/2020/07/automl-zero-evolving-code-that-learns.html\">blog post</a>)</i><br /><b><i>Esteban Real, Chen Liang, David R. So, Quoc V. Le</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.02405.pdf\">How Good is the Bayes Posterior in Deep Neural Networks Really?</a><br /><b><i>Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, Sebastian Nowozin<sup>†</sup></i></b><br /><br /><a href=\"https://arxiv.org/pdf/1905.07553.pdf\">Which Tasks Should Be Learned Together in Multi-task Learning?</a><br /><i>Trevor Standley, Amir R. Zamir, <b>Dawn Chen</b>, Leonidas Guibas, Jitendra Malik, Silvio Savarese</i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/2093-Paper.pdf\">Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit Problems</a><br /><i>Tong Yu, <b>Branislav Kveton</b>, Zheng Wen, Ruiyi Zhang, Ole J. Mengshoel</i><br /><br /><a href=\"https://arxiv.org/pdf/1912.13053.pdf\">Disentangling Trainability and Generalization in Deep Neural Networks</a><br /><b><i>Lechao Xiao, Jeffrey Pennington, Samuel S. Schoenholz</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1908.08474.pdf\">The Many Shapley Values for Model Explanation</a><br /><b><i>Mukund Sundararajan, Amir Najmi</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1911.04462.pdf\">Neural Contextual Bandits with UCB-based Exploration</a><br /><i>Dongruo Zhou, <b>Lihong Li</b>, Quanquan Gu</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.08822.pdf\">Automatic Shortcut Removal for Self-Supervised Representation Learning</a><br /><b><i>Matthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.10342.pdf\">Federated Learning with Only Positive Labels</a><br /><b><i>Felix X. Yu, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.08013.pdf\">How Recurrent Networks Implement Contextual Processing in Sentiment Analysis</a><br /><b><i>Niru Maheswaranathan, David Sussillo</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.03555.pdf\">Supervised Learning: No Loss No Cry</a><br /><i>Richard Nock, <b>Aditya Krishna Menon</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2002.02693.pdf\">Ready Policy One: World Building Through Active Learning</a><br /><i>Philip Ball, Jack Parker-Holder, Aldo Pacchiano, <b>Krzysztof Choromanski</b>, Stephen Roberts</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.02886.pdf\">Weakly-Supervised Disentanglement Without Compromises</a><br /><i>Francesco Locatello, <b>Ben Poole</b>, Gunnar Raetsch, Bernhard Schölkopf, <b>Olivier Bachem</b>, <b>Michael Tschannen</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2002.08871.pdf\">Fast Differentiable Sorting and Ranking</a><br /><b><i>Mathieu Blondel, Olivier Teboul, Quentin Berthet, Josip Djolonga</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2006.02575.pdf\">Debiased Sinkhorn barycenters</a><br /><i>Hicham Janati, <b>Marco Cuturi</b>, Alexandre Gramfort</i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/2557-Paper.pdf\">Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure</a><br /><b><i>John Sipple</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1908.10396.pdf\">Accelerating Large-Scale Inference with Anisotropic Vector Quantization</a><br /><b><i>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng<sup>†</sup>, David Simcha, Felix Chern, Sanjiv Kumar</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1907.04543.pdf\">An Optimistic Perspective on Offline Reinforcement Learning</a> <i>(see <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">blog post</a>)</i><br /><b><i>Rishabh Agarwal, Dale Schuurmans, Mohammad Norouzi</i></b><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/5840-Paper.pdf\">The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization</a><br /><b><i>Ben Adlam, Jeffrey Pennington</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.10941.pdf\">Private Query Release Assisted by Public Data</a><br /><i>Raef Bassily, Albert Cheu,<b> Shay Moran</b>, Aleksandar Nikolov, Jonathan Ullman, Zhiwei Steven Wu</i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/5401-Paper.pdf\">Learning and Evaluating Contextual Embedding of Source Code</a><br /><b><i>Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi</i></b><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/6173-Paper.pdf\">Evaluating Machine Accuracy on ImageNet</a><br /><i>Vaishaal Shankar, <b>Rebecca Roelofs</b>, Horia Mania, Alex Fang, Benjamin Recht, Ludwig Schmidt</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.08926.pdf\">Imputer: Sequence Modelling via Imputation and Dynamic Programming</a><br /><b><i>William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1909.05352.pdf\">Domain Aggregation Networks for Multi-Source Domain Adaptation</a><br /><i>Junfeng Wen, Russell Greiner, <b>Dale Schuurmans</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2005.05960.pdf\">Planning to Explore via Self-Supervised World Models</a><br /><i>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, <b>Danijar Hafner</b>, Deepak Pathak</i><br /><br /><a href=\"https://arxiv.org/pdf/2005.06800.pdf\">Context-Aware Dynamics Model for Generalization in Model-Based Reinforcement Learning</a><br /><i>Kimin Lee, Younggyo Seo, Seunghyun Lee, <b>Honglak Lee</b>, Jinwoo Shin</i><br /><br /><a href=\"https://arxiv.org/pdf/2006.15820.pdf\">Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search</a><br /><i>Binghong Chen, Chengtao Li, <b>Hanjun Dai</b>, Le Song</i><br /><br /><a href=\"https://arxiv.org/pdf/1901.11141.pdf\">On the Consistency of Top-k Surrogate Losses</a><br /><b><i>Forest Yang</i></b>, <b>Sanmi Koyejo</b><br /><br /><a href=\"https://arxiv.org/pdf/2002.10421.pdf\">Dual Mirror Descent for Online Allocation Problems</a><br /><i>Haihao Lu, Santiago Balseiro, <b>Vahab Mirrokni</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2005.07186.pdf\">Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors</a><br /><b><i>Michael W. Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-An Ma<sup>†</sup>, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, Dustin Tran</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2003.00722.pdf\">Batch Stationary Distribution Estimation</a><br /><i>Junfeng Wen, <b>Bo Dai</b>,<b> Lihong Li</b>,<b> Dale Schuurmans</b></i><br /><br /><a href=\"https://arxiv.org/pdf/1910.13540\">Small-GAN: Speeding Up GAN Training Using Core-Sets</a><br /><i>Samarth Sinha, <b>Han Zhang</b>, Anirudh Goyal, Yoshua Bengio, <b>Hugo Larochelle</b>,<b> Augustus Odena</b></i><br /><br /><a href=\"https://arxiv.org/pdf/1909.11671.pdf\">Data Valuation Using Reinforcement Learning</a><br /><b><i>Jinsung Yoon, Sercan ‎Ö. Arik, Tomas Pfister</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2004.07804.pdf\">A Game Theoretic Perspective on Model-Based Reinforcement Learning</a><br /><b><i>Aravind Rajeswaran, Igor Mordatch, Vikash Kumar</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1912.05537\">Encoding Musical Style with Transformer Autoencoders</a><br /><b><i>Kristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, Jesse Engel</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1902.05622.pdf\">The Shapley Taylor Interaction Index</a><br /><b><i>Kedar Dhamdhere, Mukund Sundararajan, Ashish Agarwal</i></b><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/1458-Paper.pdf\">Multidimensional Shape Constraints</a><br /><b><i>Maya Gupta<sup>†</sup>, Erez Louidor, Olexander Mangylov<sup>†</sup>, Nobu Morioka, Taman Narayan, Sen Zhao</i></b><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/2341-Paper.pdf\">Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead</a><br /><b><i>Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1906.04349.pdf\">Learning to Score Behaviors for Guided Policy Optimization</a><br /><i>Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, <b>Krzysztof Choromanski</b></i>,<i> Michael I. Jordan</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.04599.pdf\">Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations</a><br /><i>Florian Tramèr, Jens Behrmann, <b>Nicholas Carlini</b>,<b> Nicolas Papernot</b>, Jörn-Henrik Jacobsen</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.08605.pdf\">Optimizing Black-Box Metrics with Adaptive Surrogates</a><br /><i>Qijia Jiang, Olaoluwa Adigun, <b>Harikrishna Narasimhan</b>,<b> Mahdi Milani Fard</b>,<b> Maya Gupta<sup>†</sup></b></i><br /><br /><a href=\"https://arxiv.org/pdf/1907.01991.pdf\">Circuit-Based Intrinsic Methods to Detect Overfitting</a><br /><i><b>Sat Chatterjee, </b>Alan Mishchenko</i><br /><br /><a href=\"https://arxiv.org/pdf/1906.03028.pdf\">Automatic Reparameterisation of Probabilistic Programs</a><br /><b><i>Maria I. Gorinova, Dave Moore, Matthew D. Hoffman</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2003.13563.pdf\">Stochastic Flows and Geometric Optimization on the Orthogonal Group</a><br /><i><b>Krzysztof Choromanski, </b>David Cheikhi<b>, Jared Davis</b>, Valerii Likhosherstov, Achille Nazaret, Achraf Bahamou,<b> Xingyou Song</b>, Mrugank Akarte, Jack Parker-Holder, Jacob Bergquist, Yuan Gao, Aldo Pacchiano,<b> Tamas Sarlos</b>, Adrian Weller,<b> Vikas Sindhwani</b></i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/5310-Paper.pdf\">Black-Box Variational Inference as a Parametric Approximation to Langevin Dynamics</a><br /><b><i>Matthew Hoffman, Yi-An Ma<sup>†</sup></i></b><br /><br /><a href=\"https://arxiv.org/pdf/1810.06583.pdf\">Concise Explanations of Neural Networks Using Adversarial Training</a><br /><i>Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Somesh Jha, <b>Xi Wu</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2005.09810.pdf\">p-Norm Flow Diffusion for Local Graph Clustering</a><br /><i>Shenghao Yang, <b>Di Wang</b>, Kimon Fountoulakis</i><br /><br /><a href=\"https://arxiv.org/pdf/1907.00030.pdf\">Empirical Study of the Benefits of Overparameterization in Learning Latent Variable Models</a><br /><i>Rares-Darius Buhai, <b>Yoni Halpern</b>, Yoon Kim, Andrej Risteski, David Sontag</i><br /><br /><a href=\"https://pdfs.semanticscholar.org/70c6/e0df3f6a3d51153d42977c95caebae15e1b4.pdf?_ga=2.217697759.814897122.1593578536-1522982907.1593578536\">Robust Pricing in Dynamic Mechanism Design</a><br /><i>Yuan Deng,<b> Sébastien Lahaie</b>,<b> Vahab Mirrokni</b></i><br /><br /><a href=\"https://arxiv.org/pdf/1908.09756.pdf\">Differentiable Product Quantization for Learning Compact Embedding Layers</a><br /><i><b>Ting Chen, Lala Li, </b>Yizhou Sun</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.07348.pdf\">Adaptive Region-Based Active Learning</a><br /><i><b>Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, </b>Ningshan Zhang</i><br /><br /><a href=\"https://arxiv.org/pdf/2003.12694.pdf\">Countering Language Drift with Seeded Iterated Learning</a><br /><i>Yuchen Lu, Soumye Singhal, Florian Strub, <b>Olivier Pietquin</b>, Aaron Courville</i><br /><br /><a href=\"https://arxiv.org/pdf/2003.02819\">Does Label Smoothing Mitigate Label Noise?</a><br /><b><i>Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, Sanjiv Kumar</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.04756.pdf\">Acceleration Through Spectral Density Estimation</a><br /><i><b>Fabian Pedregosa, </b>Damien Scieur</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.03305.pdf\">Momentum Improves Normalized SGD</a><br /><b><i>Ashok Cutkosky, Harsh Mehta</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.12399.pdf\">ConQUR: Mitigating Delusional Bias in Deep Q-Learning</a><br /><b><i>Andy Su, Jayden Ooi, Tyler Lu, Dale Schuurmans, Craig Boutilier</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.04726.pdf\">Online Learning with Imperfect Hints </a><br /><i>Aditya Bhaskara,<b> Ashok Cutkosky</b>,<b> Ravi Kumar</b>,<b> Manish Purohit</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2007.00811.pdf\">Go Wide, Then Narrow: Efficient Training of Deep Thin Networks</a><br /><i><b>Denny Zhou, </b>Mao Ye<b>, Chen Chen, Tianjian Meng, Mingxing Tan, Xiaodan Song, Quoc Le, </b>Qiang Liu<b>, Dale Schuurmans</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2002.00041.pdf\">On Implicit Regularization in β-VAEs</a><br /><b><i>Abhishek Kumar, Ben Poole</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.07839.pdf\">Is Local SGD Better than Minibatch SGD?</a><br /><i>Blake Woodworth, Kumar Kshitij Patel, Sebastian U. Stich, Zhen Dai, Brian Bullins, <b>H. Brendan McMahan</b>, Ohad Shamir, Nathan Sreb</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.05709.pdf\">A Simple Framework for Contrastive Learning of Visual Representations</a><br /><b><i>Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.04664.pdf\">Universal Average-Case Optimality of Polyak Momentum</a><br /><i>Damien Scieur, <b>Fabian Pedregosa</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.16239.pdf\">An Imitation Learning Approach for Cache Replacement</a><br /><b><i>Evan Zheran Liu, Milad Hashemi, Kevin Swersky, Parthasarathy Ranganathan, Junwhan Ahn</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1910.09588.pdf\">Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems</a><br /><i><b>Zhe Dong, Bryan A. Seybold, Kevin P. Murphy, </b>Hung H. Bui</i><br /><br /><a href=\"https://arxiv.org/pdf/1911.09781.pdf\">Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</a><br /><i><b>Lu Jiang, Di Huang, </b>Mason Liu,<b> Weilong Yang</b></i><br /><br /><a href=\"https://arxiv.org/pdf/1911.10088.pdf\">Optimizing Data Usage via Differentiable Rewards</a><br /><i>Xinyi Wang, <b>Hieu Pham</b>, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, Graham Neubig</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.11296.pdf\">Sparse Sinkhorn Attention</a><br /><b><i>Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan</i></b><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/4772-Paper.pdf\">One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control</a><br /><i>Wenlong Huang, <b>Igor Mordatch</b>, Deepak Pathak</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.10002.pdf\">On Thompson Sampling with Langevin Algorithms</a><br /><i>Eric Mazumdar, Aldo Pacchiano, <b>Yi-An Ma<sup>†</sup></b>, Peter L. Bartlett, Michael I. Jordan</i><br /><br /><a href=\"https://arxiv.org/pdf/2003.01794.pdf\">Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection</a><br /><i>Mao Ye, Chengyue Gong, Lizhen Nie,<b> Denny Zhou</b>, Adam Klivans, Qiang Liu</i><br /><br /><a href=\"https://arxiv.org/pdf/2005.06392.pdf\">On the Global Convergence Rates of Softmax Policy Gradient Methods</a><br /><i><b>Jincheng Mei, </b>Chenjun Xiao, Csaba Szepesvari<b>, Dale Schuurmans</b></i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/6530-Paper.pdf\">Concept Bottleneck Models</a><br /><i>Pang Wei Koh, <b>Thao Nguyen</b>, Yew Siang Tang, Stephen Mussmann, Emma Pierson, <b>Been Kim</b>, Percy Liang</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.03229.pdf\">Supervised Quantile Normalization for Low-Rank Matrix Approximation</a><br /><i><b>Marco Cuturi, Olivier Teboul, </b>Jonathan Niles-Weed<b>, Jean-Philippe Vert</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2002.03860.pdf\">Missing Data Imputation Using Optimal Transport</a><br /><i>Boris Muzellec, <b>Julie Josse</b>, Claire Boyer, <b>Marco Cuturi</b></i><br /><br /><a href=\"https://arxiv.org/pdf/2006.16981.pdf\">Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention Over Modules</a><br /><i>Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, <b>Michael Mozer</b>, Yoshua Bengio</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.08695.pdf\">Stochastic Optimization for Regularized Wasserstein Estimators</a><br /><i>Marin Ballu, <b>Quentin Berthet</b>, Francis Bach</i><br /><br /><a href=\"https://arxiv.org/pdf/2002.07028.pdf\">Low-Rank Bottleneck in Multi-head Attention Models</a><br /><b><i>Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Jakkam Reddi, Sanjiv Kumar</i></b><br /><br /><a href=\"https://arxiv.org/pdf/1911.11134.pdf\">Rigging the Lottery: Making All Tickets Winners</a><br /><i><b>Utku Evci, Trevor Gale, </b>Jacob Menick<b>, Pablo Samuel Castro, Erich Elsen</b></i><br /><br /><a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/4700-Paper.pdf\">Online Learning with Dependent Stochastic Feedback Graphs</a><br /><i><b>Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, </b>Ningshan Zhang</i><br /><br /><a href=\"https://arxiv.org/pdf/1906.05664.pdf\">Calibration, Entropy Rates, and Memory in Language Models</a><br /><i>Mark Braverman, <b>Xinyi Chen</b>, Sham Kakade, Karthik Narasimhan, Cyril Zhang, Yi Zhang </i><br /><br /><a href=\"https://arxiv.org/pdf/2004.04772.pdf\">Composable Sketches for Functions of Frequencies: Beyond the Worst Case</a><br /><i><b>Edith Cohen, Ofir Geri,</b> Rasmus Pagh</i><br /><br /><a href=\"https://arxiv.org/pdf/2003.07521.pdf\">Energy-Based Processes for Exchangeable Data</a><br /><b><i>Mengjiao Yang, Bo Dai, Hanjun Dai, Dale Schuurmans</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.09869.pdf\">Near-Optimal Regret Bounds for Stochastic Shortest Path</a><br /><i><b>Alon Cohen, Haim Kaplan, Yishay Mansour, </b>Aviv Rosenberg</i><br /><br /><a href=\"https://arxiv.org/pdf/1912.08777\">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</a> <i>(see <a href=\"https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html\">blog post</a>)</i><br /><i>Jingqing Zhang, <b>Yao Zhao</b>,<b> Mohammad Saleh</b>,<b> Peter J. Liu</b></i><br /><br /><a href=\"https://arxiv.org/pdf/1910.01845.pdf\">The Complexity of Finding Stationary Points with Stochastic Gradient Descent</a><br /><b><i>Yoel Drori, Ohad Shamir</i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.02655.pdf\">The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks</a><br /><b><i>Jakub Swiatkowski, Kevin Roth, Bas Veeling, Linh Tran, Josh Dillon, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, Sebastian Nowozin<sup>†</sup></i></b><br /><br /><a href=\"https://arxiv.org/pdf/2002.03967.pdf\">Regularized Optimal Transport is Ground Cost Adversarial</a><br /><i>François-Pierre Paty, <b>Marco Cuturi</b></i><br /><br /><b><u>Workshops</u></b><br /><a href=\"https://nehzux.github.io/NewInML2020ICML/\">New In ML</a><br />Invited Speaker:<i> <b>Nicolas Le Roux</b></i><br />Organizers: <i>Zhen Xu, Sparkle Russell-Puleri, Zhengying Liu, Sinead A Williamson, Matthias W Seeger, Wei-Wei Tu, <b>Samy Bengio</b>, Isabelle Guyon</i><br /><br /><a href=\"https://www.latinxinai.org/icml-2020\">LatinX in AI</a><br />Workshop Advisor:<i> <b>Pablo Samuel Castro</b></i><br /><br /><a href=\"https://wimlworkshop.org/icml2020/program/\">Women in Machine Learning Un-Workshop</a><br />Invited Speaker:<b><i> Doina Precup</i></b><br />Sponsor Expo Speaker:<i> <b>Jennifer Wei</b></i><br /><br /><a href=\"https://sites.google.com/corp/view/queer-in-ai/icml-2020\">Queer in AI</a><br />Invited Speaker:<i> <b>Shakir Mohamed</b></i><br /><br /><a href=\"https://sites.google.com/corp/view/cl-icml/\">Workshop on Continual Learning</a><br />Organizers:<i> Haytham Fayek, Arslan Chaudhry, David Lopez-Paz, Eugene Belilovsky, Jonathan Schwarz, <b>Marc Pickett</b>, Rahaf Aljundi, Sayna Ebrahimi, Razvan Pascanu, Puneet Dokania</i><br /><br /><a href=\"https://sites.google.com/corp/view/whi2020\">5th ICML Workshop on Human Interpretability in Machine Learning (WHI)</a><br />Organizers:<i> Kush Varshney, Adrian Weller, Alice Xiang, Amit Dhurandhar, <b>Been Kim</b>, Dennis Wei, Umang Bhatt</i><br /><br /><a href=\"https://icml-sas.gitlab.io/\">Self-supervision in Audio and Speech</a><br />Organizers:<i> Mirco Ravanelli, Dmitriy Serdyuk, R Devon Hjelm, <b>Bhuvana Ramabhadran</b>, Titouan Parcollet</i><br /><br /><a href=\"http://manikvarma.org/events/XC20/index.html\">Workshop on eXtreme Classification: Theory and Applications</a><br />Invited Speakers:<b><i> Sanjiv Kumar</i></b><br /><br /><a href=\"https://sites.google.com/corp/view/hsys2020/home?authuser=0\">Healthcare Systems, Population Health, and the Role of Health-tech</a><br />Organizers:<i> <b>Krzysztof Choromanski</b>, David Cheikhi, <b>Jared Davis</b>, Valerii Likhosherstov, Achille Nazaret, Achraf Bahamou, <b>Xingyou Song</b>, Mrugank Akarte, Jack Parker-Holder, Jacob Bergquist, Yuan Gao, Aldo Pacchiano, <b>Tamas Sarlos</b>, Adrian Weller, <b>Vikas Sindhwani</b></i><br /><br /><a href=\"https://wensun.github.io/rl_theory_workshop_2020_ICML.github.io/\">Theoretical Foundations of Reinforcement Learning</a><br />Program Committee:<b><i> Alon Cohen, Chris Dann</i></b><br /><br /><a href=\"https://sites.google.com/corp/view/udlworkshop2020/home\">Uncertainty and Robustness in Deep Learning Workshop (UDL)</a><br />Invited Speaker:<i> <b>Justin Gilmer</b></i><br /><br />Organizers: <i>Sharon Li, <b>Balaji Lakshminarayanan</b>, Dan Hendrycks, Thomas Dietterich, <b>Jasper Snoek</b></i><br />Program Committee:<i> <b>Jeremiah Liu</b>,<b> Jie Ren, Rodolphe Jenatton</b>,<b> Zack Nado</b>,<b> Alexander Alemi</b>,<b> Florian Wenzel</b>,<b> Mike Dusenberry</b>,<b> Raphael Lopes </b></i><br /><br /><a href=\"https://sites.google.com/view/optml-icml2020/home\">Beyond First Order Methods in Machine Learning Systems</a><br />Industry Panel:<i> <b>Jonathan Hseu</b></i><br /><br /><a href=\"https://oolworkshop.github.io/index.html\">Object-Oriented Learning: Perception, Representation, and Reasoning</a><br />Invited Speakers:<i> <b>Thomas Kipf</b>,<b> Igor Mordatch </b></i><br /><br /><a href=\"https://grlplus.github.io/overview/\">Graph Representation Learning and Beyond (GRL+)</a><br />Organizers:<i> Michael Bronstein, Andreea Deac, William L. Hamilton, Jessica B. Hamrick, <b>Milad Hashemi</b>, Stefanie Jegelka, Jure Leskovec, Renjie Liao, Federico Monti, Yizhou Sun, <b>Kevin Swersky</b>, Petar Veličković, Rex Ying, Marinka Žitnik</i><br />Speakers:<i> <b>Thomas Kipf</b></i><br />Program Committee:<i> <b>Bryan Perozzi</b>,<b> Kevin Swersky</b>,<b> Milad Hashemi</b>,<b> Thomas Kipf</b>,<b> Ting Cheng</b></i><br /><br /><a href=\"https://sites.google.com/corp/view/mli4sd-icml2020\">ML Interpretability for Scientific Discovery</a><br />Organizers:<i> <b>Subhashini Venugopalan</b>,<b> Michael Brenner</b>, Scott Linderman, <b>Been Kim</b></i><br />Program Committee:<i> <b>Akinori Mitani</b>,<b> Arunachalam Narayanaswamy</b>,<b> Avinash Varadarajan</b>,<b> Awa Dieng</b>,<b> Benjamin Sanchez-Lengeling</b>,<b> Bo Dai</b>,<b> Stephan Hoyer</b>,<b> Subham Sekhar Sahoo</b>,<b> Suhani Vora</b></i><br />Steering Committee:<i><b> John Platt, Mukund Sundararajan</b>, Jon Kleinberg</i><br /><br /><a href=\"https://negative-dependence-in-ml-workshop.lids.mit.edu/\">Negative Dependence and Submodularity for Machine Learning </a><br />Organizers:<i> <b>Zelda Mariet</b>, Mike Gartrell, Michal Derezinski</i><br /><br /><a href=\"https://sites.google.com/corp/view/automl2020/home\">7th ICML Workshop on Automated Machine Learning (AutoML) </a><br />Organizers:<i> <b>Charles Weill</b>, Katharina Eggensperger, Matthias Feurer, Frank Hutter, Marius Lindauer, Joaquin Vanschoren</i><br /><br /><a href=\"http://federated-learning.org/fl-icml-2020/\">Federated Learning for User Privacy and Data Confidentiality</a><br />Keynote:<i> <b>Brendan McMahan</b></i><br />Program Committee:<i> <b>Peter Kairouz</b>,<b> Jakub Konecný </b></i><br /><br /><a href=\"https://ml-retrospectives.github.io/icml2020/\">MLRetrospectives: A Venue for Self-Reflection in ML Research</a><br />Speaker:<i> <b>Margaret Mitchell</b></i><br /><br /><a href=\"https://sites.google.com/corp/view/ml4md2020/home\">Machine Learning for Media Discovery</a><br />Speaker: <b><i>Ed Chi</i></b><br /><br /><a href=\"https://invertibleworkshop.github.io/index.html\">INNF+: Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models</a><br />Organizers<i>: Chin-Wei Huang, David Krueger, <b>Rianne van den Berg</b>, George Papamakarios, Chris Cremer, Ricky Chen, Danilo Rezende</i><br /><br /><a href=\"https://lifelongml.github.io/\">4th Lifelong Learning Workshop</a><br />Program Committee:<i> <b>George Tucker</b>,<b> Marlos C. Machado</b></i><br /><br /><a href=\"https://www.icml-hill.com/\">2nd ICML Workshop on Human in the Loop Learning (HILL)</a><br /><i>Organizers: Shanghang Zhang, Xin Wang, Fisher Yu, <b>Jiajun Wu</b>, Trevor Darrell</i><br /><br /><a href=\"https://mlforglobalhealth.org/\">Machine Learning for Global Health</a><br />Organizers<i>: Danielle Belgrave, Danielle Belgrave, Stephanie Hyland, Charles Onu, Nicholas Furnham, <b>Ernest Mwebaze</b>, Neil Lawrence</i><br /><br /><b><u>Committee</u></b><br />Social Chair:<b> <i>Adam White</i></b><br /><br /><i><sup>†</sup>Work performed while at Google</i><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=fMpogH0lgys:07TOkws5i-E:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fMpogH0lgys\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 13 Jul 2020 18:00:00 +0000","feedId":1156,"bgimg":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","linkMd5":"9ee85f58ff53824040c5cd8f1110ce5a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","destWidth":62,"destHeight":24,"sourceBytes":997,"destBytes":310,"author":"Google AI","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fMpogH0lgys":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn50@2020_3/2020/08/24/14-07-56-588_77fd3c9ea5dbdf2d.webp"},"publishedOrCreatedDate":1598278048117},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"AutoML-Zero: Evolving Code that Learns","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html","description":"<span class=\"byline-author\">Posted by Esteban Real, Staff Software Engineer, and Chen Liang, Software Engineer, Google Research, Brain Team</span> \n<br />\n<img src=\"https://3.bp.blogspot.com/-LH7lfGbziL4/Xwjykc0yfzI/AAAAAAAAGO4/n7VQble-qawbXoHeHmMtpsVeazOZWHJRACLcBGAsYHQ/s320/image2.png\" style=\"display: none;\" />\n<br />\n<br />Machine learning (ML) has seen tremendous successes recently, which were made possible by ML algorithms like \n<a href=\"https://en.wikipedia.org/wiki/Deep_learning\">deep neural networks</a> that were discovered through years of expert research. The difficulty involved in this research fueled \n<em><a href=\"https://en.wikipedia.org/wiki/Automated_machine_learning\">AutoML</a></em>, a field that aims to automate the design of ML algorithms. So far, AutoML has focused on constructing solutions by combining sophisticated hand-designed components. A typical example is that of \n<em><a href=\"https://en.wikipedia.org/wiki/Neural_architecture_search\">neural architecture search</a></em>, a subfield in which one builds neural networks automatically out of complex layers (e.g., convolutions, batch-norm, and dropout), and the \n<a href=\"https://library.oapen.org/bitstream/handle/20.500.12657/23012/1007149.pdf?sequence=1\">topic</a> of \n<a href=\"https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html\">much</a> \n<a href=\"https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\">research</a>.\n<br />\n<br />An alternative approach to using these hand-designed components in AutoML is to search for entire algorithms from scratch. This is challenging because it requires the exploration of vast and sparse search spaces, yet it has great potential benefits — it is not biased toward what we already know and potentially allows for the discovery of new and better ML architectures. By analogy, if one were building a house from scratch, there is more potential for flexibility or improvement than if one was constructing a house using only prefabricated rooms. However, the discovery of such housing designs may be more difficult because there are many more possible ways to combine the bricks and mortar than there are of combining pre-made designs of entire rooms. As such, early research into algorithm learning from scratch focused on one aspect of the algorithm, to reduce the search space and compute required, such as the \n<a href=\"https://en.wikipedia.org/wiki/Learning_rule\">learning rule</a>, and has not been revisited much \n<a href=\"https://ieeexplore.ieee.org/abstract/document/349932\">since the early 90s</a>. Until now.\n<br />\n<br />Extending \n<a href=\"https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html\">our research</a> into evolutionary AutoML, our \n<a href=\"https://arxiv.org/abs/2003.03384\">recent paper</a>, to be published at ICML 2020, demonstrates that it is possible to successfully evolve ML algorithms from scratch. The approach we propose, called \n<em>AutoML-Zero</em>, starts from empty programs and, using only basic mathematical operations as building blocks, applies evolutionary methods to automatically find the code for complete ML algorithms. Given small image classification problems, our method rediscovered fundamental ML techniques, such as 2-layer neural networks with \n<a href=\"https://en.wikipedia.org/wiki/Backpropagation\">backpropagation</a>, \n<a href=\"https://en.wikipedia.org/wiki/Linear_regression\">linear regression</a> and the like, which have been invented by researchers throughout the years. This result demonstrates the plausibility of automatically discovering more novel ML algorithms to address harder problems in the future. \n<br />\n<br />\n<b>Evolving Learning Algorithms from Scratch</b>\n<br />We use a \n<a href=\"https://www.aaai.org/ojs/index.php/AAAI/article/view/4405\">variant</a> of classic \n<a href=\"https://en.wikipedia.org/wiki/Evolutionary_algorithm\">evolutionary methods</a> to search the space of algorithms. These methods have \n<a href=\"https://en.wikipedia.org/wiki/John_Koza\">proved useful</a> in discovering computer programs \n<a href=\"http://people.idsia.ch/~juergen/diploma.html\">since the 80s</a>. Their simplicity and scalability \n<a href=\"https://en.wikipedia.org/wiki/Genetic_programming\">makes them especially suitable for the discovery of learning algorithms</a>. \n<br />\n<br />In our case, a population is initialized with empty programs. It then evolves in repeating cycles to produce better and better learning algorithms. At each cycle, two (or more) random models compete and the most accurate model gets to be a \n<em>parent</em>. The parent clones itself to produce a child, which gets \n<em>mutated</em>. That is, the child’s code is modified in a random way, which could mean, for example, arbitrarily inserting, removing or modifying a line in the code. The mutated algorithm is then evaluated on image classification tasks.\n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HZRniCIf7Kg/XwZPynbRhoI/AAAAAAAAGN4/1NYiPnmq8cAZnrhAZncq67ySs8CFKqxGQCLcBGAsYHQ/s1600/image1.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"450\" data-original-width=\"800\" height=\"360\" src=\"https://1.bp.blogspot.com/-HZRniCIf7Kg/XwZPynbRhoI/AAAAAAAAGN4/1NYiPnmq8cAZnrhAZncq67ySs8CFKqxGQCLcBGAsYHQ/s640/image1.gif\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">A population is initialized with empty programs. Many generations later, we see a more evolved population and two of its algorithms compete. The most accurate wins to produce a child. After many such events, the final population contains highly accurate classifiers.</td>\n  </tr>\n </tbody>\n</table>\n<b>Exploring a Difficult Search Space</b>\n<br />Our AutoML-Zero setup, in contrast to much previous AutoML work, makes the search space very sparse — an accurate algorithm might be as rare as 1 in 10\n<sup>12 </sup>candidates. This is due to the granularity of the building blocks provided to the algorithm, which include only basic operations such as variable assignment, addition, and matrix multiplication. In such an environment, a random search will not find a solution in a reasonable amount of time, yet evolution can be tens of thousands of times faster, according to our measurements. We distributed the search on multiple machines that occasionally exchange algorithms (analogous to \n<a href=\"https://en.wikipedia.org/wiki/Gene_flow\">migration</a> in real life). We also constructed small proxy classification tasks on which to evaluate each child algorithm, and executed this evaluation with highly optimized \n<a href=\"https://github.com/google-research/google-research/tree/master/automl_zero#automl-zero\">code</a>.\n<br />\n<br />Despite the sparsity, the evolutionary search discovers more complex and effective techniques as time passes. Initially, the simplest algorithms appear, which represent linear models with hard-coded weights. In time, \n<a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">stochastic gradient descent</a> (SGD) is invented to learn the weights, in spite of the gradient itself not having been provided as a building block. Though flawed at first, SGD gets fixed relatively quickly, starting a series of improvements to the prediction and learning algorithm. Within our toy scenario, the process discovers several concepts known to have been useful to the research community. In the end, our approach manages to construct a model that outperforms hand-designs of comparable complexity.\n<br />\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-GK43VckX7Jk/XwZP6NfZmII/AAAAAAAAGN8/OFXfCgeXB4MTjcKqn8R91mrMozB4qMmyQCLcBGAsYHQ/s1600/image2.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"950\" data-original-width=\"1600\" height=\"378\" src=\"https://1.bp.blogspot.com/-GK43VckX7Jk/XwZP6NfZmII/AAAAAAAAGN8/OFXfCgeXB4MTjcKqn8R91mrMozB4qMmyQCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">Progress of an evolution experiment. As time passes, from left to right, we see the algorithms becoming more complex and more accurate.</td>\n  </tr>\n </tbody>\n</table>\n<b>The Evolved Algorithm</b>\n<br />The figure above includes the best evolved algorithm produced by our method. This final algorithm includes techniques such as \n<a href=\"https://www.semanticscholar.org/paper/A-survey-on-Image-Data-Augmentation-for-Deep-Shorten-Khoshgoftaar/3813b88a4ec3c63919df47e9694b577f4691f7e5\">noise injection as data augmentation</a>, \n<a href=\"https://en.wikipedia.org/wiki/Bilinear_form\">bilinear model</a>, \n<a href=\"https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_9_Normalized.html\">gradient normalization</a>, and \n<a href=\"https://www.semanticscholar.org/paper/Acceleration-of-stochastic-approximation-by-Polyak-Juditsky/6dc61f37ecc552413606d8c89ffbc46ec98ed887\">weight averaging</a>, and the improvement over the baseline also transfers to datasets that are not used during search. Our paper describes how the different lines in the evolved code implement each of these techniques, and verifies their value through ablation studies.\n<br />\n<br />Through more experiments, we show that it is possible to guide the evolutionary search by controlling \"the habitat\" — i.e., the tasks on which the evolutionary process evaluates the fitness of the algorithms. For example, when we reduce the amount of data, the \n<a href=\"https://icml.cc/Conferences/2010/papers/432.pdf\">noisy ReLU</a> emerges, which helps with regularization. Or when we reduce the number of training steps, we witness the emergence of learning rate decay, which enables faster convergence. Targeted discoveries such as these are important — while it may be interesting if an automatic tool-inventing machine comes up with a hammer or a needle, it is much more interesting if it comes up with a hammer when you show it some nails and a needle when you show it some thread. By analogy, in our work the noisy ReLU (\"hammer\") is discovered when in the presence of little data (\"nails\") and the learning rate decay when in the presence of few training steps.\n<br />\n<br />\n<b>Conclusion</b>\n<br />We consider this to be preliminary work. We have yet to evolve fundamentally new algorithms, but it is encouraging that the evolved algorithm can surpass simple neural networks that exist within the search space. Right now, the search process requires significant compute.\n<a href=\"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html#1\" name=\"top1\"><sup>*</sup></a> As the coming years scale up available hardware and as the search methods become more efficient, it is likely that the search space will become more inclusive and the results will improve. We are excited at the prospects of discovering novel machine learning algorithms as we further our understanding of AutoML-Zero.\n<br />\n<br />\n<b>Acknowledgements</b>\n<br />\n<em>We want to thank our co-authors, David R. So and Quoc V. Le, and the many who helped us through discussions during the project and paper writing, including Samy Bengio, Vincent Vanhoucke, Doug Eck, Charles Sutton, Yanping Huang, Jacques Pienaar, Jeff Dean, and particularly Gabriel Bender, Hanxiao Liu, Rishabh Singh, Chiyuan Zhang, and Hieu Pham. We also want to especially thank Tom Small for contributing the animations in this post.</em>\n<br />\n<br />\n<hr width=\"100%\">\n <p><span class=\"Apple-style-span\" style=\"font-size: small;\"><a name=\"1\"><b>* </b></a>The electricity consumption for the experiments (run in 2019) was matched with the purchase of renewable energy. <a href=\"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html#top1\">↩</a></span>\n  <div class=\"feedflare\"> \n   <a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=xGFebACpRTc:XAn43bvc0fs:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\" /></a> \n  </div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/xGFebACpRTc\" height=\"1\" width=\"1\" alt=\"\" /></p>\n</hr>","descriptionType":"html","publishedDate":"Thu, 09 Jul 2020 17:08:00 +0000","feedId":1156,"bgimg":"https://3.bp.blogspot.com/-LH7lfGbziL4/Xwjykc0yfzI/AAAAAAAAGO4/n7VQble-qawbXoHeHmMtpsVeazOZWHJRACLcBGAsYHQ/s320/image2.png","linkMd5":"9905ef79d2e76ec08be633a84535c116","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn13@2020_4/2020/08/24/14-07-28-426_3d28259a6bc27e2d.webp","destWidth":320,"destHeight":190,"sourceBytes":41224,"destBytes":11662,"author":"Google AI","articleImgCdnMap":{"https://3.bp.blogspot.com/-LH7lfGbziL4/Xwjykc0yfzI/AAAAAAAAGO4/n7VQble-qawbXoHeHmMtpsVeazOZWHJRACLcBGAsYHQ/s320/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn13@2020_4/2020/08/24/14-07-28-426_3d28259a6bc27e2d.webp","https://1.bp.blogspot.com/-HZRniCIf7Kg/XwZPynbRhoI/AAAAAAAAGN4/1NYiPnmq8cAZnrhAZncq67ySs8CFKqxGQCLcBGAsYHQ/s640/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn94@2020_6/2020/08/24/14-08-21-311_2c26612e20a3b695.webp","https://1.bp.blogspot.com/-GK43VckX7Jk/XwZP6NfZmII/AAAAAAAAGN8/OFXfCgeXB4MTjcKqn8R91mrMozB4qMmyQCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn33@2020_6/2020/08/24/14-07-55-330_11087690437917d9.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/xGFebACpRTc":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn90@2020_3/2020/08/24/14-07-56-930_aab9bc89a7bfa207.webp"},"publishedOrCreatedDate":1598278048099},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Duality — A New Approach to Reinforcement Learning","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fljSD_dTaKM/duality-new-approach-to-reinforcement.html","description":"<span class=\"byline-author\">Posted by Ofir Nachum and Bo Dai, Research Scientists, Google Research</span>  <br /><br /><a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">Reinforcement learning</a> (RL) is an approach commonly used to train agents to make sequences of decisions that will be successful in complex environments, including for example, settings such as robotic navigation, where an agent controls the joint motors of a robot to seek a path to a target location, or game-playing, where the goal might be to solve a game level in minimal time. Many modern successful RL algorithms, such as <a href=\"https://arxiv.org/abs/1312.5602\">Q-learning</a> and <a href=\"https://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html\">actor-critic</a>, propose to reduce the RL problem to a <a href=\"https://en.wikipedia.org/wiki/Constraint_satisfaction_problem\">constraint-satisfaction problem</a>, where a constraint exists for every possible “state” of the environment. For example, in vision-based robotic navigation, the “states” of the environment correspond to every possible camera input.<br /><br />Despite how ubiquitous the constraint-satisfaction approach is in practice, this strategy is often difficult to reconcile with the complexity of real-world settings. In practical scenarios (like the robotic navigation example) the space of states is large, sometimes even uncountable, so how can one learn to satisfy the tremendous number of constraints associated with arbitrary input? Implementations of Q-learning and actor-critic often ignore these <a href=\"https://arxiv.org/abs/1812.02648\">mathematical issues</a> or obscure them through a series of rough approximations, which results in a stark divide between the practical implementations of these algorithms and their mathematical foundations.<br /><br />In “<a href=\"https://arxiv.org/abs/2001.01866\">Reinforcement Learning via Fenchel-Rockafellar Duality</a>” we have developed a new approach to RL that enables algorithms that are both useful in practice and mathematically principled — that is to say, the proposed algorithms avoid the use of exceedingly rough approximations to translate their mathematical foundations to practical implementation. This approach is based on <em><a href=\"https://en.wikipedia.org/wiki/Duality_(optimization)\">convex duality</a></em>, which is a well-studied mathematical tool used to transform problems expressed in one form into equivalent problems in distinct forms that may be more computationally friendly. In our case, we develop specific ways to apply duality in RL to transform the traditional constraint-satisfaction mathematical form to an unconstrained, and thus more practical, mathematical problem.<br /><br /><b>A Duality-Based Solution</b><br />The duality-based approach begins by formulating the reinforcement learning problem as a mathematical objective along with a number of constraints, potentially infinite in number. Applying duality to this mathematical problem yields a different formulation of the same problem. Still, this dual formulation has the same format as the original problem — a single objective with a large number of constraints — although the specific objective and constraints are changed. <br /><br />The next step is key to the duality-based solution. We augment the dual objective with a <em><a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)\">convex regularizer</a></em>, a method often used<em> </em>in optimization as a way to smooth a problem and make it easier to solve. The choice of the regularizer is crucial to the final step, in which we apply duality once again to yield another formulation of an equivalent problem. In our case, we use the <em><a href=\"https://en.wikipedia.org/wiki/F-divergence\">f-divergence</a></em> regularizer, which results in a final formulation that is now <em>unconstrained</em>. Although there exist other choices of convex regularizers, regularization via the f-divergence is uniquely desirable for yielding an unconstrained problem that is especially amenable to optimization in practical and real-world settings which require <a href=\"https://ai.googleblog.com/2020/04/off-policy-estimation-for-infinite.html\">off-policy</a> or <a href=\"https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html\">offline</a> learning.<br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-7FX-wmLnlMg/XwSysgdoT2I/AAAAAAAAGNk/Cqzcf7vQvRcvsijDfFGkpckNuStsOntpwCLcBGAsYHQ/s1600/image2.png\" imageanchor=\"1\"><img border=\"0\" data-original-height=\"755\" data-original-width=\"1337\" height=\"360\" src=\"https://1.bp.blogspot.com/-7FX-wmLnlMg/XwSysgdoT2I/AAAAAAAAGNk/Cqzcf7vQvRcvsijDfFGkpckNuStsOntpwCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></div><br />Notably in many cases, the applications of duality and regularization prescribed by the duality-based approach <em>do not</em> change the optimality of the original solution. In other words, although the form of the problem has changed, the solution has not. This way, the result obtained with the new formulation is the same result as for the original problem, albeit achieved in a much easier way.<br /><br /><b>Experimental Evaluation</b><br />As a test of our new approach, we implemented duality-based training on a navigational agent. The agent starts at one corner of a multi-room map and must navigate to the opposite corner. We compare our algorithm to an actor-critic approach. Although both of these algorithms are based on the same underlying mathematical problem, actor-critic uses a number of approximations due to the infeasibility of satisfying the large number of constraints. In contrast, our algorithm is more amenable to practical implementation as can be seen by comparing the performance of the two algorithms. In the figure below, we plot the average reward achieved by the learned agent against the number of iterations of training for each algorithm. The duality-based implementation achieves significantly higher reward compared to actor-critic.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-pkmXSBgOiWg/XwSyz4b7peI/AAAAAAAAGNo/qLbfYN5gc7giVvtjwIoaSV2Me9qNFidfQCLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"617\" data-original-width=\"679\" height=\"362\" src=\"https://1.bp.blogspot.com/-pkmXSBgOiWg/XwSyz4b7peI/AAAAAAAAGNo/qLbfYN5gc7giVvtjwIoaSV2Me9qNFidfQCLcBGAsYHQ/s400/image1.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A plot of the average reward achieved by an agent using the duality-based approach (blue) compared to an agent using standard actor-critic (orange). In addition to being more mathematically principled, our approach also yields better practical results.</td></tr></tbody></table><b>Conclusion</b><br />In summary, we’ve shown that if one formulates the RL problem as a mathematical objective with constraints, then repeated applications of convex duality in conjunction with a cleverly chosen convex regularizer yield an equivalent problem <em>without constraints</em>. The resulting unconstrained problem is easy to implement in practice and applicable in a wide range of settings. We’ve already applied our general framework to agent behavior <a href=\"https://arxiv.org/abs/1912.02074\">policy optimization</a> as well as <a href=\"https://arxiv.org/abs/1906.04733\">policy evaluation</a>, and <a href=\"https://arxiv.org/abs/1912.05032\">imitation learning</a>. We’ve found that our algorithms are not only more mathematically principled than existing RL methods, but they also often yield better practical performance, showing the value of unifying mathematical principles with practical implementation.<div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=fljSD_dTaKM:4Ev56rToD6c:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fljSD_dTaKM\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 08 Jul 2020 17:04:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-7FX-wmLnlMg/XwSysgdoT2I/AAAAAAAAGNk/Cqzcf7vQvRcvsijDfFGkpckNuStsOntpwCLcBGAsYHQ/s640/image2.png","linkMd5":"9f56c8339959af8f2ea5e05651ed131f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn41@2020_2/2020/08/24/14-07-28-336_3dbd0a62e85390f4.webp","destWidth":640,"destHeight":361,"sourceBytes":70150,"destBytes":23178,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-7FX-wmLnlMg/XwSysgdoT2I/AAAAAAAAGNk/Cqzcf7vQvRcvsijDfFGkpckNuStsOntpwCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn41@2020_2/2020/08/24/14-07-28-336_3dbd0a62e85390f4.webp","https://1.bp.blogspot.com/-pkmXSBgOiWg/XwSyz4b7peI/AAAAAAAAGNo/qLbfYN5gc7giVvtjwIoaSV2Me9qNFidfQCLcBGAsYHQ/s400/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn70@2020_1/2020/08/24/14-07-56-947_87f0bcbf1a64e074.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fljSD_dTaKM":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn49@2020_6/2020/08/24/14-07-58-671_efccc4d6d6a7f086.webp"},"publishedOrCreatedDate":1598278048100},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Understanding View Selection for Contrastive Learning","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","description":"<span class=\"byline-author\">Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research</span> <p>Most people take for granted the ability to view an object from several different angles, but still recognize that it's the same object— a dog viewed from the front is still a dog when viewed from the side. While people do this naturally, computer scientists need to explicitly enable machines to <a href=\"https://arxiv.org/abs/1206.5538\">learn representations</a> that are <em>view-invariant</em>, with the goal of seeking robust data representations that retain information that is useful to downstream tasks.  </p><p>Of course, in order to learn these representations, manually annotated training data can be used. However, as in many cases such annotations aren’t available, which gives rise to a series of <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\">self-</a> and <a href=\"https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html\">crossmodal</a> supervised approaches that do not require manually annotated training data. Currently, a popular paradigm for training with such data is <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\">contrastive multiview learning</a>, where two views of the same scene (for example, <a href=\"https://arxiv.org/abs/1906.05849\">different image channels</a>, <a href=\"https://arxiv.org/abs/1805.01978\">augmentations of the same image</a>, and <a href=\"https://arxiv.org/abs/1906.05743\">video and text pairs</a>) will tend to converge in representation space while two views of different scenes diverge. Despite their success, one important question remains: “If one doesn’t have annotated labels readily available, how does one select the views to which the representations should be invariant?” In other words, how does one identify an object using information that resides in the pixels of the image itself, while still remaining accurate when that image is viewed from disparate viewpoints? </p><p>In “<a href=\"https://arxiv.org/abs/2005.10243\">What makes for good views for contrastive learning</a>”, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that one should reduce the <a href=\"https://en.wikipedia.org/wiki/Mutual_information\">mutual information</a> between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their mutual information. We also consider data augmentation as a way to reduce mutual information, and show that increasing data augmentation indeed leads to decreasing mutual information while improving downstream classification accuracy. To encourage further research in this space, we have open-sourced the <a href=\"https://github.com/HobbitLong/PyContrast/tree/master/pycontrast\">code and pre-trained models</a>. </p><p><b>The InfoMin Hypothesis</b><br>The goal of contrastive multiview learning is to learn a parametric encoder, whose output representations can be used to discriminate between pairs of views with the same identities, and pairs with different identities. The amount and type of information shared between the views determines how well the resulting model performs on downstream tasks. We hypothesize that the views that yield the best results should discard as much information in the input as possible except for the task relevant information (e.g., object labels), which we call the <em>InfoMin principle</em>.  </p><p>Consider the example below in which two patches of the same image represent the different “views”. The training objective is to identify that the two views belong to the same image. It is undesirable to have views that share too much information, for example, where low-level color and texture cues can be exploited as “shortcuts” (left), or to have views that share too little information to identify that they belong to the same image (right). Rather, views at the “sweet spot” share the information related to downstream tasks, such as patches corresponding to different parts of the panda for an object classification task (center).</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s1011/image1%2B%25283%2529.jpg\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"272\" data-original-width=\"1011\" src=\"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s640/image1%2B%25283%2529.jpg\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An illustration of three regimes of information captured during contrastive multiview learning. Views should not share too much information (<b>left</b>) or too little information (<b>right</b>), but should find an optimal mix (the “sweet spot”, <b>middle</b>) that maximizes the downstream performance.</td></tr></tbody></table><p><b>A Unified View on Contrastive Learning</b><br>We design several sets of experiments to verify the InfoMin hypothesis, motivated by the fact that there are simple ways to control the mutual information shared between views without any supervision. For example, we can sample different patches from the same images, and reduce their mutual information simply by increasing the distance between the patches. Here, we estimate the mutual information using <a href=\"https://arxiv.org/abs/1807.03748\">InfoNCE</a> (I<sub>NCE</sub>), which is a quantitative measure of the mutual information lower bound.<sub>.</sub> Indeed, we observe a reverse U-shape curve: as mutual information is reduced, the downstream task accuracy first increases and then begins to decrease.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/--iaigg1Gc6c/Xz_5T8KmZsI/AAAAAAAAGcg/sZpDxlRVegIr_oIaXtODuzqIq_YqktU-gCLcBGAsYHQ/s1852/image2.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"480\" data-original-width=\"1852\" src=\"https://1.bp.blogspot.com/--iaigg1Gc6c/Xz_5T8KmZsI/AAAAAAAAGcg/sZpDxlRVegIr_oIaXtODuzqIq_YqktU-gCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Downstream classification accuracy on <a href=\"https://cs.stanford.edu/~acoates/stl10/\">STL-10</a> (<b>left</b>) and <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a> (<b>right</b>) by applying linear classifiers on representations learned with contrastive learning. Same as the previous illustration, the views are sampled as different patches from the same images. Increasing the Euclidean distance between patches leads to decreasing mutual information. A reverse U-shape curve between classification accuracy and <em>I<sub>NCE</sub></em> (patch distance) is observed.</td></tr></tbody></table><p>Furthermore, we demonstrate that several state-of-the-art contrastive learning methods (<a href=\"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html\">InstDis</a>, <a href=\"https://arxiv.org/abs/1911.05722\">MoCo</a>, <a href=\"https://arxiv.org/abs/1906.05849\">CMC</a>, <a href=\"https://arxiv.org/abs/1912.01991\">PIRL</a>, <a href=\"https://arxiv.org/abs/2002.05709\">SimCLR</a> and <a href=\"https://arxiv.org/abs/1807.03748\">CPC</a>) can be unified through the perspective of view selection: despite the differences in architecture, objective and engineering details, all recent contrastive learning methods create two views that implicitly follow the InfoMin hypothesis, where the information shared between views are controlled by the strength of data augmentation. Motivated by this, we propose a new set of data augmentations, which outperforms the prior state of the art, <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\">SimCLR</a>, by nearly 4% on the ImageNet <a href=\"https://arxiv.org/abs/1807.03748\">linear readout benchmark</a>. We also found that transferring our unsupervised pre-trained models to <a href=\"https://en.wikipedia.org/wiki/Object_detection\">object detection</a> and <a href=\"https://en.wikipedia.org/wiki/Image_segmentation\">instance segmentation</a> consistently outperforms ImageNet pre-training. </p><p><b>Learning to Generate Views</b><br>In our work, we design unsupervised and semi-supervised methods that synthesize novel views following the InfoMin hypothesis. We learn <a href=\"https://arxiv.org/abs/1505.05770\">flow-based models</a> that transfer natural color spaces into novel color spaces, from which we split the channels to get views. For the unsupervised setup, the view generators are optimized to minimize the InfoNCE bound between views. As shown in the results below, we observe a similar reverse U-shape trend while minimizing the InfoNCE bound.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-iJBtQBrfDkY/Xz_5ZtI_xuI/AAAAAAAAGck/Zb1MArg4PkwSooXe8nTRB6yvWKZGCBA-gCLcBGAsYHQ/s1860/image3.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"478\" data-original-width=\"1860\" src=\"https://1.bp.blogspot.com/-iJBtQBrfDkY/Xz_5ZtI_xuI/AAAAAAAAGck/Zb1MArg4PkwSooXe8nTRB6yvWKZGCBA-gCLcBGAsYHQ/s640/image3.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">View generators learned by unsupervised (<b>left</b>) and semi-supervised (<b>right</b>) objectives.</td></tr></tbody></table><p>To reach the sweet spot without overly minimizing mutual information, we can use the semi-supervised setup and guide the view generator to retain label information. As expected, all learned views are now centered around the sweet spot, no matter what the input color space is. </p><p><b>Code and Pretrained Models</b><br>To accelerate research in self-supervised contastive learning, we are excited to share the code and pretrained models of InfoMin with the academic community. They can be found <a href=\"https://github.com/HobbitLong/PyContrast/tree/master/pycontrast\">here</a>. </p><p><b>Acknowledgements</b><br><em>The core team includes Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid and Phillip Isola. We would like to thank Kevin Murphy for insightful discussion; Lucas Beyer for feedback on the manuscript; and the Google Cloud team for computation support.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=9DRaIjqziZI:MEM0FwITIJM:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/9DRaIjqziZI\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Fri, 21 Aug 2020 17:00:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s72-c/image1%2B%25283%2529.jpg","linkMd5":"69cfc3e9279dc4a8d0c977973ff42767","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn1@2020_6/2020/08/24/14-07-59-323_738fcd761b7ee5be.webp","destWidth":72,"destHeight":72,"sourceBytes":2719,"destBytes":1554,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s640/image1%2B%25283%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn33@2020_6/2020/08/24/14-07-58-314_911bc8e30b1e4239.webp","https://1.bp.blogspot.com/--iaigg1Gc6c/Xz_5T8KmZsI/AAAAAAAAGcg/sZpDxlRVegIr_oIaXtODuzqIq_YqktU-gCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn37@2020_2/2020/08/24/14-07-56-424_17fc5b9998f0f610.webp","https://1.bp.blogspot.com/-iJBtQBrfDkY/Xz_5ZtI_xuI/AAAAAAAAGck/Zb1MArg4PkwSooXe8nTRB6yvWKZGCBA-gCLcBGAsYHQ/s640/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn54@2020_6/2020/08/24/14-07-57-574_71df659557f09586.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/9DRaIjqziZI":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn58@2020_3/2020/08/24/14-07-55-405_21495c6297033bbc.webp"},"publishedOrCreatedDate":1598278048078},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"RepNet: Counting Repetitions in Videos","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","description":"<span class=\"byline-author\">Posted by Debidatta Dwibedi, Research Scientist, Robotics at Google</span>  <br /><br />Repeating processes ranging from natural cycles, such as phases of the moon or heartbeats and breathing, to artificial repetitive processes, like those found on manufacturing lines or in traffic patterns, are commonplace in our daily lives. Beyond just their prevalence, repeating processes are of interest to researchers for the variety of insights one can tease out of them. It may be that there is an underlying cause behind something that happens multiple times, or there may be gradual changes in a scene that may be useful for understanding. Sometimes, repeating processes provide us with unambiguous “action units”, semantically meaningful segments that make up an action. For example, if a person is chopping an onion, the action unit is the manipulation action that is repeated to produce additional slices. These units may be indicative of more complex activity and may allow us to analyze more such actions automatically at a finer time-scale without having a person annotate these units. For the above reasons, perceptual systems that aim to observe and understand our world for an extended period of time will benefit from a system that understands general repetitions.<br /><br />In “<a href=\"http://openaccess.thecvf.com/content_CVPR_2020/papers/Dwibedi_Counting_Out_Time_Class_Agnostic_Video_Repetition_Counting_in_the_CVPR_2020_paper.pdf\">Counting Out Time: Class Agnostic Video Repetition Counting in the Wild</a>”, we present <a href=\"https://sites.google.com/corp/view/repnet\">RepNet</a>, a single model that can understand a broad range of repeating processes, ranging from people exercising or using tools, to animals running and birds flapping their wings, pendulums swinging, and a wide variety of others. In contrast to <a href=\"https://ai.googleblog.com/2019/08/video-understanding-using-temporal.html.\">our previous work, which</a> used cycle-consistency constraints across different videos of the same action to understand them at a fine-grained level, in this work we present a system that can recognize repetitions within a single video. Along with this model, we are releasing a <a href=\"https://storage.googleapis.com/deepmind-media/Datasets/countix.tar.gz\">dataset</a> to benchmark class-agnostic counting in videos and a <a href=\"https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\">Colab notebook</a> to run RepNet. <br /><br /><b>RepNet</b><br />RepNet is a model that takes as input a video that contains periodic action of a variety of classes (including those unseen during training) and returns the period of repetitions found therein. <a href=\"https://ieeexplore.ieee.org/document/868681/\">In the past</a> the problem of repetition counting has been addressed by directly comparing pixel intensities in frames, but real world videos have camera motion,  occlusion by objects in the field, drastic scale difference and changes in form,  which necessitates learning of features invariant to such noise. To accomplish this we train a machine learning model in an end-to-end manner to directly estimate the period of the repetitions. The model consists of three parts: a frame encoder, an intermediate representation, called a temporal self-similarity matrix (which we will describe below), and a period predictor. <br /><br />First, the frame encoder uses the <a href=\"https://arxiv.org/abs/1512.03385\">ResNet</a> architecture as a per-frame model to generate embeddings of each frame of the video The <a href=\"https://arxiv.org/abs/1512.03385\">ResNet</a> architecture was chosen since it has been successful for a number of <a href=\"https://arxiv.org/abs/1708.02002\">image</a> and <a href=\"https://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html\">video</a> tasks. Passing each frame of a video through a ResNet-based encoder yields a sequence of embeddings. <br /><br />At this point we calculate a <em>temporal self-similarity matrix</em> (TSM) by comparing each frame’s embedding with every other frame in the video, returning a matrix that is easy for subsequent modules to analyze for counting repetitions. This process surfaces self-similarities in the stream of video frames that enable period estimation, as demonstrated in the video below.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-ZkvWVcy1Y7M/XvDoqqadcxI/AAAAAAAAGIA/KAw3Hj5f-SMlvlVC0e2onYkUa4e5IPGlgCLcBGAsYHQ/s1600/image5.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"800\" data-original-width=\"800\" height=\"640\" src=\"https://1.bp.blogspot.com/-ZkvWVcy1Y7M/XvDoqqadcxI/AAAAAAAAGIA/KAw3Hj5f-SMlvlVC0e2onYkUa4e5IPGlgCLcBGAsYHQ/s640/image5.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Demonstration of how the TSM processes images of the Earth’s day-night cycle.</td></tr></tbody></table>For each frame, we then use <a href=\"https://arxiv.org/abs/1706.03762\">Transformers</a> to predict the period of repetition and the periodicity (i.e., whether or not a frame is part of the periodic process) directly from the sequence of similarities in the TSM. Once we have the period, we obtain the per-frame count by dividing the number of frames captured in a periodic segment by the period length. We sum this up to predict the number of repetitions in the video.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-hKcKT1CyOKA/XvDpaUO2ROI/AAAAAAAAGII/twznnajZ7ZI5Q-HUbCfU7R5i_P87rsLkgCLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"594\" data-original-width=\"729\" height=\"520\" src=\"https://1.bp.blogspot.com/-hKcKT1CyOKA/XvDpaUO2ROI/AAAAAAAAGII/twznnajZ7ZI5Q-HUbCfU7R5i_P87rsLkgCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Overview of the RepNet model.</td></tr></tbody></table><b>Temporal Self-Similarity Matrix</b><br />The example of the TSM from the day-night cycle, shown above, is derived from an idealized scenario with fixed period repetitions. TSMs from real videos often reveal fascinating structures in the world, as demonstrated in the three examples below. Jumping jacks are close to the ideal periodic action with a fixed period, while in contrast, the period of a bouncing ball declines as the ball loses energy through repeated bounces. The video of someone mixing concrete demonstrates repetitive action that is preceded and followed by a period without motion. These three behaviors are clearly distinguished in the learned TSM, which requires that the model pay attention to fine changes in the scene. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-KdKAir-7vdE/XvD9RJcjT2I/AAAAAAAAGJg/xQbWbeYVFcgLiGkR-gsmRgZ_2W98UALbgCLcBGAsYHQ/s1600/image2.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"461\" data-original-width=\"720\" height=\"408\" src=\"https://1.bp.blogspot.com/-KdKAir-7vdE/XvD9RJcjT2I/AAAAAAAAGJg/xQbWbeYVFcgLiGkR-gsmRgZ_2W98UALbgCLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Jumping Jacks (constant period; video from Kinetics), Bouncing ball (decreasing period; Kinetics), Mixing concrete (aperiodic segments present in video; <a href=\"https://projects.ics.forth.gr/cvrl/pd/\">PERTUBE</a> dataset).</td></tr></tbody></table>One advantage of using the TSM as an intermediate layer in RepNet is that the subsequent processing by the transformers is done in the self-similarity space and not in the feature space. This encourages generalization to unseen classes. For example, the TSMs produced by actions as different as jumping jacks or swimming are similar as long as the action was repeated at a similar pace. This allows us to train on some classes and yet expect generalization to unseen classes.<br /><br /><b>Data</b><br />One way to train the above model would be to collect a large dataset of videos that capture repetitive activities and label them with the repetition count. The challenge in this is two-fold. First, it requires one to examine a large number of videos to identify those with repeated actions. Following that, each video must be annotated with the number of times an action was repeated. While for certain tasks annotators can skip frames (for example, to classify a video as showing jumping jacks), they still need to see the entire video in order to count how many jumping jacks were performed.<br /><br />We overcome this challenge by introducing a process for synthetic data generation that produces videos with repetitions using videos that may not contain repeating actions at all. This is accomplished by randomly selecting a segment of the video to repeat an arbitrary number of times, bookended by the original video context. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-RzCBrCZykLE/XvDpz5viqyI/AAAAAAAAGIU/Ax43DDRjtWkeAZsepspakwUEGkshGsqQwCLcBGAsYHQ/s1600/image9.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"228\" data-original-width=\"772\" height=\"188\" src=\"https://1.bp.blogspot.com/-RzCBrCZykLE/XvDpz5viqyI/AAAAAAAAGIU/Ax43DDRjtWkeAZsepspakwUEGkshGsqQwCLcBGAsYHQ/s640/image9.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Our synthetic data generation pipeline that produces videos with repetitions from any video.</td></tr></tbody></table>While this process generates a video that resembles a natural-looking video with repeating processes, it is still too simple for deep learning methods, which can learn to cheat by looking for artifacts, instead of learning to recognize repetitions. To address this, we perform extreme data augmentation, which we call <em>camera motion augmentation</em>. In this method, we modify the video to simulate a camera that smoothly moves around using 2D <a href=\"https://en.wikipedia.org/wiki/Affine_transformation\">affine motion</a> as the video progresses. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-1eYeyyE35HQ/XvDp-wfvAHI/AAAAAAAAGIc/iMpJvOl6gSgFbuzlRWzhahKC7DH66fktQCLcBGAsYHQ/s1600/image7.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"200\" data-original-width=\"480\" height=\"266\" src=\"https://1.bp.blogspot.com/-1eYeyyE35HQ/XvDp-wfvAHI/AAAAAAAAGIc/iMpJvOl6gSgFbuzlRWzhahKC7DH66fktQCLcBGAsYHQ/s640/image7.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left:</b> An example of a synthetic repeating video generated from a random video. <b>Right:</b> An example of a video with camera motion augmentation, which is tougher for the model, but results in better generalization to real repeating videos (both from Kinetics).</td></tr></tbody></table><b>Evaluation</b><br />Even though we can train a model on synthetic repeating videos, the resulting models must be able to generalize to real video of repeating processes. In order to evaluate the performance of the trained models on real videos, we collect a dataset of ~9000 videos from the <a href=\"https://deepmind.com/research/open-source/kinetics\">Kinetics dataset</a>. These videos span many action classes and capture diverse scenes, arising from the diversity of data seen on Youtube. We annotate these videos with the count of the action being repeated in the video. To encourage further research in this field, we are releasing the count annotations for this dataset, which we call <a href=\"https://storage.googleapis.com/deepmind-media/Datasets/countix.tar.gz\">Countix</a>. <br /><br /><b>Applications</b><br />A class-agnostic counting model has many useful applications. RepNet serves as a single model that can count repetitions from many different domains:<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-W0B4-YeXuno/XvDqU3z1NdI/AAAAAAAAGIs/GB_hBRziJGMpzewokDRt7C8Z5tKgU_4wACLcBGAsYHQ/s1600/image8.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"158\" data-original-width=\"600\" height=\"168\" src=\"https://1.bp.blogspot.com/-W0B4-YeXuno/XvDqU3z1NdI/AAAAAAAAGIs/GB_hBRziJGMpzewokDRt7C8Z5tKgU_4wACLcBGAsYHQ/s640/image8.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">RepNet can count repeated activities from a range of domains, such as slicing onions (<b>left</b>; video from Kinetics dataset), Earth’s diurnal cycle (<b>middle</b>; <a href=\"https://himawari8.nict.go.jp/\">Himawari satellite</a> data), or even a cheetah in motion (<b>right</b>; video from <a href=\"https://i.imgur.com/b3xs5lX.gifv/)\">imgur.com</a>).</td></tr></tbody></table>RepNet could be used to estimate heartbeat rates from echocardiogram videos even though it has not seen such videos in training:<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-BWytLL8Yzr0/XvD7ekUv3pI/AAAAAAAAGJU/y64_V7YpIOI7fgo4ITxXuoJa9HplqTtyACLcBGAsYHQ/s1600/image7.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"299\" data-original-width=\"600\" height=\"318\" src=\"https://1.bp.blogspot.com/-BWytLL8Yzr0/XvD7ekUv3pI/AAAAAAAAGJU/y64_V7YpIOI7fgo4ITxXuoJa9HplqTtyACLcBGAsYHQ/s640/image7.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Predicted heart rates: 45 bpm (<b>left</b>) and 75 bpm (<b>right</b>). True heart rates 46-50 bpm and 78-79 bpm, respectively. RepNet’s prediction of the heart rate across different devices is encouragingly close to the rate measured by the device. (Source for <a href=\"https://www.youtube.com/watch?v=iQ4GAaLB4zc\">left</a> and <a href=\"https://www.youtube.com/watch?v=h6aJSuUTVb0\">right</a>)</td></tr></tbody></table>RepNet can also be used to monitor repeating activities for any changes in speed. Below we show how the Such changes in speed can also be used in other settings for quality or process control.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-yp_f36oOQks/XvDqkBpw5ZI/AAAAAAAAGI4/agnH2a7FOS0vvEPx8bTFTl7eKqSbC9oTQCLcBGAsYHQ/s1600/image6.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"400\" data-original-width=\"400\" height=\"400\" src=\"https://1.bp.blogspot.com/-yp_f36oOQks/XvDqkBpw5ZI/AAAAAAAAGI4/agnH2a7FOS0vvEPx8bTFTl7eKqSbC9oTQCLcBGAsYHQ/s400/image6.gif\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">In this video, we see RepNet counting accelerating cellular oscillations observed under a laser microscope even though it has never seen such a video during training, (from <a href=\"https://www.nature.com/articles/s42003-018-0273-6\">Nature</a> article).</td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-_eXHKGzwHT4/XvDqsC_CNDI/AAAAAAAAGJA/4cXDqT6oHhkarqFUAcTWOT9Ge36koal_wCLcBGAsYHQ/s1600/image3.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"240\" data-original-width=\"464\" height=\"330\" src=\"https://1.bp.blogspot.com/-_eXHKGzwHT4/XvDqsC_CNDI/AAAAAAAAGJA/4cXDqT6oHhkarqFUAcTWOT9Ge36koal_wCLcBGAsYHQ/s640/image3.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left:</b> Person performing a “mountain climber” exercise. <b>Right:</b> The 1D projection of the RepNet embeddings using <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">principal component analysis</a>, capturing the moment that the person changes their speed during the exercise. (Video from Kinetics)</td></tr></tbody></table><b>Release</b><br />We are releasing Countix <a href=\"https://storage.googleapis.com/deepmind-media/Datasets/countix.tar.gz\">annotations</a> for the community to work on the problem of repetition counting. We are also releasing a <a href=\"https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\">Colab notebook</a>&nbsp;for running RepNet. Using this you can run RepNet on your videos or even using your webcam to detect periodic activities in video and count repetitions automatically in videos.<br /><br /><b>Acknowledgements</b><br /><em>This is joint work with Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Special thanks to Tom Small for designing the visual explanation of TSM.&nbsp;</em><em>The authors thank Anelia Angelova, Relja Arandjelović, Sourish Chaudhuri, Aishwarya Gomatam, Meghana Thotakuri, and Vincent Vanhoucke for their help with this project.</em><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=CPC0hNgMKL4:n1UCAILY6Z0:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/CPC0hNgMKL4\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 22 Jun 2020 20:13:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-ZkvWVcy1Y7M/XvDoqqadcxI/AAAAAAAAGIA/KAw3Hj5f-SMlvlVC0e2onYkUa4e5IPGlgCLcBGAsYHQ/s640/image5.gif","linkMd5":"454039e4a25f3a17b8deb713cd0faa82","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn45@2020_6/2020/08/24/14-07-53-641_680fbaa064e9d945.webp","destWidth":640,"destHeight":640,"sourceBytes":10196588,"destBytes":2401950,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-ZkvWVcy1Y7M/XvDoqqadcxI/AAAAAAAAGIA/KAw3Hj5f-SMlvlVC0e2onYkUa4e5IPGlgCLcBGAsYHQ/s640/image5.gif":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn45@2020_6/2020/08/24/14-07-53-641_680fbaa064e9d945.webp","https://1.bp.blogspot.com/-hKcKT1CyOKA/XvDpaUO2ROI/AAAAAAAAGII/twznnajZ7ZI5Q-HUbCfU7R5i_P87rsLkgCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn37@2020_1/2020/08/24/14-07-55-427_3050eb110825455a.webp","https://1.bp.blogspot.com/-KdKAir-7vdE/XvD9RJcjT2I/AAAAAAAAGJg/xQbWbeYVFcgLiGkR-gsmRgZ_2W98UALbgCLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn66@2020_3/2020/08/24/14-08-10-532_69460a92dce6caab.webp","https://1.bp.blogspot.com/-RzCBrCZykLE/XvDpz5viqyI/AAAAAAAAGIU/Ax43DDRjtWkeAZsepspakwUEGkshGsqQwCLcBGAsYHQ/s640/image9.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn73@2020_1/2020/08/24/14-07-58-011_f6760bfb7fef6f5d.webp","https://1.bp.blogspot.com/-1eYeyyE35HQ/XvDp-wfvAHI/AAAAAAAAGIc/iMpJvOl6gSgFbuzlRWzhahKC7DH66fktQCLcBGAsYHQ/s640/image7.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn61@2020_6/2020/08/24/14-07-57-832_ed4283f0d36e0f89.webp","https://1.bp.blogspot.com/-W0B4-YeXuno/XvDqU3z1NdI/AAAAAAAAGIs/GB_hBRziJGMpzewokDRt7C8Z5tKgU_4wACLcBGAsYHQ/s640/image8.gif":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn17@2020_1/2020/08/24/14-08-03-342_4762c3c3775a1859.webp","https://1.bp.blogspot.com/-BWytLL8Yzr0/XvD7ekUv3pI/AAAAAAAAGJU/y64_V7YpIOI7fgo4ITxXuoJa9HplqTtyACLcBGAsYHQ/s640/image7.gif":"https://cdn.jsdelivr.net/gh/myreaderx/cdn90@2020_2/2020/08/24/14-07-59-567_cb41120403bba637.webp","https://1.bp.blogspot.com/-yp_f36oOQks/XvDqkBpw5ZI/AAAAAAAAGI4/agnH2a7FOS0vvEPx8bTFTl7eKqSbC9oTQCLcBGAsYHQ/s400/image6.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn93@2020_6/2020/08/24/14-08-00-941_294a09b1d611d77b.webp","https://1.bp.blogspot.com/-_eXHKGzwHT4/XvDqsC_CNDI/AAAAAAAAGJA/4cXDqT6oHhkarqFUAcTWOT9Ge36koal_wCLcBGAsYHQ/s640/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn46@2020_1/2020/08/24/14-07-56-592_690fc631b93eb70a.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/CPC0hNgMKL4":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn21@2020_1/2020/08/24/14-07-55-354_92de5f63c67f97d2.webp"},"publishedOrCreatedDate":1598278048110},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Language-Agnostic BERT Sentence Embedding","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","description":"<span class=\"byline-author\">Posted by Yinfei Yang and Fangxiaoyu Feng, Software Engineers, Google Research</span> <p>A multilingual embedding model is a powerful tool that encodes text from different languages into a shared embedding space, enabling it to be applied to a range of downstream tasks, like <a href=\"https://en.wikipedia.org/wiki/Document_classification\">text classification</a>, <a href=\"https://en.wikipedia.org/wiki/Document_clustering\">clustering</a>, and others, while also leveraging semantic information for language understanding. Existing approaches for generating such embeddings, like <a href=\"https://research.fb.com/downloads/laser-language-agnostic-sentence-representations/\">LASER</a> or <a href=\"https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html\">m~USE</a>,  rely on parallel data, mapping a sentence from one language directly to another language in order to encourage consistency between the sentence embeddings. While these existing multilingual approaches yield good overall performance across a number of languages, they often underperform on high-resource languages compared to dedicated bilingual models, which can leverage approaches like <a href=\"https://www.aclweb.org/anthology/W18-6317.pdf\">translation ranking tasks</a> with translation pairs as training data to obtain more closely aligned representations. Further, due to limited model capacity and the often poor quality of training data for low-resource languages, it can be difficult to extend multilingual models to support a larger number of languages while maintaining good performance.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-6upFrBNGwo4/Xzwk7D60GaI/AAAAAAAAGZs/ZDgmdCvBYfQr2cc5CkWW0AfIzD11x1q4wCLcBGAsYHQ/s344/image2%2B%25284%2529.jpg\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"209\" data-original-width=\"344\" src=\"https://1.bp.blogspot.com/-6upFrBNGwo4/Xzwk7D60GaI/AAAAAAAAGZs/ZDgmdCvBYfQr2cc5CkWW0AfIzD11x1q4wCLcBGAsYHQ/s0/image2%2B%25284%2529.jpg\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Illustration of a multilingual embedding space.</td></tr></tbody></table><p>Recent efforts to improve language models include the development of <a href=\"https://www.aclweb.org/anthology/N19-1423/\">masked language model</a> (MLM) pre-training, such as that used by <a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">BERT</a>, <a href=\"https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html\">ALBERT</a> and <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa.</a> This approach has led to exceptional gains across a wide range of languages and a variety of natural language processing tasks since it only requires monolingual text. In addition, MLM pre-training has been extended to the multilingual setting by modifying MLM training to include concatenated translation pairs, known as <a href=\"https://arxiv.org/abs/1901.07291\">translation language modeling</a> (TLM), or by simply introducing pre-training data from multiple languages. However, while the internal model representations learned during MLM and TLM training are helpful when fine-tuning on downstream tasks, without a sentence level objective, they do not directly produce sentence embeddings, which are critical for translation tasks.  </p><p>In “<a href=\"https://arxiv.org/abs/2007.01852\">Language-agnostic BERT Sentence Embedding</a>”, we present a multilingual <a href=\"https://www.aclweb.org/anthology/N19-1423/\">BERT</a> embedding model, called LaBSE, that produces language-agnostic cross-lingual sentence embeddings for 109 languages. The model is trained on 17 billion monolingual sentences and 6 billion bilingual sentence pairs using MLM and TLM pre-training, resulting in a model that is effective even on low-resource languages for which there is no data available during training. Further, the model establishes a new state of the art on multiple <a href=\"https://en.wikipedia.org/wiki/Parallel_text\">parallel text</a> (a.k.a. <a href=\"https://en.wiktionary.org/wiki/bitext\">bitext</a>) retrieval tasks. We have released the pre-trained model to the community through <a href=\"https://tfhub.dev/google/LaBSE/1\">tfhub</a>, which includes modules that can be used as-is or can be fine-tuned using domain-specific data.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-eqH1ZsnTP2o/XzwlDjy3X5I/AAAAAAAAGZw/_qFDiNyDi7cMeIBtUa-qoTJ9_6ODKtLtwCLcBGAsYHQ/s1621/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"756\" data-original-width=\"1621\" src=\"https://1.bp.blogspot.com/-eqH1ZsnTP2o/XzwlDjy3X5I/AAAAAAAAGZw/_qFDiNyDi7cMeIBtUa-qoTJ9_6ODKtLtwCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The collection of the training data for 109 supported languages</td></tr></tbody></table><p><b>The Model</b><br />In <a href=\"https://www.ijcai.org/Proceedings/2019/0746.pdf\">previous work</a>, we proposed the use of a <em>translation ranking task </em>to learn a multilingual sentence embedding space. This approach tasks the model with ranking the true translation over a collection of sentences in the target language, given a sentence in the source language. The translation ranking task is trained using a dual encoder architecture with a shared <a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\">transformer</a> encoder. The resulting bilingual models achieved state-of-the-art performance on multiple parallel text retrieval tasks (including <a href=\"https://conferences.unite.un.org/UNCorpus\">United Nations</a> and <a href=\"https://comparable.limsi.fr/bucc2018/bucc2018-task.html\">BUCC</a>). However, the model suffered when the bi-lingual models were extended to support multiple languages (16 languages, in our test case) due to limitations in model capacity, vocabulary coverage, training data quality and more.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-nWk49a-caFc/Xzwl9yJ2wFI/AAAAAAAAGaU/s4lObJ58si8r_NpHh6WUTVAmf6PziFTiQCLcBGAsYHQ/s611/TranslationRanking.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"202\" data-original-width=\"611\" height=\"162\" src=\"https://1.bp.blogspot.com/-nWk49a-caFc/Xzwl9yJ2wFI/AAAAAAAAGaU/s4lObJ58si8r_NpHh6WUTVAmf6PziFTiQCLcBGAsYHQ/w489-h162/TranslationRanking.gif\" width=\"489\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Translation ranking task. Given a sentence in a given source language, the task is to find the true translation over a collection of sentences in the target language.</td></tr></tbody></table><p>For LaBSE, we leverage recent advances on language model pre-training, including MLM and TLM, on a <a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT</a>-like architecture and follow this with fine-tuning on a translation ranking task. A 12-layer <a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\">transformer</a> with a 500k token vocabulary pre-trained using MLM and TLM on 109 languages is used to increase the model and vocabulary coverage. The resulting LaBSE model offers extended <em>support to 109 languages in a single model.</em></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-2m5u8cCre6g/XzwldoUhZVI/AAAAAAAAGaE/5gjnSRA1nh8UZ1tTRPp7lmBPBH7Jim-7QCLcBGAsYHQ/s616/image3%2B-%2BEdited%2B%25281%2529.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"384\" data-original-width=\"616\" height=\"307\" src=\"https://1.bp.blogspot.com/-2m5u8cCre6g/XzwldoUhZVI/AAAAAAAAGaE/5gjnSRA1nh8UZ1tTRPp7lmBPBH7Jim-7QCLcBGAsYHQ/w493-h307/image3%2B-%2BEdited%2B%25281%2529.png\" width=\"493\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The dual encoder architecture, in which the source and target text are encoded using a shared transformer embedding network separately. The translation ranking task is applied, forcing the text that paraphrases each other to have similar representations. The transformer embedding network is initialized from a BERT checkpoint trained on MLM and TLM tasks.</td></tr></tbody></table><p><b>Performance on Cross-lingual Text Retrieval</b><br />We evaluate the proposed model using the <a href=\"https://github.com/facebookresearch/LASER\">Tatoeba corpus</a>, a dataset consisting of up to 1,000 English-aligned sentence pairs for 112 languages. For more than 30 of the languages in the dataset, the model has no training data. The model is tasked with finding the nearest neighbor translation for a given sentence, which it calculates using the <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">cosine distance</a>.  </p><p>To understand the performance of the model for languages at the head or tail of the training data distribution, we divide the set of languages into several groups and compute the average accuracy for each set. The first 14-language group is selected from the languages supported by m~USE, which cover the languages from the head of the distribution (<em>head languages</em>). We also evaluate a second language group composed of 36 languages from the <a href=\"https://ai.googleblog.com/2020/04/xtreme-massively-multilingual-multi.html\">XTREME benchmark</a>. The third 82-language group, selected from the languages covered by the LASER training data, includes many languages from the tail of the distribution (<em>tail languages</em>). Finally, we compute the average accuracy for all languages. </p><p>The table below presents the average accuracy achieved by LaBSE, compared to the m~USE and LASER models, for each language group. As expected, all models perform strongly on the 14-language group that covers most head languages. With more languages included, the averaged accuracy for both LASER and LaBSE declines. However, the reduction in accuracy from the LaBSE model with increasing numbers of languages is much less significant, outperforming LASER significantly, particularly when the full distribution of 112 languages is included (83.7% accuracy vs. 65.5%). </p><table align=\"center\" cellspacing=\"10\">  <tbody><tr>   <td><strong>Model</strong>   </td>   <td align=\"center\"><strong>14 Langs</strong>   </td>   <td align=\"center\"><strong>36 Langs</strong>   </td>   <td align=\"center\"><strong>82 Langs</strong>   </td>   <td align=\"center\"><strong>All Langs</strong>   </td>  </tr>  <tr>   <td>m~USE*    </td>   <td align=\"center\">93.9    </td>   <td align=\"center\">—    </td>   <td align=\"center\">—    </td>   <td align=\"center\">—    </td>  </tr>  <tr>   <td>LASER    </td>   <td align=\"center\">95.3    </td>   <td align=\"center\">84.4    </td>   <td align=\"center\">75.9    </td>   <td align=\"center\">65.5    </td>  </tr>  <tr>   <td>LaBSE    </td>   <td align=\"center\">95.3    </td>   <td align=\"center\">95.0    </td>   <td align=\"center\">87.3    </td>   <td align=\"center\">83.7    </td>  </tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Average Accuracy (%) on <a href=\"https://github.com/facebookresearch/LASER/tree/master/data/tatoeba/v1\">Tatoeba Datasets</a>. The “14 Langs” group consists of languages supported by m~USE; the “36 Langs” group includes languages selected by XTREME; and the “82 Langs” group represents languages covered by the LASER model. The “All Langs” group includes all languages supported by Taoteba. <br />* The m~USE model comes in two varieties, one built on a <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">convolutional neural network</a> architecture and the other a <a href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\">Transformer</a>-like architecture. Here, we compare only to the Transformer version.</td></tr></tbody></table><p><b>Support to Unsupported Languages</b><br />The average performance of all languages included in Tatoeba is very promising. Interestingly, LaBSE even performs relatively well for many of the 30+ Tatoeba languages for which it has no training data (see below). For one third of these languages the LaBSE accuracy is higher than 75% and only 8 have accuracy lower than 25%, indicating very strong transfer performance to languages without training data. Such positive language transfer is only possible due to the massively multilingual nature of LaBSE.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XKlopfLZQRI/XzwmKHMg2gI/AAAAAAAAGaY/FwXbI2K1qlQqOknkrSc_xMufwMAtnxJPwCLcBGAsYHQ/s1132/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"505\" data-original-width=\"1132\" src=\"https://1.bp.blogspot.com/-XKlopfLZQRI/XzwmKHMg2gI/AAAAAAAAGaY/FwXbI2K1qlQqOknkrSc_xMufwMAtnxJPwCLcBGAsYHQ/s640/image4.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">LaBSE accuracy for the subset of Tatoeba languages (represented with <a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\">ISO 639-1/639-2</a> codes) for which there was no training data.</td></tr></tbody></table><p><b>Mining Parallel Text from Web</b>LaBSE can be used for mining parallel text (bi-text) from web-scale data. For example, we applied LaBSE to <a href=\"https://commoncrawl.org/\">CommonCrawl</a>, a large-scale monolingual corpus, to process 560 million Chinese and 330 million German sentences for the extraction of parallel text. Each Chinese and German sentence pair is encoded using the LaBSE model and then the encoded embedding is used to find a potential translation from a pool of 7.7 billion English sentences pre-processed and encoded by the model. An <a href=\"https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor\">approximate nearest neighbor search</a> is employed to quickly search through the high-dimensional sentence embeddings. After a simple filtering, the model returns 261M and 104M potential parallel pairs for English-Chinese and English-German, respectively. The trained NMT model using the mined data reaches BLEU scores of 35.7 and 27.2 on the <a href=\"http://www.statmt.org/wmt20/\">WMT translation tasks</a> (wmt17 for English-to-Chinese and wmt14 for English-to-German). The performance is only a few points away from current state-of-art-models trained on high quality parallel data. </p><p><b>Conclusion</b>We're excited to share this research, and the model, with the community.  The pre-trained model is released at <a href=\"https://tfhub.dev/google/LaBSE/1\">tfhub</a> to support further research on this direction and possible downstream applications. We also believe that what we're showing here is just the beginning, and there are more important research problems to be addressed, such as building better models to support all languages.  </p><p><b>Acknowledgements</b><em>The core team includes Wei Wang, Naveen Arivazhagan, Daniel Cer. We would like to thank the Google Research Language team, along with our partners in other Google groups for their feedback and suggestions. Special thanks goes to Sidharth Mudgal, and Jax Law for help with data processing; as well as Jialu Liu, Tianqi Liu, Chen Chen, and Anosh Raj for help on BERT pre-training.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=DxkG9zDiWDw:f6T4QFJf50M:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/DxkG9zDiWDw\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 18 Aug 2020 19:34:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-6upFrBNGwo4/Xzwk7D60GaI/AAAAAAAAGZs/ZDgmdCvBYfQr2cc5CkWW0AfIzD11x1q4wCLcBGAsYHQ/s0/image2%2B%25284%2529.jpg","linkMd5":"1838d5b6b055d7a446dcfe2540616e69","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn94@2020_3/2020/08/24/14-07-28-397_f9c57f3b236ab4b1.webp","destWidth":344,"destHeight":209,"sourceBytes":20983,"destBytes":6018,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-6upFrBNGwo4/Xzwk7D60GaI/AAAAAAAAGZs/ZDgmdCvBYfQr2cc5CkWW0AfIzD11x1q4wCLcBGAsYHQ/s0/image2%2B%25284%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn94@2020_3/2020/08/24/14-07-28-397_f9c57f3b236ab4b1.webp","https://1.bp.blogspot.com/-eqH1ZsnTP2o/XzwlDjy3X5I/AAAAAAAAGZw/_qFDiNyDi7cMeIBtUa-qoTJ9_6ODKtLtwCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn5@2020_6/2020/08/24/14-07-57-188_1c03f8c9145038c2.webp","https://1.bp.blogspot.com/-nWk49a-caFc/Xzwl9yJ2wFI/AAAAAAAAGaU/s4lObJ58si8r_NpHh6WUTVAmf6PziFTiQCLcBGAsYHQ/w489-h162/TranslationRanking.gif":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn25@2020_3/2020/08/24/14-07-56-564_eb2e2e71a0654078.webp","https://1.bp.blogspot.com/-2m5u8cCre6g/XzwldoUhZVI/AAAAAAAAGaE/5gjnSRA1nh8UZ1tTRPp7lmBPBH7Jim-7QCLcBGAsYHQ/w493-h307/image3%2B-%2BEdited%2B%25281%2529.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn41@2020_4/2020/08/24/14-07-55-358_261c73db2ef09efd.webp","https://1.bp.blogspot.com/-XKlopfLZQRI/XzwmKHMg2gI/AAAAAAAAGaY/FwXbI2K1qlQqOknkrSc_xMufwMAtnxJPwCLcBGAsYHQ/s640/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn65@2020_4/2020/08/24/14-07-56-628_cb5ecca5dc425e01.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/DxkG9zDiWDw":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn1@2020_3/2020/08/24/14-07-58-059_ef6cabcc55c90e07.webp"},"publishedOrCreatedDate":1598278048081},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"On-device, Real-time Body Pose Tracking with MediaPipe BlazePose","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","description":"<span class=\"byline-author\">Posted by Valentin Bazarevsky and Ivan Grishchenko, Research Engineers, Google Research</span> <p>Pose estimation from video plays a critical role enabling the overlay of digital content and information on top of the physical world in <a href=\"https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html\">augmented reality</a>,<a href=\"https://www.youtube.com/watch?v=N0Vm0LXmcU4\"> sign language</a> recognition, <a href=\"https://blog.google/technology/ai/move-mirror-you-move-and-80000-images-move-you/\">full-body gesture control</a>, and even <a href=\"https://www.google.com/url?q=https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html&amp;sa=D&amp;ust=1597258979955000&amp;usg=AFQjCNFtSYz2_xPGSEe23nh5vOmwqM0p3Q\">quantifying physical exercises</a>, where it can form the basis for yoga, dance, and fitness applications. Pose estimation for fitness applications is particularly challenging due to the wide variety of possible poses (e.g., hundreds of yoga <a href=\"https://en.wikipedia.org/wiki/Asana\">asanas</a>), numerous degrees of freedom, occlusions (e.g. the body or other objects occlude limbs as seen from the camera), and a variety of appearances or outfits. </p><p></p><p id=\"gdcalert1\"></p><p></p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr>  <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-nsLiFUVt6S4/XzVpLWay6VI/AAAAAAAAGXI/oPyuvuQEFcASODqPdT9dqptyUvUuGlTvACLcBGAsYHQ/s427/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"427\" data-original-width=\"320\" height=\"274\" src=\"https://1.bp.blogspot.com/-nsLiFUVt6S4/XzVpLWay6VI/AAAAAAAAGXI/oPyuvuQEFcASODqPdT9dqptyUvUuGlTvACLcBGAsYHQ/w205-h274/image3.gif\" width=\"205\" /></a></td>  <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-3y9qZTiQ-Xg/XzVsslu98RI/AAAAAAAAGXg/hpkLt16_qmoeqtdW1NBlryODgA-6Wq-RACLcBGAsYHQ/s427/Image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"427\" data-original-width=\"293\" height=\"274\" src=\"https://1.bp.blogspot.com/-3y9qZTiQ-Xg/XzVsslu98RI/AAAAAAAAGXg/hpkLt16_qmoeqtdW1NBlryODgA-6Wq-RACLcBGAsYHQ/w188-h274/Image2.gif\" width=\"188\" /></a></td>  <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-QkAt8k-MxRk/XzVpQxW1ZCI/AAAAAAAAGXQ/NJjTJUyR-5MdSehx9U1ykh5_tWI0lOaPACLcBGAsYHQ/s427/image5.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"427\" data-original-width=\"320\" height=\"274\" src=\"https://1.bp.blogspot.com/-QkAt8k-MxRk/XzVpQxW1ZCI/AAAAAAAAGXQ/NJjTJUyR-5MdSehx9U1ykh5_tWI0lOaPACLcBGAsYHQ/w205-h274/image5.gif\" width=\"205\" /></a></td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">BlazePose results on fitness and dance use-cases.<br /></td></tr></tbody></table><p>Today we are announcing the release of a new approach to human body pose perception, <a href=\"https://solutions.mediapipe.dev/pose\">BlazePose</a>, which we <a href=\"https://arxiv.org/abs/2006.10204\">presented</a> at the <a href=\"https://mixedreality.cs.cornell.edu/workshop/2020/papers\">CV4ARVR workshop</a> at <a href=\"http://cvpr2020.thecvf.com/\">CVPR 2020</a>. Our approach provides human pose tracking by employing machine learning (ML) to infer 33, 2D landmarks of a body from a single frame. In contrast to current pose models based on the standard <a href=\"http://cocodataset.org/#keypoints-2020\">COCO topology</a>, BlazePose accurately localizes more keypoints, making it uniquely suited for fitness applications. In addition, current state-of-the-art approaches rely primarily on powerful desktop environments for inference, whereas our method achieves real-time performance on mobile phones with CPU inference. If one leverages GPU inference, BlazePose achieves super-real-time performance, enabling it to run subsequent ML models, like face or hand tracking. </p><p></p><p id=\"gdcalert4\"></p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-WNEb6bWjYII/XzVunSDlPbI/AAAAAAAAGXs/OrIPZZyMqYghCbuomvVPDO_tdKz9EeOyQCLcBGAsYHQ/s550/image10.gif\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"290\" data-original-width=\"550\" height=\"149\" src=\"https://1.bp.blogspot.com/-WNEb6bWjYII/XzVunSDlPbI/AAAAAAAAGXs/OrIPZZyMqYghCbuomvVPDO_tdKz9EeOyQCLcBGAsYHQ/w282-h149/image10.gif\" width=\"282\" /></a></td>  <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-6zLvUOR2aQY/XzVuvuFVoYI/AAAAAAAAGXw/z3yKcMX7CTMJHHAUOK7vjApm_FnX9ET8ACLcBGAsYHQ/s550/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"309\" data-original-width=\"550\" height=\"149\" src=\"https://1.bp.blogspot.com/-6zLvUOR2aQY/XzVuvuFVoYI/AAAAAAAAGXw/z3yKcMX7CTMJHHAUOK7vjApm_FnX9ET8ACLcBGAsYHQ/w265-h149/image2.gif\" width=\"265\" /></a></td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Upper-body BlazePose model in MediaPipe<br /></td></tr></tbody></table> <p><b>Topology </b><br />The current standard for human body pose is the <a href=\"http://cocodataset.org/#keypoints-2020\">COCO topology</a>, which consists of 17 landmarks across the torso, arms, legs, and face. However, the COCO keypoints only localize to the ankle and wrist points, lacking scale and orientation information for hands and feet, which is vital for practical applications like fitness and dance. The inclusion of more keypoints is crucial for the subsequent application of domain-specific pose estimation models, like those for hands, face, or feet. </p><p>With BlazePose, we present a new topology of 33 human body keypoints, which is a superset of COCO, <a href=\"https://google.github.io/mediapipe/solutions/face_detection\">BlazeFace</a> and <a href=\"https://google.github.io/mediapipe/solutions/hands\">BlazePalm</a> topologies. This allows us to determine body semantics from pose prediction alone that is consistent with face and hand models. </p><p></p><p id=\"gdcalert4\"></p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-w22Iw7BRZsg/XzWx-S7DtpI/AAAAAAAAGZg/zgpN2e5Oye8qPXfq0zLq6dm38afXaUa8gCLcBGAsYHQ/s1999/image4%2B%25281%2529.jpg\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1010\" data-original-width=\"1999\" src=\"https://1.bp.blogspot.com/-w22Iw7BRZsg/XzWx-S7DtpI/AAAAAAAAGZg/zgpN2e5Oye8qPXfq0zLq6dm38afXaUa8gCLcBGAsYHQ/s640/image4%2B%25281%2529.jpg\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">BlazePose 33 keypoint topology as COCO (colored with green) superset</td></tr></tbody></table><p><b>Overview: An ML Pipeline for Pose Tracking</b><br />For pose estimation, we utilize our <a href=\"https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html\">proven</a> two-step <a href=\"https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html\">detector-tracker ML pipeline</a>. Using a detector, this pipeline first locates the pose region-of-interest (ROI) within the frame. The tracker subsequently predicts all 33 pose keypoints from this ROI. Note that for video use cases, the detector is run only on the first frame. For subsequent frames we derive the ROI from the previous frame’s pose keypoints as discussed below. </p><p></p><p id=\"gdcalert7\"></p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-J66lTDBjlgw/XzVwzgeQJ7I/AAAAAAAAGYM/WBIhbOqzi4ICUswEOHv8r7ItJIOJgL9iwCLcBGAsYHQ/s411/image11.jpg\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"169\" data-original-width=\"411\" src=\"https://1.bp.blogspot.com/-J66lTDBjlgw/XzVwzgeQJ7I/AAAAAAAAGYM/WBIhbOqzi4ICUswEOHv8r7ItJIOJgL9iwCLcBGAsYHQ/s0/image11.jpg\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Human pose estimation pipeline overview.<br /></td></tr></tbody></table> <p><b>Pose Detection by extending BlazeFace</b><br />For real-time performance of the full ML pipeline consisting of pose detection and tracking models, each component must be very fast, using only a few milliseconds per frame. To accomplish this, we observe that the strongest signal to the neural network about the position of the torso is the person's face (due to its high-contrast features and comparably small variations in appearance). Therefore, we achieve a fast and lightweight pose detector by making the strong (yet for many mobile and web applications valid) assumption that the head should be visible for our single-person use case. </p><p>Consequently, we trained a face detector, inspired by our sub-millisecond <a href=\"https://arxiv.org/abs/1907.05047\">BlazeFace</a> model, as a proxy for a pose detector. Note, this model only detects the location of a person within the frame and can not be used to identify individuals. In contrast to the <a href=\"https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html\">Face Mesh</a> and <a href=\"https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html\">MediaPipe Hand</a> tracking pipelines, where we derive the ROI from predicted keypoints, for the human pose tracking we explicitly predict two additional <em>virtual </em>keypoints that firmly describe the human body center, rotation and scale as a circle. Inspired by <a href=\"https://en.wikipedia.org/wiki/Vitruvian_Man\">Leonardo’s Vitruvian man</a>, we predict the midpoint of a person's hips, the radius of a circle circumscribing the whole person, and the incline angle of the line connecting the shoulder and hip midpoints. This results in consistent tracking even for very complicated cases, like specific yoga asanas. The figure below illustrates the approach. </p><p></p><p id=\"gdcalert8\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-i8ZfrqzbckE/XzVxC19SuyI/AAAAAAAAGYY/XAAoXAgx34kvOtk1Ypi0tQyiCfREGADSgCLcBGAsYHQ/s878/image4.jpg\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"877\" data-original-width=\"878\" height=\"210\" src=\"https://1.bp.blogspot.com/-i8ZfrqzbckE/XzVxC19SuyI/AAAAAAAAGYY/XAAoXAgx34kvOtk1Ypi0tQyiCfREGADSgCLcBGAsYHQ/w210-h210/image4.jpg\" width=\"210\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Vitruvian man aligned via two virtual keypoints predicted by our BlazePose detector in addition to the face bounding box<br /></td></tr></tbody></table><p><b>Tracking Model</b><br />The pose estimation component of the pipeline predicts the location of all 33 person keypoints with three degrees of freedom each (<em>x, y</em> location and visibility) plus the two virtual alignment keypoints described above. Unlike current approaches that employ compute-intensive <a href=\"https://www.adrianbulat.com/human-pose-estimation\">heatmap</a> prediction, our model uses a regression approach that is <em>supervised </em>by a combined heat map/offset prediction of all keypoints, as shown below.  </p><p></p><p id=\"gdcalert9\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XxKesnBALGM/XzVxSKZNWZI/AAAAAAAAGYc/WOt31icjp_YyjMxz06RSEwTi9K3qviFxwCLcBGAsYHQ/s550/image9.jpg\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"337\" data-original-width=\"550\" height=\"270\" src=\"https://1.bp.blogspot.com/-XxKesnBALGM/XzVxSKZNWZI/AAAAAAAAGYc/WOt31icjp_YyjMxz06RSEwTi9K3qviFxwCLcBGAsYHQ/w440-h270/image9.jpg\" width=\"440\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Tracking network architecture: regression with heatmap supervision<br /></td></tr></tbody></table><p>Specifically, during training we first employ a <a href=\"https://openaccess.thecvf.com/content_CVPRW_2019/papers/Augmented%20Human%20Human-centric%20Understanding%20and%202D-3D%20Synthesis/Zhang_Exploiting_Offset-guided_Network_for_Pose_Estimation_and_Tracking_CVPRW_2019_paper.pdf\">heatmap and offset loss</a> to train the center and left tower of the network. We then remove the heatmap output and train the regression encoder (right tower), thus, effectively using the heatmap to supervise a lightweight embedding. </p><p>The table below shows an ablation study of the model quality resulting from different training strategies. As an evaluation metric, we use the Percent of Correct Points with 20% tolerance (PCK@0.2) (where we assume the point to be detected correctly if the 2D Euclidean error is smaller than 20% of the  corresponding person’s torso size).  To obtain a human baseline, we asked annotators to annotate several samples redundantly and obtained an average PCK@0.2 of 97.2. The training and validation have been done on a geo-diverse dataset of various poses, sampled uniformly. </p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-fi1yFm05xl8/XzV3ydL3ETI/AAAAAAAAGZE/arm5o-e9a4AtA0V-wmrdZIthCbE0an8DQCLcBGAsYHQ/s1260/TableA.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"295\" data-original-width=\"1260\" src=\"https://1.bp.blogspot.com/-fi1yFm05xl8/XzV3ydL3ETI/AAAAAAAAGZE/arm5o-e9a4AtA0V-wmrdZIthCbE0an8DQCLcBGAsYHQ/s640/TableA.png\" width=\"640\" /></a></div><p>To cover a wide range of customer hardware, we present two pose tracking models: lite and full, which are differentiated in the balance of speed versus quality. For performance evaluation on CPU, we use <a href=\"https://github.com/google/XNNPACK\">XNNPACK</a>; for mobile GPUs, we use the <a href=\"https://www.tensorflow.org/lite/performance/gpu\">TFLite GPU</a> backend. </p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-UgzqutJ_SBQ/XzV4jPL683I/AAAAAAAAGZU/lOQNa8SIK5UN2G14KIdnte0qdSGa0lGagCLcBGAsYHQ/s1262/TableC.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"233\" data-original-width=\"1262\" src=\"https://1.bp.blogspot.com/-UgzqutJ_SBQ/XzV4jPL683I/AAAAAAAAGZU/lOQNa8SIK5UN2G14KIdnte0qdSGa0lGagCLcBGAsYHQ/s640/TableC.png\" width=\"640\" /></a></div><p><b>Applications </b><br />Based on human pose, we can build a variety of applications, like fitness or yoga trackers. As an example, we present squats and push up counters, which can automatically count user statistics, or verify the quality of performed exercises. Such use cases can be implemented either using an additional classifier network or even with a simple joint pairwise distance lookup algorithm, which matches the closest pose in normalized pose space. </p><p></p><p id=\"gdcalert10\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Q64KtZLWOT8/XzVxdkZDMgI/AAAAAAAAGYk/qj7mLsOL3AMcDkusMgYDGrSqauRAljR9gCLcBGAsYHQ/s924/image8.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"260\" data-original-width=\"924\" src=\"https://1.bp.blogspot.com/-Q64KtZLWOT8/XzVxdkZDMgI/AAAAAAAAGYk/qj7mLsOL3AMcDkusMgYDGrSqauRAljR9gCLcBGAsYHQ/s640/image8.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The number of performed exercises counter based on detected body pose. <b>Left:</b> Squats; <b>Right:</b> Push-Ups<br /></td></tr></tbody></table><p><b>Conclusion</b><br />We have released a version of <a href=\"https://solutions.mediapipe.dev/pose\">BlazePose</a> targeting upper body use cases in <a href=\"http://mediapipe.dev\">MediaPipe</a> running on Android, iOS and Python. BlazePose will also be made available to the broader mobile developer community via the <a href=\"https://developers.google.com/ml-kit/early-access/pose-detection\">Pose detection API</a> in the upcoming release of <a href=\"https://developers.google.com/ml-kit/\">ML Kit</a>. Apart from the mobile domain, we preview our <a href=\"https://viz.mediapipe.dev/demo/pose_tracking\">web-based in-browser version</a> as well. We hope that providing this human pose perception functionality to the broader research and development community will result in an emergence of creative use cases, stimulating new applications, and new research avenues. </p><p>We plan to extend this technology with more robust and stable tracking to an even larger variety of human poses and activities. In the accompanying <a href=\"https://mediapipe.page.link/blazepose-mc\">Model Card</a>, we detail the intended uses, limitations and model fairness to ensure that use of these models aligns with <a href=\"https://www.blog.google/technology/ai/ai-principles/\">Google’s AI Principles.</a> We believe that publishing this technology can provide an impulse to new creative ideas and applications by the members of the research and developer community at large. We are excited to see what you can build with it! </p><p></p><p id=\"gdcalert11\"></p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-PFpXlfSsT9k/XzVxqz6f7EI/AAAAAAAAGYw/rRlgge0Tn5oHOae-d1WVFGFPguucsG_QgCLcBGAsYHQ/s320/image1.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"266\" data-original-width=\"320\" src=\"https://1.bp.blogspot.com/-PFpXlfSsT9k/XzVxqz6f7EI/AAAAAAAAGYw/rRlgge0Tn5oHOae-d1WVFGFPguucsG_QgCLcBGAsYHQ/s0/image1.gif\" /></a></td>    <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Fz-Z6THjArM/XzVxqpVu07I/AAAAAAAAGYs/593BjeHfjIs05HwDAgxbNajSoDoQKghNACLcBGAsYHQ/s320/image6.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"266\" data-original-width=\"320\" src=\"https://1.bp.blogspot.com/-Fz-Z6THjArM/XzVxqpVu07I/AAAAAAAAGYs/593BjeHfjIs05HwDAgxbNajSoDoQKghNACLcBGAsYHQ/s0/image6.gif\" /></a></td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">BlazePose results on yoga use-cases<br /></td></tr></tbody></table><p><b>Acknowledgments</b><br /><em>Special thanks to all our team members who worked on the tech with us: Fan Zhang, Artsiom Ablavatski, Yury Kartynnik, Tyler Zhu, Karthik Raveendran, Andrei Vakunov, Andrei Tkachenka, Marat Dukhan, Raman Sarokin, Tyler Mullen, Gregory Karpiak, Suril Shah, Buck Bourdon, Jiuqiang Tang, Ming Guang Yong, Chuo-Ling Chang, Juhyun Lee, Michael Hays, Camillo Lugaresi, Esha Uboweja, Siarhei Kazakou, Andrei Kulik, Matsvei Zhdanovich, and Matthias Grundmann.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=e0Yr7hZaFjA:HboE-gVb-kk:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/e0Yr7hZaFjA\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Thu, 13 Aug 2020 17:36:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-nsLiFUVt6S4/XzVpLWay6VI/AAAAAAAAGXI/oPyuvuQEFcASODqPdT9dqptyUvUuGlTvACLcBGAsYHQ/w205-h274/image3.gif","linkMd5":"3c71cc7626d30346f461b7b4d2ecb255","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn61@2020_1/2020/08/24/14-07-28-981_d33215879a8838f5.webp","destWidth":205,"destHeight":274,"sourceBytes":1611196,"destBytes":491500,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-nsLiFUVt6S4/XzVpLWay6VI/AAAAAAAAGXI/oPyuvuQEFcASODqPdT9dqptyUvUuGlTvACLcBGAsYHQ/w205-h274/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn61@2020_1/2020/08/24/14-07-28-981_d33215879a8838f5.webp","https://1.bp.blogspot.com/-3y9qZTiQ-Xg/XzVsslu98RI/AAAAAAAAGXg/hpkLt16_qmoeqtdW1NBlryODgA-6Wq-RACLcBGAsYHQ/w188-h274/Image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn2@2020_3/2020/08/24/14-07-56-329_bfc74348fc84bed3.webp","https://1.bp.blogspot.com/-QkAt8k-MxRk/XzVpQxW1ZCI/AAAAAAAAGXQ/NJjTJUyR-5MdSehx9U1ykh5_tWI0lOaPACLcBGAsYHQ/w205-h274/image5.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn54@2020_3/2020/08/24/14-07-59-546_cc4e2a812568a510.webp","https://1.bp.blogspot.com/-WNEb6bWjYII/XzVunSDlPbI/AAAAAAAAGXs/OrIPZZyMqYghCbuomvVPDO_tdKz9EeOyQCLcBGAsYHQ/w282-h149/image10.gif":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn86@2020_5/2020/08/24/14-07-58-700_9a6e7ea8a0791a6d.webp","https://1.bp.blogspot.com/-6zLvUOR2aQY/XzVuvuFVoYI/AAAAAAAAGXw/z3yKcMX7CTMJHHAUOK7vjApm_FnX9ET8ACLcBGAsYHQ/w265-h149/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn76@2020_3/2020/08/24/14-07-55-970_765a2308da14f275.webp","https://1.bp.blogspot.com/-w22Iw7BRZsg/XzWx-S7DtpI/AAAAAAAAGZg/zgpN2e5Oye8qPXfq0zLq6dm38afXaUa8gCLcBGAsYHQ/s640/image4%2B%25281%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn49@2020_5/2020/08/24/14-07-55-483_0c5f003462264bd8.webp","https://1.bp.blogspot.com/-J66lTDBjlgw/XzVwzgeQJ7I/AAAAAAAAGYM/WBIhbOqzi4ICUswEOHv8r7ItJIOJgL9iwCLcBGAsYHQ/s0/image11.jpg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn37@2020_4/2020/08/24/14-07-57-478_809c9c4eb4fc7d18.webp","https://1.bp.blogspot.com/-i8ZfrqzbckE/XzVxC19SuyI/AAAAAAAAGYY/XAAoXAgx34kvOtk1Ypi0tQyiCfREGADSgCLcBGAsYHQ/w210-h210/image4.jpg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_1/2020/08/24/14-07-56-305_3f9ea428c27a2045.webp","https://1.bp.blogspot.com/-XxKesnBALGM/XzVxSKZNWZI/AAAAAAAAGYc/WOt31icjp_YyjMxz06RSEwTi9K3qviFxwCLcBGAsYHQ/w440-h270/image9.jpg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn73@2020_6/2020/08/24/14-07-55-363_916630e89981b722.webp","https://1.bp.blogspot.com/-fi1yFm05xl8/XzV3ydL3ETI/AAAAAAAAGZE/arm5o-e9a4AtA0V-wmrdZIthCbE0an8DQCLcBGAsYHQ/s640/TableA.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn38@2020_1/2020/08/24/14-07-55-253_2069698b0fdb909a.webp","https://1.bp.blogspot.com/-UgzqutJ_SBQ/XzV4jPL683I/AAAAAAAAGZU/lOQNa8SIK5UN2G14KIdnte0qdSGa0lGagCLcBGAsYHQ/s640/TableC.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn85@2020_5/2020/08/24/14-07-56-240_a424e3e94533f919.webp","https://1.bp.blogspot.com/-Q64KtZLWOT8/XzVxdkZDMgI/AAAAAAAAGYk/qj7mLsOL3AMcDkusMgYDGrSqauRAljR9gCLcBGAsYHQ/s640/image8.gif":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn77@2020_3/2020/08/24/14-08-11-612_359c361fc7e252f2.webp","https://1.bp.blogspot.com/-PFpXlfSsT9k/XzVxqz6f7EI/AAAAAAAAGYw/rRlgge0Tn5oHOae-d1WVFGFPguucsG_QgCLcBGAsYHQ/s0/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn14@2020_6/2020/08/24/14-07-59-449_cc9d38e8b3da46a8.webp","https://1.bp.blogspot.com/-Fz-Z6THjArM/XzVxqpVu07I/AAAAAAAAGYs/593BjeHfjIs05HwDAgxbNajSoDoQKghNACLcBGAsYHQ/s0/image6.gif":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn10@2020_1/2020/08/24/14-08-00-120_2a7fa215f10ac08f.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/e0Yr7hZaFjA":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn62@2020_2/2020/08/24/14-07-55-367_9d3082b63ccc6018.webp"},"publishedOrCreatedDate":1598278048085},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Live HDR+ and Dual Exposure Controls on Pixel 4 and 4a","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","description":"<span class=\"byline-author\">Posted by Jiawen Chen and Sam Hasinoff, Software Engineers, Google Research</span>  <br /><br /><a href=\"https://en.wikipedia.org/wiki/High-dynamic-range_imaging\">High dynamic range (HDR) imaging</a> is a method for capturing scenes with a wide range of brightness, from deep shadows to bright highlights. On Pixel phones, the <a href=\"https://ai.googleblog.com/2018/02/introducing-hdr-burst-photography.html\">engine behind HDR imaging</a> is <a href=\"https://hdrplusdata.org/\">HDR+ burst photography</a>, which involves capturing a rapid burst of deliberately underexposed images, combining them, and rendering them in a way that preserves detail across the range of tones. Until recently, one challenge with HDR+ was that it could not be computed in real time (i.e., at 30 frames per second), which prevented the viewfinder from matching the final result. For example, bright white skies in the viewfinder might appear blue in the HDR+ result. <br /><br />Starting with Pixel 4 and 4a, we have improved the viewfinder using a machine-learning-based approximation to HDR+, which we call <em>Live HDR+</em>. This provides a real-time preview of the final result, making HDR imaging more predictable. We also created <em>dual exposure controls</em>, which generalize the classic “exposure compensation” slider into two controls for separately adjusting the rendition of shadows and highlights. Together, Live HDR+ and dual exposure controls provide HDR imaging with real-time creative control.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-tckJGXlwza0/Xyhnz018mlI/AAAAAAAAGTo/GWRtBFKMiIw-RijXcb6qYeqNZ16I7k_ZgCLcBGAsYHQ/s1600/figure0_1600px_white.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"986\" data-original-width=\"1600\" height=\"394\" src=\"https://1.bp.blogspot.com/-tckJGXlwza0/Xyhnz018mlI/AAAAAAAAGTo/GWRtBFKMiIw-RijXcb6qYeqNZ16I7k_ZgCLcBGAsYHQ/s640/figure0_1600px_white.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Live HDR+ on Pixel 4 and 4a helps the user compose their shot with a <a href=\"https://en.wikipedia.org/wiki/WYSIWYG\">WYSIWYG</a> viewfinder that closely resembles the final result. You can see individual images <a href=\"https://photos.app.goo.gl/9QrvhXyJVKKCvGWh6\">here</a>. Photos courtesy of Florian Kainz.</td></tr></tbody></table><b>The HDR+ Look</b><br />When the user presses the shutter in the Pixel camera app, it captures 3-15 underexposed images. These images are aligned and merged to <a href=\"https://ai.googleblog.com/2018/11/night-sight-seeing-in-dark-on-pixel.html\">reduce noise</a> in the shadows, producing a 14-bit intermediate “linear RGB image” with pixel values proportional to the scene brightness. What gives HDR+ images their signature look is the \"tone mapping\" of this image, reducing the range to 8 bits and making it suitable for display.<br /><br />Consider the backlit photo of a motorcyclist, below. While the linear RGB image contains detail in both the dark motorcycle and bright sky, the dynamic range is too high to see it. The simplest method to reveal more detail is to apply a “global curve”, remapping all pixels with a particular brightness to some new value. However, for an HDR scene with details in both shadows and highlights, no single curve is satisfactory.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-ChpqMODDBno/Xyhn_JHDBHI/AAAAAAAAGTs/V-MChXk-zro_8Yaj2GHTf0eW-t7q1HjMQCLcBGAsYHQ/s1600/figure1_1600px_white.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"667\" data-original-width=\"1600\" height=\"266\" src=\"https://1.bp.blogspot.com/-ChpqMODDBno/Xyhn_JHDBHI/AAAAAAAAGTs/V-MChXk-zro_8Yaj2GHTf0eW-t7q1HjMQCLcBGAsYHQ/s640/figure1_1600px_white.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Different ways to tone-map a linear RGB image. (a) The original, “un-tone-mapped” image. (b) Global curve optimizing for the sky. (c) Global curve optimizing for the subject. (d) HDR+, which preserves details everywhere. In the 2D histogram, brighter areas indicate where more pixels of a given input brightness are mapped to the same output. The overlapping shapes show that the relationship cannot be modeled using a single curve. Photo courtesy of Nicholas Wilson.</td></tr></tbody></table>In contrast to applying a single curve, HDR+ uses a <em>local</em> tone mapping algorithm to ensure that the final result contains detail everywhere, while keeping edges and textures looking natural. Effectively, this applies a different curve to different regions, depending on factors such as overall brightness, local texture, and amount of noise. Unfortunately, HDR+ is too slow to run live in the viewfinder, requiring an alternative approach for Live HDR+.<br /><br /><b>Local Curve Approximation for Live HDR+</b><br />Using a single tone curve does not produce a satisfying result for the entire image — but how about for a small region? Consider the small red patch in the figure below. Although the patch includes both shadows and highlights, the relationship between input and output brightness follows a smooth curve. Furthermore, the curve varies gradually. For the blue patch, shifted ten pixels to the right, both the image content and curve are similar. But while the curve approximation works well for small patches, it breaks down for larger patches. For the larger yellow patch, the input/output relationship is more complicated, and not well approximated by a single curve.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-jwKGEFtOWhs/XyhoEqWZZBI/AAAAAAAAGT0/iTdgSRqezO0dqarp-SBlpws4E2mmeB7SACLcBGAsYHQ/s1600/figure2_1600px_white.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"666\" data-original-width=\"1600\" height=\"266\" src=\"https://1.bp.blogspot.com/-jwKGEFtOWhs/XyhoEqWZZBI/AAAAAAAAGT0/iTdgSRqezO0dqarp-SBlpws4E2mmeB7SACLcBGAsYHQ/s640/figure2_1600px_white.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">(a) Input and HDR+ result. (b) The effect of HDR+ on a small patch (red) is approximately a smooth curve. (c) The relationship is nearly identical for the nearby blue patch. (d) However, if the patch is too big, a single curve will no longer provide a good fit.</td></tr></tbody></table>To address this challenge, we divide the input image into “tiles” of size roughly equal to the red patch in the figure above, and approximate HDR+ using a curve for each tile. Since these curves vary gradually, blending between curves is a good way to approximate the optimal curve at any pixel. To render a pixel we apply the curves from each of the four nearest tiles, then blend the results according to the distances to the respective tile centers.<br /><br />Compared to HDR+, this algorithm is particularly well suited for <a href=\"https://en.wikipedia.org/wiki/Graphics_processing_unit\">GPUs</a>. Since the tone mapping of each pixel can be computed independently, the algorithm can also be parallelized. Moreover, the representation is memory-efficient: only a small number of tiles is enough to represent HDR+ local tone mapping for the viewfinder.<br /><br />To compute local curves, we use a machine learning algorithm called <a href=\"https://groups.csail.mit.edu/graphics/hdrnet/\">HDRnet</a>, a <a href=\"https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks\">deep neural network</a> that predicts, from a linear image, per-tile curves that approximate the HDR+ look of that image. It's also fast, due to its compact architecture and the way that low-resolution input images can be used to predict the curves for the high-resolution viewfinder. We train HDRnet on thousands of images to ensure it works well on all kinds of scenes.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-dPfRkUmOrP4/XyhoRVvtpbI/AAAAAAAAGT8/eZDz-jGEbjkyb8JSH8yHyV6bBUu-DpD0ACLcBGAsYHQ/s1600/figure3_1600px_white.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"580\" data-original-width=\"1600\" height=\"232\" src=\"https://1.bp.blogspot.com/-dPfRkUmOrP4/XyhoRVvtpbI/AAAAAAAAGT8/eZDz-jGEbjkyb8JSH8yHyV6bBUu-DpD0ACLcBGAsYHQ/s640/figure3_1600px_white.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">HDRnet vs. HDR+ on a challenging scene with extreme brights and darks. The results are very similar at viewfinder resolution. Photo courtesy of Nicholas Wilson.</td></tr></tbody></table><b>Dual Exposure Controls</b><br />HDR+ is designed to produce pleasing HDR images automatically, without the need for manual controls or post-processing. But sometimes the HDR+ rendition may not match the photographer’s artistic vision. While image editing tools are a partial remedy, HDR images can be challenging to edit, because some decisions are effectively baked into the final JPG. To maximize latitude for editing, it’s possible to save RAW images for each shot (an option in the app). However, this process takes the photographer out of the moment and requires expertise with RAW editing tools as well as additional storage.<br /><br />Another approach to artistic control is to provide it live in the viewfinder. Many photographers are familiar with the <a href=\"https://en.wikipedia.org/wiki/Exposure_compensation\">exposure compensation</a> slider, which brightens or darkens the image. But overall brightness is not expressive enough for HDR photography. At a minimum two controls are needed in order to control the highlights and shadows separately.<br /><br />To address this, we introduce dual exposure controls. When the user taps on the Live HDR+ viewfinder, two sliders appear. The \"Brightness\" slider works like traditional exposure compensation, changing the overall exposure. This slider is used to recover more detail in bright skies, or intentionally blow out the background and make the subject more visible. The \"Shadows\" slider affects only dark areas — it operates by changing the tone mapping, not the exposure. This slider is most useful for high-contrast scenes, letting the user boost shadows to reveal details, or suppress them to create a silhouette.<br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-1nHmepwWcAg/Xyg8dWNKh8I/AAAAAAAAGTM/gHSNE9eMW38kgDE8w8fdkOFQKV8COpUkACLcBGAsYHQ/s1600/image1.gif\"><img border=\"0\" data-original-height=\"303\" data-original-width=\"640\" height=\"302\" src=\"https://1.bp.blogspot.com/-1nHmepwWcAg/Xyg8dWNKh8I/AAAAAAAAGTM/gHSNE9eMW38kgDE8w8fdkOFQKV8COpUkACLcBGAsYHQ/s640/image1.gif\" width=\"640\" /></a></div><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XRSmG5X_Yp4/XyhogLlk1AI/AAAAAAAAGUE/Yyy4IX94riA8-Qp5F2k1irKvYQ1RetLcQCLcBGAsYHQ/s1600/figure5_1600px_white.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"556\" data-original-width=\"1600\" height=\"222\" src=\"https://1.bp.blogspot.com/-XRSmG5X_Yp4/XyhogLlk1AI/AAAAAAAAGUE/Yyy4IX94riA8-Qp5F2k1irKvYQ1RetLcQCLcBGAsYHQ/s640/figure5_1600px_white.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Screen capture of dual exposure controls in action on an outdoor HDR scene with HDR+ results below. You can see individual images <a href=\"https://photos.app.goo.gl/9QrvhXyJVKKCvGWh6\">here</a>. Photos courtesy of Florian Kainz.</td></tr></tbody></table>Here are some of the dramatic renditions we were able to achieve using dual exposure controls.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-y5LV-PfaOe8/Xyg8rbe79aI/AAAAAAAAGTY/IRWbV0s_O78llTqpx14NoLoD0j29K47-QCLcBGAsYHQ/s1600/image7.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1165\" data-original-width=\"1080\" height=\"640\" src=\"https://1.bp.blogspot.com/-y5LV-PfaOe8/Xyg8rbe79aI/AAAAAAAAGTY/IRWbV0s_O78llTqpx14NoLoD0j29K47-QCLcBGAsYHQ/s640/image7.png\" width=\"592\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Different renditions using Dual Exposure Controls. You can see individual images <a href=\"https://photos.app.goo.gl/9QrvhXyJVKKCvGWh6\">here</a>. Photo credits: Jiawen Chen, Florian Kainz, Alexander Schiffhauer.</td></tr></tbody></table>Dual Exposure Controls gives you the flexibility to capture dramatically different versions of the same subject. They are not limited to tough HDR scenes, so don’t be afraid to experiment with different subjects and lighting. You may be surprised at how much these sliders will change how you shoot!<br /><br /><b>Acknowledgements</b><br /><em>Live HDR+ and Dual Exposure Controls is the result of a collaboration between Google Research, Android, Hardware, and UX Design teams. Key contributors include: Francois Bleibel, Sean Callanan, Yulun Chang, Eric Chen, Michelle Chen, Kourosh Derakshan, Ryan Geiss, Zhijun He, Joy Hsu, Liz Koh, Marc Levoy, Chia-Kai Liang, Diane Liang, Timothy Lin, Gaurav Malik, Hossein Mohtasham, Nandini Mukherjee, Sushil Nath, Gabriel Nava, Karl Rasche, YiChang Shih, Daniel Solomon, Gary Sun, Kelly Tsai, Sung-fang Tsai, Ted Tsai, Ruben Velarde, Lida Wang, Tianfan Xue, Junlan Yang.</em><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=2B94l2vj1K4:oiIdDpILfyY:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2B94l2vj1K4\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 03 Aug 2020 18:23:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-tckJGXlwza0/Xyhnz018mlI/AAAAAAAAGTo/GWRtBFKMiIw-RijXcb6qYeqNZ16I7k_ZgCLcBGAsYHQ/s640/figure0_1600px_white.png","linkMd5":"977eb1a6b95954423c643687bd4266ff","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn1@2020_4/2020/08/24/14-07-28-393_24a383d4be71fac0.webp","destWidth":640,"destHeight":394,"sourceBytes":251873,"destBytes":34202,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-tckJGXlwza0/Xyhnz018mlI/AAAAAAAAGTo/GWRtBFKMiIw-RijXcb6qYeqNZ16I7k_ZgCLcBGAsYHQ/s640/figure0_1600px_white.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn1@2020_4/2020/08/24/14-07-28-393_24a383d4be71fac0.webp","https://1.bp.blogspot.com/-ChpqMODDBno/Xyhn_JHDBHI/AAAAAAAAGTs/V-MChXk-zro_8Yaj2GHTf0eW-t7q1HjMQCLcBGAsYHQ/s640/figure1_1600px_white.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn4@2020_3/2020/08/24/14-07-56-386_85428e093c4782ec.webp","https://1.bp.blogspot.com/-jwKGEFtOWhs/XyhoEqWZZBI/AAAAAAAAGT0/iTdgSRqezO0dqarp-SBlpws4E2mmeB7SACLcBGAsYHQ/s640/figure2_1600px_white.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn5@2020_2/2020/08/24/14-07-58-292_d29de5e00a94d3f6.webp","https://1.bp.blogspot.com/-dPfRkUmOrP4/XyhoRVvtpbI/AAAAAAAAGT8/eZDz-jGEbjkyb8JSH8yHyV6bBUu-DpD0ACLcBGAsYHQ/s640/figure3_1600px_white.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn2@2020_2/2020/08/24/14-07-57-202_e8c2ae452dd445bf.webp","https://1.bp.blogspot.com/-1nHmepwWcAg/Xyg8dWNKh8I/AAAAAAAAGTM/gHSNE9eMW38kgDE8w8fdkOFQKV8COpUkACLcBGAsYHQ/s640/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn61@2020_2/2020/08/24/14-08-03-139_383b087a83a3572d.webp","https://1.bp.blogspot.com/-XRSmG5X_Yp4/XyhogLlk1AI/AAAAAAAAGUE/Yyy4IX94riA8-Qp5F2k1irKvYQ1RetLcQCLcBGAsYHQ/s640/figure5_1600px_white.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn33@2020_4/2020/08/24/14-07-56-417_c3f13219e5ac3100.webp","https://1.bp.blogspot.com/-y5LV-PfaOe8/Xyg8rbe79aI/AAAAAAAAGTY/IRWbV0s_O78llTqpx14NoLoD0j29K47-QCLcBGAsYHQ/s640/image7.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn74@2020_4/2020/08/24/14-07-59-390_72d8631d55dc0348.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2B94l2vj1K4":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn42@2020_6/2020/08/24/14-07-56-379_274a43ec68bfb6c2.webp"},"publishedOrCreatedDate":1598278048091},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"SmartReply for YouTube Creators","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BDNx_vWFo_M/smartreply-for-youtube-creators.html","description":"<span class=\"byline-author\">Posted by Rami Al-Rfou, Research Scientist, Google Research</span>  <br /><br />It has been more than 4 years since <a href=\"https://research.google/pubs/pub45189/\">SmartReply</a> was launched, and since then, it has expanded to more users with the <a href=\"https://ai.googleblog.com/2017/05/efficient-smart-reply-now-for-gmail.html\">Gmail launch</a>&nbsp;and&nbsp;<a href=\"https://www.blog.google/products/messages/five-new-features-try-messages/\">Android Messages</a>&nbsp;and to more devices with <a href=\"https://ai.googleblog.com/2017/11/on-device-conversational-modeling-with.html\">Android Wear</a>. Developers now use SmartReply to respond to reviews within the <a href=\"https://android-developers.googleblog.com/2019/05/whats-new-in-play.html\">Play Developer Console</a> and can set up their own versions using APIs offered within <a href=\"https://developers.google.com/ml-kit/language/smart-reply\">MLKit</a> and <a href=\"https://www.tensorflow.org/lite/models/smart_reply/overview\">TFLite</a>. With each launch there has been a unique challenge in modeling and serving that required customizing SmartReply for the task requirements.<br /><br />We are now excited to share an updated SmartReply built for YouTube and implemented in YouTube Studio that helps creators engage more easily with their viewers. This model learns comment and reply representation through a computationally efficient dilated self-attention network, and represents the first cross-lingual and character byte-based SmartReply model. SmartReply for YouTube is currently available for English and Spanish creators, and this approach simplifies the process of extending the SmartReply feature to many more languages in the future. <br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-O5smp1-Rfrk/XvvEsptj6aI/AAAAAAAAGMo/QyU8k6bcojkB5VofNOf-yv0Wqjyo_Xx4ACLcBGAsYHQ/s1600/image2.gif\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"1000\" data-original-width=\"1000\" height=\"640\" src=\"https://1.bp.blogspot.com/-O5smp1-Rfrk/XvvEsptj6aI/AAAAAAAAGMo/QyU8k6bcojkB5VofNOf-yv0Wqjyo_Xx4ACLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></div>YouTube creators receive a large volume of responses to their videos. Moreover, the community of creators and viewers on YouTube is diverse, as reflected by the creativity of their comments, discussions and videos. In comparison to emails, which tend to be long and dominated by formal language, YouTube comments reveal complex patterns of language switching, abbreviated words, slang, inconsistent usage of punctuation, and heavy utilization of emoji. Following is a sample of comments that illustrate this challenge: <br /><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-IaXHUwl2yHI/XvvEkCH08uI/AAAAAAAAGMk/Ub9Yx8vzKXM3CDUv1wwHewslKXT4WRfCgCLcBGAsYHQ/s1600/Emoji.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"210\" data-original-width=\"1220\" height=\"110\" src=\"https://1.bp.blogspot.com/-IaXHUwl2yHI/XvvEkCH08uI/AAAAAAAAGMk/Ub9Yx8vzKXM3CDUv1wwHewslKXT4WRfCgCLcBGAsYHQ/s640/Emoji.png\" width=\"640\" /></a></div><b>Deep Retrieval</b><br />The initial release of <a href=\"https://ai.googleblog.com/2015/11/computer-respond-to-this-email.html\">SmartReply for Inbox</a> encoded input emails word-by-word with a <a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\">recurrent neural network</a>, and then decoded potential replies with yet another word-level recurrent neural network. Despite the expressivity of this approach, it was computationally expensive. Instead, <a href=\"https://arxiv.org/abs/1705.00652\">we found</a> that one can achieve the same ends by designing a system that searches through a predefined list of suggestions for the most appropriate response. <br /><br />This retrieval system encoded the message and its suggestion independently. First, the text was preprocessed to extract words and short phrases. This preprocessing included, but was not limited to, language identification, tokenization, and normalization. Two neural networks then simultaneously and independently encoded the message and the suggestion. This factorization allowed one to pre-compute the suggestion encodings and then search through the set of suggestions using an efficient <a href=\"https://arxiv.org/abs/1903.10391\">maximum inner product search</a> data structure. This deep retrieval approach enabled us to <a href=\"https://ai.googleblog.com/2017/05/efficient-smart-reply-now-for-gmail.html\">expand SmartReply to Gmail</a> and since then, it has been the foundation for several SmartReply systems including the current YouTube system.<br /><br /><b>Beyond Words</b><br />The previous SmartReply systems described above relied on word level preprocessing that is well tuned for a limited number of languages and narrow genres of writing. Such systems face significant challenges in the YouTube case, where a typical comment might include heterogeneous content, like emoji, ASCII art, language switching, etc. In light of this, and taking inspiration from <a href=\"https://www.aaai.org/ojs/index.php/AAAI/article/view/4182\">our recent work</a> on byte and character language modeling, we decided to encode the text without any preprocessing. This approach is supported by <a href=\"https://arxiv.org/abs/1908.10322\">research</a> demonstrating that a deep <a href=\"https://arxiv.org/abs/1706.03762\">Transformer</a> network is able to model words and phrases from the ground up just by feeding it text as a sequence of characters or bytes, with comparable quality to word-based models.<br /><br />Although initial results were promising, especially for processing comments with emoji or typos, the inference speed was too slow for production due to the fact that character sequences are longer than word equivalents and the computational complexity of <a href=\"https://arxiv.org/abs/1706.03762\">self-attention layers</a>&nbsp;grows quadratically as a function of sequence length. We found that shrinking the sequence length by applying temporal reduction layers at each layer of the network, similar to the dilation technique applied in <a href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\">WaveNet</a>, provides a good trade-off between computation and quality.<br /><br />The figure below presents a dual encoder network that encodes both the comment and the reply to maximize the mutual information between their latent representations by training the network with a <a href=\"https://arxiv.org/abs/1705.00652\">contrastive objective</a>. The encoding starts with feeding the transformer a sequence of bytes after they have been embedded. The input for each subsequent layer will be reduced by dropping a percentage of characters at equal offsets. After applying several transformer layers the sequence length is greatly truncated, significantly reducing the computational complexity. This sequence compression scheme could be substituted by other operators such as <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D\">average pooling</a>, though we did not notice any gains from more sophisticated methods, and therefore, opted to use dilation for simplicity.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-l-98i5rzG4w/XvvHoaIpGGI/AAAAAAAAGNQ/cBoe7_pWqQw5tpbkI_k_3NhbmU8Y_u08wCLcBGAsYHQ/s1600/animation.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"900\" data-original-width=\"1600\" height=\"360\" src=\"https://1.bp.blogspot.com/-l-98i5rzG4w/XvvHoaIpGGI/AAAAAAAAGNQ/cBoe7_pWqQw5tpbkI_k_3NhbmU8Y_u08wCLcBGAsYHQ/s640/animation.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A dual encoder network that maximizes the mutual information between the comments and their replies through a <a href=\"https://arxiv.org/abs/1705.00652\">contrastive objective</a>. Each encoder is fed a sequence of bytes and is  implemented as a computationally efficient dilated transformer network.</td></tr></tbody></table><b>A Model to Learn Them All</b><br />Instead of training a separate model for each language, we opted to train a single cross-lingual model for all supported languages. This allows the support of mixed-language usage in the comments, and enables the model to utilize the learning of common elements in one language for understanding another, such as emoji and numbers. Moreover, having a single model simplifies the logistics of maintenance and updates. While the model has been  rolled out to English and Spanish, the flexibility inherent in this approach will enable it to be expanded to other languages in the future.<br /><br />Inspecting the encodings of a multilingual set of suggestions produced by the model reveals that the model clusters appropriate replies, regardless of the language to which they belong. This cross-lingual capability emerged without exposing the model during training to any parallel corpus. We demonstrate in the figure below for three languages how the replies are clustered by their meaning when the model is probed with an input comment. For example, the English comment “<em>This is a great video,</em>” is surrounded by appropriate replies, such as “<em>Thanks!</em>” Moreover, inspection of the nearest replies in other languages reveal them also to be appropriate and similar in meaning to the English reply. The 2D projection also shows several other cross-lingual clusters that consist of replies of similar meaning.  This clustering demonstrates how the model can support a rich cross-lingual user experience in the supported languages.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-XEoGSy-SXDo/XvvFdqDIYhI/AAAAAAAAGM8/NNQNjtfDo-Ek7_kq0F_qSF4DrB-oAsspwCLcBGAsYHQ/s1600/image4.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"719\" data-original-width=\"1499\" height=\"306\" src=\"https://1.bp.blogspot.com/-XEoGSy-SXDo/XvvFdqDIYhI/AAAAAAAAGM8/NNQNjtfDo-Ek7_kq0F_qSF4DrB-oAsspwCLcBGAsYHQ/s640/image4.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A 2D projection of the model encodings when presented with a hypothetical comment and a small list of potential replies. The neighborhood surrounding English comments (black color) consists of appropriate replies in English and their counterparts in Spanish and Arabic. Note that the network learned to align English replies with their translations without access to any parallel corpus.</td></tr></tbody></table><b>When to Suggest?</b><br />Our goal is to help creators, so we have to make sure that SmartReply only makes suggestions when it is very likely to be useful. Ideally, suggestions would only be displayed when it is likely that the creator would reply to the comment and when the model has a high chance of providing a sensible and specific response. To accomplish this, we trained auxiliary models to identify which comments should trigger the SmartReply feature. <br /><br /><b>Conclusion</b><br />We’ve launched YouTube SmartReply, starting with English and Spanish comments, the first cross-lingual and character byte-based SmartReply. YouTube is a global product with a diverse user base that generates heterogeneous content. Consequently, it is important that we continuously improve comments for this global audience, and SmartReply represents a strong step in this direction. <br /><br /><b>Acknowledgements</b><br /><i>SmartReply for YouTube creators was developed by Golnaz Farhadi, Ezequiel Baril, Cheng Lee, Claire Yuan, Coty Morrison‎, Joe Simunic‎, Rachel Bransom‎, Rajvi Mehta, Jorge Gonzalez‎, Mark Williams, Uma Roy and many more. We are grateful for the leadership support from Nikhil Dandekar, Eileen Long, Siobhan Quinn, Yun-hsuan Sung, Rachel Bernstein, and Ray Kurzweil.</i><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=BDNx_vWFo_M:Q84W-fgZOHc:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/BDNx_vWFo_M\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 01 Jul 2020 17:01:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-O5smp1-Rfrk/XvvEsptj6aI/AAAAAAAAGMo/QyU8k6bcojkB5VofNOf-yv0Wqjyo_Xx4ACLcBGAsYHQ/s640/image2.gif","linkMd5":"48b1d4a884366dd0c8c7dff7923ccf98","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_5/2020/08/24/14-07-30-891_ab51825e059f6f3d.webp","destWidth":640,"destHeight":640,"sourceBytes":612085,"destBytes":334118,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-O5smp1-Rfrk/XvvEsptj6aI/AAAAAAAAGMo/QyU8k6bcojkB5VofNOf-yv0Wqjyo_Xx4ACLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_5/2020/08/24/14-07-30-891_ab51825e059f6f3d.webp","https://1.bp.blogspot.com/-IaXHUwl2yHI/XvvEkCH08uI/AAAAAAAAGMk/Ub9Yx8vzKXM3CDUv1wwHewslKXT4WRfCgCLcBGAsYHQ/s640/Emoji.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn38@2020_3/2020/08/24/14-07-58-432_b85b09923fac5ee7.webp","https://1.bp.blogspot.com/-l-98i5rzG4w/XvvHoaIpGGI/AAAAAAAAGNQ/cBoe7_pWqQw5tpbkI_k_3NhbmU8Y_u08wCLcBGAsYHQ/s640/animation.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn49@2020_1/2020/08/24/14-07-57-711_570c1af80b4c5ab0.webp","https://1.bp.blogspot.com/-XEoGSy-SXDo/XvvFdqDIYhI/AAAAAAAAGM8/NNQNjtfDo-Ek7_kq0F_qSF4DrB-oAsspwCLcBGAsYHQ/s640/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn97@2020_4/2020/08/24/14-07-58-088_e5de388353adc967.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/BDNx_vWFo_M":"https://cdn.jsdelivr.net/gh/myreaderx/cdn10@2020_6/2020/08/24/14-07-57-149_24aa9badf3a085be.webp"},"publishedOrCreatedDate":1598278048101},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Leveraging Temporal Context for Object Detection","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","description":"<span class=\"byline-author\">Posted by Sara Beery, Student Researcher, and Jonathan Huang, Research Scientist, Google Research</span>  <br /><br />Ecological monitoring helps researchers to understand the dynamics of global ecosystems, quantify biodiversity, and measure the effects of climate change and human activity, including the efficacy of conservation and remediation efforts. In order to monitor effectively, ecologists need high-quality data, often expending significant efforts to place monitoring sensors, such as static cameras, in the field. While it is increasingly cost effective to build and operate networks of such sensors, the manual data analysis of global biodiversity data remains a bottleneck to accurate, global, real-time ecological monitoring.  While there are ways to automate this analysis via machine learning, the data from static cameras, widely used to monitor the world around us for purposes ranging from <a href=\"https://www.wsdot.com/traffic/passes/snoqualmie/\">mountain pass road conditions</a> to <a href=\"https://phenocam.sr.unh.edu/webcam/gallery/\">ecosystem phenology</a>, still pose a strong challenge for traditional computer vision systems — due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. <br /><br />In order to perform well in this setting, computer vision models must be robust to objects of interest that are often off-center, out of focus, poorly lit, or at a variety of scales. In addition, a static camera will always take images of the same scene unless it is moved, which causes the data from any one camera to be highly repetitive. Without sufficient data variability, machine learning models may learn to focus on correlations in the background, <a href=\"https://arxiv.org/abs/1807.04975\">leading to poor generalization to novel deployments</a>. The machine learning and ecological communities have been working together through venues like <a href=\"http://lila.science/\">LILA BC</a> and <a href=\"https://www.wildlifeinsights.org/home\">Wildlife Insights</a> to curate expert-labeled training data from many research groups, each of which may operate anywhere from one to hundreds of camera traps, in order to increase data variability. This process of data collection and annotation is slow, and is confounded by the need to have diverse, representative data across geographic regions and taxonomic groups. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-A1jVMpuUj7g/XvTcZUCFDNI/AAAAAAAAGKk/v3pHkHOhpQYwLF2S53FCT9unitrkVfWMgCLcBGAsYHQ/s1600/image3.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1136\" data-original-width=\"1600\" height=\"454\" src=\"https://1.bp.blogspot.com/-A1jVMpuUj7g/XvTcZUCFDNI/AAAAAAAAGKk/v3pHkHOhpQYwLF2S53FCT9unitrkVfWMgCLcBGAsYHQ/s640/image3.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">What’s in this image? Objects in images from static cameras can be very challenging to detect and categorize. Here, a foggy morning has made it very difficult to see a herd of wildebeest walking along the crest of a hill. [Image from <a href=\"http://lila.science/datasets/snapshot-serengeti\">Snapshot Serengeti</a>]</td></tr></tbody></table>In <em><a href=\"https://arxiv.org/abs/1912.03538\">Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection</a></em>, we present a complementary approach that increases global scalability by improving generalization to novel camera deployments algorithmically. This new object detection architecture leverages contextual clues across time for each camera deployment in a network, improving recognition of objects in novel camera deployments without relying on additional training data from a large number of cameras. Echoing the approach a person might use when faced with challenging images, Context R-CNN leverages up to a month’s worth of images from the same camera for context to determine what objects might be present and identify them.&nbsp;Using this method, the model outperforms a single-frame <a href=\"https://arxiv.org/abs/1506.01497\">Faster R-CNN</a> baseline by significant margins across multiple domains, including wildlife camera traps. We have open sourced the code and models for this work as part of the <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/README.md#june-17th-2020\">TF Object Detection API</a> to make it easy to train and test Context R-CNN models on new static camera datasets.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-PJxZGdDg2CQ/XvTcfv7MO9I/AAAAAAAAGKo/7zEBw-Qp4XA2Q-vffmYyNoXs2v1tyYoWwCLcBGAsYHQ/s1600/image2.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"768\" data-original-width=\"1600\" height=\"306\" src=\"https://1.bp.blogspot.com/-PJxZGdDg2CQ/XvTcfv7MO9I/AAAAAAAAGKo/7zEBw-Qp4XA2Q-vffmYyNoXs2v1tyYoWwCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Here, we can see how additional examples from the same scene help experts determine that the object is an animal and not background. Context such as the shape &amp; size of the object, its attachment to a herd, and habitual grazing at certain times of day help determine that the species is a wildebeest. Useful examples occur throughout the month.</td></tr></tbody></table><b>The Context R-CNN Model</b><br />Context R-CNN is designed to take advantage of the high degree of correlation within images taken by a static camera to boost performance on challenging data and improve generalization to new camera deployments without additional human data labeling. It  is an adaptation of  <a href=\"https://arxiv.org/abs/1506.01497\">Faster R-CNN</a>, a popular two-stage object detection architecture. To extract context for a camera, we first use a frozen feature extractor to build up a contextual <em>memory bank</em> from images across a large time horizon (up to a month or more). Next, objects are detected in each image using Context R-CNN which aggregates relevant context from the memory bank to help detect objects under challenging conditions (such as the heavy fog obscuring the wildebeests in our previous example). This aggregation is performed using <a href=\"https://arxiv.org/abs/1706.03762\">attention</a>, which is robust to the sparse and irregular sampling rates often seen in static monitoring cameras. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-egMF8wDiUf4/XvTcoH-TwxI/AAAAAAAAGKs/znyPBxNOuyIGIjWlNi5UHAbsnk-kvE1ygCLcBGAsYHQ/s1600/image8.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"689\" data-original-width=\"1600\" height=\"274\" src=\"https://1.bp.blogspot.com/-egMF8wDiUf4/XvTcoH-TwxI/AAAAAAAAGKs/znyPBxNOuyIGIjWlNi5UHAbsnk-kvE1ygCLcBGAsYHQ/s640/image8.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">High-level architecture diagram, showing how Context R-CNN incorporates long-term context within the <a href=\"https://arxiv.org/abs/1506.01497\">Faster R-CNN</a> model architecture.</td></tr></tbody></table>The first stage of Faster R-CNN proposes potential objects, and the second stage categorizes each proposal as either background or one of the target classes. In Context R-CNN, we take the proposed objects from the first stage of Faster R-CNN, and for each one we use <a href=\"https://arxiv.org/abs/1706.03762\">similarity-based attention</a> to determine how relevant each of the features in our <em>memory bank</em> (<em>M</em>) is to the current object, and construct a per-object context feature by taking a relevance-weighted sum over <em>M</em> and adding it back to the original object features. Then each object, now with added contextual information, is finally categorized using the second stage of Faster R-CNN. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HucWjMOChHk/XvTcuRir9nI/AAAAAAAAGK0/SfhWxyARqdI5FrvNntwjNnafvBxYk8fhgCLcBGAsYHQ/s1600/image4.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"512\" data-original-width=\"1600\" height=\"204\" src=\"https://1.bp.blogspot.com/-HucWjMOChHk/XvTcuRir9nI/AAAAAAAAGK0/SfhWxyARqdI5FrvNntwjNnafvBxYk8fhgCLcBGAsYHQ/s640/image4.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Context R-CNN is able to leverage context (spanning up to 1 month) to correctly categorize the challenging wildebeest example we saw above. The green values are the corresponding attention weights for each boxed object.</td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HgIzPDSJ0xw/XvTdQH1Vs3I/AAAAAAAAGLE/gTlB3RfhFDEHhainPzV6FnQWKo1JuFyqACLcBGAsYHQ/s1600/ContextRCNN.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1095\" data-original-width=\"956\" height=\"640\" src=\"https://1.bp.blogspot.com/-HgIzPDSJ0xw/XvTdQH1Vs3I/AAAAAAAAGLE/gTlB3RfhFDEHhainPzV6FnQWKo1JuFyqACLcBGAsYHQ/s640/ContextRCNN.png\" width=\"558\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Compared to a Faster R-CNN baseline (<b>left</b>), Context R-CNN (<b>right</b>) is able to capture challenging objects such as an elephant occluded by a tree, two poorly-lit impala, and a vervet monkey leaving the frame. [Images from <a href=\"http://lila.science/datasets/snapshot-serengeti\">Snapshot Serengeti</a>]</td></tr></tbody></table><b>Results</b><br />We have tested Context R-CNN on <a href=\"http://lila.science/datasets/snapshot-serengeti\">Snapshot Serengeti</a> (SS) and <a href=\"http://lila.science/datasets/caltech-camera-traps\">Caltech Camera Traps</a> (CCT), both ecological datasets of animal species in camera traps but from highly different  geographic regions (Tanzania vs. the Southwestern United States). Improvements over the Faster R-CNN baseline for each dataset can be seen in the table below. Notably, we see a 47.5% relative increase in <a href=\"https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173#:~:text=mAP%20(mean%20average%20precision)%20is,difference%20between%20AP%20and%20mAP.\">mean average precision</a> (mAP) on SS, and a 34.3% relative mAP increase on CCT. We also compare Context R-CNN to <a href=\"https://arxiv.org/pdf/1712.04851\">S3D</a> (a 3D convolution based baseline) and see performance improve from 44.7% mAP to 55.9% mAP (a 25.1% relative increase). Finally, we find that the performance increases as the contextual time horizon increases, from a minute of context to a month. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-AUfpsk256iM/XvaMrDxcAKI/AAAAAAAAGLk/WMBxTRGTLBUcbBwh0owpVs6PDYpFAlcxQCLcBGAsYHQ/s1600/image1%2B%25281%2529.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"306\" data-original-width=\"1164\" height=\"105\" src=\"https://1.bp.blogspot.com/-AUfpsk256iM/XvaMrDxcAKI/AAAAAAAAGLk/WMBxTRGTLBUcbBwh0owpVs6PDYpFAlcxQCLcBGAsYHQ/s400/image1%2B%25281%2529.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Comparison to a single frame Faster R-CNN baseline, showing both <a href=\"https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\">mean average precision</a> (mAP) and <a href=\"https://manalelaidouni.github.io/manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html#average-recall-ar\">average recall</a> (AR) detection metrics.</td></tr></tbody></table><b>Ongoing and Future Work</b><br />We are working to implement Context R-CNN within the <a href=\"https://www.wildlifeinsights.org/\">Wildlife Insights</a> platform, to facilitate large-scale, global ecological monitoring via camera traps. We also host competitions such as the yearly <a href=\"https://www.kaggle.com/c/iwildcam-2020-fgvc7\">iWildCam</a> species identification competition at the <a href=\"https://ai.googleblog.com/2020/05/announcing-7th-fine-grained-visual.html\">CVPR Fine-Grained Visual Recognition Workshop</a>&nbsp;to help bring these challenges to the attention of the computer vision community. The challenges seen in automatic species identification in static cameras are shared by numerous applications of static cameras outside of the ecological monitoring domain, as well as other static sensors used to monitor biodiversity, such as audio and sonar devices. Our method is general, and we anticipate the per-sensor context approach taken by Context R-CNN would be beneficial for any static sensor.<br /><br /><b>Acknowledgements</b><br /><em>This post reflects the work of the authors as well as the following group of core contributors: Vivek Rathod, Guanhang Wu, Ronny Votel.  We are also grateful to Zhichao Lu, David Ross, Tanya Birch and the Wildlife Insights AI team, and Pietro Perona and the <a href=\"http://www.vision.caltech.edu/\">Caltech Computational Vision Lab</a>.</em><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=WADybDJ966w:VatUcOLGWH0:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/WADybDJ966w\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Fri, 26 Jun 2020 16:59:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-A1jVMpuUj7g/XvTcZUCFDNI/AAAAAAAAGKk/v3pHkHOhpQYwLF2S53FCT9unitrkVfWMgCLcBGAsYHQ/s640/image3.png","linkMd5":"c171dd959a368832e85f24dfeaf3eb73","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn18@2020_3/2020/08/24/14-07-28-401_20f2d8bb4b8cef68.webp","destWidth":640,"destHeight":454,"sourceBytes":128906,"destBytes":3678,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-A1jVMpuUj7g/XvTcZUCFDNI/AAAAAAAAGKk/v3pHkHOhpQYwLF2S53FCT9unitrkVfWMgCLcBGAsYHQ/s640/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn18@2020_3/2020/08/24/14-07-28-401_20f2d8bb4b8cef68.webp","https://1.bp.blogspot.com/-PJxZGdDg2CQ/XvTcfv7MO9I/AAAAAAAAGKo/7zEBw-Qp4XA2Q-vffmYyNoXs2v1tyYoWwCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn85@2020_5/2020/08/24/14-07-57-025_3db80c84ebe28b97.webp","https://1.bp.blogspot.com/-egMF8wDiUf4/XvTcoH-TwxI/AAAAAAAAGKs/znyPBxNOuyIGIjWlNi5UHAbsnk-kvE1ygCLcBGAsYHQ/s640/image8.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn93@2020_3/2020/08/24/14-07-55-459_488fca40d139e2f8.webp","https://1.bp.blogspot.com/-HucWjMOChHk/XvTcuRir9nI/AAAAAAAAGK0/SfhWxyARqdI5FrvNntwjNnafvBxYk8fhgCLcBGAsYHQ/s640/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn41@2020_5/2020/08/24/14-07-55-429_a0007aca11526e38.webp","https://1.bp.blogspot.com/-HgIzPDSJ0xw/XvTdQH1Vs3I/AAAAAAAAGLE/gTlB3RfhFDEHhainPzV6FnQWKo1JuFyqACLcBGAsYHQ/s640/ContextRCNN.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn45@2020_4/2020/08/24/14-07-57-247_87b778df1366e459.webp","https://1.bp.blogspot.com/-AUfpsk256iM/XvaMrDxcAKI/AAAAAAAAGLk/WMBxTRGTLBUcbBwh0owpVs6PDYpFAlcxQCLcBGAsYHQ/s400/image1%2B%25281%2529.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn26@2020_6/2020/08/24/14-07-57-362_0be1cd04029c810e.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/WADybDJ966w":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn12@2020_6/2020/08/24/14-07-57-023_3a958b473c03347c.webp"},"publishedOrCreatedDate":1598278048109},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"MediaPipe Iris: Real-time Iris Tracking & Depth Estimation","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","description":"<span class=\"byline-author\">Posted by Andrey Vakunov and Dmitry Lagun, Research Engineers, Google Research</span> <p>A wide range of real-world applications, including computational photography (e.g., <a href=\"https://ai.googleblog.com/2019/12/improvements-to-portrait-mode-on-google.html\">portrait mode</a> and glint reflections) and <a href=\"https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html\">augmented reality effects</a> (e.g., virtual avatars) rely on estimating eye position by tracking the iris. Once accurate iris tracking is available, we show that it is possible to determine the metric distance from the camera to the user — without the use of a dedicated depth sensor. This, in-turn, can improve a variety of use cases, ranging from computational photography, over virtual try-on of properly sized glasses and hats to usability enhancements that adopt the font size depending on the viewer’s distance.  </p><p>Iris tracking is a challenging task to solve on mobile devices, due to limited computing resources, variable light conditions and the presence of occlusions, such as hair or people squinting. Often, sophisticated specialized hardware is employed, limiting the range of devices on which the solution could be applied. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-Lpgeiec1LTI/Xywye-E8YOI/AAAAAAAAGUU/bAytHYPS1wIHV_EXWWBAXIdnp3p1H0D1wCLcBGAsYHQ/s675/image3.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"266\" data-original-width=\"675\" height=\"252\" src=\"https://1.bp.blogspot.com/-Lpgeiec1LTI/Xywye-E8YOI/AAAAAAAAGUU/bAytHYPS1wIHV_EXWWBAXIdnp3p1H0D1wCLcBGAsYHQ/w640-h252/image3.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><a href=\"https://google.github.io/mediapipe/solutions/face_mesh.html\">FaceMesh</a> can be adopted to drive virtual avatars (<b>middle</b>). By additionally employing iris tracking (<b>right</b>), the avatar’s liveliness is significantly enhanced.</td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-jF_HuzW-smE/Xywy_JurMuI/AAAAAAAAGUc/cMX_9GtWxM4RhOqMhURDYB831eDqJ0ZIwCLcBGAsYHQ/s586/image13.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"164\" data-original-width=\"586\" height=\"164\" src=\"https://1.bp.blogspot.com/-jF_HuzW-smE/Xywy_JurMuI/AAAAAAAAGUc/cMX_9GtWxM4RhOqMhURDYB831eDqJ0ZIwCLcBGAsYHQ/w586-h164/image13.gif\" width=\"586\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An example of eye re-coloring enabled by MediaPipe Iris.</td></tr></tbody></table><p>Today, we announce the release of <a href=\"https://solutions.mediapipe.dev/iris\">MediaPipe Iris</a>, a new machine learning model for accurate iris estimation. Building on our work on <a href=\"https://solutions.mediapipe.dev/face_mesh\">MediaPipe Face Mesh</a>, this model is able to track landmarks involving the iris, pupil and the eye contours using a single RGB camera, in real-time, without the need for specialized hardware. Through use of iris landmarks, the model is also able to determine the metric distance between the subject and the camera with relative error less than 10% without the use of depth sensor. Note that iris tracking does not infer the location at which people are looking, nor does it provide any form of identity recognition. Thanks to the fact that this system is implemented in <a href=\"http://mediapipe.dev/\">MediaPipe</a> — an open source cross-platform framework for researchers and developers to build world-class ML solutions and applications — it can run on most modern mobile phones, desktops, laptops and even <a href=\"https://viz.mediapipe.dev/demo/iris_tracking\">on the web</a>.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-GJxNLlVwxqI/XywzLGfARTI/AAAAAAAAGUg/zkvGnNLmHmwyb_nDQ4j46mi95wPRYyP7gCLcBGAsYHQ/s540/image12.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"540\" data-original-width=\"512\" src=\"https://1.bp.blogspot.com/-GJxNLlVwxqI/XywzLGfARTI/AAAAAAAAGUg/zkvGnNLmHmwyb_nDQ4j46mi95wPRYyP7gCLcBGAsYHQ/s0/image12.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Usability prototype for far-sighted individuals: observed font size remains constant independent of the device distance from the user.</td></tr></tbody></table><p><b>An ML Pipeline for Iris Tracking</b><br />The first step in the pipeline leverages our previous work on <a href=\"https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html\">3D Face Meshes</a>, which uses high-fidelity facial landmarks to generate a mesh of the approximate face geometry. From this mesh, we isolate the eye region in the original image for use in the iris tracking model. The problem is then divided into two parts: eye contour estimation and iris location. We designed a multi-task model consisting of a unified encoder with a separate component for each task, which allowed us to use task-specific training data.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-N494e9_yk00/XywzbwgHMgI/AAAAAAAAGUo/4rWZgcvMPaQVphDK6SSeDZp8-79REaIAwCLcBGAsYHQ/s546/image8.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"500\" data-original-width=\"546\" src=\"https://1.bp.blogspot.com/-N494e9_yk00/XywzbwgHMgI/AAAAAAAAGUo/4rWZgcvMPaQVphDK6SSeDZp8-79REaIAwCLcBGAsYHQ/s0/image8.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Examples of iris (blue) and eyelid (red) tracking.</td></tr></tbody></table><p>To train the model from the cropped eye region, we manually annotated ~50k images, representing a variety of illumination conditions and head poses from geographically diverse regions, as shown below. <br /></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-9lEqMQQGr9o/Xywz7ACbOQI/AAAAAAAAGU4/fOZBI8anLv8fZiVFkrA7Q_HSREf1hxJlACLcBGAsYHQ/s1200/EyeAnnotation.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"400\" data-original-width=\"1200\" src=\"https://1.bp.blogspot.com/-9lEqMQQGr9o/Xywz7ACbOQI/AAAAAAAAGU4/fOZBI8anLv8fZiVFkrA7Q_HSREf1hxJlACLcBGAsYHQ/s640/EyeAnnotation.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Eye region annotated with eyelid (red) and iris (blue) contours.</td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-SxYvxDLQSp4/Xyw0EvBSPhI/AAAAAAAAGU8/MDN_cwLNerUMpzXWikbx4qBbBT4yG7iTgCLcBGAsYHQ/s621/image7.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"175\" data-original-width=\"621\" src=\"https://1.bp.blogspot.com/-SxYvxDLQSp4/Xyw0EvBSPhI/AAAAAAAAGU8/MDN_cwLNerUMpzXWikbx4qBbBT4yG7iTgCLcBGAsYHQ/s0/image7.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Cropped eye regions form the input to the model, which predicts landmarks via separate components.</td></tr></tbody></table><p><b>Depth-from-Iris: Depth Estimation from a Single Image</b><br />Our iris-tracking model is able to determine the metric distance of a subject to the camera with less than 10% error, without requiring any specialized hardware. This is done by relying on the fact that the horizontal iris diameter of the human eye remains roughly constant at 11.7±0.5 mm across a wide population [<a href=\"https://onlinelibrary.wiley.com/doi/full/10.1111/cxo.12583\">1</a>, <a href=\"https://insights.ovid.com/crossref?an=00003226-200504000-00003\">2</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S0886335003007211\">3</a>, <a href=\"https://www.sciencedirect.com/science/article/pii/S2452232515000025\">4</a>], along with some simple geometric arguments. For illustration, consider a pinhole camera model projecting onto a sensor of square pixels. The distance to a subject can be estimated from facial landmarks by using the <a href=\"https://en.wikipedia.org/wiki/Focal_length\">focal length</a> of the camera, which can be obtained using camera capture APIs or directly from the <a href=\"https://en.wikipedia.org/wiki/Exif\">EXIF metadata</a> of a captured image, along with other camera intrinsic parameters. Given the focal length, the distance from the subject to the camera is directly proportional to the physical size of the subject’s eye, as visualized below.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-q5suTIBLMpY/Xyw0LhfLZOI/AAAAAAAAGVA/A5xO_Ybtws4UvChYYKJFOfwkiLxtrpzDQCLcBGAsYHQ/s1638/image5.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"1524\" data-original-width=\"1638\" height=\"476\" src=\"https://1.bp.blogspot.com/-q5suTIBLMpY/Xyw0LhfLZOI/AAAAAAAAGVA/A5xO_Ybtws4UvChYYKJFOfwkiLxtrpzDQCLcBGAsYHQ/w512-h476/image5.png\" width=\"512\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The distance of the subject (<em>d</em>) can be computed from the focal length (<em>f</em>) and the size of the iris using similar triangles.</td></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-5sT_B1DWTGg/Xyw0UM40atI/AAAAAAAAGVM/foqPr2XPszQYxAATdmORKjJ8BK9bu6DFgCLcBGAsYHQ/s398/image2.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"360\" data-original-width=\"398\" src=\"https://1.bp.blogspot.com/-5sT_B1DWTGg/Xyw0UM40atI/AAAAAAAAGVM/foqPr2XPszQYxAATdmORKjJ8BK9bu6DFgCLcBGAsYHQ/s0/image2.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left: </b>MediaPipe Iris predicting metric distance in cm on a Pixel 2 from iris tracking alone, without the use of a depth sensor. <b>Right: </b>Ground-truth depth.</td></tr></tbody></table><p>In order to quantify the accuracy of the method, we compared it to the depth sensor on an iPhone 11 by collecting front-facing, synchronized video and depth images on over 200 participants. We experimentally verified the error of the iPhone 11 depth sensor to be &lt; 2% for distances up to 2 meters, using a laser ranging device. Our evaluation shows that our approach for depth estimation using iris size has a mean relative error of 4.3% and standard deviation of 2.4%. We tested our approach on participants with and without eyeglasses (not accounting for contact lenses on participants) and found that eyeglasses increase the mean relative error slightly to 4.8% (standard deviation 3.1%). We did not test this approach on participants with any eye diseases (like <a href=\"https://en.wikipedia.org/wiki/Arcus_senilis\">arcus senilis</a> or <a href=\"https://en.wikipedia.org/wiki/Pannus\">pannus</a>). Considering MediaPipe Iris requires no specialized hardware, these results suggest it may be possible to obtain metric depth from a single image on devices with a wide range of cost-points. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-rAQyquFR03U/Xyw0oeMop-I/AAAAAAAAGVU/xisQmcE68PAMWyK23r3fUsqk20D3zuEZQCLcBGAsYHQ/s1176/DistanceEstimation.png\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"420\" data-original-width=\"1176\" src=\"https://1.bp.blogspot.com/-rAQyquFR03U/Xyw0oeMop-I/AAAAAAAAGVU/xisQmcE68PAMWyK23r3fUsqk20D3zuEZQCLcBGAsYHQ/s640/DistanceEstimation.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Histogram of estimation errors (<b>left</b>) and comparison of actual to estimated distance by iris (<b>right</b>).</td></tr></tbody></table><p><b>Release of MediaPipe Iris</b><br />We are releasing the iris and depth estimation models as a cross-platform MediaPipe pipeline that can run on desktop, mobile and the web. As described in our recent <a href=\"https://mediapipe.page.link/webdevblog\">Google Developer Blog post on MediaPipe on the web</a>, we leverage <a href=\"https://en.wikipedia.org/wiki/WebAssembly\">WebAssembly</a> and <a href=\"https://github.com/google/XNNPACK\">XNNPACK</a> to run our Iris ML pipeline locally in the browser, without any data being sent to the cloud.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-co_8stdJGNw/Xyw_QSggW1I/AAAAAAAAGVs/-OsEhVKP6Fgi5gBJGFVDHCYDi3VFhhYyACLcBGAsYHQ/s400/MediaPipeWASM.gif\" imageanchor=\"1\" style=\"display: block; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"400\" data-original-width=\"281\" src=\"https://1.bp.blogspot.com/-co_8stdJGNw/Xyw_QSggW1I/AAAAAAAAGVs/-OsEhVKP6Fgi5gBJGFVDHCYDi3VFhhYyACLcBGAsYHQ/s0/MediaPipeWASM.gif\" /></a></td><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-r-_CNdpVVOw/Xyw0xjn4rsI/AAAAAAAAGVc/6-dWyJbqd2caNcYtX8GxacyEWhGf_oISgCLcBGAsYHQ/s400/image11.gif\" style=\"display: block; margin-left: auto; margin-right: auto; padding: 1em 0px;\"><img border=\"0\" data-original-height=\"400\" data-original-width=\"277\" src=\"https://1.bp.blogspot.com/-r-_CNdpVVOw/Xyw0xjn4rsI/AAAAAAAAGVc/6-dWyJbqd2caNcYtX8GxacyEWhGf_oISgCLcBGAsYHQ/s0/image11.gif\" /></a></td></tr><tr></tr></tbody></table><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td class=\"tr-caption\" style=\"text-align: center;\">Using <a href=\"https://developers.googleblog.com/2020/01/mediapipe-on-web.html\">MediaPipe’s WASM</a> stack, you can run the models locally in your browser! <b>Left:</b> Iris tracking. <b>Right:</b> Depth from Iris computed just from a photo with EXIF data. Iris tracking can be tried out <a href=\"https://viz.mediapipe.dev/demo/iris_tracking\">here</a> and iris depth measurements <a href=\"https://viz.mediapipe.dev/demo/iris_depth\">here</a>.</td></tr></tbody></table><p><b>Future Directions</b><br />We plan to extend our MediaPipe Iris model with even more stable tracking for lower error and deploy it for accessibility use cases. We strongly believe in sharing code that enables reproducible research, rapid experimentation, and development of new ideas in different areas. In our <a href=\"https://solutions.mediapipe.dev/iris\">documentation</a> and the accompanying <a href=\"https://mediapipe.page.link/iris-mc\">Model Card</a>, we detail the intended uses, limitations and model fairness to ensure that use of these models aligns with <a href=\"https://www.blog.google/technology/ai/ai-principles/\">Google’s AI Principles</a>. Note, that any form of surveillance or identification is explicitly out of scope and not enabled by this technology. We hope that <a href=\"https://solutions.mediapipe.dev/iris\">providing this iris perception functionality</a> to the wider research and development community will result in an emergence of creative use cases, stimulating responsible new applications and new research avenues. </p><p>For more ML solutions from MediaPipe, please see our <a href=\"https://solutions.mediapipe.dev/\">solutions page</a> and <a href=\"https://developers.googleblog.com/search/label/MediaPipe\">Google Developer blog</a> for the latest updates. </p><p><b>Acknowledgements</b><br /><em>We would like to thank Artsiom Ablavatski, Andrei Tkachenka, Buck Bourdon, Ivan Grishchenko and Gregory Karpiak for support in model evaluation and data collection; Yury Kartynnik, Valentin Bazarevsky, Artsiom Ablavatski for developing the mesh technology; Aliaksandr Shyrokau and the annotation team for their diligence to data preparation; Vidhya Navalpakkam, Tomer Shekel, Kai Kohlhoff for their domain expertise, Fan Zhang, Esha Uboweja, Tyler Mullen, Michael Hays and Chuo-Ling Chang for help to integrate the model to MediaPipe; Matthias Grundmann, Florian Schroff and Ming Guang Yong for continuous help for building this technology.</em></p><p></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=T5ahr9GBor4:R7pRvwzOvgw:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/T5ahr9GBor4\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Thu, 06 Aug 2020 17:36:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-Lpgeiec1LTI/Xywye-E8YOI/AAAAAAAAGUU/bAytHYPS1wIHV_EXWWBAXIdnp3p1H0D1wCLcBGAsYHQ/w640-h252/image3.gif","linkMd5":"6177c6f6f007891265bd244d276b5967","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn98@2020_3/2020/08/24/14-07-34-023_56223dc13987983b.webp","destWidth":640,"destHeight":252,"sourceBytes":6764175,"destBytes":2192456,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-Lpgeiec1LTI/Xywye-E8YOI/AAAAAAAAGUU/bAytHYPS1wIHV_EXWWBAXIdnp3p1H0D1wCLcBGAsYHQ/w640-h252/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn98@2020_3/2020/08/24/14-07-34-023_56223dc13987983b.webp","https://1.bp.blogspot.com/-jF_HuzW-smE/Xywy_JurMuI/AAAAAAAAGUc/cMX_9GtWxM4RhOqMhURDYB831eDqJ0ZIwCLcBGAsYHQ/w586-h164/image13.gif":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn53@2020_3/2020/08/24/14-07-58-048_a4365a518099002e.webp","https://1.bp.blogspot.com/-GJxNLlVwxqI/XywzLGfARTI/AAAAAAAAGUg/zkvGnNLmHmwyb_nDQ4j46mi95wPRYyP7gCLcBGAsYHQ/s0/image12.gif":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn70@2020_1/2020/08/24/14-08-02-343_4d83820b77faa5ed.webp","https://1.bp.blogspot.com/-N494e9_yk00/XywzbwgHMgI/AAAAAAAAGUo/4rWZgcvMPaQVphDK6SSeDZp8-79REaIAwCLcBGAsYHQ/s0/image8.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn30@2020_1/2020/08/24/14-07-59-537_1fac569e67d27203.webp","https://1.bp.blogspot.com/-9lEqMQQGr9o/Xywz7ACbOQI/AAAAAAAAGU4/fOZBI8anLv8fZiVFkrA7Q_HSREf1hxJlACLcBGAsYHQ/s640/EyeAnnotation.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn82@2020_3/2020/08/24/14-07-55-678_f2f1ab1cba580337.webp","https://1.bp.blogspot.com/-SxYvxDLQSp4/Xyw0EvBSPhI/AAAAAAAAGU8/MDN_cwLNerUMpzXWikbx4qBbBT4yG7iTgCLcBGAsYHQ/s0/image7.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn81@2020_1/2020/08/24/14-07-56-138_a1eb06ab3e8ed74a.webp","https://1.bp.blogspot.com/-q5suTIBLMpY/Xyw0LhfLZOI/AAAAAAAAGVA/A5xO_Ybtws4UvChYYKJFOfwkiLxtrpzDQCLcBGAsYHQ/w512-h476/image5.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn62@2020_1/2020/08/24/14-07-57-680_897744e0fffd0d12.webp","https://1.bp.blogspot.com/-5sT_B1DWTGg/Xyw0UM40atI/AAAAAAAAGVM/foqPr2XPszQYxAATdmORKjJ8BK9bu6DFgCLcBGAsYHQ/s0/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn53@2020_5/2020/08/24/14-08-01-121_0b24e330a9583d93.webp","https://1.bp.blogspot.com/-rAQyquFR03U/Xyw0oeMop-I/AAAAAAAAGVU/xisQmcE68PAMWyK23r3fUsqk20D3zuEZQCLcBGAsYHQ/s640/DistanceEstimation.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn48@2020_3/2020/08/24/14-07-55-306_9da12be4b020ac47.webp","https://1.bp.blogspot.com/-co_8stdJGNw/Xyw_QSggW1I/AAAAAAAAGVs/-OsEhVKP6Fgi5gBJGFVDHCYDi3VFhhYyACLcBGAsYHQ/s0/MediaPipeWASM.gif":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn69@2020_2/2020/08/24/14-07-58-771_282f476a1c0d780f.webp","https://1.bp.blogspot.com/-r-_CNdpVVOw/Xyw0xjn4rsI/AAAAAAAAGVc/6-dWyJbqd2caNcYtX8GxacyEWhGf_oISgCLcBGAsYHQ/s0/image11.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn30@2020_6/2020/08/24/14-07-56-527_e8196c63772f5a67.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/T5ahr9GBor4":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn77@2020_1/2020/08/24/14-07-57-962_a9b2820039b730d1.webp"},"publishedOrCreatedDate":1598278048117},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"On-device Supermarket Product Recognition","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/uEq7NDB-AgY/on-device-supermarket-product.html","description":"<span class=\"byline-author\">Posted by Chao Chen, Software Engineer, Google Research</span>  <br /><br />One of the greatest challenges faced by users who are visually impaired is identifying packaged foods, both in a grocery store and also in their kitchen cupboard at home. This is because many foods share the same packaging, such as boxes, tins, bottles and jars, and only differ in the text and imagery printed on the label. However, the ubiquity of smart mobile devices provides an opportunity to address such challenges using machine learning (ML).<br /><br />In recent years, there have been significant improvements in the accuracy of on-device neural networks for various perception tasks. When coupled with the increased computing power in modern smartphones, it is now possible for many vision tasks to yield high performance while running entirely on a mobile device. The development of on-device models such as <a href=\"https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html\">MnasNet</a> and <a href=\"https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html\">MobileNets</a> (based on resource-aware architecture search) in combination with on-device indexing allows one to run a full computer vision system, such as labeled product recognition, entirely on-device, in real time.<br /><br />Leveraging developments such as these, we recently released <a href=\"https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.reveal\">Lookout</a>, an Android app that uses computer vision to make the physical world more accessible for users who are visually impaired. When the user aims their smartphone camera at the product, Lookout identifies it and speaks aloud the brand name and product size. To accomplish this, Lookout includes a supermarket product detection and recognition model with an on-device product index, along with <a href=\"https://mediapipe.dev/\">MediaPipe</a> object tracking and an optical character recognition model. The resulting architecture is efficient enough to run in real-time entirely on-device.<br /><br /><b>Why On-Device?</b><br />A completely on-device system has the benefit of being low latency and with no reliance on network connectivity. However, this means that for a product recognition system to be truly useful to the users, it must have a on-device database with good product coverage. These requirements drive the design of the datasets used by Lookout, which consist of two million popular products chosen dynamically according to the user’s geographic location.<br /><br /><b>Traditional Solutions</b><br />Product recognition using computer vision has traditionally been solved using local image features extracted by, for example, the <a href=\"https://en.wikipedia.org/wiki/Scale-invariant_feature_transform\">SIFT</a> algorithm. These non ML-based approaches provide fairly reliable matching but are storage intensive per index image (typically ranging from 10KB to 40KB per image) and are less robust to poor lighting and blur in images. Additionally, the local nature of these descriptors means that it typically does not capture more global aspects of the product’s appearance.<br /><br />An alternative approach that has a number of advantages would be to use ML and run an <a href=\"https://en.wikipedia.org/wiki/Optical_character_recognition\">optical character recognition</a> (OCR) system over the query image and database images to extract the text present on the product packaging. The text on the query image can be matched to the database using <a href=\"https://en.wikipedia.org/wiki/N-gram\">N-Grams</a> to be robust to OCR errors such as spelling mistakes, misrecognitions, failed recognition of words on product packaging. N-Grams can also allow for partial match between query document and index document using measures such as <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">Jaccard similarity coefficient</a>, as opposed to requiring an exact match. However, with OCR, the index document size can grow very large since one would  need to store N-Grams for product packaging text along with other signals like <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">TF-IDF</a>. Furthermore, the reliability of the matches is a concern with the OCR+N-Gram approach since it can easily over trigger in situations where there are a lot of common words present on the packaging of two different products.<br /><br />In contrast to both the SIFT and OCR+N-Gram methods, our neural network-based approach, which generates a global descriptor (i.e., an embedding) for each image, requires only 64 bytes, significantly reducing the storage requirements from the 10-40KB per image needed for each SIFT feature index entry,  or the few KBs per image for the less reliable OCR+N-gram approach. With fewer bytes consumed for each index image, more products can be included as a part of the index, yielding more complete product coverage and a better overall user experience.<br /><br /><b>Design</b><br />The Lookout system consists of a frame cache, frame selector, detector, object tracker, embedder, index searcher, OCR, scorer and result presenter.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-cdc1x9OZGbA/XyHa7C04FGI/AAAAAAAAGR8/3JEtp7s9Qj8U5wvsBJACaAcdvPSM-tW1gCLcBGAsYHQ/s1600/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1037\" data-original-width=\"1318\" height=\"502\" src=\"https://1.bp.blogspot.com/-cdc1x9OZGbA/XyHa7C04FGI/AAAAAAAAGR8/3JEtp7s9Qj8U5wvsBJACaAcdvPSM-tW1gCLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Product recognition pipeline internal architecture.</td></tr></tbody></table><ul><li><em>Frame cache</em><br />The frame cache manages the lifecycle of the input camera frames in the pipeline. It efficiently delivers the data, including YUV/RGB/gray images, as requested by the other model components and manages the data life cycle to avoid duplicated conversions for the same camera frame requested by multiple components.   </li><li><em>Frame selector</em><br />When a user points the camera viewfinder towards a product, a lightweight <a href=\"https://en.wikipedia.org/wiki/Inertial_measurement_unit\">IMU</a>-based frame selector is run as a prefiltering stage. It selects the frames that best match a certain quality criterion (e.g., balanced image quality and latency) from the continuously incoming image stream, based on the jitter as measured by the angular rotation rate (deg/sec). This approach minimizes energy consumption by selectively processing only the high quality image frames and skipping the blurry frames.   </li><li><em>Detector</em><br />Each selected frame is then passed to a product detector model, which proposes regions of interest(a.k.a. Detection bounding boxes) in the frames. The detector model architecture is a single-shot detector with an <a href=\"https://arxiv.org/abs/1807.11626\">MnasNet</a> backbone that strikes a balance between high quality and low latency.   </li><li><em>Object tracker</em><br /><a href=\"https://ai.googleblog.com/2018/02/the-instant-motion-tracking-behind.html\">MediaPipe Box tracking</a> is used to track the detected box in real-time, and plays an important role in filling the gap between the detection of different objects and reducing the detection frequency, thus reducing energy consumption. The object tracker also maintains an object map in which each object is assigned a unique object ID during runtime, which are later used by the <em>result presenter </em>to differentiate between objects and to avoid repeating the announcement of a single object. For each detection result, the tracker either registers a new object in the map or updates an existing object with the detection bounding box, using the <a href=\"https://en.wikipedia.org/wiki/Jaccard_index\">Intersection over Union</a> (IoU) between existing object bounding boxes with the detection result.   </li><li><em>Embedder</em><br />The regions of interest (ROIs) from the detector are sent to the embedder model, which then computes a 64-dimension embedding. The embedder model is initially trained from a large classification model (i.e., the <em>teacher</em> model, based on <a href=\"https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html\">NASNet</a>), which spans tens of thousands of classes. An embedding layer is added in the model to project the input image into an ‘embedding space’, i.e., a vector space where two points being close means that the images they represent are visually similar (e.g., two images show the same product). Analyzing only the embeddings ensures that the model is flexible and does not need to be retrained every time it is to be expanded to new products. However, because the teacher model is too large to be used directly on-device, the embeddings it generates are used to train a smaller, mobile-friendly <em>student</em> model that learns to map the input images to the same points in the embedding space as the teacher network. Finally, we apply <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">principal component analysis</a> (PCA) to reduce the dimensionality of the embedding vectors from 256 to 64, streamlining the embeddings for storing on-device.   </li><li><em>Index searcher</em><br />The index searcher performs <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">KNN search</a> over a pre-built, compatible <a href=\"https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html\">ScaNN</a> index using a query embedding. As a result, it returns the top-ranked index documents containing their metadata, such as product names, packaging size, etc. To reduce the index lookup latency, all embeddings are <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">k-means</a> clustered into clusters. At query time, the relevant clusters of data are loaded in memory for the actual distance computation. To reduce the index size without sacrificing quality, we use <a href=\"https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf\">product quantization</a> at indexing time.   </li><li><em>OCR</em><br />OCR is executed on the ROI for each camera frame in order to extract additional information, such as packet size, product flavor variant, etc. Whereas traditional solutions used the OCR result for index searching, here we only use it for scoring. A proper scoring algorithm informed by the OCR text assists the scorer (below) in determining the correct result and improves the precision, especially in the case where multiple products have similar packages.   </li><li><em>Scorer</em><br />The scorer takes the input from the embeddings (with index results) and the OCR module and scores each of the previously retrieved index documents (embeddings and metadata retrieved via the index searcher). The top result after scoring is used as the final recognition from the system.   </li><li><em>Result presenter</em><br />Result presenter takes in all the results above, and surfaces the results to users by speaking the product name via text-to-speech service.</li></ul><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-evIOqC7twgY/XyHbBfrwVeI/AAAAAAAAGSA/6fO0Q1txQvAkbhTtqc2o_-idvrLS9QnwgCLcBGAsYHQ/s1600/image2.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"658\" data-original-width=\"320\" height=\"400\" src=\"https://1.bp.blogspot.com/-evIOqC7twgY/XyHbBfrwVeI/AAAAAAAAGSA/6fO0Q1txQvAkbhTtqc2o_-idvrLS9QnwgCLcBGAsYHQ/s400/image2.gif\" width=\"193\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Early experiments with on-device product recognition in a Swiss supermarket.</td></tr></tbody></table><b>Conclusion/Future Work</b><br />The on-device system outlined here can be used to enable a spectrum of new in-store experiences, including the display of detailed product information (nutritional facts, allergens, etc.), customer ratings, product comparisons, smart shopping lists, price tracking, and more. We are excited to explore some of these future applications, while continuing research into advancing the quality and robustness of the underlying on-device models.<br /><br /><b>Acknowledgements</b><br /><em>The work described here was authored by Abhanshu Sharma, Chao Chen, Lukas Mach, Matt Sharifi, Matteo Agosti, Sasa Petrovic and Tom Binder. This work wouldn’t have been possible without the support and help we received from Alec Go, Alessandro Bissacco, Cédric Deltheil, Eunyoung Kim, Haoran Qi, Jeff Gilbert and Mingxing Tan.</em><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=uEq7NDB-AgY:KqkHOYduzkE:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/uEq7NDB-AgY\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 11 Aug 2020 18:10:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-cdc1x9OZGbA/XyHa7C04FGI/AAAAAAAAGR8/3JEtp7s9Qj8U5wvsBJACaAcdvPSM-tW1gCLcBGAsYHQ/s640/image1.png","linkMd5":"32fb0c3b0fecf8789d62476c3497c382","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn9@2020_5/2020/08/24/14-07-28-358_7f02eb85257eb3e8.webp","destWidth":640,"destHeight":504,"sourceBytes":76093,"destBytes":13128,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-cdc1x9OZGbA/XyHa7C04FGI/AAAAAAAAGR8/3JEtp7s9Qj8U5wvsBJACaAcdvPSM-tW1gCLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn9@2020_5/2020/08/24/14-07-28-358_7f02eb85257eb3e8.webp","https://1.bp.blogspot.com/-evIOqC7twgY/XyHbBfrwVeI/AAAAAAAAGSA/6fO0Q1txQvAkbhTtqc2o_-idvrLS9QnwgCLcBGAsYHQ/s400/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn57@2020_6/2020/08/24/14-07-58-988_9d8f63e428110bb6.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/uEq7NDB-AgY":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn74@2020_2/2020/08/24/14-07-56-878_25fa6336e1f9ce49.webp"},"publishedOrCreatedDate":1598278048091},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"REALM: Integrating Retrieval into Language Representation Models","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","description":"<span class=\"byline-author\">Posted by Ming-Wei Chang and Kelvin Guu, Research Scientists, Google Research</span> <p>Recent advances in natural language processing have largely built upon the power of <em>unsupervised pre-training</em>, which trains general purpose language representation models using a large amount of text, without human annotations or labels. These pre-trained models, such as <a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">BERT</a> and <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a>, have been shown to <em><a href=\"https://arxiv.org/pdf/1909.01066.pdf\">memorize a surprising amount of world knowledge</a></em>, such as “the birthplace of <a href=\"https://en.wikipedia.org/wiki/Francesco_Bartolomeo_Conti\">Francesco Bartolomeo Conti</a>”, “the developer of <a href=\"https://en.wikipedia.org/wiki/Java_Development_Kit\">JDK</a>” and “the owner of <a href=\"https://en.wikipedia.org/wiki/ITV_Border\">Border TV</a>”. While the ability to encode knowledge is especially important for certain natural language processing tasks such as question answering, information retrieval and text generation, these models memorize knowledge <em>implicitly</em> — i.e., world knowledge is captured in an abstract way in the model weights — making it difficult to determine what knowledge has been stored and where it is kept in the model. Furthermore, the storage space, and hence the accuracy of the model, is limited by the size of the network. To capture more world knowledge, the standard practice is to train ever-larger networks, which can be prohibitively slow or expensive.  </p><p>Instead, what if there was a method for pre-training that could access knowledge <i>explicitly</i>, e.g., by referencing an additional large external text corpus, in order to achieve accurate results without increasing the model size or complexity?&nbsp; For example, a sentence found in an external document collection, \"Francesco Bartolomeo Conti was born in Florence,\" could be referenced by the model to determine the birthplace of the musician, rather than relying on the model's opaque ability to access the knowledge stored in its own parameters. The ability to retrieve text containing explicit knowledge such as this would improve the efficiency of pre-training while enabling the model to perform well on knowledge-intensive tasks without using billions of parameters.</p><p>In “<a href=\"https://arxiv.org/abs/2002.08909\">REALM: Retrieval-Augmented Language Model Pre-Training</a>”, accepted at the <em><a href=\"https://icml.cc/Conferences/2020\">2020 International Conference on Machine Learning</a></em>, we share a novel paradigm for language model pre-training, which augments a language representation model with a <em>knowledge retriever</em>, allowing REALM models to retrieve textual world knowledge <em>explicitly</em> from raw text documents, instead of memorizing all the knowledge in the model parameters. We have also open sourced the <a href=\"https://github.com/google-research/language/tree/master/language/realm\">REALM codebase</a> to demonstrate how one can train the retriever and the language representation jointly. </p><p><b>Background: Pre-training Language Representation Models</b><br>To understand how standard language representation models memorize world knowledge, one should first review how these models are pre-trained. Since the invention of <a href=\"https://en.wikipedia.org/wiki/BERT_(language_model)\">BERT</a>, the fill-in-the-blank task, called <a href=\"http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">masked language modeling</a>, has been widely used for pre-training language representation models.  Given any text with certain words masked out, the task is to fill back the missing words. An example of this task looks like: </p>   <code>I am so thirsty. I need to __ water.</code>  <p>During pre-training, a model will go over a large number of examples and adjust the parameters in order to predict the missing words (<code>answer: <em>drink</em></code>, in the above example). Interestingly, the fill-in-the-blank task makes the model memorize certain facts about the world. For example, the knowledge of Einstein's birthplace is required to fill the missing word in the following example:  </p><p><code>Einstein was a __-born scientist. </code>(<code>answer: <em>German</em></code>)  </p><p>However, because the world knowledge captured by the model is stored in the model weights, it is abstract, making it difficult to understand what information is stored.  </p><p><b>Our Proposal: Retrieval-Augmented Language Representation Model Pre-training</b><br>In contrast to standard language representation models, REALM augments the language representation model with a <em>knowledge retriever</em> that first retrieves another piece of text from an external document collection as the supporting knowledge — in our experiments, we use the <a href=\"https://archive.org/details/wikimediadownloads\">Wikipedia text corpus</a> — and then feeds this supporting text as well as the original text into a language representation model. </p><p>The key intuition of REALM is that a retrieval system should improve the model's ability to fill in missing words. Therefore, a retrieval that provides more context for filling the missing words should be rewarded. If the retrieved information does not help the model make its predictions, it should be discouraged, making room for better retrievals. </p><p>How does one train a knowledge retriever, given that only unlabeled text is available during pre-training? It turns out that one can use the task of filling words to train the knowledge retriever indirectly, without any human annotations. Assume the input of the query is: </p>   <code>We paid twenty __ at the Buckingham Palace gift shop.</code>  <p>Filling the missing word (<code>answer:<em>pounds</em></code>) in this sentence without retrieval can be tricky, as the model would need to have implicitly stored knowledge of the country in which the Buckingham Palace is located and the associated currency, as well as make the connection between the two. It would be easier for the model to fill in the missing word if it was presented with a passage that explicitly connects some of the necessary knowledge, retrieved from an external corpus.   </p><p> In this example, the retriever would be rewarded for retrieving the following sentence. </p>   <code>Buckingham Palace is the London residence of the British monarchy.</code>  <p>Since the retrieval step needs to add more context, there may be multiple retrieval targets that could be helpful in filling the missing word, for example, “<code>The official currency of the United Kingdom is the Pound.</code>”  The whole process is demonstrated in the next figure: </p><p>  </p><p id=\"gdcalert1\"></p><p></p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s1600/image1.gif\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"655\" data-original-width=\"1600\" src=\"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s640/image1.gif\" width=\"640\" /></a></div><p><b>Computational Challenges for REALM</b><br>Scaling REALM pre-training such that models can retrieve knowledge from millions of documents is challenging. In REALM, the selection of the best document is formulated as <a href=\"https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf\">maximum inner product search</a> (MIPS). To perform retrieval, MIPS models need to first encode all of the documents in the collection, such that each document has a corresponding document vector. When an input arrives, it is encoded as a query vector. In MIPS, given a query, the document in the collection that has the maximum inner product value between its document vector and the query vector is retrieved, as shown in the following figure: </p><p>  </p><p id=\"gdcalert2\"></p><p></p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-OuZB6r1b-pc/XzRPJYOMNsI/AAAAAAAAGWw/qBKVlzIudt4itmvTrYBtOSJaR1N1MB4OgCLcBGAsYHQ/s1123/image2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"600\" data-original-width=\"1123\" src=\"https://1.bp.blogspot.com/-OuZB6r1b-pc/XzRPJYOMNsI/AAAAAAAAGWw/qBKVlzIudt4itmvTrYBtOSJaR1N1MB4OgCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></div>In REALM, we use the <a href=\"https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html\">ScaNN</a> package to conduct MIPS efficiently, which makes finding the maximum inner product value relatively cheap, given that the document vectors are pre-computed. However, if the model parameters were updated during training, it is typically necessary to re-encode the document vectors for the entire collection of documents. To address the computational challenges, we structure the retriever so that the computation performed for each document can be cached and asynchronously updated. We also found that updating document vectors every 500 training steps, instead of every step, is able to achieve good performance and make training tractable. <p></p><p><b>Applying REALM to Open-domain Question Answering</b><br>We evaluate the effectiveness of REALM by applying it to <a href=\"https://en.wikipedia.org/wiki/Question_answering#Open_domain_question_answering\">open-domain question answering</a> (Open-QA), one of the most knowledge-intensive tasks in natural language processing. The goal of the task is to answer questions, such as “What is the angle of the equilateral triangle?”  </p><p>In standard question answering tasks (e.g.,  <a href=\"https://arxiv.org/abs/1606.05250\">SQuAD</a> or <a href=\"https://ai.google.com/research/NaturalQuestions/\">Natural Questions</a>), the supporting document is provided as part of input, so a model only needs to look up the answer in the given document. In Open-QA, there are no given documents, so that Open-QA models need to look up the knowledge by themselves — this makes Open-QA an excellent task to examine the effectiveness of REALM. </p><p>The following figure shows the results on the OpenQA version of <a href=\"https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html\">Natural Question</a>. We mainly compared our results with <a href=\"http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\">T5</a>, another approach that trains models without annotated supporting documents. From the figure, one can clearly see that REALM pre-training generates very powerful Open-QA models, and even outperforms the much larger T5 (11B) model by almost 4 points, using only a fraction of the parameters (300M). </p><p>  </p><p id=\"gdcalert3\"></p><p></p><p></p><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https://1.bp.blogspot.com/-BEw65BexITY/XzRPPdN3lfI/AAAAAAAAGW4/bu583zs7aHEPzR_zcsy5HkV0s4bLqRUfwCLcBGAsYHQ/s1086/image3.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"952\" data-original-width=\"1086\" height=\"359\" src=\"https://1.bp.blogspot.com/-BEw65BexITY/XzRPPdN3lfI/AAAAAAAAGW4/bu583zs7aHEPzR_zcsy5HkV0s4bLqRUfwCLcBGAsYHQ/w410-h359/image3.png\" width=\"410\" /></a></div><p><b>Conclusion</b><br>The release of REALM has helped drive interest in developing end-to-end retrieval-augmented models, including a recent <a href=\"https://arxiv.org/abs/2005.11401\">retrieval-augmented generative model</a>. We look forward to the possibility of extending this line of work in several ways, including 1) applying REALM-like methods to new applications that require knowledge-intensive reasoning and interpretable provenance (beyond Open-QA), and 2) exploring the benefits of retrieving other forms of knowledge, such as images, knowledge graph structures, or even text in other languages. We are also excited to see what the research community does with the open source <a href=\"https://github.com/google-research/language/tree/master/language/realm\">REALM codebase</a>! </p><p><b>Acknowledgements</b><br><em>This work has been a collaborative effort involving Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.</em></p><p></p><p></p><p></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=FjfQTAdG8E8:YUw16ZmmgRI:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/FjfQTAdG8E8\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 12 Aug 2020 21:18:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s72-c/image1.gif","linkMd5":"744d32fc2907f7da5c39c5fc5aaed997","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn5@2020_5/2020/08/24/14-07-59-709_d3cf3d57c9ed09b3.webp","destWidth":72,"destHeight":72,"sourceBytes":41224,"destBytes":16028,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s640/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn69@2020_3/2020/08/24/14-08-04-982_bdfeb2aecf2f5b66.webp","https://1.bp.blogspot.com/-OuZB6r1b-pc/XzRPJYOMNsI/AAAAAAAAGWw/qBKVlzIudt4itmvTrYBtOSJaR1N1MB4OgCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn69@2020_2/2020/08/24/14-07-55-424_3d533ba1974a881e.webp","https://1.bp.blogspot.com/-BEw65BexITY/XzRPPdN3lfI/AAAAAAAAGW4/bu583zs7aHEPzR_zcsy5HkV0s4bLqRUfwCLcBGAsYHQ/w410-h359/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn89@2020_4/2020/08/24/14-07-58-030_649e3e56985b6733.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/FjfQTAdG8E8":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn18@2020_4/2020/08/24/14-07-55-335_3d79b8706294ef84.webp"},"publishedOrCreatedDate":1598278048083},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"A Simulation Suite for Tackling Applied Reinforcement Learning Challenges","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","description":"<span class=\"byline-author\">Posted by Daniel J. Mankowitz, Research Scientist, DeepMind and Gabriel Dulac-Arnold, Research Scientist, Google Research</span> <p><a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">Reinforcement Learning</a> (RL) has proven to be effective in solving numerous complex problems ranging from <a href=\"https://deepmind.com/research/case-studies/alphago-the-story-so-far\">Go,</a> <a href=\"https://deepmind.com/research/publications/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\">StarCraft</a> and <a href=\"https://arxiv.org/abs/1604.07255\">Minecraft</a> to <a href=\"https://ai.googleblog.com/2020/05/agile-and-intelligent-locomotion-via.html\">robot locomotion</a> and <a href=\"https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html\">chip design</a>. In each of these cases, a simulator is available or the real environment is quick and inexpensive to access. Yet, there are still considerable challenges to deploying RL to real-world products and systems. For example, in physical control systems, such as robotics and autonomous driving, RL controllers are trained to solve tasks like grasping objects or driving on a highway. These controllers are susceptible to effects such as sensor noise, system delays, or normal wear-and-tear that can reduce the quality of input to the controller, leading to incorrect decision-making and potentially catastrophic failures.  </p><p>  </p><p id=\"gdcalert1\"></p><p></p><p></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-vgePiImUoOU/XzLQFtSR7bI/AAAAAAAAGV8/G6qugs86Y_AuBOKlB3VRkRE0AaQbNF_CwCLcBGAsYHQ/s1200/image1.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"900\" data-original-width=\"1200\" height=\"384\" src=\"https://1.bp.blogspot.com/-vgePiImUoOU/XzLQFtSR7bI/AAAAAAAAGV8/G6qugs86Y_AuBOKlB3VRkRE0AaQbNF_CwCLcBGAsYHQ/w513-h384/image1.png\" width=\"513\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A physical control system: Robots learning how to grasp and sort objects using RL at the <a href=\"https://x.company/projects/everyday-robots/\">Everyday Robot Project</a> at <a href=\"https://x.company/\">X</a>. These types of systems are subject to many of the real-world challenges detailed here.<br /></td></tr></tbody></table><p></p><p>In “<a href=\"https://arxiv.org/abs/1904.12901\">Challenges of Real-World Reinforcement Learning</a>”, we identify and discuss nine different challenges that hinder the application of current RL algorithms to applied systems. We then follow up this work with an <a href=\"https://arxiv.org/abs/2003.11881\">empirical investigation</a> in which we simulated versions of these challenges on state-of-the-art RL algorithms, and benchmark the effects of each. We have open-sourced these simulated challenges in the <a href=\"https://github.com/google-research/realworldrl_suite\">Real-World RL</a> (RWRL) task suite to help draw attention to these important issues, as well as accelerate research toward solving them.  </p><p><b>The RWRL Suite </b><br>The RWRL suite is a set of simulated tasks inspired by applied reinforcement learning challenges, the goal of which is to enable fast algorithmic iterations for both researchers and practitioners, without having to run slow, expensive experiments on real-systems. While there will be additional challenges transitioning from RL algorithms that were trained in simulation to real-world applications, this suite intends to close some of the more fundamental, algorithmic gaps. At present, RWRL supports a subset of the <a href=\"https://github.com/deepmind/dm_control\">DeepMind Control Suite</a> domains, but the goal is to broaden the suite to support an even more diverse domain set. </p><p><b>Easy-to-Use &amp; Flexible</b><br>We designed the suite with two main goals in mind. (1) It should be easy to use — a user should be able to start running experiments within minutes of downloading the suite, simply by changing a few lines of code. (2) It should be flexible — a user should be able to incorporate any combination of challenges into the environment with very little effort. </p><p><b>A Delayed Action Example</b><br>To illustrate the ease of use of the RWRL suite, imagine a researcher or practitioner wants to implement action delays (i.e., temporal delays on actions being sent to the environment). To use the RWRL suite, simply import the <code>rwrl</code> module. Next, load an environment (e.g., cartpole) with the <code>delay_spec</code> argument. This optional argument is specified as a dictionary configuring delay applied to actions, observations, or rewards and the number of timesteps the corresponding element is delayed (e.g., 20 timesteps). Once the environment is loaded, the effects of actions are automatically delayed without any other changes to the experiment. This makes it easy to test an RL algorithm with action delays in a range of different environments supported by the RWRL suite.</p><p id=\"gdcalert2\"></p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-H8zldR24sTQ/XzLQ2wYcz0I/AAAAAAAAGWI/xvxV8GxLdM8Tx852QR5oK3CxgBuEQgfKgCLcBGAsYHQ/s1092/RWRL_Overview.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"386\" data-original-width=\"1092\" src=\"https://1.bp.blogspot.com/-H8zldR24sTQ/XzLQ2wYcz0I/AAAAAAAAGWI/xvxV8GxLdM8Tx852QR5oK3CxgBuEQgfKgCLcBGAsYHQ/s640/RWRL_Overview.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A high-level overview of the RWRL suite. Add a challenge (e.g., action delays) into the environment with a few lines of code, run a hyperparameter sweep and produce a graph shown on the right<br /></td></tr></tbody></table><p></p><p>A user can combine different challenges or choose from a set of predefined benchmark challenges by simply adding additional arguments to the load function, all of which are specified in the open-source RWRL suite <a href=\"https://github.com/google-research/realworldrl_suite\">codebase</a>.  </p><p><b>Supported Challenges</b><br>The RWRL suite provides functionality to support experiments related to eight of the nine different challenges that make applying current RL algorithms on applied systems difficult: sample efficiency; system delays; high-dimensional state and action spaces; constraints; partial observability, stochasticity and non-stationarity; multiple objectives; real-time inference; and training from offline logs. RWRL excludes the explainability challenge, which is abstract and non-trivial to define. The supported experiments are non-exhaustive and provide researchers and practitioners with the ability to analyze the capabilities of their agent with respect to each challenge dimension. Examples of the supported challenges include: </p><ul><li><em>System Delays</em><br>Most real systems have delays in either sensing, actuation or reward feedback, all of which can be configured and applied to any task within the RWRL suite.The graphs below show the performance of a <a href=\"https://arxiv.org/pdf/1804.08617.pdf\">D4PG</a> agent as actions (left), observations (middle) and rewards (right) are increasingly delayed.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-F8KvPhGskK8/XzLRFouAnMI/AAAAAAAAGWM/mMr9kw3WWn03kFDJzT3WkoP3m7QTJ-p-QCLcBGAsYHQ/s1999/image2.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"479\" data-original-width=\"1999\" src=\"https://1.bp.blogspot.com/-F8KvPhGskK8/XzLRFouAnMI/AAAAAAAAGWM/mMr9kw3WWn03kFDJzT3WkoP3m7QTJ-p-QCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The effect of increasing the action (left), observation (middle) and reward (right) delays respectively on a state-of-the art RL agent in four <a href=\"http://www.mujoco.org/\">MuJoCo</a> domains.<br /></td></tr></tbody></table>   <p>As can be seen in the graphs, a researcher or practitioner can quickly gain insights as to which type of delay affects their agent’s performance. These delays can also be combined together to observe their combined effect. </p></li><li><em>Constraints</em><br>Almost all applied systems have some form of constraints embedded into the overall objective, which is not common in most RL environments. The RWRL suite implements a series of constraints for each task, with varying difficulties, to facilitate research in constrained RL.  An example of a complex local angular velocity constraint being violated is visualized in the video below. <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-G5xcb5HLe5w/XzLRQiK5UXI/AAAAAAAAGWU/QF78i71-hpQuvyRuyoDpBcFAjMjtkC5zACLcBGAsYHQ/s640/image4.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"480\" data-original-width=\"640\" height=\"308\" src=\"https://1.bp.blogspot.com/-G5xcb5HLe5w/XzLRQiK5UXI/AAAAAAAAGWU/QF78i71-hpQuvyRuyoDpBcFAjMjtkC5zACLcBGAsYHQ/w410-h308/image4.gif\" width=\"410\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An example of constraint violations for cartpole. The red screen indicates that a violation has occurred on localized angular velocity.<br /></td></tr></tbody></table>  </li><li><em>Non-Stationarity</em><br>The user can introduce non-stationarity by perturbing environment parameters. These perturbations are in contrast to the pixel level adversarial perturbations that have recently gained popularity in research on supervised deep learning. For example, in the human walker domain, the size of the head and friction of the ground can be modified throughout training to simulate changing conditions. A variety of schedulers are available in the RWRL suite (see our <a href=\"https://github.com/google-research/realworldrl_suite/blob/f7143e830a0ef915457cef9de48abae6238c0d2d/realworldrl_suite/environments/realworld_env.py#L25\">codebase</a> for more details), along with multiple default parameter perturbations, which were carefully defined to handicap the learning capabilities of state-of-the-art learning algorithms. <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-olx2WMIKhek/XzLRayW8BOI/AAAAAAAAGWc/jhlCkKyW2SoxSnGy8kmtfx3m1SfzjNjuQCLcBGAsYHQ/s960/image3.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"240\" data-original-width=\"960\" src=\"https://1.bp.blogspot.com/-olx2WMIKhek/XzLRayW8BOI/AAAAAAAAGWc/jhlCkKyW2SoxSnGy8kmtfx3m1SfzjNjuQCLcBGAsYHQ/s640/image3.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Non-stationary perturbations. The suite supports perturbing environment parameters across episodes such as changing head size (<b>center</b>) and contact friction (<b>right</b>).<br /></td></tr></tbody></table>  </li><li><em>Training from Offline Log Data </em><br>In most applied systems, it is both slow and expensive to run experiments. There are often logs of data available from previous experiments that can be utilized to train a policy. However, it is often difficult to outperform the previous model in production due to the data being limited, of low variance, or of poor quality. To address this, we have generated offline datasets of the combined RWRL benchmark challenges, which we made available as part of a wider <a href=\"https://arxiv.org/abs/2006.13888\">offline dataset release</a>.  More information can be found in this <a href=\"https://github.com/deepmind/deepmind-research/blob/master/rl_unplugged/rwrl_d4pg.ipynb\">notebook</a>. </li></ul><p>  <b>Conclusion</b><br>Most systems rarely manifest only a single challenge, and we are excited to see how algorithms can deal with an environment in which there are multiple challenges combined with increasing levels of difficulty (‘Easy’, ‘Medium’ and ‘Hard’). We highly encourage the research community to try and solve these challenges, as we believe that solving them will facilitate more widespread applications of RL to products and real-world systems.  </p><p>While the initial set of RWRL suite features and experiments provide a starting point for closing the gap between the current state of RL and the challenges of applied systems, there is still much work to do. The supported experiments are not exhaustive and we welcome new ideas from the wider community to better evaluate the capabilities of our RL agents. Our main goal with this suite is to highlight and encourage research on the core problems that limit the effectiveness of RL algorithms in applied products and systems and to accelerate progress towards enabling future RL applications. </p><p>  <b>Acknowledgements</b><br><em>We would like to thank our core contributor and co-author Nir Levine for his invaluable help. We would also like to thank our co-authors Jerry Li, Sven Gowal, Todd Hester and Cosmin Paduraru as well as Robert Dadashi, the ACME team, Dan A. Calian, Juliet Rothenberg and Timothy Mann for their contributions.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=fVhMJKABb9s:gCxGG6kB-mc:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fVhMJKABb9s\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 12 Aug 2020 17:33:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-vgePiImUoOU/XzLQFtSR7bI/AAAAAAAAGV8/G6qugs86Y_AuBOKlB3VRkRE0AaQbNF_CwCLcBGAsYHQ/w513-h384/image1.png","linkMd5":"e78d187929399261dd59b6fd6be7b186","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn79@2020_5/2020/08/24/14-07-28-356_ffa833e18d92ff51.webp","destWidth":512,"destHeight":384,"sourceBytes":371836,"destBytes":39170,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-vgePiImUoOU/XzLQFtSR7bI/AAAAAAAAGV8/G6qugs86Y_AuBOKlB3VRkRE0AaQbNF_CwCLcBGAsYHQ/w513-h384/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn79@2020_5/2020/08/24/14-07-28-356_ffa833e18d92ff51.webp","https://1.bp.blogspot.com/-H8zldR24sTQ/XzLQ2wYcz0I/AAAAAAAAGWI/xvxV8GxLdM8Tx852QR5oK3CxgBuEQgfKgCLcBGAsYHQ/s640/RWRL_Overview.gif":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn98@2020_1/2020/08/24/14-07-59-469_317bc5565fbad83b.webp","https://1.bp.blogspot.com/-F8KvPhGskK8/XzLRFouAnMI/AAAAAAAAGWM/mMr9kw3WWn03kFDJzT3WkoP3m7QTJ-p-QCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn82@2020_5/2020/08/24/14-07-59-114_d034b4310d17ed4c.webp","https://1.bp.blogspot.com/-G5xcb5HLe5w/XzLRQiK5UXI/AAAAAAAAGWU/QF78i71-hpQuvyRuyoDpBcFAjMjtkC5zACLcBGAsYHQ/w410-h308/image4.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn57@2020_1/2020/08/24/14-08-03-387_b496f357e876f99c.webp","https://1.bp.blogspot.com/-olx2WMIKhek/XzLRayW8BOI/AAAAAAAAGWc/jhlCkKyW2SoxSnGy8kmtfx3m1SfzjNjuQCLcBGAsYHQ/s640/image3.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn94@2020_5/2020/08/24/14-08-01-702_543bc053de1b8ed8.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fVhMJKABb9s":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn65@2020_4/2020/08/24/14-07-57-823_8900986825bb1376.webp"},"publishedOrCreatedDate":1598278048085},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Sensing Force-Based Gestures on the Pixel 4","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","description":"<span class=\"byline-author\">Posted by Philip Quinn and Wenxin Feng, Research Scientists, Android UX</span>  <br /><br />Touch input has traditionally focussed on two-dimensional finger pointing. Beyond tapping and swiping gestures, long pressing has been the main alternative path for interaction. However, a long press is sensed with a time-based threshold where a user’s finger must remain stationary for 400–500 ms. By its nature, a time-based threshold has negative effects for usability and discoverability as the lack of immediate feedback disconnects the user’s action from the system’s response. Fortunately, fingers are dynamic input devices that can express more than just location: when a user touches a surface, their finger can also express some level of force, which can be used as an alternative to a time-based threshold.<br /><br />While a variety of force-based interactions have been pursued, sensing touch force requires dedicated hardware sensors that are expensive to design and integrate. Further, research indicates that touch force is difficult for people to control, and so most practical force-based interactions focus on discrete levels of force (e.g., a <em>soft</em> vs. <em>firm</em> touch) — which do not require the full capabilities of a hardware force sensor.<br /><br />For a <a href=\"https://www.blog.google/products/pixel/more-pixel-features-dropping/\">recent update to the Pixel 4</a>, we developed a method for sensing force gestures that allowed us to deliver a more expressive touch interaction experience  By studying how the human finger interacts with touch sensors, we designed the experience to complement and support the long-press interactions that apps already have, but with a more natural gesture. In this post we describe the core principles of touch sensing and finger interaction, how we designed a machine learning algorithm to recognise press gestures from touch sensor data, and how we integrated it into the user experience for Pixel devices.<br /><br /><b>Touch Sensor Technology and Finger Biomechanics</b><br />A capacitive touch sensor is constructed from two conductive electrodes (a <em>drive electrode</em> and a <em>sense electrode</em>) that are separated by a non-conductive <a href=\"https://en.wikipedia.org/wiki/Dielectric\">dielectric</a> (e.g., glass). The two electrodes form a tiny capacitor (a <em>cell</em>) that can hold some charge. When a finger (or another conductive object) approaches this cell, it ‘steals’ some of the charge, which can be measured as a drop in capacitance. Importantly, the finger doesn’t have to come into contact with the electrodes (which are protected under another layer of glass) as the amount of charge stolen is inversely proportional to the distance between the finger and the electrodes.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-19LyC1CdLmY/XvONkCibrzI/AAAAAAAAGJ8/E7H0OBHOCWs6TFHONyeSRUAnpMIndfhvQCLcBGAsYHQ/s1600/CapacitiveTouch.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"682\" data-original-width=\"1218\" height=\"358\" src=\"https://1.bp.blogspot.com/-19LyC1CdLmY/XvONkCibrzI/AAAAAAAAGJ8/E7H0OBHOCWs6TFHONyeSRUAnpMIndfhvQCLcBGAsYHQ/s640/CapacitiveTouch.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left:</b> A finger interacts with a touch sensor cell by ‘stealing’ charge from the projected field around two electrodes. <b>Right:</b> A capacitive touch sensor is constructed from rows and columns of electrodes, separated by a dielectric. The electrodes overlap at cells, where capacitance is measured.</td></tr></tbody></table>The cells are arranged as a matrix over the display of a device, but with a much lower density than the display pixels. For instance, the Pixel 4 has a 2280 × 1080 pixel display, but a 32 × 15 cell touch sensor. When scanned at a high resolution (at least 120 Hz), readings from these cells form a video of the finger’s interaction.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-TXNzzC8j40w/XvOONxuPdVI/AAAAAAAAGKE/pGGmwHQDztMU5ns3O8OhIWQKU10Fxez7QCLcBGAsYHQ/s1600/image1.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"434\" data-original-width=\"1414\" height=\"196\" src=\"https://1.bp.blogspot.com/-TXNzzC8j40w/XvOONxuPdVI/AAAAAAAAGKE/pGGmwHQDztMU5ns3O8OhIWQKU10Fxez7QCLcBGAsYHQ/s640/image1.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Slowed touch sensor recordings of a user tapping (<b>left</b>), pressing (<b>middle</b>), and scrolling (<b>right</b>).</td></tr></tbody></table>Capacitive touch sensors don’t respond to changes in force <em>per se</em>, but are tuned to be highly sensitive to changes in distance within a couple of millimeters above the display. That is, a finger contact on the display glass should saturate the sensor near its centre, but will retain a high dynamic range around the perimeter of the finger’s contact (where the finger curls up).<br /><br />When a user’s finger presses against a surface, its soft tissue deforms and spreads out. The nature of this spread depends on the size and shape of the user’s finger, and its angle to the screen. At a high level, we can observe a couple of key features in this spread (shown in the figures): it is asymmetric around the initial contact point, and the overall centre of mass shifts along the axis of the finger. This is also a dynamic change that occurs over some period of time, which differentiates it from contacts that have a long duration or a large area.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-K0JRDPfNB6Q/XvOOUsx4oII/AAAAAAAAGKI/uPAzLFjiMlEWA55KOrN11tKUDND7l_q0wCLcBGAsYHQ/s1600/image3.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1098\" data-original-width=\"1600\" height=\"273\" src=\"https://1.bp.blogspot.com/-K0JRDPfNB6Q/XvOOUsx4oII/AAAAAAAAGKI/uPAzLFjiMlEWA55KOrN11tKUDND7l_q0wCLcBGAsYHQ/s400/image3.png\" width=\"400\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Touch sensor signals are saturated around the centre of the finger’s contact, but fall off at the edges. This allows us to sense small deformations in the finger’s contact shape caused by changes in the finger’s force.</td></tr></tbody></table>However, the differences between users (and fingers) makes it difficult to encode these observations with heuristic rules. We therefore designed a machine learning solution that would allow us to learn these features and their variances directly from user interaction samples.<br /><br /><b>Machine Learning for Touch Interaction</b><br />We approached the analysis of these touch signals as a gesture classification problem. That is, rather than trying to predict an abstract parameter, such as force or contact spread, we wanted to sense a <em>press gesture</em> — as if engaging a button or a switch. This allowed us to connect the classification to a well-defined user experience, and allowed users to perform the gesture during training at a comfortable force and posture.<br /><br />Any classification model we designed had to operate within users’ high expectations for touch experiences. In particular, touch interaction is extremely latency-sensitive and demands real-time feedback. Users expect applications to be responsive to their finger movements as they make them, and application developers expect the system to deliver timely information about the gestures a user is performing. This means that classification of a press gesture needs to occur in real-time, and be able to trigger an interaction at the moment the finger’s force reaches its apex.<br /><br />We therefore designed a neural network that combined <a href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\">convolutional</a> (CNN) and <a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\">recurrent</a> (RNN) components. The CNN could attend to the spatial features we observed in the signal, while the RNN could attend to their temporal development. The RNN also helps provide a consistent runtime experience: each frame is processed by the network as it is received from the touch sensor, and the RNN state vectors are preserved between frames (rather than processing them in batches). The network was intentionally kept simple to minimise on-device inference costs when running concurrently with other applications (taking approximately 50 µs of processing per frame and less than 1 MB of memory using <a href=\"https://www.tensorflow.org/lite\">TensorFlow Lite</a>).<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-2pW9VB0LSsw/XvOOcT8nqoI/AAAAAAAAGKM/xBbYeiD-iEUhzefT63IePbrQ9YWjSA4rQCLcBGAsYHQ/s1600/image4.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"416\" data-original-width=\"1600\" height=\"166\" src=\"https://1.bp.blogspot.com/-2pW9VB0LSsw/XvOOcT8nqoI/AAAAAAAAGKM/xBbYeiD-iEUhzefT63IePbrQ9YWjSA4rQCLcBGAsYHQ/s640/image4.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An overview of the classification model’s architecture.</td></tr></tbody></table>The model was trained on a dataset of press gestures and other common touch interactions (tapping, scrolling, dragging, and long-pressing without force). As the model would be evaluated after each frame, we designed a loss function that temporally shaped the label probability distribution of each sample, and applied a time-increasing weight to errors. This ensured that the output probabilities were temporally smooth and converged towards the correct gesture classification.<br /><br /><b>User Experience Integration</b><br />Our UX research found that it was hard for users to discover force-based interactions, and that users frequently confused a force press with a long press because of the difficulty in coordinating the amount of force they were applying with the duration of their contact. Rather than creating a new interaction modality based on force, we therefore focussed on improving the user experience of long press interactions by accelerating them with force in a unified <em>press</em> gesture. A press gesture has the same outcome as a long press gesture, whose time threshold remains effective, but provides a stronger connection between the outcome and the user’s action when force is used.<br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-5YsKCaL2M4U/XvOOsy7muxI/AAAAAAAAGKY/ghER-egTAAoUR6RZy-ZoZN2A-ab_p8YMwCLcBGAsYHQ/s1600/image2.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"300\" data-original-width=\"600\" height=\"320\" src=\"https://1.bp.blogspot.com/-5YsKCaL2M4U/XvOOsy7muxI/AAAAAAAAGKY/ghER-egTAAoUR6RZy-ZoZN2A-ab_p8YMwCLcBGAsYHQ/s640/image2.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">A user long pressing (<b>left</b>) and firmly pressing (<b>right</b>) on a launcher icon.</td></tr></tbody></table>This also means that users can take advantage of this gesture without developers needing to update their apps. Applications that use Android’s <a href=\"https://developer.android.com/reference/android/view/GestureDetector\">GestureDetector</a> or <a href=\"https://developer.android.com/reference/android/view/View.OnLongClickListener\">View</a> APIs will automatically get these press signals through their existing long-press handlers. Developers that implement custom long-press detection logic can receive these press signals through the <a href=\"https://developer.android.com/reference/android/view/MotionEvent.html#getClassification()\">MotionEvent classification</a> API introduced in Android Q.<br /><br />Through this integration of machine-learning algorithms and careful interaction design, we were able to deliver a more expressive touch experience for Pixel users. We plan to continue researching and developing these capabilities to refine the touch experience on Pixel, and explore new forms of touch interaction.<br /><br /><b>Acknowledgements</b><br /><em>This project is a collaborative effort between the Android UX, Pixel software, and Android framework teams.</em><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=M5E2htUALhA:e_RNAL_Uzfw:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/M5E2htUALhA\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 24 Jun 2020 18:24:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-19LyC1CdLmY/XvONkCibrzI/AAAAAAAAGJ8/E7H0OBHOCWs6TFHONyeSRUAnpMIndfhvQCLcBGAsYHQ/s640/CapacitiveTouch.png","linkMd5":"8fa1dc05a356f402a1ad28227ac1a2ab","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn70@2020_6/2020/08/24/14-07-28-482_04b6cc038a49bd46.webp","destWidth":640,"destHeight":358,"sourceBytes":94115,"destBytes":18776,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-19LyC1CdLmY/XvONkCibrzI/AAAAAAAAGJ8/E7H0OBHOCWs6TFHONyeSRUAnpMIndfhvQCLcBGAsYHQ/s640/CapacitiveTouch.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn70@2020_6/2020/08/24/14-07-28-482_04b6cc038a49bd46.webp","https://1.bp.blogspot.com/-TXNzzC8j40w/XvOONxuPdVI/AAAAAAAAGKE/pGGmwHQDztMU5ns3O8OhIWQKU10Fxez7QCLcBGAsYHQ/s640/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn46@2020_2/2020/08/24/14-07-56-180_20931dffcccaa11d.webp","https://1.bp.blogspot.com/-K0JRDPfNB6Q/XvOOUsx4oII/AAAAAAAAGKI/uPAzLFjiMlEWA55KOrN11tKUDND7l_q0wCLcBGAsYHQ/s400/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn65@2020_3/2020/08/24/14-07-55-460_6584253a262e21fb.webp","https://1.bp.blogspot.com/-2pW9VB0LSsw/XvOOcT8nqoI/AAAAAAAAGKM/xBbYeiD-iEUhzefT63IePbrQ9YWjSA4rQCLcBGAsYHQ/s640/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn53@2020_4/2020/08/24/14-07-57-085_95c4410379c19ea7.webp","https://1.bp.blogspot.com/-5YsKCaL2M4U/XvOOsy7muxI/AAAAAAAAGKY/ghER-egTAAoUR6RZy-ZoZN2A-ab_p8YMwCLcBGAsYHQ/s640/image2.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn34@2020_5/2020/08/24/14-08-00-908_3138f44e133d7529.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/M5E2htUALhA":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn25@2020_2/2020/08/24/14-07-58-332_e7e7574bdea46e90.webp"},"publishedOrCreatedDate":1598278048109},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Exploring Faster Screening with Fewer Tests via Bayesian Group Testing","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/ioYPP_o3naI/exploring-faster-screening-with-fewer.html","description":"<span class=\"byline-author\">Posted by Marco Cuturi and Jean-Philippe Vert, Research Scientists, Google Research, Brain Team</span>  <br /><br />How does one find a needle in a haystack? At the turn of World War II, that question took on a very concrete form when doctors wondered how to efficiently detect diseases among those who had been drafted into the war effort. Inspired by this challenge, <a href=\"https://en.wikipedia.org/wiki/Group_testing#History_and_development\">Robert Dorfman</a>, a young statistician at that time (later to become Harvard professor of economics), proposed in a seminal <a href=\"https://projecteuclid.org/euclid.aoms/1177731363\">paper</a> a 2-stage approach to detect infected individuals, whereby individual blood samples first are pooled in groups of four before being tested for the presence or absence of a pathogen. If a group is negative, then it is safe to assume that everyone in the group is free of the pathogen. In that case, the reduction in the number of required tests is substantial: an entire group of four people has been cleared with a single test. On the other hand, if a group tests positive, which is expected to happen rarely if the pathogen’s prevalence is small, at least one or more people within that group must be positive; therefore, a few more tests to determine the infected individuals are needed. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-F74XTb7HKXw/XwzPBox_POI/AAAAAAAAGPE/PObFcMyE_VYN3gOjWruWxgSgJVYND3iAACLcBGAsYHQ/s1600/image1.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"475\" data-original-width=\"1273\" height=\"238\" src=\"https://1.bp.blogspot.com/-F74XTb7HKXw/XwzPBox_POI/AAAAAAAAGPE/PObFcMyE_VYN3gOjWruWxgSgJVYND3iAACLcBGAsYHQ/s640/image1.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left:</b>  Sixteen individual tests are required to screen 16 people — only one person’s test is positive, while 15 return negative. <b>Right:</b> Following Dorfman’s procedure, samples are pooled into four groups of four individuals, and tests are executed on the pooled samples. Because only the second group tests positive, 12 individuals are cleared and only those four belonging to the positive group need to be retested. This approach requires only eight tests, instead of the 16 needed for an exhaustive testing campaign.</td></tr></tbody></table>Dorfman’s proposal triggered many follow-up works with connections to several areas in computer science, such as <a href=\"https://arxiv.org/abs/1902.06002\">information theory</a>, <a href=\"https://www.cs.umn.edu/sites/cs.umn.edu/files/tech_reports/00-007.pdf\">combinatorics</a> or <a href=\"https://arxiv.org/abs/0907.1061\">compressive sensing</a>, and several variants of his approach have been proposed, notably those leveraging <a href=\"https://en.wikipedia.org/wiki/Group_testing#Generalised_binary-splitting_algorithm\">binary splitting</a> or side knowledge on <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4495770/\">individual</a> <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3197971/\">infection probability</a> rates. The field has grown to the extent that several sub-problems are recognized and deserving of an entire literature on their own. Some algorithms are tailored for the <em>noiseless </em>case in which tests are perfectly reliable, whereas some consider instead the more realistic case where tests are <em>noisy</em> and may produce false negatives or positives. Finally, some strategies are <em>adaptive</em>, proposing groups based on test results already observed (including Dorfman’s, since it proposes to re-test individuals that appeared in positive groups), whereas others stick to a <em>non-adaptive</em> setting in which groups are known beforehand or drawn at random.<br /><br />In “<a href=\"https://arxiv.org/abs/2004.12508\">Noisy Adaptive Group Testing using Bayesian Sequential Experimental Design</a>”, we present an approach to group testing that can operate in a noisy setting (i.e., where tests can be mistaken) to decide adaptively by looking at past results which groups to test next, with the goal to converge on a reliable detection as quickly, and with as few tests, as possible. Large scale simulations suggest that this approach may result in significant improvements over both adaptive and non-adaptive baselines, and are far more efficient than individual tests when disease prevalence is low. As such, this approach is particularly well suited for situations that require large numbers of tests to be conducted with limited resources, as may be the case for pandemics, such as that corresponding to the spread of COVID-19. We have open-sourced the code to the community through <a href=\"https://github.com/google-research/google-research/tree/master/grouptesting\">our GitHub repo</a>.<br /><br /><b>Noisy and Adaptive Group Testing in a Non-Asymptotic Regime</b><br />A group testing strategy is an algorithm that is tasked with guessing who, among a list of <em>n</em> people, carries a particular pathogen. To do so, the strategy provides instructions for pooling individuals into groups. Assuming a laboratory can execute <em>k</em> tests at a time, the strategy will form a <em>k</em> ⨉ <em>n</em> pooling matrix that defines these groups. Once the tests are carried out, the results are used to decide whether sufficient information has been gathered to determine who is or is not infected, and if not, how to form new groups for another round of testing.<br /><br />We designed a group testing approach for the realistic setting where the testing strategy can be adaptive and where tests are noisy — the probability that the test of an infected sample is positive (<em><a href=\"https://en.wikipedia.org/wiki/Sensitivity_and_specificity\">sensitivity</a></em>) is less than 100%, as is the <em><a href=\"https://en.wikipedia.org/wiki/Sensitivity_and_specificity\">specificity</a></em>, the probability that a non-infected sample returns negative.<br /><br /><b>Screening More People with Fewer Tests Using Bayesian Optimal Experimental Design</b><br />The strategy we propose proceeds the way a detective would investigate a case. They first form several hypotheses about who may or may not be infected, using evidence from all tests (if any) that have been carried out so far and prior information on the infection rate (a). Using these hypotheses, our detectives produce an actionable item to continue the investigation, namely a next wave of groups that may help in validating or invalidating as many hypotheses as possible (b), and then loop back to (a) until the set of plausible hypotheses is small enough to unambiguously identify the target of the search. More precisely,<br /><ol type=\"a\"><li>Given a population of <em>n</em> people, an infection state is a binary vector of length <em>n</em> that describes who is infected (marked with a 1), and who is not (marked with a 0). At a certain time, a population is in a given state (most likely a few 1’s and mostly 0’s). The goal of group testing is to identify that state using as few tests as possible. Given a prior belief on the infection rate (the disease is rare) and test results observed so far (if any), we expect that only a small share of those infection states will be plausible. Rather than evaluating the plausibility of all <em>2<sup>n</sup></em> possible states (an <a href=\"https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem\">extremely large number</a> even for small <em>n</em>), we resort to a more efficient method to <em>sample </em>plausible hypotheses using a <a href=\"https://en.wikipedia.org/wiki/Particle_filter\">sequential Monte Carlo</a> (SMC) <em>sampler</em>. Although quite costly by common standards (a few minutes using a GPU in our experimental setup), we show in this work that SMC samplers remain tractable even for large <em>n</em>, opening new possibilities for group testing. In short, in return for a few minutes of computations, our detectives get an extensive list of thousands of relevant hypotheses that may explain tests observed so far.<br /><br /></li><li>Equipped with a relevant list of hypotheses, our strategy proceeds, as detectives would, by selectively gathering additional evidence. If <em>k</em> tests can be carried out at the next iteration, our strategy will propose to test <em>k</em> new groups, which are computed using the framework of <a href=\"https://en.wikipedia.org/wiki/Bayesian_experimental_design\">Bayesian optimal experimental design</a>. Intuitively, if <em>k=1</em> and one can only propose a single new group to test, there would be clear advantage in building that group such that its test outcome is as <em>uncertain</em> as possible, i.e., with a probability that it returns positive as close to 50% as possible, given the current set of hypotheses. Indeed, to progress in an investigation, it is best to maximize the surprise factor (or <a href=\"https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\">information gain</a>) provided by new test results, as opposed to using them to confirm further what we already hold to be very likely. To generalize that idea to a set of <em>k&gt;1 </em>new groups, we score this surprise factor by computing the <a href=\"https://en.wikipedia.org/wiki/Mutual_information\">mutual information</a> of these “virtual” group tests <em>vs.</em> the distribution of hypotheses. We also consider a more involved approach that computes the expected <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\">area under the ROC curve</a> (AUC) one would obtain from testing these new groups using the distribution of hypotheses. The maximization of these two criteria is carried out using a <a href=\"https://en.wikipedia.org/wiki/Greedy_algorithm\">greedy</a> approach, resulting in two <em>group selectors</em>, GMIMAX and GAUCMAX (greedy maximization of mutual information or AUC, respectively).<br /></li></ol>The interaction between a laboratory (<code>wet_lab</code>) carrying out testing, and our strategy, composed of a <code>sampler</code> and a <code>group selector</code>, is summarized in the following drawing, which uses names of classes implemented in our open source package. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-S5cRq33MXjo/Xw3gT9FdyoI/AAAAAAAAGPo/7QGTjK16kxIl8hyphqjSybLx8nrR43wNgCLcBGAsYHQ/s1600/image2%2B%25282%2529.jpg\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"627\" data-original-width=\"965\" height=\"414\" src=\"https://1.bp.blogspot.com/-S5cRq33MXjo/Xw3gT9FdyoI/AAAAAAAAGPo/7QGTjK16kxIl8hyphqjSybLx8nrR43wNgCLcBGAsYHQ/s640/image2%2B%25282%2529.jpg\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Our group testing framework describes an interaction between a testing environment, the <code>wet_lab</code>, whose pooled test results are used by the <code>sampler</code> to draw thousands of plausible hypotheses on the infection status of all individuals. These hypotheses are then used by an optimization procedure, <code>group_selector,</code> that figures out what groups may be the most relevant to test in order to narrow down on the true infection status. Once formed, these new groups are then tested again, closing the loop. At any point in the procedure, the hypotheses formed by the sampler can be averaged to obtain the average probability of infection for each patient. From these probabilities, a decision on whether a patient is infected or not can be done by thresholding these probabilities at a certain confidence level.</td></tr></tbody></table><b>Benchmarking </b> <br />We benchmarked our two strategies GMIMAX and GAUCMAX against various baselines in a wide variety of settings (infection rates, test noise levels), reporting performance as the number of tests increases. In addition to simple Dorfman strategies, the baselines we considered included a mix of non-adaptive strategies (<a href=\"https://www.smarterbetter.design/origamiassays/default/index\">origami assays</a>, random designs) complemented at later stages with the so-called <a href=\"https://pubmed.ncbi.nlm.nih.gov/21762119/\">informative Dorfman approach</a>. Our approaches significantly outperform the others in all settings.  <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-DidBahLhPzE/XwzPhPjuH_I/AAAAAAAAGPY/cacUVTeDgoI2He46mzu4jN8xbAU_D1eGQCLcBGAsYHQ/s1600/image2.png\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"335\" data-original-width=\"512\" height=\"418\" src=\"https://1.bp.blogspot.com/-DidBahLhPzE/XwzPhPjuH_I/AAAAAAAAGPY/cacUVTeDgoI2He46mzu4jN8xbAU_D1eGQCLcBGAsYHQ/s640/image2.png\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">We executed 5000 simulations on a sample population of 70 individuals with an infection rate of 2%. We have assumed sensitivity/specificity values of 85% / 97% for tests with groups of maximal size 10, which are representative of <a href=\"https://pubmed.ncbi.nlm.nih.gov/32358960/\">current</a> <a href=\"https://en.wikipedia.org/wiki/Thermal_cycler\">PCR machines</a>. This figure demonstrates that our approach outperforms the other baselines with as few as 24 tests (up to 8 tests used in 3 cycles), including both adaptive and non-adaptive varieties, and performs significantly better than individual tests (plotted in the sensitivity/specificity plane as a hexagon, requiring 70 tests), highlighting the savings potential offered by group testing. See preprint for other setups.</td></tr></tbody></table><b>Conclusion </b> <br />Screening a population for a pathogen is a fundamental problem, one that we currently face during the current COVID-19 epidemic. Seventy years ago, Dorfman proposed a simple approach currently adopted by <a href=\"https://www.nature.com/articles/d41586-020-02053-6\">various institutions</a>. Here, we have proposed a method to extend the basic group testing approach in several ways. Our first contribution is to adopt a probabilistic perspective, and form thousands of plausible hypotheses of infection distributions given test outcomes, rather than trust test results to be 100% reliable as Dorfman did. This perspective allows us to seamlessly incorporate additional prior knowledge on infection, such as when we suspect some individuals to be more likely than others to carry the pathogen, based for instance on contact tracing data or answers to a questionnaire. This provides our algorithms, which can be compared to detectives investigating a case, the advantage of knowing what are the most likely infection hypotheses that agree with prior beliefs and tests carried out so far. Our second contribution is to propose algorithms that can take advantage of these hypotheses to form new groups, and therefore direct the gathering of new evidence, to narrow down as quickly as possible to the \"true\" infection hypothesis, and close the case with as little testing effort as possible. <br /><br /><b>Acknowledgements </b> <br /><em>We would like to thank our collaborators on this work, Olivier Teboul, in particular, for his help preparing figures, as well as Arnaud Doucet and Quentin Berthet</em>. <em>We also thank</em> <em>Kevin Murphy and Olivier Bousquet (Google) for their suggestions at the earliest stages of this project, as well as Dan Popovici for his unwavering support pushing this forward; Ignacio Anegon, Jeremie Poschmann and Laurent Tesson (INSERM) for providing us background information on RT-PCR tests and Nicolas Chopin (CREST) for giving guidance on his work to define SMCs for binary spaces.</em> <div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=ioYPP_o3naI:e5Cu888Vl6E:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/ioYPP_o3naI\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 14 Jul 2020 16:58:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-F74XTb7HKXw/XwzPBox_POI/AAAAAAAAGPE/PObFcMyE_VYN3gOjWruWxgSgJVYND3iAACLcBGAsYHQ/s640/image1.png","linkMd5":"3891ccfc82c595644b56c5ac424cdf4d","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_6/2020/08/24/14-07-28-382_9143e9983577d564.webp","destWidth":640,"destHeight":239,"sourceBytes":49387,"destBytes":14084,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-F74XTb7HKXw/XwzPBox_POI/AAAAAAAAGPE/PObFcMyE_VYN3gOjWruWxgSgJVYND3iAACLcBGAsYHQ/s640/image1.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_6/2020/08/24/14-07-28-382_9143e9983577d564.webp","https://1.bp.blogspot.com/-S5cRq33MXjo/Xw3gT9FdyoI/AAAAAAAAGPo/7QGTjK16kxIl8hyphqjSybLx8nrR43wNgCLcBGAsYHQ/s640/image2%2B%25282%2529.jpg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn85@2020_5/2020/08/24/14-07-59-245_3717b3682cc27bf0.webp","https://1.bp.blogspot.com/-DidBahLhPzE/XwzPhPjuH_I/AAAAAAAAGPY/cacUVTeDgoI2He46mzu4jN8xbAU_D1eGQCLcBGAsYHQ/s640/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn82@2020_3/2020/08/24/14-07-58-055_33b9d2e3022fc8ef.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/ioYPP_o3naI":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn46@2020_3/2020/08/24/14-07-58-537_88f2127b61ee2bd6.webp"},"publishedOrCreatedDate":1598278048115},{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","title":"Presenting a Challenge and Workshop in Efficient Open-Domain Question Answering","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2y9HgaPGngY/presenting-challenge-and-workshop-in.html","description":"<span class=\"byline-author\">Posted by Eunsol Choi, Visiting Faculty Researcher and Tom Kwiatkowski, Research Scientist, Google Research</span><br /><br />One of the primary goals of natural language processing is to build systems that can answer a user's questions. To do this, computers need to be able to understand questions, represent world knowledge, and reason their way to answers. Traditionally, answers have been retrieved from a collection of documents or a <a href=\"https://en.wikipedia.org/wiki/Knowledge_Graph\">knowledge graph</a>. For example, to answer the question, “When was the declaration of independence officially signed?” a system might first find the most relevant article from <a href=\"https://www.wikipedia.org/\">Wikipedia</a>, and then locate a sentence containing the answer, “August 2, 1776”.  However, more recent approaches, like <a href=\"https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\">T5</a>, have also shown that neural models, trained on large amounts of web-text, can also answer questions directly, without retrieving documents or facts from a knowledge graph. This has led to significant debate about how knowledge should be stored for use by our question answering systems — in human readable text and structured formats, or in the learned parameters of a neural network. <br /><br />Today, we are proud to announce the <a href=\"https://efficientqa.github.io/\">EfficientQA competition and workshop</a> at <a href=\"https://neurips.cc/Conferences/2020\">NeurIPS 2020</a>, organized in cooperation with <a href=\"https://www.cs.princeton.edu/~danqic/\">Princeton University</a> and the <a href=\"https://h2lab.cs.washington.edu/\">University of Washington</a>. The goal is to develop an end-to-end <a href=\"https://en.wikipedia.org/wiki/Question_answering\">question answering</a> system that contains all of the knowledge required to answer open-domain questions. There are no constraints on how the knowledge is stored — it could be in documents, databases, the parameters of a neural network, or any other form — but entries will be evaluated based on the number of bytes used to access this knowledge, including code, corpora, and model parameters. There will also be an unconstrained track, in which the goal is to achieve the best possible question answering performance regardless of system size. To build small, yet robust systems, participants will have to explore new methods of knowledge representation and reasoning. <br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto; text-align: center;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-9Q6HXopT7uI/XvElCdveChI/AAAAAAAAGJs/dZdfHqcB1FwWsQxaCjipgYe9J2caIGaGwCLcBGAsYHQ/s1600/image1.gif\" imageanchor=\"1\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"750\" data-original-width=\"1200\" height=\"400\" src=\"https://1.bp.blogspot.com/-9Q6HXopT7uI/XvElCdveChI/AAAAAAAAGJs/dZdfHqcB1FwWsQxaCjipgYe9J2caIGaGwCLcBGAsYHQ/s640/image1.gif\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">An illustration of how the memory budget changes as a neural network and retrieval corpus grow and shrink. It is possible that successful systems will also use other resources such as a knowledge graph.</td></tr></tbody></table><b>Competition Overview</b><br />The competition will be evaluated using the open-domain variant of the <a href=\"https://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html\">Natural Questions dataset</a>. We will also provide further human evaluation of all the top performing entries to account for the fact that there are many correct ways to answer a question, not all of which will be covered by any set of reference answers. For example, for the question “What type of car is a Jeep considered?” both “off-road vehicles” and “crossover SUVs” are valid answers. <br /><br />The competition is divided between four separate tracks: best performing system under 500 Mb; best performing system under 6 Gb; smallest system to get at least 25% accuracy; and the best performing system with no constraints. The winners of each of these tracks will be invited to present their work during the <a href=\"https://neurips.cc/Conferences/2020/CompetitionTrack\">competition track at NeurIPS 2020</a>, which will be hosted virtually. We will also put each of the winning systems up against human trivia experts (the <a href=\"https://sites.google.com/corp/view/qanta/past-events?authuser=0\">2017 NeurIPS Human-Computer competition</a> featured Jeopardy! and Who Wants to Be a Millionaire champions) in a real-time contest at the virtual conference.<br /><br /><b>Participation</b><br />To participate, go to the <a href=\"https://efficientqa.github.io/\">competition site</a> where you will find the data and evaluation code available for download, as well as dates and instructions on how to participate, and a sign-up form for updates. Along with our academic collaborators, we have provided some example systems to help you get started.<br /><br />We believe that the field of natural language processing will benefit from a greater exploration and comparison of small system question answering options. We hope that by encouraging the development of very small systems, this competition will pave the way for on-device question answering.<br /><br /><b>Acknowledgements</b><br />Creating this challenge and workshop has been a large team effort including <em>Adam Roberts, Colin Raffel, Chris Alberti, Jordan Boyd-Graber, Jennimaria Palomaki, Kenton Lee, Kelvin Guu, </em>and<em> Michael Collins</em> from Google; as well as <em>Sewon Min </em>and<em> Hannaneh Hajishirz</em>i from the University of Washington; and <em>Danqi Chen</em> from Princeton University.<div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=2y9HgaPGngY:hu7og43wU5Q:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2y9HgaPGngY\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 23 Jun 2020 18:13:00 +0000","feedId":1156,"bgimg":"https://1.bp.blogspot.com/-9Q6HXopT7uI/XvElCdveChI/AAAAAAAAGJs/dZdfHqcB1FwWsQxaCjipgYe9J2caIGaGwCLcBGAsYHQ/s640/image1.gif","linkMd5":"8792dadc9e236b54f5fb1f69fb96a705","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn57@2020_3/2020/08/24/14-07-38-946_1a092a5e78146a61.webp","destWidth":640,"destHeight":400,"sourceBytes":735981,"destBytes":400256,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-9Q6HXopT7uI/XvElCdveChI/AAAAAAAAGJs/dZdfHqcB1FwWsQxaCjipgYe9J2caIGaGwCLcBGAsYHQ/s640/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn57@2020_3/2020/08/24/14-07-38-946_1a092a5e78146a61.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2y9HgaPGngY":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn45@2020_6/2020/08/24/14-07-57-526_e8167fb1f549dd5a.webp"},"publishedOrCreatedDate":1598278048106}],"record":{"createdTime":"2020-08-24 22:07:28","updatedTime":"2020-08-24 22:07:28","feedId":1156,"fetchDate":"Mon, 24 Aug 2020 14:07:28 +0000","fetchMs":484,"handleMs":1557,"totalMs":57627,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"34.205.229.226","hostName":"us-032.herokuapp.com","requestId":"7a7a8066e51b46f3b3576e302123de5f_1156","contentType":"text/xml; charset=UTF-8","totalBytes":44437960,"bgimgsTotal":25,"bgimgsGithubTotal":25,"articlesImgsTotal":136,"articlesImgsGithubTotal":136,"successGithubMap":{"myreaderx14":5,"myreaderx8":5,"myreaderx7":5,"myreaderx15":5,"myreaderx6":4,"myreaderx16":6,"myreaderx4":5,"myreaderx32":5,"myreaderx10":4,"myreaderx11":5,"myreaderx33":5,"myreaderx3":6,"myreaderx12":5,"myreaderx2":5,"myreaderx1":5,"myreaderx13":5,"myreaderx30":5,"myreaderx31":5,"myreaderx18":5,"myreaderx19":5,"myreaderx":4,"myreaderx25":6,"myreaderx27":4,"myreaderx21":5,"myreaderx22":5,"myreaderx23":5,"myreaderx24":5,"myreaderx5oss":5,"myreaderx29":5},"failGithubMap":{}},"feed":{"createdTime":"2020-08-24 21:31:27","updatedTime":"2020-08-24 21:31:27","id":1156,"name":"Google AI Blog","url":"http://googleresearch.blogspot.com/atom.xml","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn2@2020_2/2020/08/24/14-07-27-260_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":"http://ai.googleblog.com"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":7098940,"tmpBodyImgCdnBytes":37316490,"tmpBgImgCdnBytes":22530,"extra4":{"start":1598278045994,"total":0,"statList":[{"spend":587,"msg":"获取xml内容"},{"spend":1557,"msg":"解释文章"},{"spend":1243,"msg":"上传封面图到cdn"},{"spend":4419,"msg":"修正封面图上传失败重新上传"},{"spend":28448,"msg":"正文链接上传到cdn"}]},"extra5":136,"extra6":136,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Ojg1wuVgxWs/introducing-model-card-toolkit-for.html_mailto:model-cards@google.com":"mailto:model-cards@google.com","http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html_#1":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html#1","http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html_#top1":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html#top1","http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html_#top1":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html#top1","http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html_#1":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html#1"},"extra111_proxyServerAndStatMap":{"http://us-018.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-24.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-039.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-53.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-007.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-015.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe70.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-011.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe-23.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-031.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-019.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-010.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-023.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-027.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Lb1GXOp7Ilk/XyC54ZsgAeI/AAAAAAAAGRo/Z7ZScvrPVZ4YXepnkU2eM4Vpelt2OmsPwCLcBGAsYHQ/s640/image2%2B%25283%2529.jpg","sourceStatusCode":200,"destWidth":640,"destHeight":235,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn89@2020_6/2020/08/24/14-07-28-322_290fee323372c895.webp","sourceBytes":29538,"destBytes":11064,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":814,"convertSpendMs":9,"createdTime":"2020-08-24 22:07:28","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Ojg1wuVgxWs/introducing-model-card-toolkit-for.html","linkMd5ListStr":"a43cbc46c792f96b6fd8e05e6a83fd1c,a43cbc46c792f96b6fd8e05e6a83fd1c","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"28.8 KB","destSize":"10.8 KB","compressRate":"37.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn6@2020_4/2020/08/24/14-07-28-353_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":819,"convertSpendMs":4,"createdTime":"2020-08-24 22:07:28","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2y9HgaPGngY/presenting-challenge-and-workshop-in.html","linkMd5ListStr":"b09a356862e25dc3f311f71d606af524,6fa5004149ccce453a3e5645aa8b9477,82c16e7daa20dcc8b6c6188bc66eba60,120a22df18c057ba75ed58782d9abeaf,a6859779ff2bda767c63a2b9c5a2c0a9,a43cbc46c792f96b6fd8e05e6a83fd1c,6473b353c163127f73885d9cb5187135,f368ca441fd668170d34ab61787b6561,9f56c8339959af8f2ea5e05651ed131f,9905ef79d2e76ec08be633a84535c116,9ee85f58ff53824040c5cd8f1110ce5a,69cfc3e9279dc4a8d0c977973ff42767,454039e4a25f3a17b8deb713cd0faa82,1838d5b6b055d7a446dcfe2540616e69,c171dd959a368832e85f24dfeaf3eb73,48b1d4a884366dd0c8c7dff7923ccf98,977eb1a6b95954423c643687bd4266ff,3c71cc7626d30346f461b7b4d2ecb255,6177c6f6f007891265bd244d276b5967,32fb0c3b0fecf8789d62476c3497c382,744d32fc2907f7da5c39c5fc5aaed997,8fa1dc05a356f402a1ad28227ac1a2ab,e78d187929399261dd59b6fd6be7b186,3891ccfc82c595644b56c5ac424cdf4d,8792dadc9e236b54f5fb1f69fb96a705,b09a356862e25dc3f311f71d606af524,9ee85f58ff53824040c5cd8f1110ce5a","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-vgePiImUoOU/XzLQFtSR7bI/AAAAAAAAGV8/G6qugs86Y_AuBOKlB3VRkRE0AaQbNF_CwCLcBGAsYHQ/w513-h384/image1.png","sourceStatusCode":200,"destWidth":512,"destHeight":384,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn79@2020_5/2020/08/24/14-07-28-356_ffa833e18d92ff51.webp","sourceBytes":371836,"destBytes":39170,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":911,"convertSpendMs":26,"createdTime":"2020-08-24 22:07:28","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","linkMd5ListStr":"e78d187929399261dd59b6fd6be7b186,e78d187929399261dd59b6fd6be7b186","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"363.1 KB","destSize":"38.3 KB","compressRate":"10.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-tckJGXlwza0/Xyhnz018mlI/AAAAAAAAGTo/GWRtBFKMiIw-RijXcb6qYeqNZ16I7k_ZgCLcBGAsYHQ/s640/figure0_1600px_white.png","sourceStatusCode":200,"destWidth":640,"destHeight":394,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn1@2020_4/2020/08/24/14-07-28-393_24a383d4be71fac0.webp","sourceBytes":251873,"destBytes":34202,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":974,"convertSpendMs":24,"createdTime":"2020-08-24 22:07:28","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff,977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"246 KB","destSize":"33.4 KB","compressRate":"13.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-7FX-wmLnlMg/XwSysgdoT2I/AAAAAAAAGNk/Cqzcf7vQvRcvsijDfFGkpckNuStsOntpwCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":361,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn41@2020_2/2020/08/24/14-07-28-336_3dbd0a62e85390f4.webp","sourceBytes":70150,"destBytes":23178,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1001,"convertSpendMs":14,"createdTime":"2020-08-24 22:07:28","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fljSD_dTaKM/duality-new-approach-to-reinforcement.html","linkMd5ListStr":"9f56c8339959af8f2ea5e05651ed131f,9f56c8339959af8f2ea5e05651ed131f","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"68.5 KB","destSize":"22.6 KB","compressRate":"33%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-CemiVojz7C0/XwekwEp2UxI/AAAAAAAAGOM/xIAJa9i8DXA7Ev3B6qcx6JcQ7MJ9n0xqgCLcBGAsYHQ/s640/image2%2B%25281%2529.jpg","sourceStatusCode":200,"destWidth":525,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn66@2020_6/2020/08/24/14-07-28-409_6902584c7a1e23fc.webp","sourceBytes":92161,"destBytes":41896,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1015,"convertSpendMs":43,"createdTime":"2020-08-24 22:07:28","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/wmN0OhKV4I4/grounding-natural-language-instructions.html","linkMd5ListStr":"82c16e7daa20dcc8b6c6188bc66eba60,82c16e7daa20dcc8b6c6188bc66eba60","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"90 KB","destSize":"40.9 KB","compressRate":"45.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-19LyC1CdLmY/XvONkCibrzI/AAAAAAAAGJ8/E7H0OBHOCWs6TFHONyeSRUAnpMIndfhvQCLcBGAsYHQ/s640/CapacitiveTouch.png","sourceStatusCode":200,"destWidth":640,"destHeight":358,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn70@2020_6/2020/08/24/14-07-28-482_04b6cc038a49bd46.webp","sourceBytes":94115,"destBytes":18776,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1015,"convertSpendMs":71,"createdTime":"2020-08-24 22:07:28","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","linkMd5ListStr":"8fa1dc05a356f402a1ad28227ac1a2ab,8fa1dc05a356f402a1ad28227ac1a2ab","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"91.9 KB","destSize":"18.3 KB","compressRate":"20%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-cdc1x9OZGbA/XyHa7C04FGI/AAAAAAAAGR8/3JEtp7s9Qj8U5wvsBJACaAcdvPSM-tW1gCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":504,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn9@2020_5/2020/08/24/14-07-28-358_7f02eb85257eb3e8.webp","sourceBytes":76093,"destBytes":13128,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":979,"convertSpendMs":13,"createdTime":"2020-08-24 22:07:28","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/uEq7NDB-AgY/on-device-supermarket-product.html","linkMd5ListStr":"32fb0c3b0fecf8789d62476c3497c382,32fb0c3b0fecf8789d62476c3497c382","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"74.3 KB","destSize":"12.8 KB","compressRate":"17.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-A1jVMpuUj7g/XvTcZUCFDNI/AAAAAAAAGKk/v3pHkHOhpQYwLF2S53FCT9unitrkVfWMgCLcBGAsYHQ/s640/image3.png","sourceStatusCode":200,"destWidth":640,"destHeight":454,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn18@2020_3/2020/08/24/14-07-28-401_20f2d8bb4b8cef68.webp","sourceBytes":128906,"destBytes":3678,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1087,"convertSpendMs":14,"createdTime":"2020-08-24 22:07:28","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73,c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"125.9 KB","destSize":"3.6 KB","compressRate":"2.9%"},{"code":1,"isDone":false,"source":"https://3.bp.blogspot.com/-LH7lfGbziL4/Xwjykc0yfzI/AAAAAAAAGO4/n7VQble-qawbXoHeHmMtpsVeazOZWHJRACLcBGAsYHQ/s320/image2.png","sourceStatusCode":200,"destWidth":320,"destHeight":190,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn13@2020_4/2020/08/24/14-07-28-426_3d28259a6bc27e2d.webp","sourceBytes":41224,"destBytes":11662,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1103,"convertSpendMs":9,"createdTime":"2020-08-24 22:07:28","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html","linkMd5ListStr":"9905ef79d2e76ec08be633a84535c116,9905ef79d2e76ec08be633a84535c116","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"40.3 KB","destSize":"11.4 KB","compressRate":"28.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-F74XTb7HKXw/XwzPBox_POI/AAAAAAAAGPE/PObFcMyE_VYN3gOjWruWxgSgJVYND3iAACLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":239,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_6/2020/08/24/14-07-28-382_9143e9983577d564.webp","sourceBytes":49387,"destBytes":14084,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1136,"convertSpendMs":9,"createdTime":"2020-08-24 22:07:28","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/ioYPP_o3naI/exploring-faster-screening-with-fewer.html","linkMd5ListStr":"3891ccfc82c595644b56c5ac424cdf4d,3891ccfc82c595644b56c5ac424cdf4d","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"48.2 KB","destSize":"13.8 KB","compressRate":"28.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-6upFrBNGwo4/Xzwk7D60GaI/AAAAAAAAGZs/ZDgmdCvBYfQr2cc5CkWW0AfIzD11x1q4wCLcBGAsYHQ/s0/image2%2B%25284%2529.jpg","sourceStatusCode":200,"destWidth":344,"destHeight":209,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn94@2020_3/2020/08/24/14-07-28-397_f9c57f3b236ab4b1.webp","sourceBytes":20983,"destBytes":6018,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1179,"convertSpendMs":25,"createdTime":"2020-08-24 22:07:28","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","linkMd5ListStr":"1838d5b6b055d7a446dcfe2540616e69,1838d5b6b055d7a446dcfe2540616e69","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.5 KB","destSize":"5.9 KB","compressRate":"28.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s1600/image3.png","sourceStatusCode":200,"destWidth":1600,"destHeight":412,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn49@2020_6/2020/08/24/14-07-28-535_dc187b158ce2fd24.webp","sourceBytes":55785,"destBytes":18714,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1348,"convertSpendMs":25,"createdTime":"2020-08-24 22:07:28","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135,6473b353c163127f73885d9cb5187135","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"54.5 KB","destSize":"18.3 KB","compressRate":"33.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-nsLiFUVt6S4/XzVpLWay6VI/AAAAAAAAGXI/oPyuvuQEFcASODqPdT9dqptyUvUuGlTvACLcBGAsYHQ/w205-h274/image3.gif","sourceStatusCode":200,"destWidth":205,"destHeight":274,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn61@2020_1/2020/08/24/14-07-28-981_d33215879a8838f5.webp","sourceBytes":1611196,"destBytes":491500,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1970,"convertSpendMs":578,"createdTime":"2020-08-24 22:07:28","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255,3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.5 MB","destSize":"480 KB","compressRate":"30.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-oSVo5zrtc_Y/Xxc5yMjPndI/AAAAAAAAGP8/yLbgy8VsfeoSY_5TTgJSjupW5QGRtjjewCLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":640,"destHeight":162,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn73@2020_4/2020/08/24/14-07-28-896_283470d036354fef.webp","sourceBytes":1203387,"destBytes":313796,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2250,"convertSpendMs":467,"createdTime":"2020-08-24 22:07:28","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/A_hvdqM_OXc/improving-holistic-scene-understanding.html","linkMd5ListStr":"120a22df18c057ba75ed58782d9abeaf,120a22df18c057ba75ed58782d9abeaf","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.1 MB","destSize":"306.4 KB","compressRate":"26.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-O5smp1-Rfrk/XvvEsptj6aI/AAAAAAAAGMo/QyU8k6bcojkB5VofNOf-yv0Wqjyo_Xx4ACLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":640,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_5/2020/08/24/14-07-30-891_ab51825e059f6f3d.webp","sourceBytes":612085,"destBytes":334118,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":3812,"convertSpendMs":2496,"createdTime":"2020-08-24 22:07:28","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BDNx_vWFo_M/smartreply-for-youtube-creators.html","linkMd5ListStr":"48b1d4a884366dd0c8c7dff7923ccf98,48b1d4a884366dd0c8c7dff7923ccf98","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"597.7 KB","destSize":"326.3 KB","compressRate":"54.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Lpgeiec1LTI/Xywye-E8YOI/AAAAAAAAGUU/bAytHYPS1wIHV_EXWWBAXIdnp3p1H0D1wCLcBGAsYHQ/w640-h252/image3.gif","sourceStatusCode":200,"destWidth":640,"destHeight":252,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn98@2020_3/2020/08/24/14-07-34-023_56223dc13987983b.webp","sourceBytes":6764175,"destBytes":2192456,"targetWebpQuality":60,"feedId":1156,"totalSpendMs":7904,"convertSpendMs":2457,"createdTime":"2020-08-24 22:07:28","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967,6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.5 MB","destSize":"2.1 MB","compressRate":"32.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Rsymb9XvPOE/Xx8rfRnmTHI/AAAAAAAAGRQ/U2n_bBNXS4IBstYrx2IalrFXufLUvmn2gCLcBGAsYHQ/s640/ScaNN%2Btom%2Bexport.gif","sourceStatusCode":200,"destWidth":512,"destHeight":245,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn78@2020_5/2020/08/24/14-07-35-843_d3214e2bba50cc30.webp","sourceBytes":827250,"destBytes":728984,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":8873,"convertSpendMs":7433,"createdTime":"2020-08-24 22:07:28","host":"us-030*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html","linkMd5ListStr":"6fa5004149ccce453a3e5645aa8b9477,6fa5004149ccce453a3e5645aa8b9477","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"807.9 KB","destSize":"711.9 KB","compressRate":"88.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-9Q6HXopT7uI/XvElCdveChI/AAAAAAAAGJs/dZdfHqcB1FwWsQxaCjipgYe9J2caIGaGwCLcBGAsYHQ/s640/image1.gif","sourceStatusCode":200,"destWidth":640,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn57@2020_3/2020/08/24/14-07-38-946_1a092a5e78146a61.webp","sourceBytes":735981,"destBytes":400256,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":12006,"convertSpendMs":10319,"createdTime":"2020-08-24 22:07:28","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2y9HgaPGngY/presenting-challenge-and-workshop-in.html","linkMd5ListStr":"8792dadc9e236b54f5fb1f69fb96a705,8792dadc9e236b54f5fb1f69fb96a705","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"718.7 KB","destSize":"390.9 KB","compressRate":"54.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-ZkvWVcy1Y7M/XvDoqqadcxI/AAAAAAAAGIA/KAw3Hj5f-SMlvlVC0e2onYkUa4e5IPGlgCLcBGAsYHQ/s640/image5.gif","sourceStatusCode":200,"destWidth":640,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn45@2020_6/2020/08/24/14-07-53-641_680fbaa064e9d945.webp","sourceBytes":10196588,"destBytes":2401950,"targetWebpQuality":37,"feedId":1156,"totalSpendMs":26874,"convertSpendMs":12499,"createdTime":"2020-08-24 22:07:28","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82,454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.7 MB","destSize":"2.3 MB","compressRate":"23.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-fi1yFm05xl8/XzV3ydL3ETI/AAAAAAAAGZE/arm5o-e9a4AtA0V-wmrdZIthCbE0an8DQCLcBGAsYHQ/s640/TableA.png","sourceStatusCode":200,"destWidth":640,"destHeight":150,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn38@2020_1/2020/08/24/14-07-55-253_2069698b0fdb909a.webp","sourceBytes":23760,"destBytes":8680,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":766,"convertSpendMs":8,"createdTime":"2020-08-24 22:07:55","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23.2 KB","destSize":"8.5 KB","compressRate":"36.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s72-c/Environments.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn29@2020_2/2020/08/24/14-07-55-296_3d283dac124096aa.webp","sourceBytes":8468,"destBytes":2168,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":822,"convertSpendMs":5,"createdTime":"2020-08-24 22:07:55","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"8.3 KB","destSize":"2.1 KB","compressRate":"25.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-rAQyquFR03U/Xyw0oeMop-I/AAAAAAAAGVU/xisQmcE68PAMWyK23r3fUsqk20D3zuEZQCLcBGAsYHQ/s640/DistanceEstimation.png","sourceStatusCode":200,"destWidth":640,"destHeight":229,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn48@2020_3/2020/08/24/14-07-55-306_9da12be4b020ac47.webp","sourceBytes":43914,"destBytes":11230,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":818,"convertSpendMs":9,"createdTime":"2020-08-24 22:07:55","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"42.9 KB","destSize":"11 KB","compressRate":"25.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/wmN0OhKV4I4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn13@2020_4/2020/08/24/14-07-55-325_c664d740dde00de9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":846,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:55","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/wmN0OhKV4I4/grounding-natural-language-instructions.html","linkMd5ListStr":"82c16e7daa20dcc8b6c6188bc66eba60","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s72-c/image3.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn34@2020_5/2020/08/24/14-07-55-385_730a903ab0b285f6.webp","sourceBytes":11638,"destBytes":2780,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":832,"convertSpendMs":54,"createdTime":"2020-08-24 22:07:55","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","linkMd5ListStr":"f368ca441fd668170d34ab61787b6561","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"11.4 KB","destSize":"2.7 KB","compressRate":"23.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/FjfQTAdG8E8","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn18@2020_4/2020/08/24/14-07-55-335_3d79b8706294ef84.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":875,"convertSpendMs":8,"createdTime":"2020-08-24 22:07:55","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","linkMd5ListStr":"744d32fc2907f7da5c39c5fc5aaed997","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-GK43VckX7Jk/XwZP6NfZmII/AAAAAAAAGN8/OFXfCgeXB4MTjcKqn8R91mrMozB4qMmyQCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":380,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn33@2020_6/2020/08/24/14-07-55-330_11087690437917d9.webp","sourceBytes":124173,"destBytes":39102,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":896,"convertSpendMs":18,"createdTime":"2020-08-24 22:07:55","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html","linkMd5ListStr":"9905ef79d2e76ec08be633a84535c116","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"121.3 KB","destSize":"38.2 KB","compressRate":"31.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-XxKesnBALGM/XzVxSKZNWZI/AAAAAAAAGYc/WOt31icjp_YyjMxz06RSEwTi9K3qviFxwCLcBGAsYHQ/w440-h270/image9.jpg","sourceStatusCode":200,"destWidth":440,"destHeight":270,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn73@2020_6/2020/08/24/14-07-55-363_916630e89981b722.webp","sourceBytes":49503,"destBytes":17972,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":831,"convertSpendMs":17,"createdTime":"2020-08-24 22:07:55","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"48.3 KB","destSize":"17.6 KB","compressRate":"36.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/e0Yr7hZaFjA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn62@2020_2/2020/08/24/14-07-55-367_9d3082b63ccc6018.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":866,"convertSpendMs":36,"createdTime":"2020-08-24 22:07:55","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/Ojg1wuVgxWs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn78@2020_2/2020/08/24/14-07-55-362_281eb60395c68e9a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":860,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:55","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Ojg1wuVgxWs/introducing-model-card-toolkit-for.html","linkMd5ListStr":"a43cbc46c792f96b6fd8e05e6a83fd1c","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s72-c/image1.gif","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn26@2020_1/2020/08/24/14-07-55-391_d3cf3d57c9ed09b3.webp","sourceBytes":41224,"destBytes":16028,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":977,"convertSpendMs":95,"createdTime":"2020-08-24 22:07:55","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","linkMd5ListStr":"744d32fc2907f7da5c39c5fc5aaed997","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"40.3 KB","destSize":"15.7 KB","compressRate":"38.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-l4VY-q1YcEE/XxsvREuIEvI/AAAAAAAAGQs/zzJNUHTZ9SU8LtKzm2rgl0oQCuiJ9fhIwCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":205,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn98@2020_4/2020/08/24/14-07-55-408_cb4f9bcf2189e939.webp","sourceBytes":71169,"destBytes":17378,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":967,"convertSpendMs":108,"createdTime":"2020-08-24 22:07:55","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html","linkMd5ListStr":"6fa5004149ccce453a3e5645aa8b9477","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"69.5 KB","destSize":"17 KB","compressRate":"24.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/CPC0hNgMKL4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn21@2020_1/2020/08/24/14-07-55-354_92de5f63c67f97d2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":922,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:55","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-2m5u8cCre6g/XzwldoUhZVI/AAAAAAAAGaE/5gjnSRA1nh8UZ1tTRPp7lmBPBH7Jim-7QCLcBGAsYHQ/w493-h307/image3%2B-%2BEdited%2B%25281%2529.png","sourceStatusCode":200,"destWidth":493,"destHeight":307,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn41@2020_4/2020/08/24/14-07-55-358_261c73db2ef09efd.webp","sourceBytes":56564,"destBytes":15874,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":985,"convertSpendMs":14,"createdTime":"2020-08-24 22:07:55","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","linkMd5ListStr":"1838d5b6b055d7a446dcfe2540616e69","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"55.2 KB","destSize":"15.5 KB","compressRate":"28.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-oULUcK5QgtE/Xvpd-UHfDkI/AAAAAAAAGMA/UP9pK20VOlAUcU394ElECpqu2poxuDfOgCLcBGAsYHQ/s400/image1.png","sourceStatusCode":200,"destWidth":400,"destHeight":251,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn6@2020_4/2020/08/24/14-07-55-478_0bd29865a4eafd86.webp","sourceBytes":23387,"destBytes":7110,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":984,"convertSpendMs":32,"createdTime":"2020-08-24 22:07:55","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"22.8 KB","destSize":"6.9 KB","compressRate":"30.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/9DRaIjqziZI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn58@2020_3/2020/08/24/14-07-55-405_21495c6297033bbc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":918,"convertSpendMs":7,"createdTime":"2020-08-24 22:07:55","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","linkMd5ListStr":"69cfc3e9279dc4a8d0c977973ff42767","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-rOS64hhFqaY/XvpdnFjiZbI/AAAAAAAAGL0/I2oGEgdWWsoLzTxKetvdinIMGARJIRTBACLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":225,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn10@2020_2/2020/08/24/14-07-55-450_cfaa7a0cdc525a07.webp","sourceBytes":21690,"destBytes":8108,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1086,"convertSpendMs":38,"createdTime":"2020-08-24 22:07:55","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.2 KB","destSize":"7.9 KB","compressRate":"37.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s72-c/image1%2B%25283%2529.jpg","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn21@2020_4/2020/08/24/14-07-55-300_738fcd761b7ee5be.webp","sourceBytes":2719,"destBytes":1554,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1199,"convertSpendMs":58,"createdTime":"2020-08-24 22:07:55","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","linkMd5ListStr":"69cfc3e9279dc4a8d0c977973ff42767","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.7 KB","destSize":"1.5 KB","compressRate":"57.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-K0JRDPfNB6Q/XvOOUsx4oII/AAAAAAAAGKI/uPAzLFjiMlEWA55KOrN11tKUDND7l_q0wCLcBGAsYHQ/s400/image3.png","sourceStatusCode":200,"destWidth":400,"destHeight":274,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn65@2020_3/2020/08/24/14-07-55-460_6584253a262e21fb.webp","sourceBytes":21625,"destBytes":7792,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":988,"convertSpendMs":6,"createdTime":"2020-08-24 22:07:55","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","linkMd5ListStr":"8fa1dc05a356f402a1ad28227ac1a2ab","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.1 KB","destSize":"7.6 KB","compressRate":"36%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HucWjMOChHk/XvTcuRir9nI/AAAAAAAAGK0/SfhWxyARqdI5FrvNntwjNnafvBxYk8fhgCLcBGAsYHQ/s640/image4.png","sourceStatusCode":200,"destWidth":640,"destHeight":204,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn41@2020_5/2020/08/24/14-07-55-429_a0007aca11526e38.webp","sourceBytes":79134,"destBytes":15670,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1052,"convertSpendMs":15,"createdTime":"2020-08-24 22:07:55","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"77.3 KB","destSize":"15.3 KB","compressRate":"19.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-OuZB6r1b-pc/XzRPJYOMNsI/AAAAAAAAGWw/qBKVlzIudt4itmvTrYBtOSJaR1N1MB4OgCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":342,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn69@2020_2/2020/08/24/14-07-55-424_3d533ba1974a881e.webp","sourceBytes":49209,"destBytes":16910,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1105,"convertSpendMs":23,"createdTime":"2020-08-24 22:07:55","host":"europe-23*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","linkMd5ListStr":"744d32fc2907f7da5c39c5fc5aaed997","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"48.1 KB","destSize":"16.5 KB","compressRate":"34.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-gerljyblrUs/Xxc56h-1AuI/AAAAAAAAGQA/YB6id3ki1P87w3ZzYdwt6n6SuOGQR07awCLcBGAsYHQ/s640/image3.png","sourceStatusCode":200,"destWidth":534,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn71@2020_6/2020/08/24/14-07-55-615_07e460ae2b3ef04c.webp","sourceBytes":73838,"destBytes":21814,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1237,"convertSpendMs":38,"createdTime":"2020-08-24 22:07:55","host":"us-010*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/A_hvdqM_OXc/improving-holistic-scene-understanding.html","linkMd5ListStr":"120a22df18c057ba75ed58782d9abeaf","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"72.1 KB","destSize":"21.3 KB","compressRate":"29.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-w22Iw7BRZsg/XzWx-S7DtpI/AAAAAAAAGZg/zgpN2e5Oye8qPXfq0zLq6dm38afXaUa8gCLcBGAsYHQ/s640/image4%2B%25281%2529.jpg","sourceStatusCode":200,"destWidth":640,"destHeight":323,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn49@2020_5/2020/08/24/14-07-55-483_0c5f003462264bd8.webp","sourceBytes":56955,"destBytes":27984,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1055,"convertSpendMs":174,"createdTime":"2020-08-24 22:07:55","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"55.6 KB","destSize":"27.3 KB","compressRate":"49.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-xHxGJ9xuFWY/XvpgD6BE_fI/AAAAAAAAGMM/yM8aF2fv8U4kqvWOmAEBv26kWB_p9DgaQCLcBGAsYHQ/s640/COCOPerformance.png","sourceStatusCode":200,"destWidth":640,"destHeight":336,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn57@2020_4/2020/08/24/14-07-55-382_4042c46e2fa06811.webp","sourceBytes":53143,"destBytes":22198,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1242,"convertSpendMs":17,"createdTime":"2020-08-24 22:07:55","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.9 KB","destSize":"21.7 KB","compressRate":"41.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s640/Environments.png","sourceStatusCode":200,"destWidth":640,"destHeight":472,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn85@2020_1/2020/08/24/14-07-55-793_712572a622809d76.webp","sourceBytes":277692,"destBytes":30280,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1321,"convertSpendMs":239,"createdTime":"2020-08-24 22:07:55","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"271.2 KB","destSize":"29.6 KB","compressRate":"10.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-egMF8wDiUf4/XvTcoH-TwxI/AAAAAAAAGKs/znyPBxNOuyIGIjWlNi5UHAbsnk-kvE1ygCLcBGAsYHQ/s640/image8.png","sourceStatusCode":200,"destWidth":640,"destHeight":275,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn93@2020_3/2020/08/24/14-07-55-459_488fca40d139e2f8.webp","sourceBytes":26631,"destBytes":14396,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1377,"convertSpendMs":28,"createdTime":"2020-08-24 22:07:55","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"26 KB","destSize":"14.1 KB","compressRate":"54.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-9lEqMQQGr9o/Xywz7ACbOQI/AAAAAAAAGU4/fOZBI8anLv8fZiVFkrA7Q_HSREf1hxJlACLcBGAsYHQ/s640/EyeAnnotation.png","sourceStatusCode":200,"destWidth":640,"destHeight":213,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn82@2020_3/2020/08/24/14-07-55-678_f2f1ab1cba580337.webp","sourceBytes":147061,"destBytes":9790,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1367,"convertSpendMs":13,"createdTime":"2020-08-24 22:07:55","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"143.6 KB","destSize":"9.6 KB","compressRate":"6.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HNNrEpGoZyY/Xxc6AlgpRxI/AAAAAAAAGQE/QqqLwHukUuQxiIFeJkT_sBw5HPZ_avvVwCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":426,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn25@2020_5/2020/08/24/14-07-55-538_615660cbc8bd0abb.webp","sourceBytes":73874,"destBytes":22390,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1395,"convertSpendMs":16,"createdTime":"2020-08-24 22:07:55","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/A_hvdqM_OXc/improving-holistic-scene-understanding.html","linkMd5ListStr":"120a22df18c057ba75ed58782d9abeaf","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"72.1 KB","destSize":"21.9 KB","compressRate":"30.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-hKcKT1CyOKA/XvDpaUO2ROI/AAAAAAAAGII/twznnajZ7ZI5Q-HUbCfU7R5i_P87rsLkgCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":521,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn37@2020_1/2020/08/24/14-07-55-427_3050eb110825455a.webp","sourceBytes":205582,"destBytes":53792,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1425,"convertSpendMs":31,"createdTime":"2020-08-24 22:07:55","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"200.8 KB","destSize":"52.5 KB","compressRate":"26.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s640/image3.png","sourceStatusCode":200,"destWidth":640,"destHeight":316,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn61@2020_5/2020/08/24/14-07-55-448_51f7346668ccac4d.webp","sourceBytes":274515,"destBytes":51342,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1524,"convertSpendMs":20,"createdTime":"2020-08-24 22:07:55","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","linkMd5ListStr":"f368ca441fd668170d34ab61787b6561","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"268.1 KB","destSize":"50.1 KB","compressRate":"18.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/sKILqR_zvQI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn97@2020_1/2020/08/24/14-07-56-223_d463e7644372228c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":767,"convertSpendMs":4,"createdTime":"2020-08-24 22:07:56","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html","linkMd5ListStr":"6fa5004149ccce453a3e5645aa8b9477","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-SxYvxDLQSp4/Xyw0EvBSPhI/AAAAAAAAGU8/MDN_cwLNerUMpzXWikbx4qBbBT4yG7iTgCLcBGAsYHQ/s0/image7.png","sourceStatusCode":200,"destWidth":621,"destHeight":175,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn81@2020_1/2020/08/24/14-07-56-138_a1eb06ab3e8ed74a.webp","sourceBytes":30025,"destBytes":10226,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":990,"convertSpendMs":31,"createdTime":"2020-08-24 22:07:56","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"29.3 KB","destSize":"10 KB","compressRate":"34.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/pliU9YmOakA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn22@2020_1/2020/08/24/14-07-56-344_343b7f3261e59ba1.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":774,"convertSpendMs":5,"createdTime":"2020-08-24 22:07:56","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/pliU9YmOakA/google-at-acl-2020.html","linkMd5ListStr":"b09a356862e25dc3f311f71d606af524","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-0HxtiXvnyTU/Xx8xNOgfUSI/AAAAAAAAGRc/Vgf0gK50N9cIG1aA9TWFLx7nqAYwuP5TQCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":222,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn18@2020_4/2020/08/24/14-07-56-416_c53290c1c77806fa.webp","sourceBytes":33696,"destBytes":12412,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":910,"convertSpendMs":40,"createdTime":"2020-08-24 22:07:56","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html","linkMd5ListStr":"6fa5004149ccce453a3e5645aa8b9477","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"32.9 KB","destSize":"12.1 KB","compressRate":"36.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/--iaigg1Gc6c/Xz_5T8KmZsI/AAAAAAAAGcg/sZpDxlRVegIr_oIaXtODuzqIq_YqktU-gCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":166,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn37@2020_2/2020/08/24/14-07-56-424_17fc5b9998f0f610.webp","sourceBytes":31726,"destBytes":12328,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":775,"convertSpendMs":20,"createdTime":"2020-08-24 22:07:56","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","linkMd5ListStr":"69cfc3e9279dc4a8d0c977973ff42767","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31 KB","destSize":"12 KB","compressRate":"38.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-UgzqutJ_SBQ/XzV4jPL683I/AAAAAAAAGZU/lOQNa8SIK5UN2G14KIdnte0qdSGa0lGagCLcBGAsYHQ/s640/TableC.png","sourceStatusCode":200,"destWidth":640,"destHeight":118,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn85@2020_5/2020/08/24/14-07-56-240_a424e3e94533f919.webp","sourceBytes":22637,"destBytes":8870,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":972,"convertSpendMs":11,"createdTime":"2020-08-24 22:07:56","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"22.1 KB","destSize":"8.7 KB","compressRate":"39.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-i8ZfrqzbckE/XzVxC19SuyI/AAAAAAAAGYY/XAAoXAgx34kvOtk1Ypi0tQyiCfREGADSgCLcBGAsYHQ/w210-h210/image4.jpg","sourceStatusCode":200,"destWidth":210,"destHeight":210,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_1/2020/08/24/14-07-56-305_3f9ea428c27a2045.webp","sourceBytes":19171,"destBytes":9368,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":890,"convertSpendMs":5,"createdTime":"2020-08-24 22:07:56","host":"europe-23*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"18.7 KB","destSize":"9.1 KB","compressRate":"48.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-TXjVAPwTxks/XwelMUqNFBI/AAAAAAAAGOc/9bFKQnVg4ZYF8ne_qr2st5LxxBb7iY6QwCLcBGAsYHQ/s640/image3%2B%25281%2529.jpg","sourceStatusCode":200,"destWidth":640,"destHeight":297,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn9@2020_5/2020/08/24/14-07-56-399_b2e0a7ccad17f532.webp","sourceBytes":49545,"destBytes":24508,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":915,"convertSpendMs":14,"createdTime":"2020-08-24 22:07:56","host":"us-039*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/wmN0OhKV4I4/grounding-natural-language-instructions.html","linkMd5ListStr":"82c16e7daa20dcc8b6c6188bc66eba60","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"48.4 KB","destSize":"23.9 KB","compressRate":"49.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-XRSmG5X_Yp4/XyhogLlk1AI/AAAAAAAAGUE/Yyy4IX94riA8-Qp5F2k1irKvYQ1RetLcQCLcBGAsYHQ/s640/figure5_1600px_white.png","sourceStatusCode":200,"destWidth":640,"destHeight":222,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn33@2020_4/2020/08/24/14-07-56-417_c3f13219e5ac3100.webp","sourceBytes":238554,"destBytes":28640,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":942,"convertSpendMs":31,"createdTime":"2020-08-24 22:07:56","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"233 KB","destSize":"28 KB","compressRate":"12%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2B94l2vj1K4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn42@2020_6/2020/08/24/14-07-56-379_274a43ec68bfb6c2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":954,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:56","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/A_hvdqM_OXc","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn90@2020_5/2020/08/24/14-07-56-312_97d2e8923dd09c96.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1097,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:56","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/A_hvdqM_OXc/improving-holistic-scene-understanding.html","linkMd5ListStr":"120a22df18c057ba75ed58782d9abeaf","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-nWk49a-caFc/Xzwl9yJ2wFI/AAAAAAAAGaU/s4lObJ58si8r_NpHh6WUTVAmf6PziFTiQCLcBGAsYHQ/w489-h162/TranslationRanking.gif","sourceStatusCode":200,"destWidth":489,"destHeight":161,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn25@2020_3/2020/08/24/14-07-56-564_eb2e2e71a0654078.webp","sourceBytes":25160,"destBytes":10900,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1082,"convertSpendMs":141,"createdTime":"2020-08-24 22:07:56","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","linkMd5ListStr":"1838d5b6b055d7a446dcfe2540616e69","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"24.6 KB","destSize":"10.6 KB","compressRate":"43.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-XKlopfLZQRI/XzwmKHMg2gI/AAAAAAAAGaY/FwXbI2K1qlQqOknkrSc_xMufwMAtnxJPwCLcBGAsYHQ/s640/image4.png","sourceStatusCode":200,"destWidth":640,"destHeight":286,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn65@2020_4/2020/08/24/14-07-56-628_cb5ecca5dc425e01.webp","sourceBytes":37106,"destBytes":18332,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":852,"convertSpendMs":11,"createdTime":"2020-08-24 22:07:56","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","linkMd5ListStr":"1838d5b6b055d7a446dcfe2540616e69","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"36.2 KB","destSize":"17.9 KB","compressRate":"49.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fMpogH0lgys","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn50@2020_3/2020/08/24/14-07-56-588_77fd3c9ea5dbdf2d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1014,"convertSpendMs":4,"createdTime":"2020-08-24 22:07:56","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fMpogH0lgys/google-at-icml-2020.html","linkMd5ListStr":"9ee85f58ff53824040c5cd8f1110ce5a","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-ChpqMODDBno/Xyhn_JHDBHI/AAAAAAAAGTs/V-MChXk-zro_8Yaj2GHTf0eW-t7q1HjMQCLcBGAsYHQ/s640/figure1_1600px_white.png","sourceStatusCode":200,"destWidth":640,"destHeight":267,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn4@2020_3/2020/08/24/14-07-56-386_85428e093c4782ec.webp","sourceBytes":106347,"destBytes":19294,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1213,"convertSpendMs":11,"createdTime":"2020-08-24 22:07:56","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"103.9 KB","destSize":"18.8 KB","compressRate":"18.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-74GDeejuQSg/XwiZLA3EmVI/AAAAAAAAGOs/EekF8fpsUKMGHreoMDBbQQPFWWrMge4eACLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":266,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn58@2020_4/2020/08/24/14-07-56-743_afb98d70ce3e2007.webp","sourceBytes":43407,"destBytes":16600,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":996,"convertSpendMs":12,"createdTime":"2020-08-24 22:07:56","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/wmN0OhKV4I4/grounding-natural-language-instructions.html","linkMd5ListStr":"82c16e7daa20dcc8b6c6188bc66eba60","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"42.4 KB","destSize":"16.2 KB","compressRate":"38.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-r-_CNdpVVOw/Xyw0xjn4rsI/AAAAAAAAGVc/6-dWyJbqd2caNcYtX8GxacyEWhGf_oISgCLcBGAsYHQ/s0/image11.gif","sourceStatusCode":200,"destWidth":277,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn30@2020_6/2020/08/24/14-07-56-527_e8196c63772f5a67.webp","sourceBytes":118413,"destBytes":219510,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2427,"convertSpendMs":1229,"createdTime":"2020-08-24 22:07:55","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"115.6 KB","destSize":"214.4 KB","compressRate":"185.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/xGFebACpRTc","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn90@2020_3/2020/08/24/14-07-56-930_aab9bc89a7bfa207.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":803,"convertSpendMs":7,"createdTime":"2020-08-24 22:07:56","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html","linkMd5ListStr":"9905ef79d2e76ec08be633a84535c116","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/WADybDJ966w","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn12@2020_6/2020/08/24/14-07-57-023_3a958b473c03347c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":999,"convertSpendMs":191,"createdTime":"2020-08-24 22:07:56","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/uEq7NDB-AgY","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn74@2020_2/2020/08/24/14-07-56-878_25fa6336e1f9ce49.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1030,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:56","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/uEq7NDB-AgY/on-device-supermarket-product.html","linkMd5ListStr":"32fb0c3b0fecf8789d62476c3497c382","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-PJxZGdDg2CQ/XvTcfv7MO9I/AAAAAAAAGKo/7zEBw-Qp4XA2Q-vffmYyNoXs2v1tyYoWwCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":307,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn85@2020_5/2020/08/24/14-07-57-025_3db80c84ebe28b97.webp","sourceBytes":166446,"destBytes":24422,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1057,"convertSpendMs":16,"createdTime":"2020-08-24 22:07:56","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"162.5 KB","destSize":"23.8 KB","compressRate":"14.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-_eXHKGzwHT4/XvDqsC_CNDI/AAAAAAAAGJA/4cXDqT6oHhkarqFUAcTWOT9Ge36koal_wCLcBGAsYHQ/s640/image3.gif","sourceStatusCode":200,"destWidth":464,"destHeight":240,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn46@2020_1/2020/08/24/14-07-56-592_690fc631b93eb70a.webp","sourceBytes":4700083,"destBytes":571782,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2525,"convertSpendMs":832,"createdTime":"2020-08-24 22:07:55","host":"us-018*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.5 MB","destSize":"558.4 KB","compressRate":"12.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/BDNx_vWFo_M","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn10@2020_6/2020/08/24/14-07-57-149_24aa9badf3a085be.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":812,"convertSpendMs":21,"createdTime":"2020-08-24 22:07:57","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BDNx_vWFo_M/smartreply-for-youtube-creators.html","linkMd5ListStr":"48b1d4a884366dd0c8c7dff7923ccf98","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-pkmXSBgOiWg/XwSyz4b7peI/AAAAAAAAGNo/qLbfYN5gc7giVvtjwIoaSV2Me9qNFidfQCLcBGAsYHQ/s400/image1.png","sourceStatusCode":200,"destWidth":400,"destHeight":363,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn70@2020_1/2020/08/24/14-07-56-947_87f0bcbf1a64e074.webp","sourceBytes":37967,"destBytes":13292,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1236,"convertSpendMs":9,"createdTime":"2020-08-24 22:07:56","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fljSD_dTaKM/duality-new-approach-to-reinforcement.html","linkMd5ListStr":"9f56c8339959af8f2ea5e05651ed131f","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"37.1 KB","destSize":"13 KB","compressRate":"35%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-XJMFuqkmy_w/Xz2M2UH1I0I/AAAAAAAAGaw/KkXuhBMdiSYSG36OECfxpF8GYUOmRFmYwCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":362,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn97@2020_5/2020/08/24/14-07-57-031_da466c188dac83fd.webp","sourceBytes":28672,"destBytes":13672,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":948,"convertSpendMs":16,"createdTime":"2020-08-24 22:07:56","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","linkMd5ListStr":"f368ca441fd668170d34ab61787b6561","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"28 KB","destSize":"13.4 KB","compressRate":"47.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-6zLvUOR2aQY/XzVuvuFVoYI/AAAAAAAAGXw/z3yKcMX7CTMJHHAUOK7vjApm_FnX9ET8ACLcBGAsYHQ/w265-h149/image2.gif","sourceStatusCode":200,"destWidth":265,"destHeight":149,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn76@2020_3/2020/08/24/14-07-55-970_765a2308da14f275.webp","sourceBytes":1355213,"destBytes":407164,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2602,"convertSpendMs":579,"createdTime":"2020-08-24 22:07:55","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"397.6 KB","compressRate":"30%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-2pW9VB0LSsw/XvOOcT8nqoI/AAAAAAAAGKM/xBbYeiD-iEUhzefT63IePbrQ9YWjSA4rQCLcBGAsYHQ/s640/image4.png","sourceStatusCode":200,"destWidth":640,"destHeight":166,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn53@2020_4/2020/08/24/14-07-57-085_95c4410379c19ea7.webp","sourceBytes":21357,"destBytes":10510,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1134,"convertSpendMs":29,"createdTime":"2020-08-24 22:07:56","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","linkMd5ListStr":"8fa1dc05a356f402a1ad28227ac1a2ab","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.9 KB","destSize":"10.3 KB","compressRate":"49.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-dPfRkUmOrP4/XyhoRVvtpbI/AAAAAAAAGT8/eZDz-jGEbjkyb8JSH8yHyV6bBUu-DpD0ACLcBGAsYHQ/s640/figure3_1600px_white.png","sourceStatusCode":200,"destWidth":640,"destHeight":232,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn2@2020_2/2020/08/24/14-07-57-202_e8c2ae452dd445bf.webp","sourceBytes":259211,"destBytes":32458,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":962,"convertSpendMs":62,"createdTime":"2020-08-24 22:07:57","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"253.1 KB","destSize":"31.7 KB","compressRate":"12.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Ks99sDT1Rv4/XyC5-OoGSWI/AAAAAAAAGRs/76W7kZPo0_wgsVKNE8veNk9DxwZ2VqCgACLcBGAsYHQ/s1600/image1.png","sourceStatusCode":200,"destWidth":1067,"destHeight":1600,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn81@2020_3/2020/08/24/14-07-57-050_e4b2185d748452ca.webp","sourceBytes":472645,"destBytes":133028,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1274,"convertSpendMs":201,"createdTime":"2020-08-24 22:07:56","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/Ojg1wuVgxWs/introducing-model-card-toolkit-for.html","linkMd5ListStr":"a43cbc46c792f96b6fd8e05e6a83fd1c","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"461.6 KB","destSize":"129.9 KB","compressRate":"28.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-TXNzzC8j40w/XvOONxuPdVI/AAAAAAAAGKE/pGGmwHQDztMU5ns3O8OhIWQKU10Fxez7QCLcBGAsYHQ/s640/image1.gif","sourceStatusCode":200,"destWidth":640,"destHeight":196,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn46@2020_2/2020/08/24/14-07-56-180_20931dffcccaa11d.webp","sourceBytes":416353,"destBytes":177510,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2705,"convertSpendMs":509,"createdTime":"2020-08-24 22:07:55","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","linkMd5ListStr":"8fa1dc05a356f402a1ad28227ac1a2ab","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"406.6 KB","destSize":"173.3 KB","compressRate":"42.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-eqH1ZsnTP2o/XzwlDjy3X5I/AAAAAAAAGZw/_qFDiNyDi7cMeIBtUa-qoTJ9_6ODKtLtwCLcBGAsYHQ/s640/image1.png","sourceStatusCode":200,"destWidth":640,"destHeight":298,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn5@2020_6/2020/08/24/14-07-57-188_1c03f8c9145038c2.webp","sourceBytes":39445,"destBytes":14154,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":938,"convertSpendMs":10,"createdTime":"2020-08-24 22:07:57","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","linkMd5ListStr":"1838d5b6b055d7a446dcfe2540616e69","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"38.5 KB","destSize":"13.8 KB","compressRate":"35.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-AUfpsk256iM/XvaMrDxcAKI/AAAAAAAAGLk/WMBxTRGTLBUcbBwh0owpVs6PDYpFAlcxQCLcBGAsYHQ/s400/image1%2B%25281%2529.png","sourceStatusCode":200,"destWidth":400,"destHeight":105,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn26@2020_6/2020/08/24/14-07-57-362_0be1cd04029c810e.webp","sourceBytes":17997,"destBytes":7592,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":964,"convertSpendMs":6,"createdTime":"2020-08-24 22:07:57","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"17.6 KB","destSize":"7.4 KB","compressRate":"42.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-swx5QnTgaTI/Xvpd2eTw2VI/AAAAAAAAGL4/TWZk2xzAzBM6-bLI2VUYApGOlAfXpDKtwCLcBGAsYHQ/s640/image3.png","sourceStatusCode":200,"destWidth":640,"destHeight":165,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_1/2020/08/24/14-07-57-482_a7cb0ed5cd7c385d.webp","sourceBytes":18456,"destBytes":6762,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":898,"convertSpendMs":7,"createdTime":"2020-08-24 22:07:57","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"18 KB","destSize":"6.6 KB","compressRate":"36.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HgIzPDSJ0xw/XvTdQH1Vs3I/AAAAAAAAGLE/gTlB3RfhFDEHhainPzV6FnQWKo1JuFyqACLcBGAsYHQ/s640/ContextRCNN.png","sourceStatusCode":200,"destWidth":559,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn45@2020_4/2020/08/24/14-07-57-247_87b778df1366e459.webp","sourceBytes":454402,"destBytes":51228,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1807,"convertSpendMs":89,"createdTime":"2020-08-24 22:07:56","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/WADybDJ966w/leveraging-temporal-context-for-object.html","linkMd5ListStr":"c171dd959a368832e85f24dfeaf3eb73","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"443.8 KB","destSize":"50 KB","compressRate":"11.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-3y9qZTiQ-Xg/XzVsslu98RI/AAAAAAAAGXg/hpkLt16_qmoeqtdW1NBlryODgA-6Wq-RACLcBGAsYHQ/w188-h274/Image2.gif","sourceStatusCode":200,"destWidth":188,"destHeight":274,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn2@2020_3/2020/08/24/14-07-56-329_bfc74348fc84bed3.webp","sourceBytes":1914759,"destBytes":555414,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2914,"convertSpendMs":734,"createdTime":"2020-08-24 22:07:55","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.8 MB","destSize":"542.4 KB","compressRate":"29%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-iJBtQBrfDkY/Xz_5ZtI_xuI/AAAAAAAAGck/Zb1MArg4PkwSooXe8nTRB6yvWKZGCBA-gCLcBGAsYHQ/s640/image3.png","sourceStatusCode":200,"destWidth":640,"destHeight":164,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn54@2020_6/2020/08/24/14-07-57-574_71df659557f09586.webp","sourceBytes":31587,"destBytes":12156,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":786,"convertSpendMs":15,"createdTime":"2020-08-24 22:07:57","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","linkMd5ListStr":"69cfc3e9279dc4a8d0c977973ff42767","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"30.8 KB","destSize":"11.9 KB","compressRate":"38.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/2y9HgaPGngY","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn45@2020_6/2020/08/24/14-07-57-526_e8167fb1f549dd5a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":881,"convertSpendMs":8,"createdTime":"2020-08-24 22:07:57","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2y9HgaPGngY/presenting-challenge-and-workshop-in.html","linkMd5ListStr":"8792dadc9e236b54f5fb1f69fb96a705","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-q5suTIBLMpY/Xyw0LhfLZOI/AAAAAAAAGVA/A5xO_Ybtws4UvChYYKJFOfwkiLxtrpzDQCLcBGAsYHQ/w512-h476/image5.png","sourceStatusCode":200,"destWidth":512,"destHeight":476,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn62@2020_1/2020/08/24/14-07-57-680_897744e0fffd0d12.webp","sourceBytes":36270,"destBytes":14934,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":928,"convertSpendMs":21,"createdTime":"2020-08-24 22:07:57","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"35.4 KB","destSize":"14.6 KB","compressRate":"41.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fVhMJKABb9s","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn65@2020_4/2020/08/24/14-07-57-823_8900986825bb1376.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":848,"convertSpendMs":9,"createdTime":"2020-08-24 22:07:57","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","linkMd5ListStr":"e78d187929399261dd59b6fd6be7b186","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-l-98i5rzG4w/XvvHoaIpGGI/AAAAAAAAGNQ/cBoe7_pWqQw5tpbkI_k_3NhbmU8Y_u08wCLcBGAsYHQ/s640/animation.gif","sourceStatusCode":200,"destWidth":640,"destHeight":360,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn49@2020_1/2020/08/24/14-07-57-711_570c1af80b4c5ab0.webp","sourceBytes":62870,"destBytes":44344,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1077,"convertSpendMs":128,"createdTime":"2020-08-24 22:07:57","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BDNx_vWFo_M/smartreply-for-youtube-creators.html","linkMd5ListStr":"48b1d4a884366dd0c8c7dff7923ccf98","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"61.4 KB","destSize":"43.3 KB","compressRate":"70.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/TQbqJYtduXE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn42@2020_6/2020/08/24/14-07-57-585_c8d48371b012b000.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1231,"convertSpendMs":52,"createdTime":"2020-08-24 22:07:57","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-BEw65BexITY/XzRPPdN3lfI/AAAAAAAAGW4/bu583zs7aHEPzR_zcsy5HkV0s4bLqRUfwCLcBGAsYHQ/w410-h359/image3.png","sourceStatusCode":200,"destWidth":409,"destHeight":359,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn89@2020_4/2020/08/24/14-07-58-030_649e3e56985b6733.webp","sourceBytes":27192,"destBytes":9200,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":850,"convertSpendMs":12,"createdTime":"2020-08-24 22:07:57","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","linkMd5ListStr":"744d32fc2907f7da5c39c5fc5aaed997","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"26.6 KB","destSize":"9 KB","compressRate":"33.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-uEkUMJ-m8Gw/Xz2zWt4CGzI/AAAAAAAAGcI/CXOmIimUTDcD4wFBB4fN8bfgyhXVpV4yQCLcBGAsYHQ/w328-h96/image3.png","sourceStatusCode":200,"destWidth":328,"destHeight":96,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn93@2020_2/2020/08/24/14-07-58-028_c9d67371e06d509b.webp","sourceBytes":21436,"destBytes":7840,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":863,"convertSpendMs":4,"createdTime":"2020-08-24 22:07:58","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.9 KB","destSize":"7.7 KB","compressRate":"36.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-RzCBrCZykLE/XvDpz5viqyI/AAAAAAAAGIU/Ax43DDRjtWkeAZsepspakwUEGkshGsqQwCLcBGAsYHQ/s640/image9.png","sourceStatusCode":200,"destWidth":640,"destHeight":188,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn73@2020_1/2020/08/24/14-07-58-011_f6760bfb7fef6f5d.webp","sourceBytes":28177,"destBytes":22242,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":965,"convertSpendMs":42,"createdTime":"2020-08-24 22:07:57","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"27.5 KB","destSize":"21.7 KB","compressRate":"78.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-J66lTDBjlgw/XzVwzgeQJ7I/AAAAAAAAGYM/WBIhbOqzi4ICUswEOHv8r7ItJIOJgL9iwCLcBGAsYHQ/s0/image11.jpg","sourceStatusCode":200,"destWidth":411,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn37@2020_4/2020/08/24/14-07-57-478_809c9c4eb4fc7d18.webp","sourceBytes":35090,"destBytes":7184,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1447,"convertSpendMs":5,"createdTime":"2020-08-24 22:07:57","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"34.3 KB","destSize":"7 KB","compressRate":"20.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/T5ahr9GBor4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn77@2020_1/2020/08/24/14-07-57-962_a9b2820039b730d1.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":942,"convertSpendMs":8,"createdTime":"2020-08-24 22:07:57","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-59MnhMRiRQs/Xz2rScdINAI/AAAAAAAAGbc/O6nSOZaF-FsWJ6DZ8FwNRHRTZreK5tN0ACLcBGAsYHQ/w410-h154/image6.png","sourceStatusCode":200,"destWidth":410,"destHeight":154,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn30@2020_5/2020/08/24/14-07-58-363_1c437a54de8d29b6.webp","sourceBytes":5432,"destBytes":3336,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":775,"convertSpendMs":13,"createdTime":"2020-08-24 22:07:58","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.3 KB","destSize":"3.3 KB","compressRate":"61.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/DxkG9zDiWDw","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn1@2020_3/2020/08/24/14-07-58-059_ef6cabcc55c90e07.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1048,"convertSpendMs":2,"createdTime":"2020-08-24 22:07:58","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/DxkG9zDiWDw/language-agnostic-bert-sentence.html","linkMd5ListStr":"1838d5b6b055d7a446dcfe2540616e69","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-1eYeyyE35HQ/XvDp-wfvAHI/AAAAAAAAGIc/iMpJvOl6gSgFbuzlRWzhahKC7DH66fktQCLcBGAsYHQ/s640/image7.gif","sourceStatusCode":200,"destWidth":480,"destHeight":200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn61@2020_6/2020/08/24/14-07-57-832_ed4283f0d36e0f89.webp","sourceBytes":2323417,"destBytes":603990,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2557,"convertSpendMs":953,"createdTime":"2020-08-24 22:07:56","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.2 MB","destSize":"589.8 KB","compressRate":"26%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-jwKGEFtOWhs/XyhoEqWZZBI/AAAAAAAAGT0/iTdgSRqezO0dqarp-SBlpws4E2mmeB7SACLcBGAsYHQ/s640/figure2_1600px_white.png","sourceStatusCode":200,"destWidth":640,"destHeight":266,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn5@2020_2/2020/08/24/14-07-58-292_d29de5e00a94d3f6.webp","sourceBytes":110081,"destBytes":21438,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":999,"convertSpendMs":87,"createdTime":"2020-08-24 22:07:58","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"107.5 KB","destSize":"20.9 KB","compressRate":"19.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s640/image1%2B%25283%2529.jpg","sourceStatusCode":200,"destWidth":640,"destHeight":172,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn33@2020_6/2020/08/24/14-07-58-314_911bc8e30b1e4239.webp","sourceBytes":31342,"destBytes":16282,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":886,"convertSpendMs":7,"createdTime":"2020-08-24 22:07:58","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","linkMd5ListStr":"69cfc3e9279dc4a8d0c977973ff42767","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"30.6 KB","destSize":"15.9 KB","compressRate":"51.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-XEoGSy-SXDo/XvvFdqDIYhI/AAAAAAAAGM8/NNQNjtfDo-Ek7_kq0F_qSF4DrB-oAsspwCLcBGAsYHQ/s640/image4.png","sourceStatusCode":200,"destWidth":640,"destHeight":307,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn97@2020_4/2020/08/24/14-07-58-088_e5de388353adc967.webp","sourceBytes":49497,"destBytes":28586,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1161,"convertSpendMs":17,"createdTime":"2020-08-24 22:07:58","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BDNx_vWFo_M/smartreply-for-youtube-creators.html","linkMd5ListStr":"48b1d4a884366dd0c8c7dff7923ccf98","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"48.3 KB","destSize":"27.9 KB","compressRate":"57.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-IaXHUwl2yHI/XvvEkCH08uI/AAAAAAAAGMk/Ub9Yx8vzKXM3CDUv1wwHewslKXT4WRfCgCLcBGAsYHQ/s640/Emoji.png","sourceStatusCode":200,"destWidth":640,"destHeight":110,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn38@2020_3/2020/08/24/14-07-58-432_b85b09923fac5ee7.webp","sourceBytes":34298,"destBytes":11984,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":884,"convertSpendMs":25,"createdTime":"2020-08-24 22:07:58","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/BDNx_vWFo_M/smartreply-for-youtube-creators.html","linkMd5ListStr":"48b1d4a884366dd0c8c7dff7923ccf98","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"33.5 KB","destSize":"11.7 KB","compressRate":"34.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-TyWiSe5e8hM/XvpgQjxMlnI/AAAAAAAAGMQ/gg0yBASTjP0IfwXz-HJYkm3vnLTwAVYRQCLcBGAsYHQ/s640/iNaturalistPerformance.png","sourceStatusCode":200,"destWidth":640,"destHeight":339,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn22@2020_2/2020/08/24/14-07-58-258_73c14236e5173644.webp","sourceBytes":50642,"destBytes":22720,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1108,"convertSpendMs":18,"createdTime":"2020-08-24 22:07:58","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"49.5 KB","destSize":"22.2 KB","compressRate":"44.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/sokv3SkAWtI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn42@2020_3/2020/08/24/14-07-58-510_0d21a9d9b92b6723.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":890,"convertSpendMs":7,"createdTime":"2020-08-24 22:07:58","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","linkMd5ListStr":"f368ca441fd668170d34ab61787b6561","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fljSD_dTaKM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn49@2020_6/2020/08/24/14-07-58-671_efccc4d6d6a7f086.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":796,"convertSpendMs":4,"createdTime":"2020-08-24 22:07:58","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fljSD_dTaKM/duality-new-approach-to-reinforcement.html","linkMd5ListStr":"9f56c8339959af8f2ea5e05651ed131f","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-DidBahLhPzE/XwzPhPjuH_I/AAAAAAAAGPY/cacUVTeDgoI2He46mzu4jN8xbAU_D1eGQCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":512,"destHeight":335,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn82@2020_3/2020/08/24/14-07-58-055_33b9d2e3022fc8ef.webp","sourceBytes":76202,"destBytes":26586,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1496,"convertSpendMs":24,"createdTime":"2020-08-24 22:07:57","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/ioYPP_o3naI/exploring-faster-screening-with-fewer.html","linkMd5ListStr":"3891ccfc82c595644b56c5ac424cdf4d","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"74.4 KB","destSize":"26 KB","compressRate":"34.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/ioYPP_o3naI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn46@2020_3/2020/08/24/14-07-58-537_88f2127b61ee2bd6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":943,"convertSpendMs":4,"createdTime":"2020-08-24 22:07:58","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/ioYPP_o3naI/exploring-faster-screening-with-fewer.html","linkMd5ListStr":"3891ccfc82c595644b56c5ac424cdf4d","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-jF_HuzW-smE/Xywy_JurMuI/AAAAAAAAGUc/cMX_9GtWxM4RhOqMhURDYB831eDqJ0ZIwCLcBGAsYHQ/w586-h164/image13.gif","sourceStatusCode":200,"destWidth":586,"destHeight":164,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn53@2020_3/2020/08/24/14-07-58-048_a4365a518099002e.webp","sourceBytes":4458084,"destBytes":1562076,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":4214,"convertSpendMs":2535,"createdTime":"2020-08-24 22:07:55","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.3 MB","destSize":"1.5 MB","compressRate":"35%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fqWE2c6TI9o","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn77@2020_5/2020/08/24/14-07-58-945_d08657f8cc5c378e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":808,"convertSpendMs":6,"createdTime":"2020-08-24 22:07:58","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fqWE2c6TI9o/spinenet-novel-architecture-for-object.html","linkMd5ListStr":"6473b353c163127f73885d9cb5187135","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-WNEb6bWjYII/XzVunSDlPbI/AAAAAAAAGXs/OrIPZZyMqYghCbuomvVPDO_tdKz9EeOyQCLcBGAsYHQ/w282-h149/image10.gif","sourceStatusCode":200,"destWidth":282,"destHeight":149,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn86@2020_5/2020/08/24/14-07-58-700_9a6e7ea8a0791a6d.webp","sourceBytes":1757089,"destBytes":385966,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2005,"convertSpendMs":773,"createdTime":"2020-08-24 22:07:57","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 MB","destSize":"376.9 KB","compressRate":"22%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-F8KvPhGskK8/XzLRFouAnMI/AAAAAAAAGWM/mMr9kw3WWn03kFDJzT3WkoP3m7QTJ-p-QCLcBGAsYHQ/s640/image2.png","sourceStatusCode":200,"destWidth":640,"destHeight":153,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn82@2020_5/2020/08/24/14-07-59-114_d034b4310d17ed4c.webp","sourceBytes":86639,"destBytes":20044,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":941,"convertSpendMs":77,"createdTime":"2020-08-24 22:07:59","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","linkMd5ListStr":"e78d187929399261dd59b6fd6be7b186","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"84.6 KB","destSize":"19.6 KB","compressRate":"23.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-co_8stdJGNw/Xyw_QSggW1I/AAAAAAAAGVs/-OsEhVKP6Fgi5gBJGFVDHCYDi3VFhhYyACLcBGAsYHQ/s0/MediaPipeWASM.gif","sourceStatusCode":200,"destWidth":281,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn69@2020_2/2020/08/24/14-07-58-771_282f476a1c0d780f.webp","sourceBytes":3080696,"destBytes":1187598,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":4699,"convertSpendMs":3228,"createdTime":"2020-08-24 22:07:55","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 MB","destSize":"1.1 MB","compressRate":"38.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-iW-taWytIiw/Xz_50_iafnI/AAAAAAAAGc0/3XUoFTdIvt8HztfS3cK1kc5r5KVHPxmkgCLcBGAsYHQ/s72-c/image1%2B%25283%2529.jpg","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn1@2020_6/2020/08/24/14-07-59-323_738fcd761b7ee5be.webp","sourceBytes":2719,"destBytes":1554,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":799,"convertSpendMs":3,"createdTime":"2020-08-24 22:07:59","host":"us-007*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/9DRaIjqziZI/understanding-view-selection-for.html","linkMd5ListStr":"69cfc3e9279dc4a8d0c977973ff42767","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.7 KB","destSize":"1.5 KB","compressRate":"57.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-S5cRq33MXjo/Xw3gT9FdyoI/AAAAAAAAGPo/7QGTjK16kxIl8hyphqjSybLx8nrR43wNgCLcBGAsYHQ/s640/image2%2B%25282%2529.jpg","sourceStatusCode":200,"destWidth":640,"destHeight":416,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn85@2020_5/2020/08/24/14-07-59-245_3717b3682cc27bf0.webp","sourceBytes":108353,"destBytes":51358,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":946,"convertSpendMs":25,"createdTime":"2020-08-24 22:07:59","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/ioYPP_o3naI/exploring-faster-screening-with-fewer.html","linkMd5ListStr":"3891ccfc82c595644b56c5ac424cdf4d","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"105.8 KB","destSize":"50.2 KB","compressRate":"47.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-y5LV-PfaOe8/Xyg8rbe79aI/AAAAAAAAGTY/IRWbV0s_O78llTqpx14NoLoD0j29K47-QCLcBGAsYHQ/s640/image7.png","sourceStatusCode":200,"destWidth":593,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn74@2020_4/2020/08/24/14-07-59-390_72d8631d55dc0348.webp","sourceBytes":633903,"destBytes":57288,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1237,"convertSpendMs":115,"createdTime":"2020-08-24 22:07:59","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"619 KB","destSize":"55.9 KB","compressRate":"9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Kc4t05mihzY/Xz2yxD4QdpI/AAAAAAAAGcA/JR643zBGlA8ntm4_G0_5aLI_328PLgGiACLcBGAsYHQ/s72-c/Environments.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn10@2020_5/2020/08/24/14-07-59-404_3d283dac124096aa.webp","sourceBytes":8468,"destBytes":2168,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":900,"convertSpendMs":5,"createdTime":"2020-08-24 22:07:59","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"8.3 KB","destSize":"2.1 KB","compressRate":"25.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-p-rZ4EcQ6MM/Xz2Mn0XxHlI/AAAAAAAAGao/3Mhvgjv55jUlIACaffUbjLCSx6YVMQlCQCLcBGAsYHQ/s72-c/image3.png","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn13@2020_4/2020/08/24/14-07-59-515_730a903ab0b285f6.webp","sourceBytes":11638,"destBytes":2780,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":947,"convertSpendMs":21,"createdTime":"2020-08-24 22:07:59","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","linkMd5ListStr":"f368ca441fd668170d34ab61787b6561","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"11.4 KB","destSize":"2.7 KB","compressRate":"23.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-H8zldR24sTQ/XzLQ2wYcz0I/AAAAAAAAGWI/xvxV8GxLdM8Tx852QR5oK3CxgBuEQgfKgCLcBGAsYHQ/s640/RWRL_Overview.gif","sourceStatusCode":200,"destWidth":640,"destHeight":226,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn98@2020_1/2020/08/24/14-07-59-469_317bc5565fbad83b.webp","sourceBytes":124639,"destBytes":70690,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1148,"convertSpendMs":202,"createdTime":"2020-08-24 22:07:59","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","linkMd5ListStr":"e78d187929399261dd59b6fd6be7b186","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"121.7 KB","destSize":"69 KB","compressRate":"56.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/M5E2htUALhA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn25@2020_2/2020/08/24/14-07-58-332_e7e7574bdea46e90.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":848,"convertSpendMs":5,"createdTime":"2020-08-24 22:07:58","host":"us-54*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","linkMd5ListStr":"8fa1dc05a356f402a1ad28227ac1a2ab","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-OLiDcAzEtCg/Xz2rbvae8KI/AAAAAAAAGbk/bQvUDWg-l60yVhBUJXmZv2NFHXFWaENFQCLcBGAsYHQ/s0/image1.gif","sourceStatusCode":200,"destWidth":610,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn66@2020_1/2020/08/24/14-07-59-448_e9f25e0788533521.webp","sourceBytes":736437,"destBytes":596954,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":5568,"convertSpendMs":4158,"createdTime":"2020-08-24 22:07:55","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"719.2 KB","destSize":"583 KB","compressRate":"81.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-PFpXlfSsT9k/XzVxqz6f7EI/AAAAAAAAGYw/rRlgge0Tn5oHOae-d1WVFGFPguucsG_QgCLcBGAsYHQ/s0/image1.gif","sourceStatusCode":200,"destWidth":320,"destHeight":266,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn14@2020_6/2020/08/24/14-07-59-449_cc9d38e8b3da46a8.webp","sourceBytes":2580667,"destBytes":537684,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2690,"convertSpendMs":1225,"createdTime":"2020-08-24 22:07:58","host":"us-015*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 MB","destSize":"525.1 KB","compressRate":"20.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s72-c/image1.gif","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn5@2020_5/2020/08/24/14-07-59-709_d3cf3d57c9ed09b3.webp","sourceBytes":41224,"destBytes":16028,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":1446,"convertSpendMs":294,"createdTime":"2020-08-24 22:07:59","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","linkMd5ListStr":"744d32fc2907f7da5c39c5fc5aaed997","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"40.3 KB","destSize":"15.7 KB","compressRate":"38.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-QkAt8k-MxRk/XzVpQxW1ZCI/AAAAAAAAGXQ/NJjTJUyR-5MdSehx9U1ykh5_tWI0lOaPACLcBGAsYHQ/w205-h274/image5.gif","sourceStatusCode":200,"destWidth":205,"destHeight":274,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn54@2020_3/2020/08/24/14-07-59-546_cc4e2a812568a510.webp","sourceBytes":1409302,"destBytes":281036,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":2322,"convertSpendMs":823,"createdTime":"2020-08-24 22:07:58","host":"us-023*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"274.4 KB","compressRate":"19.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-BWytLL8Yzr0/XvD7ekUv3pI/AAAAAAAAGJU/y64_V7YpIOI7fgo4ITxXuoJa9HplqTtyACLcBGAsYHQ/s640/image7.gif","sourceStatusCode":200,"destWidth":600,"destHeight":299,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn90@2020_2/2020/08/24/14-07-59-567_cb41120403bba637.webp","sourceBytes":4649601,"destBytes":1063364,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":5717,"convertSpendMs":3833,"createdTime":"2020-08-24 22:07:55","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.4 MB","destSize":"1 MB","compressRate":"22.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-evIOqC7twgY/XyHbBfrwVeI/AAAAAAAAGSA/6fO0Q1txQvAkbhTtqc2o_-idvrLS9QnwgCLcBGAsYHQ/s400/image2.gif","sourceStatusCode":200,"destWidth":195,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn57@2020_6/2020/08/24/14-07-58-988_9d8f63e428110bb6.webp","sourceBytes":4278502,"destBytes":1671768,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":3579,"convertSpendMs":1236,"createdTime":"2020-08-24 22:07:57","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/uEq7NDB-AgY/on-device-supermarket-product.html","linkMd5ListStr":"32fb0c3b0fecf8789d62476c3497c382","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.1 MB","destSize":"1.6 MB","compressRate":"39.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Fz-Z6THjArM/XzVxqpVu07I/AAAAAAAAGYs/593BjeHfjIs05HwDAgxbNajSoDoQKghNACLcBGAsYHQ/s0/image6.gif","sourceStatusCode":200,"destWidth":320,"destHeight":266,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn10@2020_1/2020/08/24/14-08-00-120_2a7fa215f10ac08f.webp","sourceBytes":2696056,"destBytes":402010,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":3309,"convertSpendMs":1923,"createdTime":"2020-08-24 22:07:58","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 MB","destSize":"392.6 KB","compressRate":"14.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-N494e9_yk00/XywzbwgHMgI/AAAAAAAAGUo/4rWZgcvMPaQVphDK6SSeDZp8-79REaIAwCLcBGAsYHQ/s0/image8.gif","sourceStatusCode":200,"destWidth":546,"destHeight":500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn30@2020_1/2020/08/24/14-07-59-537_1fac569e67d27203.webp","sourceBytes":11258539,"destBytes":1345726,"targetWebpQuality":30,"feedId":1156,"totalSpendMs":5919,"convertSpendMs":2781,"createdTime":"2020-08-24 22:07:56","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"10.7 MB","destSize":"1.3 MB","compressRate":"12%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-olx2WMIKhek/XzLRayW8BOI/AAAAAAAAGWc/jhlCkKyW2SoxSnGy8kmtfx3m1SfzjNjuQCLcBGAsYHQ/s640/image3.gif","sourceStatusCode":200,"destWidth":640,"destHeight":160,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn94@2020_5/2020/08/24/14-08-01-702_543bc053de1b8ed8.webp","sourceBytes":2921431,"destBytes":840384,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":3857,"convertSpendMs":2353,"createdTime":"2020-08-24 22:07:59","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","linkMd5ListStr":"e78d187929399261dd59b6fd6be7b186","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 MB","destSize":"820.7 KB","compressRate":"28.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-yp_f36oOQks/XvDqkBpw5ZI/AAAAAAAAGI4/agnH2a7FOS0vvEPx8bTFTl7eKqSbC9oTQCLcBGAsYHQ/s400/image6.gif","sourceStatusCode":200,"destWidth":400,"destHeight":400,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn93@2020_6/2020/08/24/14-08-00-941_294a09b1d611d77b.webp","sourceBytes":8648668,"destBytes":2067406,"targetWebpQuality":45,"feedId":1156,"totalSpendMs":6450,"convertSpendMs":3952,"createdTime":"2020-08-24 22:07:56","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"8.2 MB","destSize":"2 MB","compressRate":"23.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-5YsKCaL2M4U/XvOOsy7muxI/AAAAAAAAGKY/ghER-egTAAoUR6RZy-ZoZN2A-ab_p8YMwCLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":600,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn34@2020_5/2020/08/24/14-08-00-908_3138f44e133d7529.webp","sourceBytes":4116661,"destBytes":1872742,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":5959,"convertSpendMs":3112,"createdTime":"2020-08-24 22:07:57","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/M5E2htUALhA/sensing-force-based-gestures-on-pixel-4.html","linkMd5ListStr":"8fa1dc05a356f402a1ad28227ac1a2ab","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.9 MB","destSize":"1.8 MB","compressRate":"45.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-5sT_B1DWTGg/Xyw0UM40atI/AAAAAAAAGVM/foqPr2XPszQYxAATdmORKjJ8BK9bu6DFgCLcBGAsYHQ/s0/image2.gif","sourceStatusCode":200,"destWidth":398,"destHeight":360,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn53@2020_5/2020/08/24/14-08-01-121_0b24e330a9583d93.webp","sourceBytes":14889533,"destBytes":1640964,"targetWebpQuality":4,"feedId":1156,"totalSpendMs":6902,"convertSpendMs":4173,"createdTime":"2020-08-24 22:07:56","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"14.2 MB","destSize":"1.6 MB","compressRate":"11%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-GJxNLlVwxqI/XywzLGfARTI/AAAAAAAAGUg/zkvGnNLmHmwyb_nDQ4j46mi95wPRYyP7gCLcBGAsYHQ/s0/image12.gif","sourceStatusCode":200,"destWidth":512,"destHeight":540,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn70@2020_1/2020/08/24/14-08-02-343_4d83820b77faa5ed.webp","sourceBytes":17714489,"destBytes":1312224,"targetWebpQuality":4,"feedId":1156,"totalSpendMs":6065,"convertSpendMs":3418,"createdTime":"2020-08-24 22:07:57","host":"us-019*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/T5ahr9GBor4/mediapipe-iris-real-time-iris-tracking.html","linkMd5ListStr":"6177c6f6f007891265bd244d276b5967","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.9 MB","destSize":"1.3 MB","compressRate":"7.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-FZa1l-V1xUk/Xz2Mus9zzUI/AAAAAAAAGas/pcMTYi0RCf41dzn9evv4D3mxgy-R9ih_ACLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":640,"destHeight":250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn87@2020_6/2020/08/24/14-08-02-712_e641fe3b7ebe5395.webp","sourceBytes":531532,"destBytes":410120,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":4684,"convertSpendMs":3411,"createdTime":"2020-08-24 22:07:59","host":"us-53*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sokv3SkAWtI/understanding-deep-learning-on.html","linkMd5ListStr":"f368ca441fd668170d34ab61787b6561","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"519.1 KB","destSize":"400.5 KB","compressRate":"77.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Yg5us32h5YE/Xz2rogG6KHI/AAAAAAAAGbo/Ay4oOI1bQncrCt-RbE0kI6icoFUDG9oXQCLcBGAsYHQ/s640/image5.gif","sourceStatusCode":200,"destWidth":640,"destHeight":208,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn13@2020_1/2020/08/24/14-08-02-206_679438de95c6e5d4.webp","sourceBytes":1214024,"destBytes":846668,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":6965,"convertSpendMs":4717,"createdTime":"2020-08-24 22:07:57","host":"europe70*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"826.8 KB","compressRate":"69.7%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-W0B4-YeXuno/XvDqU3z1NdI/AAAAAAAAGIs/GB_hBRziJGMpzewokDRt7C8Z5tKgU_4wACLcBGAsYHQ/s640/image8.gif","sourceStatusCode":200,"destWidth":600,"destHeight":158,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn17@2020_1/2020/08/24/14-08-03-342_4762c3c3775a1859.webp","sourceBytes":2535021,"destBytes":739588,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":6580,"convertSpendMs":4765,"createdTime":"2020-08-24 22:07:58","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 MB","destSize":"722.3 KB","compressRate":"29.2%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-G5xcb5HLe5w/XzLRQiK5UXI/AAAAAAAAGWU/QF78i71-hpQuvyRuyoDpBcFAjMjtkC5zACLcBGAsYHQ/w410-h308/image4.gif","sourceStatusCode":200,"destWidth":410,"destHeight":308,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn57@2020_1/2020/08/24/14-08-03-387_b496f357e876f99c.webp","sourceBytes":2541144,"destBytes":1667158,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":6048,"convertSpendMs":4616,"createdTime":"2020-08-24 22:07:58","host":"us-011*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fVhMJKABb9s/a-simulation-suite-for-tackling-applied.html","linkMd5ListStr":"e78d187929399261dd59b6fd6be7b186","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 MB","destSize":"1.6 MB","compressRate":"65.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-1nHmepwWcAg/Xyg8dWNKh8I/AAAAAAAAGTM/gHSNE9eMW38kgDE8w8fdkOFQKV8COpUkACLcBGAsYHQ/s640/image1.gif","sourceStatusCode":200,"destWidth":640,"destHeight":303,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn61@2020_2/2020/08/24/14-08-03-139_383b087a83a3572d.webp","sourceBytes":17452752,"destBytes":1148734,"targetWebpQuality":4,"feedId":1156,"totalSpendMs":6226,"convertSpendMs":3598,"createdTime":"2020-08-24 22:07:58","host":"europe-24*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/2B94l2vj1K4/live-hdr-and-dual-exposure-controls-on.html","linkMd5ListStr":"977eb1a6b95954423c643687bd4266ff","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.6 MB","destSize":"1.1 MB","compressRate":"6.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/--mbMV8fQY28/XxsvbGL_l-I/AAAAAAAAGQ0/Br9B3XGnBa07barUxC4XTi8hSDxYzwAEgCLcBGAsYHQ/s640/image5.png","sourceStatusCode":200,"destWidth":640,"destHeight":377,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn21@2020_5/2020/08/24/14-08-04-796_df28e85253ab9cd7.webp","sourceBytes":103839,"destBytes":33800,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":952,"convertSpendMs":19,"createdTime":"2020-08-24 22:08:04","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/sKILqR_zvQI/announcing-scann-efficient-vector.html","linkMd5ListStr":"6fa5004149ccce453a3e5645aa8b9477","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"101.4 KB","destSize":"33 KB","compressRate":"32.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-gjJyqRYR_5o/XzRO635ImzI/AAAAAAAAGWs/1mgK5iropsIINNQYx7wpZCf3cu5iiRK3ACLcBGAsYHQ/s640/image1.gif","sourceStatusCode":200,"destWidth":640,"destHeight":262,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn69@2020_3/2020/08/24/14-08-04-982_bdfeb2aecf2f5b66.webp","sourceBytes":430262,"destBytes":212262,"targetWebpQuality":75,"feedId":1156,"totalSpendMs":7463,"convertSpendMs":6014,"createdTime":"2020-08-24 22:07:58","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/FjfQTAdG8E8/realm-integrating-retrieval-into.html","linkMd5ListStr":"744d32fc2907f7da5c39c5fc5aaed997","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"420.2 KB","destSize":"207.3 KB","compressRate":"49.3%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-aMeUM4tOpxM/Xz2sHEc0WEI/AAAAAAAAGb0/xAeSG6rn0q0Y2SnIHxGZwY1_fRssvPfPwCLcBGAsYHQ/s0/image4.gif","sourceStatusCode":200,"destWidth":590,"destHeight":256,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn18@2020_6/2020/08/24/14-08-01-930_a309715d45d891d9.webp","sourceBytes":6463404,"destBytes":1928266,"targetWebpQuality":60,"feedId":1156,"totalSpendMs":10181,"convertSpendMs":4169,"createdTime":"2020-08-24 22:07:57","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/TQbqJYtduXE/tackling-open-challenges-in-offline.html","linkMd5ListStr":"a6859779ff2bda767c63a2b9c5a2c0a9","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.2 MB","destSize":"1.8 MB","compressRate":"29.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-KdKAir-7vdE/XvD9RJcjT2I/AAAAAAAAGJg/xQbWbeYVFcgLiGkR-gsmRgZ_2W98UALbgCLcBGAsYHQ/s640/image2.gif","sourceStatusCode":200,"destWidth":640,"destHeight":410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn66@2020_3/2020/08/24/14-08-10-532_69460a92dce6caab.webp","sourceBytes":22934908,"destBytes":1724184,"targetWebpQuality":4,"feedId":1156,"totalSpendMs":13750,"convertSpendMs":4111,"createdTime":"2020-08-24 22:07:58","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/CPC0hNgMKL4/repnet-counting-repetitions-in-videos.html","linkMd5ListStr":"454039e4a25f3a17b8deb713cd0faa82","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.9 MB","destSize":"1.6 MB","compressRate":"7.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-Q64KtZLWOT8/XzVxdkZDMgI/AAAAAAAAGYk/qj7mLsOL3AMcDkusMgYDGrSqauRAljR9gCLcBGAsYHQ/s640/image8.gif","sourceStatusCode":200,"destWidth":640,"destHeight":180,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn77@2020_3/2020/08/24/14-08-11-612_359c361fc7e252f2.webp","sourceBytes":8401020,"destBytes":2944700,"targetWebpQuality":45,"feedId":1156,"totalSpendMs":16865,"convertSpendMs":13474,"createdTime":"2020-08-24 22:07:56","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/e0Yr7hZaFjA/on-device-real-time-body-pose-tracking.html","linkMd5ListStr":"3c71cc7626d30346f461b7b4d2ecb255","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"8 MB","destSize":"2.8 MB","compressRate":"35.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HZRniCIf7Kg/XwZPynbRhoI/AAAAAAAAGN4/1NYiPnmq8cAZnrhAZncq67ySs8CFKqxGQCLcBGAsYHQ/s640/image1.gif","sourceStatusCode":200,"destWidth":640,"destHeight":360,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn94@2020_6/2020/08/24/14-08-21-311_2c26612e20a3b695.webp","sourceBytes":10004195,"destBytes":5009318,"targetWebpQuality":37,"feedId":1156,"totalSpendMs":27407,"convertSpendMs":13839,"createdTime":"2020-08-24 22:07:56","host":"us-036*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/xGFebACpRTc/automl-zero-evolving-code-that-learns.html","linkMd5ListStr":"9905ef79d2e76ec08be633a84535c116","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.5 MB","destSize":"4.8 MB","compressRate":"50.1%"}],"successGithubMap":{"myreaderx14":5,"myreaderx8":5,"myreaderx7":5,"myreaderx15":5,"myreaderx6":4,"myreaderx16":6,"myreaderx4":5,"myreaderx32":5,"myreaderx10":4,"myreaderx11":5,"myreaderx33":5,"myreaderx3":6,"myreaderx12":5,"myreaderx2":5,"myreaderx1":5,"myreaderx13":5,"myreaderx30":5,"myreaderx31":5,"myreaderx18":5,"myreaderx19":5,"myreaderx":4,"myreaderx25":6,"myreaderx27":4,"myreaderx21":5,"myreaderx22":5,"myreaderx23":5,"myreaderx24":5,"myreaderx5oss":5,"myreaderx29":5},"failGithubMap":{}}