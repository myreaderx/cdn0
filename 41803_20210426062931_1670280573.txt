{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-04-26 14:28:40","updatedTime":"2021-04-26 14:28:40","title":"Scrapy update: Better broad crawl performance","link":"https://www.zyte.com/?p=4602","description":"\n<p>When crawling the web, there’s always a speed limit. A spider can't fetch faster than the host willing to send the pages. Page serving takes some amount of resources - CPU, disk, network bandwidth, etc. These resources cost money. Unrestricted serving and extensive crawling are the worst combinations. Such a combination could bring applications to halt and deny service to users. Taking all this into account, limiting serving capacity is natural.</p>\n\n\n\n<p>This article explains which Scrapy settings help you honor these limits and how to achieve better performance during broad crawls in the presence of these limits.</p>\n\n\n\n<h2>Problem statement</h2>\n\n\n\n<p>First of all, we need a way to differentiate entities behind domain names. The simplest and fastest one is \"entities never share a single exact domain name\". http://example1.com and http://example2.com are different, so do http://www.example.com and http://about.example.com. Another is \"entities never share a single IP\". Scrapy has to send DNS queries to resolve domain names to IP addresses before deciding. This solves the problem of different domain names served from a single host, so http://www.example.com and http://about.example.com are the same entity.</p>\n\n\n\n<p>For every entity, there is a slot in a<a href=\"https://docs.scrapy.org/en/latest/topics/architecture.html#downloader\" rel=\"noopener\"> Downloader</a>. The number of requests sent to each entity simultaneously is limited by<a href=\"https://docs.scrapy.org/en/latest/topics/settings.html#concurrent-requests-per-domain\" rel=\"noopener\"> CONCURRENT_REQUESTS_PER_DOMAIN</a> or<a href=\"https://docs.scrapy.org/en/latest/topics/settings.html#concurrent-requests-per-ip\" class=\"rank-math-link\" rel=\"noopener\"> CONCURRENT_REQUESTS_PER_IP</a> options. Which one to choose depends on the selected way to differentiate between entities. Such requests can be called active or running. Requests enqueued into Downloader and not sent to the host are called inactive. Every slot has a queue for inactive requests. After finishing an active request and before running one from an inactive queue, Scrapy waits for<a href=\"https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\" rel=\"noopener\"> DOWNLOAD_DELAY</a> seconds.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/downloader.gif\" alt=\"\"/></figure>\n\n\n\n<p>This approach requires tuning to keep a balance between performance and limits honoring. A less tuning-sensitive one is implemented in the AutoThrottle extension. <a href=\"https://docs.scrapy.org/en/latest/topics/autothrottle.html\" class=\"rank-math-link\" rel=\"noopener\">Documentation</a> provides a very good background and description of how it works.</p>\n\n\n\n<h2>Possible approaches</h2>\n\n\n\n<p>The downloader doesn't decide which requests are enqueued into it. It is done by Scheduler. The first implementation of such decision-making doesn't take entities into account. It works well in the case of crawling a specific entity. The situation is very different in the case of <a href=\"https://docs.scrapy.org/en/latest/topics/broad-crawls.html\" class=\"rank-math-link\" rel=\"noopener\">broad crawls</a>. In broad crawls inactive queues for some slots are too long and active requests are not running at full capacity for others.</p>\n\n\n\n<p>After presenting the concept of entities to a Scheduler, there are different strategies to select one for the next request.</p>\n\n\n\n<p><a href=\"https://en.wikipedia.org/wiki/Round-robin_scheduling\" rel=\"noopener\">Round-robin algorithm</a> can be used for request scheduling:</p>\n\n\n\n<ul><li>store all entities in FIFO queue <em>Q</em></li><li>when the next request should be scheduled pop one entity <em>E1</em> (from the top)</li><li>issue a request to <em>E1</em></li><li>push <em>E1</em> back into <em>Q</em> (to the bottom)</li></ul>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/roundrobin.gif\" alt=\"\"/></figure>\n\n\n\n<p>This approach provides an equal flow of requests. For it to work well, every crawled entity should serve pages with the same speed. In the real world, it is not the case. Different hosts have different rendering times and network latencies are also different.</p>\n\n\n\n<p>Let's look at what’s happening in such a situation.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/roundrobin_problem.gif\" alt=\"\"/></figure>\n\n\n\n<p>In the end, the target is to keep the Downloader's queue of inactive requests as short as possible. Generic Computer Science algorithm doesn't work well in real-life conditions. A more task-specific approach needs to be presented.</p>\n\n\n\n<p>Instead of using a model, real information can be used. Downloader knows the length of every queue. Providing such knowledge to a Scheduler solves the problem.</p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img src=\"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/aware.gif\" alt=\"\"/></figure>\n\n\n\n<h2>Experimental results</h2>\n\n\n\n<p>Both of these approaches were implemented in Scrapy. To select the fastest one, broadworm mode of scrapy-bench was used. In this mode, 1000 entities are emulated by 1000 domain aliases to the same server. This server introduces artificial delays on top of serving time. The result is presented in the table below.</p>\n\n\n\n<figure class=\"wp-block-table is-style-regular\"><table class=\"has-subtle-light-gray-background-color has-background\"><tbody><tr><td><strong>Algorithm</strong></td><td><strong>Items per second crawled</strong></td><td><strong>Speedup</strong></td></tr><tr><td>Entity-unaware</td><td>2.34</td><td>1x</td></tr><tr><td>Round-robin</td><td>7.56</td><td>3x</td></tr><tr><td>Ask the downloader</td><td>23.12</td><td>10x</td></tr></tbody></table></figure>\n\n\n\n<p>Based on numbers it was decided to not preserve round-robin implementation inside Scrapy’s codebase, but you can still <a href=\"https://github.com/scrapy/scrapy/blob/821f5bb26077d7f9a6b2b1a72f210f81779f5393/scrapy/pqueues.py#L155-L182\" class=\"rank-math-link\" rel=\"noopener\">find it</a> in the commit history.</p>\n","descriptionType":"html","publishedDate":"Thu, 18 Feb 2021 09:41:33 +0000","feedId":41803,"bgimg":"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/downloader.gif","linkMd5":"a05756a0ea8db7c160ce8d9d9155f8c3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn68@2020_1/2021/04/26/06-29-06-787_6900570cb4ce79b4.webp","destWidth":854,"destHeight":480,"sourceBytes":3717796,"destBytes":7141134,"author":"Nikita Vostretsov","articleImgCdnMap":{"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/downloader.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn68@2020_1/2021/04/26/06-29-06-787_6900570cb4ce79b4.webp","https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/roundrobin.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn16@2020_2/2021/04/26/06-29-14-618_42dbc6f45bdcfbf7.webp","https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/roundrobin_problem.gif":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn11@2020_2/2021/04/26/06-29-27-029_d61039031923e0b3.webp","https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/aware.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn17@2020_6/2021/04/26/06-29-19-743_0e5ee60072bb8d61.webp"},"publishedOrCreatedDate":1619418520430}],"record":{"createdTime":"2021-04-26 14:28:40","updatedTime":"2021-04-26 14:28:40","feedId":41803,"fetchDate":"Mon, 26 Apr 2021 06:28:40 +0000","fetchMs":159,"handleMs":9,"totalMs":51601,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"79b59b7ab14b8d981e3d8a220e20848a","hostName":"us-013*","requestId":"6f342001713f4fb2a13bb324a990549c_41803","contentType":"application/rss+xml; charset=UTF-8","totalBytes":17920962,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":4,"articlesImgsGithubTotal":4,"successGithubMap":{"myreaderx32":1,"myreaderx2":1,"myreaderx1":1,"myreaderx29":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 03:25:21","updatedTime":"2020-09-07 05:42:10","id":41803,"name":"THE SCRAPINGHUB BLOG","url":"https://blog.scrapinghub.com/rss.xml","subscriber":77,"website":null,"icon":"https://blog.scrapinghub.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn66@2020_3/2020/09/06/21-42-07-189_b51c938999942bec.webp","description":"Learn about web data extraction from the experts. 10+ years web scraping veterans share their knowledge.","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":7141134,"tmpBodyImgCdnBytes":10779828,"tmpBgImgCdnBytes":0,"extra4":{"start":1619418520259,"total":0,"statList":[{"spend":161,"msg":"获取xml内容"},{"spend":9,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":23156,"msg":"正文链接上传到cdn"}]},"extra5":4,"extra6":4,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://europe69.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-034.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-035.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/downloader.gif","sourceStatusCode":200,"destWidth":854,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn68@2020_1/2021/04/26/06-29-06-787_6900570cb4ce79b4.webp","sourceBytes":3717796,"destBytes":7141134,"targetWebpQuality":75,"feedId":41803,"totalSpendMs":28130,"convertSpendMs":25639,"createdTime":"2021-04-26 14:28:40","host":"europe-23*","referer":"https://www.zyte.com/?p=4602","linkMd5ListStr":"a05756a0ea8db7c160ce8d9d9155f8c3,a05756a0ea8db7c160ce8d9d9155f8c3","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"6.8 MB","compressRate":"192.1%","sourceSize":"3.5 MB"},{"code":1,"isDone":false,"source":"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/roundrobin.gif","sourceStatusCode":200,"destWidth":854,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn16@2020_2/2021/04/26/06-29-14-618_42dbc6f45bdcfbf7.webp","sourceBytes":2072603,"destBytes":2528546,"targetWebpQuality":75,"feedId":41803,"totalSpendMs":6374,"convertSpendMs":5696,"createdTime":"2021-04-26 14:29:08","host":"us-034*","referer":"https://www.zyte.com/?p=4602","linkMd5ListStr":"a05756a0ea8db7c160ce8d9d9155f8c3","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"2.4 MB","compressRate":"122%","sourceSize":"2 MB"},{"code":1,"isDone":false,"source":"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/aware.gif","sourceStatusCode":200,"destWidth":854,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn17@2020_6/2021/04/26/06-29-19-743_0e5ee60072bb8d61.webp","sourceBytes":1735641,"destBytes":3168574,"targetWebpQuality":75,"feedId":41803,"totalSpendMs":11630,"convertSpendMs":10803,"createdTime":"2021-04-26 14:29:08","host":"us-035*","referer":"https://www.zyte.com/?p=4602","linkMd5ListStr":"a05756a0ea8db7c160ce8d9d9155f8c3","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"3 MB","compressRate":"182.6%","sourceSize":"1.7 MB"},{"code":1,"isDone":false,"source":"https://gist.githubusercontent.com/whalebot-helmsman/1654b913c98375047a258ff44e06697b/raw/a0d232651819f5806c92780372e063256df4a9b9/roundrobin_problem.gif","sourceStatusCode":200,"destWidth":854,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn11@2020_2/2021/04/26/06-29-27-029_d61039031923e0b3.webp","sourceBytes":3298110,"destBytes":5082708,"targetWebpQuality":75,"feedId":41803,"totalSpendMs":23014,"convertSpendMs":17560,"createdTime":"2021-04-26 14:29:08","host":"europe69*","referer":"https://www.zyte.com/?p=4602","linkMd5ListStr":"a05756a0ea8db7c160ce8d9d9155f8c3","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"4.8 MB","compressRate":"154.1%","sourceSize":"3.1 MB"}],"successGithubMap":{"myreaderx32":1,"myreaderx2":1,"myreaderx1":1,"myreaderx29":1},"failGithubMap":{}}