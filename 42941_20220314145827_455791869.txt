{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2022-03-14 22:56:23","updatedTime":"2022-03-14 22:56:23","title":"Learn OpenTelemetry tracing with this lightweight microservices demo","link":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","description":"<img src=\"https://www.timescale.com/blog/content/images/2022/02/otel-demo-blogpost-timescale-1.png\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\"><p>OpenTelemetry is an open source observability framework for cloud-native service and infrastructure instrumentation hosted by the <a href=\"https://www.cncf.io/\">Cloud Native Computing Foundation</a> (CNCF). It has gained a lot of momentum with contributions from all major cloud providers (AWS, Google, Microsoft) as well as observability vendors (including Timescale) to the point it has become the second project with the <a href=\"https://all.devstats.cncf.io/d/1/activity-repository-groups?orgId=1\">most activity</a> and contributors only after Kubernetes.</p><p>Last October, we added <a href=\"https://twitter.com/ramonguiu/status/1447552147592261638\">beta support for OpenTelemetry traces</a> to <a href=\"https://www.timescale.com/promscale\">Promscale</a>, the observability backend powered by SQL. A trace (or distributed trace) is a connected representation of the sequence of operations that were performed across all microservices involved in order to fulfill an individual request. Each of those operations is represented using a span. Each span includes a reference to the parent span except the first one which is called the root span. As a result, a trace is a tree of spans. We told you everything about OpenTelemetry traces in <a href=\"https://blog.timescale.com/blog/what-are-traces-and-how-sql-yes-sql-and-opentelemetry-can-help-us-get-more-value-out-of-traces-to-build-better-software/\">this blog post</a>.</p><p>However, it&#x2019;s better to learn by doing. When talking about OpenTelemetry traces and Promscale, we often wished we had access to a microservices demo application instrumented with OpenTelemetry, so users could play with it to directly experience the potential of tracing. For example, <a href=\"https://www.timescale.com/blog/what-are-traces-and-how-sql-yes-sql-and-opentelemetry-can-help-us-get-more-value-out-of-traces-to-build-better-software/\">in the blog post we referenced earlier</a>, we used <a href=\"https://github.com/honeycombio/microservices-demo\">Honeycomb&#x2019;s fork of Google&apos;s microservices demo</a>, which adds OpenTelemetry instrumentation to those services. But while it is a great demo of a microservices environment, we found it too complex and resource-heavy to run it locally on a laptop for the purposes of playing with OpenTelemetry, since it requires a Kubernetes cluster with 4 cores and 4 GB of available memory. </p><p>This inspired us to build a more accessible and lightweight OpenTelemetry demo application: </p><p><a href=\"https://github.com/timescale/opentelemetry-demo/\"><strong>https://github.com/timescale/opentelemetry-demo/</strong></a></p><p>In this blog post, we&#x2019;ll introduce you to this demo, which consists of a password generator overdesigned as a microservices application. We&#x2019;ll dive into its architecture, we&#x2019;ll explain how we instrumented the code to produce OpenTelemetry traces, how to get the demo up and running on your computer in a few minutes, and how to analyze those traces with Jaeger, Grafana, Promscale, and SQL to understand how the system behaves and to troubleshoot problems. </p><p>But that&apos;s not all: later in the post, we will walk you through the six (!) prebuilt Grafana dashboards <a href=\"https://github.com/timescale/opentelemetry-demo/#examples\">that come with the demo</a>. These are powerful dashboards that will give you a deep understanding of your systems by monitoring throughput, latency, and error rate. You will also be able to understand upstream and downstream service dependencies.</p><p>Even if we built this demo to make it easier for users to experiment with Promscale, this demo application is not Promscale-specific. It can be easily configured to send the telemetry it generates to any OpenTelemetry-compatible backend and we hope the broader community, even those not using Promcale, will find it useful. </p><p>To learn about Promscale, check out our <a href=\"https://www.timescale.com/promscale\">website</a>. If you start using us, <a href=\"https://slack.timescale.com\">join the #promscale channel in our Slack Community</a> and chat with us about observability, Promscale, and anything in between - the Observability team is very active in #promscale! Plus, we just launched our <a href=\"https://www.timescale.com/forum/\">Timescale Community Forum</a>. Feel free to shoot us any technical questions there as well.</p><p>One more thing: if you share our mission of serving developers worldwide &#x1F30F; and want to join our fully remote, global team... <a href=\"https://www.timescale.com/careers\">We are hiring broadly across many roles</a>!</p><h2 id=\"demo-architecture\">Demo Architecture</h2><p>The <a href=\"https://github.com/timescale/opentelemetry-demo\">demo application</a> is a password generator that has been overdesigned to run as a microservices application. It includes five microservices:</p><ul><li>The<em> </em><strong>generator service </strong>is the entry point for requests to generate a new password. It calls all other services to create a random password.</li><li>The<em> </em><strong>upper service</strong> returns random uppercase letters (A-Z).</li><li>The <strong>lower service</strong> returns random lowercase letters (a-z).</li><li>The <strong>digit service</strong> returns random digits (0-9).</li><li>The <strong>special service</strong><em> </em>returns random special characters.</li></ul><p>Apart from the microservices, the demo also deploys a pre-configured OpenTelemetry observability stack composed of the OpenTelemetry Collector, Promscale, Jaeger, and Grafana:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/03/architecture-diagram-demo-env.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1126\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/03/architecture-diagram-demo-env.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/03/architecture-diagram-demo-env.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/03/architecture-diagram-demo-env.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/03/architecture-diagram-demo-env.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Architecture diagram of the demo environment</figcaption></figure><p>Obviously, this is a silly design - this is not how you would design a password generator. But we decided to use this application because it&#x2019;s a very easy-to-understand example (everybody is familiar with creating secure passwords), the code is simple, and it&#x2019;s very lightweight, so you can easily run it on a computer with Docker (no Kubernetes required).</p><p>To make the traces generated more interesting, the code of the services introduces random wait times and errors. There is also a bug in the code of one of the services (an Easter egg &#xA0;&#x1F440; for you to find!).</p><p>All services are built in Python, except the lower service, which is built in Ruby. The demo also includes a load generator that makes requests to the generator service to create new passwords. It instantiates three instances of that load generator.</p><h2 id=\"instrumentation\">Instrumentation<br></h2><p>The first step to start getting visibility into the performance and behaviors of the different microservices is to instrument the code with OpenTelemetry to generate traces. &#xA0;The five microservices included in the demo have already been instrumented; in this section, however, we&apos;ll explain how we did it, in case you&#x2019;re interested in learning how to instrument your own. </p><p>OpenTelemetry provides SDKs and libraries to instrument code in a variety of different languages. SDKs allow you to manually instrument your services; at the time of this writing, the OpenTelemetry project provides SDKs for 12 (!) languages (C++, .NET, Erlang/Elixir, Go, Java, Javascript, PHP, Python, Ruby, Rust, and Swift). The maturity of those SDKs varies though - check the <a href=\"https://opentelemetry.io/docs/instrumentation/\">OpenTelemetry instrumentation documentation</a> for more details.</p><p>Additionally, OpenTelemetry provides automatic instrumentation for many libraries in a number of languages like <a href=\"https://opentelemetry.io/docs/instrumentation/python/automatic/\">Python</a>, <a href=\"https://opentelemetry.io/docs/instrumentation/ruby/automatic/\">Ruby</a>, <a href=\"https://opentelemetry.io/docs/instrumentation/java/automatic/\">Java</a>, <a href=\"https://opentelemetry.io/docs/instrumentation/js/getting-started/nodejs/#instrumentation-modules\">Javascript</a>, or .<a href=\"https://opentelemetry.io/docs/instrumentation/net/automatic/\">NET</a>. Automatic instrumentation typically works through library hooks or monkey-patching library code. This helps you get a lot of visibility into your code with very little work, dramatically reducing the amount of effort required to start using distributed tracing to improve your applications.</p><p>For services written in Python, you could leverage auto-instrumentation and not have to touch a single line of code. For example, for the generator service and after setting the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable to the URL of the OTLP endpoint (the OpenTelemetry Collector for example) where you want data to be sent, you would run the following command to auto-instrument the service:</p><pre><code class=\"language-SQL\">opentelemetry-instrument --traces_exporter otlp python generator.py</code></pre><p>As we just mentioned, we already instrumented the code of the different microservices in our password generator demo. In our case, we decided to manually instrument the code, to show how it is done and in order to add some additional instrumentation, but that is not required. </p><p>To instrument the Python services, first we imported a number of OpenTelemetry libraries:</p><pre><code class=\"language-SQL\">from opentelemetry import trace\nfrom opentelemetry.trace import StatusCode, Status\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n</code></pre><p>Then, we initialized and set up the tracing instrumentation for the service, including setting up the Flask and HTTP auto-instrumentation:</p><pre><code class=\"language-SQL\">trace.set_tracer_provider(TracerProvider(resource=Resource.create({&#34;service.name&#34;: &#34;generator&#34;})))\nspan_exporter = OTLPSpanExporter(endpoint=&#34;collector:4317&#34;)\ntrace.get_tracer_provider().add_span_processor(BatchSpanProcessor(span_exporter))\nFlaskInstrumentor().instrument_app(app)\nRequestsInstrumentor().instrument()\ntracer = trace.get_tracer(__name__)</code></pre><p>And finally, we added some manual spans with &#xA0; <code>tracer.start_as_current_span(<em>name</em>)</code> for each operation, and some events with <code>span.add_event(<em>name</em>)</code>:</p><pre><code class=\"language-SQL\">def uppers() -&#62; Iterable[str]:\n    with tracer.start_as_current_span(&#34;generator.uppers&#34;) as span:\n        x = []\n        for i in range(random.randint(0, 3)):\n            span.add_event(f&#34;iteration_{i}&#34;, {&apos;iteration&apos;: i})\n            try:\n                response = requests.get(&#34;http://upper:5000/&#34;)\n                c = response.json()[&apos;char&apos;]\n            except Exception as e:\n                e = Exception(f&#34;FAILED to fetch a upper char&#34;)\n                span.record_exception(e)\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                raise e\n            x.append(c)\n        return x\n</code></pre><p>Check the code of the <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/generator/generator.py\">generator</a>, <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/upper/upper.py\">upper</a>, <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/digit/digit.py\">digit</a> or <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/special/special.py\">special</a> services for complete examples.</p><p>For the Ruby service, we did something similar. Check <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/lower/lower.rb\">the code of the lower service</a> for more details!</p><h2 id=\"running-the-demo\">Running the demo</h2><p>The demo uses <code>docker-compose</code> to get all components up and running on your computer. Therefore, before running it, you first need to install the <a href=\"https://docs.docker.com/engine/install/\">Docker Engine</a> and <a href=\"https://docs.docker.com/compose/install/\">Docker Compose</a> as prerequisites. We&#x2019;ve tested the demo on recent versions of MacOS (both Intel and M1 processors), Linux (tested on Ubuntu) and Windows.</p><p>The <a href=\"https://github.com/timescale/opentelemetry-demo\">opentelemetry-demo GitHub repo</a> has everything you need to run the demo. First, clone the repo:</p><pre><code class=\"language-SQL\">git clone https://github.com/timescale/opentelemetry-demo.git</code></pre><p>Or if you prefer, just <a href=\"https://github.com/timescale/opentelemetry-demo/archive/refs/heads/main.zip\">download it</a> and unzip it.</p><p>Next, go into the opentelemetry-demo folder (or opentelemetry-demo-main if you used the download option):</p><pre><code class=\"language-SQL\">cd opentelemetry-demo</code></pre><p>And run:</p><pre><code class=\"language-SQL\">docker-compose up --detach</code></pre><p><em>Note: on Linux systems the default installation only gives the root user permissions to connect to the Docker Engine and the previous command would throw permission errors. Running sudo docker-compose up --detach would fix the permissions problems by running the command as root but you may want to </em><a href=\"https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user\"><em>grant your user account permission to manage Docker</em></a><em>.</em></p><p>This will execute the instructions in the <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/docker-compose.yaml\">docker-compose.yaml</a> file, which will build a Docker image for each service. This could take a few minutes. Then, run all the different components of the demo environment. When <code>docker-compose</code> completes, you should see something like the following:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/output-docker-compose-otel-demo.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1492\" height=\"728\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/output-docker-compose-otel-demo.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/output-docker-compose-otel-demo.png 1000w, https://www.timescale.com/blog/content/images/2022/02/output-docker-compose-otel-demo.png 1492w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Once the microservices demo installation is complete, you will see this output from docker-compose.</em></figcaption></figure><p>And that&#x2019;s it. Congratulations! You have a microservices application instrumented with OpenTelemetry sending traces to an OpenTelemetry observability stack.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/running-otel-demo-short.gif\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1152\" height=\"720\"><figcaption><em>Installing the microservices demo with docker-compose.</em></figcaption></figure><p>If at any point you want to stop the demo, just run <code>docker-compose down</code>.</p><p>We&apos;ll talk about Jaeger and Grafana extensively in the next sections of this blogpost, but as a quick note - Jaeger runs on <a href=\"http://localhost:16686/\">http://localhost:16686/</a> and Grafana on <a href=\"http://localhost:3000/\">http://localhost:3000/</a> . Grafana will require credentials to log in for the first time: use <code>admin</code> for the username, and also <code>admin</code> for the password. &#xA0;You will be able to update your password immediately after. </p><h2 id=\"visualize-individual-traces-with-jaeger\">Visualize individual traces with Jaeger<br></h2><p>Now that you have the entire demo up, including the password generator microservices and the preconfigured OpenTelemetry observability stack, you are ready to start playing around with the generated traces. </p><p>As we explained earlier, traces represent a sequence of operations, typically across multiple services, that are executed in order to fulfill a request. Each operation corresponds to a span in the trace. Traces follow a tree-like structure, and thus most tools for visualizing traces offer a tree view of all the spans that make up a trace.</p><p>Jaeger is the most well-known open source tool for visualizing individual traces. Actually, Jaeger is more than just visualization, as it also provides libraries to instrument code with traces (which <a href=\"https://medium.com/jaegertracing/migrating-from-jaeger-client-to-opentelemetry-sdk-bd337d796759\">will be deprecated soon in favor of OpenTelemetry</a>), a backend service to ingest those traces, and an in-memory and local disk storage. Jaeger also provides a plugin mechanism and a gRPC API for integrating with external storage systems. (Promscale implements such gRPC API to integrate with Jaeger; actually, the Promscale team <a href=\"https://github.com/jaegertracing/jaeger/pull/3383\">contributed it to the project</a>!)</p><p>Jaeger is particularly helpful when you know your system is having a problem and you have enough information to narrow down the search to a specific set of requests. For example, if you know that some users of your application are experiencing very slow performance when making requests to a specific API, you could run a search for requests to that API that have taken more than X seconds, and from there visualize several individual traces to find a pattern of which service and operation is the bottleneck.</p><p>Let&#x2019;s turn to our simple demo application to show an example of this. If you have the demo application running, <a href=\"http://localhost:16686/search\">you can access Jaeger here.</a></p><p>Now, let&#x2019;s say we know there are some requests to generate a password that are taking more than 10 seconds. To find the problematic requests, we could <a href=\"http://localhost:16686/search?end=1644971490615000&#38;limit=20&#38;lookback=1h&#38;maxDuration&#38;minDuration=10s&#38;operation=%2F&#38;service=generator&#38;start=1644967890615000\">filter traces in Jaeger to the generator service that have taken more than 10 seconds</a>, and sort them by &#x201C;Longest First&#x201D;:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/jaeger-search-ui-otel-demo.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1918\" height=\"1120\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/jaeger-search-ui-otel-demo.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/jaeger-search-ui-otel-demo.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/jaeger-search-ui-otel-demo.png 1600w, https://www.timescale.com/blog/content/images/2022/02/jaeger-search-ui-otel-demo.png 1918w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Jaeger search UI (if you have the microservices demo running, </em><a href=\"http://localhost:16686/search\"><em>you can access Jaeger here</em></a><em>).</em></figcaption></figure><p>In our case, we are seeing a request with 200 spans taking more than 30 seconds. Let&#x2019;s open that trace and see why it is taking so long:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1141\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png 1600w, https://www.timescale.com/blog/content/images/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png 2058w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Visualizing a distributed trace in Jaeger (if you have the microservices demo running, </em><a href=\"http://localhost:16686/search\"><em>you can access Jaeger here</em></a><em>).</em></figcaption></figure><p>After inspecting it carefully, we can see that the <code>random_digit</code> operation in the digit service is taking more than 1 second every time it runs, while other operations are usually run in a few microseconds or milliseconds. If we go back to the list of traces, and open another slow trace, we will see that it points again to the same problem. </p><p>With that information, we can now go inspect the code of the <code>random_digit</code> operation in the digit service, which reveals the problem:</p><pre><code class=\"language-SQL\">def random_digit() -&#62; str:\n    with tracer.start_as_current_span(&#34;random_digit&#34;) as span:\n        work(0.0003, 0.0001)\n\n        # slowness varies with the minute of the hour\n        time.sleep(sin(time.localtime().tm_min) + 1.0)\n\n        c = random.choice(string.digits)\n        span.set_attribute(&apos;char&apos;, c)\n        return c\n</code></pre><p>Now we can see why this operation is giving us trouble: it&apos;s because the code is calling the <code>time.sleep</code> function, which is slowing it down! </p><p>Besides trace visualization, Jaeger also provides <a href=\"http://localhost:16686/dependencies\">a service map</a> of your microservices architecture generated from trace data. This is useful to get an overview of service dependencies, which can help identify unexpected dependencies and also ways to simplify or optimize the architecture of your application. For example, the map below shows us how the generator service calls all the other services:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/jaeger-map-view-otel-demo.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1858\" height=\"890\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/jaeger-map-view-otel-demo.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/jaeger-map-view-otel-demo.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/jaeger-map-view-otel-demo.png 1600w, https://www.timescale.com/blog/content/images/2022/02/jaeger-map-view-otel-demo.png 1858w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Service map view in Jaeger (if you have the microservices demo running, </em><a href=\"http://localhost:16686/search\"><em>you can access Jaeger here</em></a><em>).</em></figcaption></figure><h2 id=\"analyze-the-performance-and-behavior-of-the-application-with-grafana-and-sql\">Analyze the performance and behavior of the application with Grafana and SQL<br></h2><p>Jaeger is a fantastic tool to understand individual traces, but it doesn&#x2019;t provide a query language to analyze and aggregate data as needed. Traces have a wealth of information that you can use to better understand how your system is performing and behaving - &#xA0;if you have the ability to run arbitrary queries on the data. For example, you can&#x2019;t compute the throughput, latency, and error rate of your services with Jaeger.</p><p>This is where Promscale can help you. The observability stack deployed by the password generator demo stores all OpenTelemetry traces in Promscale through its endpoint that implements the <a href=\"https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md\">OpenTelemetry Protocol</a> (OTLP). And because Promscale is built on top of TimescaleDB and PostgreSQL, it allows you to run full SQL to analyze the data.</p><p>In this section, we&#x2019;ll tell you how to use Grafana to analyze traces with a set of predefined dashboards that query the data in Promscale using SQL. These dashboards allow you to monitor the three golden signals often used to measure the performance of an application: throughput, latency, and error rate. We&apos;ve also built dashboards that will help you understand your upstream and downstream service dependencies. </p><p>We have built six predefined dashboards to do different analysis, <a href=\"https://github.com/timescale/opentelemetry-demo/#examples\">all of them included in our microservices demo</a>:</p><ol><li><a href=\"http://localhost:3000/d/QoZDH91nk/01-request-rate?orgId=1\">Request rates or throughput</a></li><li><a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\">Request durations or latency</a></li><li><a href=\"http://localhost:3000/d/CiE9l917z/02-error-rates\">Error rates</a></li><li><a href=\"http://localhost:3000/d/scyq99J7k/04-service-dependencies?orgId=1\">Service dependencies</a></li><li><a href=\"http://localhost:3000/d/lyIow61nz/05-upstream-spans?orgId=1&#38;var-service=digit&#38;var-operation=%2F\">Upstream dependencies</a></li><li><a href=\"http://localhost:3000/d/SdzI3eJnk/06-downstream-spans?orgId=1\">Downstream dependencies</a></li></ol><p>Let&apos;s get into them. </p><h3 id=\"request-rates\">Request rates &#xA0;</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/request-rates-dashboard.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1046\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/request-rates-dashboard.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/request-rates-dashboard.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/request-rates-dashboard.png 1600w, https://www.timescale.com/blog/content/images/2022/02/request-rates-dashboard.png 2238w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Request rates dashboard in Grafana. </em><a href=\"http://localhost:3000/d/QoZDH91nk/01-request-rate?orgId=1\"><em>You can access it here</em></a><em> if you have the microservices demo running.</em></figcaption></figure><p>It&#x2019;s possible to compute throughput of your services from traces, if you can query and aggregate them like you can do with SQL. Before showing you how to do that though, it is important to notice that this requires that all trace data is stored, or the results will not be accurate. It is not uncommon to use downsampling to only keep a representative set of traces to reduce the compute and storage requirements, but if traces are downsampled, the computed request rate won&#x2019;t be exact.</p><p>In our demo, the <a href=\"http://localhost:3000/d/QoZDH91nk/01-request-rate?orgId=1\">Request Rate dashboard</a> shows the evolution of throughput (i.e. request rate) over time. It has two charts: the first one computes the requests per second using one-minute time buckets (lower resolution, equivalent to the average requests per second in each minute), and the other one uses one-second time buckets.</p><p>To generate the requests per second with one-second time buckets (the bottom chart), we used the following SQL query:</p><pre><code class=\"language-SQL\">SELECT\n    time_bucket(&apos;1 second, start_time) as time,\n    count(*) as req_per_sec\nFROM ps_trace.span s\nWHERE $__timeFilter(start_time)\nAND parent_span_id is null -- just the root spans\nGROUP BY 1\nORDER BY 1\n</code></pre><p>The query follows the well-known <code>SELECT</code>, <code>FROM</code>, <code>WHERE</code> SQL structure. Promscale provides the <code>ps_trace.span</code> view that gives you access to all spans stored in the database. If you are not familiar with SQL views, from a query perspective you can think of them as relational tables for simplicity.</p><p>This query is just counting all spans <code>(count(*))</code> in one second buckets <code>(time_bucket(&apos;1 second, start_time))</code> in the selected time window <code>($__timeFilter(start_time))</code> that are root spans <code>(parent_span_id is null)</code>. Root spans are spans that don&#x2019;t have a parent: they are the very first span of a trace. So effectively, this condition filters out internal requests between the microservices that make up the password generator while keeping those that originate from outside of the system - that is, the actual requests that users make to the application.</p><p>If you want to run the query outside of Grafana, replace <code>$__timeFilter(start_time)</code> with the corresponding condition on <code>start_time</code>. For example, to limit the query to the last 30 minutes, replace <code>$__timeFilter(start_time)</code> with <code>start_time &#60; NOW() - INTERVAL &apos;30 minutes&apos;</code> .</p><h3 id=\"request-durations\">Request durations </h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/request-durations-dashboard.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1111\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/request-durations-dashboard.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/request-durations-dashboard.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/request-durations-dashboard.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/02/request-durations-dashboard.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Request durations dashboard in Grafana. </em><a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\"><em>You can access it here</em></a><em> if you have the microservices demo running.</em></figcaption></figure><p>Latency is also key to understanding if your application is responding fast and delivering a good experience to your users. The most common way to look at latency is by using histograms and percentiles. Histograms indicate the distribution, while percentiles help you understand what percentage of requests took X amount of time or less. Common percentiles are 99th, 95th and 50th, which is usually known as the median.</p><p>The <a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\">Request Durations dashboard</a> uses a number of different Grafana panels to analyze request latency in our password generator demo. </p><p>How did we build this? The easiest way to display a histogram is to use Grafana&#x2019;s Histogram panel with a query that returns the duration of each request to the system in the selected time window:</p><pre><code class=\"language-SQL\">SELECT duration_ms\nFROM ps_trace.span\nWHERE $__timeFilter(start_time)\nAND parent_span_id is null\n</code></pre><p>The histogram view shows that the majority of user requests take less than one second. However, there is a long tail of requests that are taking a long time, up to 30 seconds, which is a very poor user experience for a password generator:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/histogram-request-durations.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1928\" height=\"942\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/histogram-request-durations.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/histogram-request-durations.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/histogram-request-durations.png 1600w, https://www.timescale.com/blog/content/images/2022/02/histogram-request-durations.png 1928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Latency histogram in Grafana (from the </em><a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\"><em>Request Durations </em></a><em>dashboard).</em></figcaption></figure><p>The previous query is not very efficient if you have lots of requests. A more optimized way of doing this would be to use TimescaleDB&#x2019;s <a href=\"https://docs.timescale.com/api/latest/hyperfunctions/histogram/\">histogram</a> function and compute the histogram buckets in the database:</p><pre><code class=\"language-SQL\">SELECT histogram(duration_ms, 100, 10000, 20)\nFROM ps_trace.span\nWHERE $__timeFilter(start_time)\nAND parent_span_id is null\n</code></pre><p>The query above would generate a histogram for the duration_ms column with a lower bound of 100 ms, an upper bound of 10,000 ms (i.e. 10 seconds), and 20 buckets. The output of that query would look something like this:</p><pre><code class=\"language-SQL\">{7,98,202,90,33,23,21,20,15,16,9,10,11,11,8,5,5,4,5,6,4,91}\n</code></pre><p>Each of those values corresponds to one bucket in the histogram. Unfortunately, there isn&#x2019;t an out-of-the-box Grafana panel that can use this input to display a chart. However, with some SQL magic it can be converted into something that could easily be displayed in a Grafana panel.</p><p>When using metric instrumentation instead of trace instrumentation, you have to define the buckets when you initially define the metric and therefore you can&#x2019;t change the histogram buckets later for data that has already been stored. For example, imagine that you set a lower bound of 100ms, but you realize later when investigating an issue that it would have been valuable to you to have a lower bound of 10ms.</p><p>With traces though, you can change the structure of your histogram as needed, since they are computed at query time. Additionally, you have the flexibility to filter the histogram to show the latency of requests having certain tags - like the latency of requests made by a specific customer, for instance. </p><p>TimescaleDB also provides <a href=\"https://docs.timescale.com/api/latest/hyperfunctions/percentile-approximation/approx_percentile/\">functions to compute percentiles</a>. The following query can be used to see the evolution of different percentiles over time:</p><pre><code class=\"language-SQL\">SELECT\n    time_bucket(&apos;1 minute&apos;, start_time) as time,\n    ROUND(approx_percentile(0.99, percentile_agg(duration_ms))::numeric, 3) as duration_p99,\n    ROUND(approx_percentile(0.95, percentile_agg(duration_ms))::numeric, 3) as duration_p95,\n    ROUND(approx_percentile(0.90, percentile_agg(duration_ms))::numeric, 3) as duration_p90,\n    ROUND(approx_percentile(0.50, percentile_agg(duration_ms))::numeric, 3) as duration_p50\nFROM span\nWHERE\n    $__timeFilter(start_time)\n    AND parent_span_id is null\nGROUP BY time\nORDER BY time\n</code></pre><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/request-duration-percentile.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1035\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/request-duration-percentile.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/request-duration-percentile.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/request-duration-percentile.png 1600w, https://www.timescale.com/blog/content/images/2022/02/request-duration-percentile.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Latency percentiles in Grafana (from the </em><a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\"><em>Request Durations </em></a><em>dashboard).</em></figcaption></figure><p>Finally, you can also use SQL to just return the slowest requests across all traces, something that is actually not possible to do with Jaeger when the volume of traces is high. Jaeger limits the amount of traces you can retrieve to 1,500 and sorting is implemented in the UI. Therefore, if there are more than 1,500 matching traces in the selected timeframe, you&#x2019;ll have no guarantee that you&#x2019;ll actually see the slowest traces.</p><p>With Promscale and SQL, you are sure to get the latest traces regardless of how many there are because sorting is performed in the database:</p><pre><code class=\"language-SQL\">SELECT\n  trace_id,\n  start_time,\n  duration_ms\nFROM ps_trace.span\nWHERE $__timeFilter(start_time)\nAND parent_span_id is null\nORDER BY duration_ms DESC\nLIMIT 10;</code></pre><p>One more interesting thing. In the <a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\">Request Durations dashboard</a>, the query we use is slightly different:</p><pre><code class=\"language-SQL\">SELECT\n  replace(trace_id::text, &apos;-&apos;::text, &apos;&apos;::text) as trace_id,\n  start_time,\n  duration_ms\nFROM ps_trace.span\nWHERE $__timeFilter(start_time)\nAND parent_span_id is null\nORDER BY duration_ms DESC\nLIMIT 10;\n</code></pre><p>The difference is that we remove &#x201C;-&#x201D; from the trace id. We do that because we store the original trace id as reported by OpenTelemetry, but in the dashboard, we do a nice trick to link the trace id directly to the Grafana UI, so we can visualize the corresponding trace (Grafana expects a trace id with &#x201C;-&#x201D; removed). </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/slowest-requests.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1520\" height=\"820\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/slowest-requests.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/slowest-requests.png 1000w, https://www.timescale.com/blog/content/images/2022/02/slowest-requests.png 1520w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>List of slowest traces (from the </em><a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1&#38;viewPanel=6\"><em>Request Durations </em></a><em>dashboard).</em></figcaption></figure><p>If you want to see this in action, open <a href=\"http://localhost:3000/d/GkrS6rJ7z/03-request-durations?orgId=1\">the dashboard</a> and click on a trace id. It&#x2019;s pretty cool, and it works thanks to the high number of options for customizing your dashboard that Grafana provides. A shootout to the Grafana team!</p><h3 id=\"error-rates\">Error rates &#xA0;</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/error-rate-dashboard.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1133\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/error-rate-dashboard.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/error-rate-dashboard.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/error-rate-dashboard.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/02/error-rate-dashboard.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Error rates dashboard in Grafana. </em><a href=\"http://localhost:3000/d/CiE9l917z/02-error-rates\"><em>You can access it here</em></a><em> if you have the microservices demo running.</em></figcaption></figure><p>Error rate is the third golden signal for measuring application and service performance. When an operation in a request does not complete successfully, OpenTelemetry marks the corresponding span(s) with an error status. In Promscale, a span with error is indicated with <code>status_code = STATUS_CODE_ERROR</code></p><p>To compute the error rate per operation in the selected time window in Grafana, you would run the following query:</p><pre><code class=\"language-SQL\">SELECT\n    x.service_name,\n    x.span_name,\n    x.num_err::numeric / x.num_total as err_rate\nFROM\n(\n    SELECT\n        service_name,\n        span_name,\n        count(*) filter (where status_code = &apos;STATUS_CODE_ERROR&apos;) as num_err,\n        count(*) as num_total\n    FROM ps_trace.span\n    WHERE $__timeFilter(start_time)\n    GROUP BY 1, 2\n) x\nORDER BY err_rate desc\n</code></pre><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/error-rates-table.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1934\" height=\"958\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/error-rates-table.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/error-rates-table.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/error-rates-table.png 1600w, https://www.timescale.com/blog/content/images/2022/02/error-rates-table.png 1934w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Error rates by service operation (from the </em><a href=\"http://localhost:3000/d/CiE9l917z/02-error-rates\"><em>Error Rates</em></a><em> dashboard).</em></figcaption></figure><p>All OpenTelemetry spans include a <code>service_name</code> to indicate the name of the service that performed the operation represented by the span, and a <code>span_name</code> to identify the specific operation. </p><h3 id=\"service-dependencies\">Service dependencies &#xA0;</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/service-dependencies-dashboard.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1060\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/service-dependencies-dashboard.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/service-dependencies-dashboard.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/service-dependencies-dashboard.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/02/service-dependencies-dashboard.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Service dependencies dashboard in Grafana. </em><a href=\"http://localhost:3000/d/scyq99J7k/04-service-dependencies?orgId=1\"><em>You can access it here</em></a><em> if you have the microservices demo running.</em></figcaption></figure><p>So far we have learned how to analyze data across all services, per service, or per operation to compute the three golden signals: throughput, latency, and error rate. In the next three sections, we are going to leverage the information in traces to learn about how services communicate with each other to understand the behavior of the system.</p><p>We saw that Jaeger can display a service map out of the traces being ingested. Actually, with SQL and Grafana you can get a lot more valuable insights from the same trace data. Using the <a href=\"https://grafana.com/docs/grafana/latest/visualizations/node-graph/\">Grafana node graph</a> panel we can generate a service map similar to the one Jaeger provides that also indicates the number of requests between services:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/service-dependencies-node-graph.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"868\" height=\"1192\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/service-dependencies-node-graph.png 600w, https://www.timescale.com/blog/content/images/2022/02/service-dependencies-node-graph.png 868w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Service map using the Grafana node graph panel (from the </em><a href=\"http://localhost:3000/d/scyq99J7k/04-service-dependencies?orgId=1\"><em>Service Dependencies</em></a><em> dashboard).</em></figcaption></figure><p>This dependency map already tells us that something seems to be wrong, as we see that the lower service is calling the digit service - that doesn&#x2019;t make any sense! If we&apos;d do a careful inspection of the code, it would reveal that there is a bug <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/lower/lower.rb#L56\">calling the digit service</a> every time it receives a request to generate a lowercase character for no reason. Problem solved! </p><p>The node graph panel requires two inputs to build this visualization: a first query that returns the list of nodes, and a second query that returns the edges that connect those nodes.</p><p>The nodes in our service map are the services that are part of the application. The query below retrieves all the different <code>service_names</code> that appear in spans during the selected timeframe. The node graph panel requires an id and a title for each node, and we use the same value for both:</p><pre><code class=\"language-SQL\">SELECT \n   service_name as id,\n   service_name as title\nFROM ps_trace.span\nWHERE $__timeFilter(start_time)\nGROUP BY service_name\n</code></pre><p>Then, we need a query to get the list of edges. The edges indicate dependencies between services, that is, that service A calls service B.</p><pre><code class=\"language-SQL\">SELECT\n    p.service_name || &apos;-&#62;&apos; || k.service_name || &apos;:&apos; || k.span_name as id,\n    p.service_name as source,\n    k.service_name as target,\n    k.span_name as &#34;mainStat&#34;,\n    count(*) as &#34;secondaryStat&#34;\nFROM ps_trace.span p\nINNER JOIN ps_trace.span k\nON (p.trace_id = k.trace_id\nAND p.span_id = k.parent_span_id\nAND p.service_name != k.service_name)\nWHERE $__timeFilter(p.start_time)\nGROUP BY 1, 2, 3, 4\n</code></pre><p>This query joins the span view with itself because an individual span does not have any information about the parent span other than the parent span id. And so, we need to do a join on the parent span id to identify calls between different services (i.e. the span and the parent span were generated by different services):</p><pre><code class=\"language-SQL\">p.trace_id = k.trace_id\nAND p.span_id = k.parent_span_id\nAND p.service_name != k.service_name\n</code></pre><p>The node graph panel requires an id, a source node, a target node, and optionally a main statistic and a secondary statistic to display in the graph. The id generated in the query is an arbitrary string we defined that includes the service making the requests, the service receiving those requests and which operation the parent service called. </p><p>By doing that, if service A calls different operations in service B, the service map will show multiple arrows (one for each operation) instead of just one, which helps you understand the dependencies between services in more detail. This is not the case in the demo environment (as each service only calls one operation in another service), but it could be useful in other situations. And if you prefer to group all operations together, you would just need to remove <code>k.span_name</code> from the id in the query. </p><p>This is the power of using SQL for all of this: you have full flexibility to get the information you are interested in!</p><p>One more thing: you can compute additional statistics to help you further understand how services are interacting. For example, you may be interested in seeing not only how many times a service is calling another service, but also how much total time is spent by the receiving service in those calls, and what&#x2019;s the average (you could also compute the median, or p99, or anything you want really) of each of those calls.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/service-dependencies-table.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1928\" height=\"450\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/service-dependencies-table.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/service-dependencies-table.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/service-dependencies-table.png 1600w, https://www.timescale.com/blog/content/images/2022/02/service-dependencies-table.png 1928w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Summary table of service dependencies (from the </em><a href=\"http://localhost:3000/d/scyq99J7k/04-service-dependencies?orgId=1\"><em>Service Dependencies</em></a><em> dashboard).</em></figcaption></figure><p>The table above was generated with a Grafana table panel using the query below, that again joins the span view with itself:</p><pre><code class=\"language-SQL\">SELECT\n    p.service_name as source,\n    k.service_name as target,\n    k.span_name,\n    count(*) as calls,\n    sum(k.duration_ms) as total_exec_ms,\n    avg(k.duration_ms) as avg_exec_ms\nFROM ps_trace.span p\nINNER JOIN ps_trace.span k\nON (p.trace_id = k.trace_id\nAND p.span_id = k.parent_span_id\nAND p.service_name != k.service_name)\nWHERE $__timeFilter(p.start_time)\nGROUP BY 1, 2, 3\nORDER BY total_exec_ms DESC</code></pre><h3 id=\"investigating-upstream-dependencies\"> Investigating upstream dependencies</h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/upstream-spans-dashboard.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1132\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/upstream-spans-dashboard.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/upstream-spans-dashboard.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/upstream-spans-dashboard.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/02/upstream-spans-dashboard.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Upstream spans dashboard in Grafana. </em><a href=\"http://localhost:3000/d/lyIow61nz/05-upstream-spans?orgId=1&#38;var-service=digit&#38;var-operation=%2F\"><em>You can access it here</em></a><em> if you have the microservices demo running.</em></figcaption></figure><p>When you are troubleshooting a problem in a microservice it&#x2019;s often very helpful to understand what is calling that service. Imagine that you saw a sudden increase in the throughput of a particular operation in a microservice: you would like to understand what caused that increase in throughput - that is, what upstream service and operation started making a lot more requests to that microservice operation.</p><p>This problem is more complicated than the dependency map problem, as we need to analyze all spans that represent requests to that microservice operation; then, retrieve the calling services; then, do the same for those... And repeat. Basically, we need to recursively traverse the tree of spans to extract all those dependencies.</p><p>To visualize the dependencies, we will again use the node graph panel, which requires a query for the list of nodes and a query for the list edges. These are not as straightforward as in the dependency map, because we only want to display services that are in the upstream chain of a specific service and operation. We need a way to traverse the tree of spans for that service and operation backwards. </p><p>Luckily, we have the SQL superpowers of Postgres: we can use a <a href=\"https://www.postgresql.org/docs/13/queries-with.html\">recursive query</a> to do this! </p><p>The structure of a recursive query in PostgreSQL is the following:</p><pre><code class=\"language-SQL\">WITH RECURSIVE table_name AS (\n\n     initial_query\n\n\n     UNION [ALL]\n     recursive_query\n)\nfinal_query\n</code></pre><p>Let&apos;s break it down: </p><ul><li><code>initial_query</code>: the query that returns the base result set that will be run against the recursive query.</li><li><code>recursive_query</code>: a query against <code>table_name</code> that is initially run against the result set of <code>initial_query</code>. The results of that query are run against the recursive_query again, and then that result set is run against the recursive_query again and so on until the result set returned by the recursive query is empty.</li><li><code>final_query</code>: the results of the initial_query execution and the subsequent recursive_query execution are unioned (if using UNION instead of UNION ALL duplicate rows are discarded). That is, all results from initial_query and all results from subsequent recursive_query executions are put together into one working table and then run against the final_query to generate the query results that are finally returned by the database.</li></ul><p>Given a service and operation, this is the query that returns all upstream services and operations by using the <code>WITH RECURSIVE</code> PostgreSQL statement:</p><pre><code class=\"language-SQL\">WITH RECURSIVE x AS\n(\n    SELECT\n        trace_id,\n        span_id,\n        parent_span_id,\n        service_name,\n        span_name\n    FROM ps_trace.span\n    WHERE $__timeFilter(start_time)\n    AND service_name = &apos;${service}&apos;\n    AND span_name = &apos;${operation}&apos;\n    UNION ALL\n    SELECT\n        s.trace_id,\n        s.span_id,\n        s.parent_span_id,\n        s.service_name,\n        s.span_name\n    FROM x\n    INNER JOIN ps_trace.span s\n    ON (x.trace_id = s.trace_id\n    AND x.parent_span_id = s.span_id)\n)\nSELECT\n    md5(service_name || &apos;-&apos; || span_name) as id,\n    span_name as title,\n    service_name as &#34;subTitle&#34;,\n    count(distinct span_id) as &#34;mainStat&#34;\nFROM x\nGROUP BY service_name, span_name\n</code></pre><p>The condition <code>x.parent_span_id = s.span_id</code> in the <code>recursive_query</code> is the one that ensures only parent spans are returned.</p><p>Each node represents a specific service and operation. Then for each node, this query will display the number of executions for the corresponding operation <code>(count(distinct span_id) as &#x201C;mainStat&#x201D;)</code>. </p><p><code>${service}</code> and <code>${operation}</code> are Grafana variables that are defined at the top of the dashboard and that specify which is the service and operation for which we want to show upstream services and operations.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/variable-selectors-grafana.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1724\" height=\"1112\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/variable-selectors-grafana.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/variable-selectors-grafana.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/variable-selectors-grafana.png 1600w, https://www.timescale.com/blog/content/images/2022/02/variable-selectors-grafana.png 1724w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Variable selectors (from the </em><a href=\"http://localhost:3000/d/lyIow61nz/05-upstream-spans?orgId=1&#38;var-service=digit&#38;var-operation=%2F\"><em>Upstream Spans</em></a><em> dashboard).</em></figcaption></figure><p>The query for edges is similar to the query for nodes:</p><pre><code class=\"language-SQL\">WITH RECURSIVE x AS\n(\n    SELECT\n        trace_id,\n        span_id,\n        parent_span_id,\n        service_name,\n        span_name,\n        null::text as id,\n        null::text as target,\n        null::text as source\n    FROM ps_trace.span\n    WHERE $__timeFilter(start_time)\n    AND service_name = &apos;${service}&apos;\n    AND span_name = &apos;${operation}&apos;\n    UNION ALL\n    SELECT\n        s.trace_id,\n        s.span_id,\n        s.parent_span_id,\n        s.service_name,\n        s.span_name,\n        md5(s.service_name || &apos;-&apos; || s.span_name || &apos;-&apos; || x.service_name || &apos;-&apos; || x.span_name) as id,\n        md5(x.service_name || &apos;-&apos; || x.span_name) as target,\n        md5(s.service_name || &apos;-&apos; || s.span_name) as source\n    FROM x\n    INNER JOIN ps_trace.span s\n    ON (x.trace_id = s.trace_id\n    AND x.parent_span_id = s.span_id)\n)\nSELECT DISTINCT\n    x.id,\n    x.target,\n    x.source \nFROM x\nWHERE id is not null\n</code></pre><p>In this case, we are setting <code>id</code>, <code>target</code> and <code>source</code> to null in the <code>initial_query</code> because these columns don&#x2019;t exist in the <code>ps_trace.span</code> view. However, we need to define these columns to be able to do a <code>UNION</code> with the <code>recursive_query</code>, which does define them (the columns in the <code>SELECT</code> clause must be the same for both queries). In the final query, we only keep those rows where id is not null.</p><p>This is the end result:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/upstream-dependencies-node-graph.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"1770\" height=\"1004\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/upstream-dependencies-node-graph.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/upstream-dependencies-node-graph.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/upstream-dependencies-node-graph.png 1600w, https://www.timescale.com/blog/content/images/2022/02/upstream-dependencies-node-graph.png 1770w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Grafana node graph showing upstream services and operations (from the </em><a href=\"http://localhost:3000/d/lyIow61nz/05-upstream-spans?orgId=1&#38;var-service=digit&#38;var-operation=%2F\"><em>Upstream Spans</em></a><em> dashboard).</em></figcaption></figure><p>For example, we can see that the digit service is serving a similar number of requests from the lower service as from the generator service.</p><h3 id=\"investigating-downstream-dependencies\">Investigating downstream dependencies </h3><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://www.timescale.com/blog/content/images/2022/02/downstream-spans-dashboard.png\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\" width=\"2000\" height=\"1124\" srcset=\"https://www.timescale.com/blog/content/images/size/w600/2022/02/downstream-spans-dashboard.png 600w, https://www.timescale.com/blog/content/images/size/w1000/2022/02/downstream-spans-dashboard.png 1000w, https://www.timescale.com/blog/content/images/size/w1600/2022/02/downstream-spans-dashboard.png 1600w, https://www.timescale.com/blog/content/images/size/w2400/2022/02/downstream-spans-dashboard.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Downstream spans dashboard in Grafana. </em><a href=\"http://localhost:3000/d/SdzI3eJnk/06-downstream-spans?orgId=1\"><em>You can access it here</em></a><em> if you have the microservices demo running.</em></figcaption></figure><p>When investigating how to improve the performance of a specific service operation it is extremely useful to get a sense of all the different downstream services and operations that are involved in executing all executions of that service operation. </p><p>To create a map of downstream dependencies using traces in Promscale, we&apos;ll use the same approach as for upstream services: we&apos;ll use the node graph panel with recursive SQL queries. The queries will be the same as the ones we used for upstream dependencies but replacing the <code>x.parent_span_id = s.span_id</code> condition in the <code>recursive_query</code> with <code>x.span_id = s.parent_span_id</code> to return the child spans instead of the parent span.</p><p>If in the dashboard we select <code>Service: generator</code> and <code>Operation: /</code>, then we basically get the entire map of how services interact, since that service and operation is the only entry point to the password generator application.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh4.googleusercontent.com/eFlcH_SlRhkxBC9zlAeOgpngS6oe2tq-Gn3HbaCJTG3hL-kzHW5r1HMvUPAYnWdPzm57cIcqK14FmB6I1i83x2RcaAcUv6xyfgIf0hq6R5-lzdSwa2CAEY4LT-HaaOBySjfjVhw2\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\"><figcaption><em>Grafana node graph showing downstream services and operations (from the </em><a href=\"http://localhost:3000/d/SdzI3eJnk/06-downstream-spans?orgId=1\"><em>Downstream Spans</em></a><em> dashboard).</em></figcaption></figure><p>By analyzing downstream dependencies, we can also do other extremely valuable analyses, like looking at which downstream service operation consumes the majority of the execution time for the selected service and operation. This would be very helpful, for example, to decide where to start if we want to improve the performance of a specific service operation. </p><p>However, that calculation can&apos;t be done directly by using the duration of each span, since it includes the duration of its child spans - and therefore spans that are higher in the hierarchy will always show as taking the longest to execute. To solve that problem, we subtract to the duration of each span the time spent in child spans.</p><p>Let&#x2019;s look at a specific example to illustrate the problem. The diagram below shows a trace with four spans representing different operations. The time effectively consumed by <code>Operation A</code> is <code>t1 + t3 + t6</code>, or expressed differently, &#xA0;the duration of the span for <code>Operation A</code> minus the duration of the span for <code>Operation B</code> minus the duration of the span for <code>Operation C</code> . The time effectively consumed by <code>Operation B</code> is <code>t2</code>, by <code>Operation C</code> is <code>t4</code>, and by <code>Operation D</code> is <code>t5</code>. In this case, <code>Operation C</code> is the one that has consumed most of the time in the trace.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh5.googleusercontent.com/o1r1r_lLwT7uGC6djtLEUtGcx7DKn9w7ecCWmBsnYPbzfSKg2OrP3pcXSjrxfLxw3kvfv8r0D4RiJW2a0bv7oFvkdlPae7aRxVK3BHCGXD467iOv3dtk2VNqDL8qbmcSyOGc2B_c\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\"><figcaption><em>Diagram illustrating the time effectively consumed by each operation for a particular trace.</em></figcaption></figure><p>To perform this calculation in aggregate for all traces starting with a specific service and operation, we can run the following recursive query:</p><pre><code class=\"language-SQL\">WITH RECURSIVE x AS\n(\n    SELECT\n        s.trace_id,\n        s.span_id,\n        s.parent_span_id,\n        s.service_name,\n        s.span_name,\n        s.duration_ms - coalesce(\n        (\n            SELECT sum(z.duration_ms)\n            FROM ps_trace.span z\n            WHERE s.trace_id = z.trace_id\n            AND s.span_id = z.parent_span_id\n        ), 0.0) as duration_ms\n    FROM ps_trace.span s\n    WHERE $__timeFilter(s.start_time)\n    AND s.service_name = &apos;${service}&apos;\n    AND s.span_name = &apos;${operation}&apos;\n    UNION ALL\n    SELECT\n        s.trace_id,\n        s.span_id,\n        s.parent_span_id,\n        s.service_name,\n        s.span_name,\n        s.duration_ms - coalesce(\n        (\n            SELECT sum(z.duration_ms)\n            FROM ps_trace.span z\n            WHERE s.trace_id = z.trace_id\n            AND s.span_id = z.parent_span_id\n        ), 0.0) as duration_ms\n    FROM x\n    INNER JOIN ps_trace.span s\n    ON (x.trace_id = s.trace_id\n    AND x.span_id = s.parent_span_id)\n)\nSELECT\n    service_name,\n    span_name,\n    sum(duration_ms) as total_exec_time\nFROM x\nGROUP BY 1, 2\nORDER BY 3 DESC</code></pre><p>This query is very similar to the one we used to get the list edges for the node graph of downstream dependencies, but it includes a calculation of the effective time spent in each operation by subtracting to the duration of each span the duration of all their child spans. </p><p>We&apos;re doing this using another cool SQL feature: <a href=\"https://www.postgresql.org/docs/8.1/functions-subquery.html\">subqueries.</a></p><pre><code class=\"language-SQL\">s.duration_ms - coalesce(\n(\n    SELECT sum(z.duration_ms)\n    FROM ps_trace.span z\n    WHERE s.trace_id = z.trace_id\n    AND s.span_id = z.parent_span_id\n), 0.0) as duration_ms\n</code></pre><p><code>coalesce</code> will return <code>0.0</code> if the subquery returns no results, that is if the span doesn&#x2019;t have any children. In this case, the effective time spent in the corresponding operation is the total duration of the span.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh3.googleusercontent.com/Uhy9qLXROkdgaU_el7j2kKIU-bwgDBVitmsapyMQnKQnPMli7bZP6susmv52qSZDrCZMAKGxGZdQZ-5uFreVCkHz9KuuJHwe8yMmCBUga_zKAuKDl5mu1qdhew2BsfHKNs7xX9_p\" class=\"kg-image\" alt=\"Learn OpenTelemetry tracing with this lightweight microservices demo\" loading=\"lazy\"><figcaption><em>Graph showing the operations with the highest execution time (from the </em><a href=\"http://localhost:3000/d/SdzI3eJnk/06-downstream-spans?orgId=1\"><em>Downstream Spans</em></a><em> dashboard).</em></figcaption></figure><p>In the case of the <code>/</code> operation of the generator service, 87% of the time is spent in the <code>random_digit</code> operation of the digit service. Inspecting <a href=\"https://github.com/timescale/opentelemetry-demo/blob/main/digit/digit.py#L35\">the code</a> reveals why; this issue is caused by the sleep code we already identified when looking at slow traces in Jaeger. </p><p>Keep in mind that this method for computing effective time consumed by an operation only works if all downstream operations are synchronous. If your system uses asynchronous operations and those are being tracked as child spans, then the calculations would likely be incorrect, because child spans could happen outside the boundaries of their parent span.</p><h2 id=\"summary\">Summary</h2><p>With this lightweight OpenTelemetry demo, you can get an easy-to-deploy microservices environment with a complete OpenTelemetry observability stack running on your computer in just a few minutes.</p><p>This will help you get started with distributed tracing visualization tools like Jaeger, which are easy to use and let you dig into specific traces to troubleshoot a problem if you know what you are looking for.</p><p>With Promscale, Grafana, and SQL, you can go deeper by interrogating your trace data in any way you need. This will allow you to quickly derive new insights about your distributed systems, helping you surface and identify hard-to-find problems that you cannot find with other tools, or that would require a lot of manual work from you. </p><p>If you want to learn more about Promscale, check our <a href=\"https://www.timescale.com/promscale\">website</a> and our <a href=\"https://docs.timescale.com/promscale/latest/\">documentation</a>. If you start using us, don&#x2019;t forget to join the #promscale channel in our <a href=\"http://slack.timescale.com\">Community Slack </a>to directly interact with the team building the product. You can also ask us any technical questions in our new <a href=\"https://www.timescale.com/forum/\">Community Forum</a>.</p>","descriptionType":"html","publishedDate":"Thu, 24 Feb 2022 14:03:54 +0000","feedId":42941,"bgimg":"https://www.timescale.com/blog/content/images/2022/02/otel-demo-blogpost-timescale-1.png","linkMd5":"69fd17512556765abaf5ebdcf98afbb4","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn50@2020_2/2022/03/14/14-56-23-531_f7f4d805b5001664.webp","destWidth":2316,"destHeight":1424,"sourceBytes":647696,"destBytes":105016,"author":"Ramon Guiu","articleImgCdnMap":{"https://www.timescale.com/blog/content/images/2022/02/otel-demo-blogpost-timescale-1.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn50@2020_2/2022/03/14/14-56-23-531_f7f4d805b5001664.webp","https://www.timescale.com/blog/content/images/2022/03/architecture-diagram-demo-env.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn14@2020_2/2022/03/14/14-56-24-027_6bbfe69fc7e8806c.webp","https://www.timescale.com/blog/content/images/2022/02/output-docker-compose-otel-demo.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn66@2020_1/2022/03/14/14-56-24-160_9b4154808ec903f3.webp","https://www.timescale.com/blog/content/images/2022/02/running-otel-demo-short.gif":null,"https://www.timescale.com/blog/content/images/2022/02/jaeger-search-ui-otel-demo.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn90@2020_1/2022/03/14/14-56-24-239_edf6610bc3d6b0c2.webp","https://www.timescale.com/blog/content/images/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn6@2020_5/2022/03/14/14-56-24-292_5e0b405b9ce7b5ef.webp","https://www.timescale.com/blog/content/images/2022/02/jaeger-map-view-otel-demo.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn77@2020_1/2022/03/14/14-56-24-215_e706b72c68c28e6c.webp","https://www.timescale.com/blog/content/images/2022/02/request-rates-dashboard.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn98@2020_3/2022/03/14/14-56-24-209_93c6a98bcd22bef7.webp","https://www.timescale.com/blog/content/images/2022/02/request-durations-dashboard.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn40@2020_6/2022/03/14/14-56-24-207_dc47b192576d1f14.webp","https://www.timescale.com/blog/content/images/2022/02/histogram-request-durations.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn61@2020_4/2022/03/14/14-56-25-996_d12543767df01cf4.webp","https://www.timescale.com/blog/content/images/2022/02/request-duration-percentile.png":null,"https://www.timescale.com/blog/content/images/2022/02/slowest-requests.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn68@2020_6/2022/03/14/14-56-24-033_955389f3596b5a0a.webp","https://www.timescale.com/blog/content/images/2022/02/error-rate-dashboard.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn53@2020_5/2022/03/14/14-56-24-197_025c47891380ff6f.webp","https://www.timescale.com/blog/content/images/2022/02/error-rates-table.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn22@2020_3/2022/03/14/14-56-26-198_bf80dcee7b6c4f4d.webp","https://www.timescale.com/blog/content/images/2022/02/service-dependencies-dashboard.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn37@2020_2/2022/03/14/14-56-24-322_8cb0ca38013b40a5.webp","https://www.timescale.com/blog/content/images/2022/02/service-dependencies-node-graph.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn73@2020_4/2022/03/14/14-56-23-954_963784cfb5e1d63a.webp","https://www.timescale.com/blog/content/images/2022/02/service-dependencies-table.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn26@2020_2/2022/03/14/14-56-25-966_7648496febefe3ef.webp","https://www.timescale.com/blog/content/images/2022/02/upstream-spans-dashboard.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn15@2020_6/2022/03/14/14-56-24-128_c9d951a6d437a989.webp","https://www.timescale.com/blog/content/images/2022/02/variable-selectors-grafana.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn86@2020_3/2022/03/14/14-56-24-086_7feabc992d518048.webp","https://www.timescale.com/blog/content/images/2022/02/upstream-dependencies-node-graph.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn29@2020_6/2022/03/14/14-56-23-981_395d82fb797b88f2.webp","https://www.timescale.com/blog/content/images/2022/02/downstream-spans-dashboard.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn93@2020_4/2022/03/14/14-56-24-020_9a16eb4863e0cb1f.webp","https://lh4.googleusercontent.com/eFlcH_SlRhkxBC9zlAeOgpngS6oe2tq-Gn3HbaCJTG3hL-kzHW5r1HMvUPAYnWdPzm57cIcqK14FmB6I1i83x2RcaAcUv6xyfgIf0hq6R5-lzdSwa2CAEY4LT-HaaOBySjfjVhw2":null,"https://lh5.googleusercontent.com/o1r1r_lLwT7uGC6djtLEUtGcx7DKn9w7ecCWmBsnYPbzfSKg2OrP3pcXSjrxfLxw3kvfv8r0D4RiJW2a0bv7oFvkdlPae7aRxVK3BHCGXD467iOv3dtk2VNqDL8qbmcSyOGc2B_c":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn99@2020_5/2022/03/14/14-56-24-132_5032c6b61a691e84.webp","https://lh3.googleusercontent.com/Uhy9qLXROkdgaU_el7j2kKIU-bwgDBVitmsapyMQnKQnPMli7bZP6susmv52qSZDrCZMAKGxGZdQZ-5uFreVCkHz9KuuJHwe8yMmCBUga_zKAuKDl5mu1qdhew2BsfHKNs7xX9_p":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn58@2020_6/2022/03/14/14-56-26-427_87b4f017dba83878.webp"},"publishedOrCreatedDate":1647269783264}],"record":{"createdTime":"2022-03-14 22:56:23","updatedTime":"2022-03-14 22:56:23","feedId":42941,"fetchDate":"Mon, 14 Mar 2022 14:56:23 +0000","fetchMs":400,"handleMs":16,"totalMs":124513,"newArticles":0,"totalArticles":15,"status":1,"type":0,"ip":"af0629e1ae74a27744b4cbd27b40a78e","hostName":"us-004*","requestId":"329f2b528b5b43a2bb8a9efb6202b5b5_42941","contentType":"text/xml; charset=utf-8","totalBytes":1072340,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":24,"articlesImgsGithubTotal":21,"successGithubMap":{"myreaderx25":1,"myreaderx7":1,"myreaderx15":1,"myreaderx16":1,"myreaderx27":1,"myreaderx32":1,"myreaderx21":1,"myreaderx4":1,"myreaderx10":1,"myreaderx11":1,"myreaderx33":1,"myreaderx22":1,"myreaderx2":1,"myreaderx12":1,"myreaderx24":1,"myreaderx1":1,"myreaderx13":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx29":1,"myreaderx":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}},"feed":{"createdTime":"2020-09-07 03:29:19","updatedTime":"2020-09-07 05:46:33","id":42941,"name":"Timescale Blog","url":"https://blog.timescale.com/feed","subscriber":75,"website":null,"icon":"https://blog.timescale.com/favicon.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx62/cdn31@2020_5/2020/09/06/21-46-26-213_ee8abd6ce89d5048.png","description":"The latest thoughts, tutorials, and technical posts on TimescaleDB, SQL, time-series data, and PostgreSQL. With IoT, network monitoring, and DevOps use cases.","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2022-03-14 22:58:27","updatedTime":"2022-03-14 22:58:27","id":null,"feedId":42941,"linkMd5":"69fd17512556765abaf5ebdcf98afbb4"}],"tmpCommonImgCdnBytes":105016,"tmpBodyImgCdnBytes":967324,"tmpBgImgCdnBytes":0,"extra4":{"start":1647269782837,"total":0,"statList":[{"spend":411,"msg":"xml"},{"spend":16,"msg":""},{"spend":0,"msg":"cdn"},{"spend":0,"msg":""},{"spend":123515,"msg":"cdn"}]},"extra5":24,"extra6":23,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/request-duration-percentile.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1035,"sourceBytes":835425,"destBytes":59134,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":225,"convertSpendMs":69,"createdTime":"2022-03-14 22:56:23","host":"us-028*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn10/contents/2022/03/14/14-56-24-029_58f6b3a3057feb76.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 14:56:24 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D9CC:6B07:370034:15A443E:622F5798"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647271455"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn10/contents/2022/03/14/14-56-24-029_58f6b3a3057feb76.webp","historyStatusCode":[],"spendMs":34},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"57.7 KB","compressRate":"7.1%","sourceSize":"815.8 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/request-duration-percentile.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1035,"sourceBytes":835425,"destBytes":59134,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":225,"convertSpendMs":78,"createdTime":"2022-03-14 22:56:24","host":"us-028*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn10/contents/2022/03/14/14-56-24-251_58f6b3a3057feb76.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 14:56:24 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D9CC:6B07:370036:15A4448:622F5798"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647271455"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn10/contents/2022/03/14/14-56-24-251_58f6b3a3057feb76.webp","historyStatusCode":[],"spendMs":42},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"57.7 KB","compressRate":"7.1%","sourceSize":"815.8 KB"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/eFlcH_SlRhkxBC9zlAeOgpngS6oe2tq-Gn3HbaCJTG3hL-kzHW5r1HMvUPAYnWdPzm57cIcqK14FmB6I1i83x2RcaAcUv6xyfgIf0hq6R5-lzdSwa2CAEY4LT-HaaOBySjfjVhw2","sourceStatusCode":200,"destWidth":1600,"destHeight":781,"sourceBytes":187803,"destBytes":35954,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":622,"convertSpendMs":75,"createdTime":"2022-03-14 22:56:23","host":"us-020*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn33/contents/2022/03/14/14-56-24-355_8ecd755214494f73.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 14:56:24 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A944:5DBC:28E3BF1:56FB9D0:622F5798"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647271455"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn33/contents/2022/03/14/14-56-24-355_8ecd755214494f73.webp","historyStatusCode":[],"spendMs":110},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"35.1 KB","compressRate":"19.1%","sourceSize":"183.4 KB"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/eFlcH_SlRhkxBC9zlAeOgpngS6oe2tq-Gn3HbaCJTG3hL-kzHW5r1HMvUPAYnWdPzm57cIcqK14FmB6I1i83x2RcaAcUv6xyfgIf0hq6R5-lzdSwa2CAEY4LT-HaaOBySjfjVhw2","sourceStatusCode":200,"destWidth":1600,"destHeight":781,"sourceBytes":187803,"destBytes":35954,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":239,"convertSpendMs":111,"createdTime":"2022-03-14 22:56:24","host":"us-020*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn33/contents/2022/03/14/14-56-24-610_8ecd755214494f73.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 14:56:24 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A390:53F7:425EE00:835FB28:622F5798"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647271455"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn33/contents/2022/03/14/14-56-24-610_8ecd755214494f73.webp","historyStatusCode":[],"spendMs":102},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"35.1 KB","compressRate":"19.1%","sourceSize":"183.4 KB"},null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-002.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-58.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe61.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe70.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://europe69.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-019.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-003.herokuapp.com/":{"failCount":1,"successCount":2,"resultList":[200,200,null]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/otel-demo-blogpost-timescale-1.png","sourceStatusCode":200,"destWidth":2316,"destHeight":1424,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn50@2020_2/2022/03/14/14-56-23-531_f7f4d805b5001664.webp","sourceBytes":647696,"destBytes":105016,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":558,"convertSpendMs":149,"createdTime":"2022-03-14 22:56:23","host":"us-016*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4,69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"102.6 KB","compressRate":"16.2%","sourceSize":"632.5 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/service-dependencies-node-graph.png","sourceStatusCode":200,"destWidth":868,"destHeight":1192,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn73@2020_4/2022/03/14/14-56-23-954_963784cfb5e1d63a.webp","sourceBytes":194884,"destBytes":14688,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":384,"convertSpendMs":29,"createdTime":"2022-03-14 22:56:23","host":"us-008*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"14.3 KB","compressRate":"7.5%","sourceSize":"190.3 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/upstream-dependencies-node-graph.png","sourceStatusCode":200,"destWidth":1770,"destHeight":1004,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn29@2020_6/2022/03/14/14-56-23-981_395d82fb797b88f2.webp","sourceBytes":175655,"destBytes":26042,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":428,"convertSpendMs":49,"createdTime":"2022-03-14 22:56:23","host":"us-032*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"25.4 KB","compressRate":"14.8%","sourceSize":"171.5 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/03/architecture-diagram-demo-env.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1126,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn14@2020_2/2022/03/14/14-56-24-027_6bbfe69fc7e8806c.webp","sourceBytes":763491,"destBytes":49616,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":446,"convertSpendMs":77,"createdTime":"2022-03-14 22:56:23","host":"us-002*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"48.5 KB","compressRate":"6.5%","sourceSize":"745.6 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/downstream-spans-dashboard.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1124,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn93@2020_4/2022/03/14/14-56-24-020_9a16eb4863e0cb1f.webp","sourceBytes":943964,"destBytes":88674,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":527,"convertSpendMs":88,"createdTime":"2022-03-14 22:56:23","host":"us-012*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"86.6 KB","compressRate":"9.4%","sourceSize":"921.8 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/slowest-requests.png","sourceStatusCode":200,"destWidth":1520,"destHeight":820,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn68@2020_6/2022/03/14/14-56-24-033_955389f3596b5a0a.webp","sourceBytes":437318,"destBytes":73216,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":538,"convertSpendMs":88,"createdTime":"2022-03-14 22:56:23","host":"us-020*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"71.5 KB","compressRate":"16.7%","sourceSize":"427.1 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/variable-selectors-grafana.png","sourceStatusCode":200,"destWidth":1724,"destHeight":1112,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn86@2020_3/2022/03/14/14-56-24-086_7feabc992d518048.webp","sourceBytes":250322,"destBytes":29004,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":551,"convertSpendMs":136,"createdTime":"2022-03-14 22:56:23","host":"us-036*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"28.3 KB","compressRate":"11.6%","sourceSize":"244.5 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/upstream-spans-dashboard.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1132,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn15@2020_6/2022/03/14/14-56-24-128_c9d951a6d437a989.webp","sourceBytes":493395,"destBytes":36200,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":564,"convertSpendMs":144,"createdTime":"2022-03-14 22:56:23","host":"us-003*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"35.4 KB","compressRate":"7.3%","sourceSize":"481.8 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/output-docker-compose-otel-demo.png","sourceStatusCode":200,"destWidth":1492,"destHeight":728,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn66@2020_1/2022/03/14/14-56-24-160_9b4154808ec903f3.webp","sourceBytes":350543,"destBytes":76854,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":650,"convertSpendMs":52,"createdTime":"2022-03-14 22:56:23","host":"us-019*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"75.1 KB","compressRate":"21.9%","sourceSize":"342.3 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/error-rate-dashboard.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1133,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn53@2020_5/2022/03/14/14-56-24-197_025c47891380ff6f.webp","sourceBytes":1076607,"destBytes":111172,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":685,"convertSpendMs":205,"createdTime":"2022-03-14 22:56:23","host":"us-003*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"108.6 KB","compressRate":"10.3%","sourceSize":"1 MB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/jaeger-search-ui-otel-demo.png","sourceStatusCode":200,"destWidth":1918,"destHeight":1120,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn90@2020_1/2022/03/14/14-56-24-239_edf6610bc3d6b0c2.webp","sourceBytes":556688,"destBytes":63840,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":697,"convertSpendMs":97,"createdTime":"2022-03-14 22:56:23","host":"us-024*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"62.3 KB","compressRate":"11.5%","sourceSize":"543.6 KB"},{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/o1r1r_lLwT7uGC6djtLEUtGcx7DKn9w7ecCWmBsnYPbzfSKg2OrP3pcXSjrxfLxw3kvfv8r0D4RiJW2a0bv7oFvkdlPae7aRxVK3BHCGXD467iOv3dtk2VNqDL8qbmcSyOGc2B_c","sourceStatusCode":200,"destWidth":1514,"destHeight":1008,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn99@2020_5/2022/03/14/14-56-24-132_5032c6b61a691e84.webp","sourceBytes":180169,"destBytes":24652,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":669,"convertSpendMs":49,"createdTime":"2022-03-14 22:56:23","host":"europe62*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"24.1 KB","compressRate":"13.7%","sourceSize":"175.9 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/Screen-Shot-2022-02-22-at-1.54.23-PM.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1141,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn6@2020_5/2022/03/14/14-56-24-292_5e0b405b9ce7b5ef.webp","sourceBytes":913261,"destBytes":55738,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":756,"convertSpendMs":74,"createdTime":"2022-03-14 22:56:23","host":"us-040*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"54.4 KB","compressRate":"6.1%","sourceSize":"891.9 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/request-rates-dashboard.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1046,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn98@2020_3/2022/03/14/14-56-24-209_93c6a98bcd22bef7.webp","sourceBytes":393385,"destBytes":42204,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":688,"convertSpendMs":69,"createdTime":"2022-03-14 22:56:23","host":"europe61*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"41.2 KB","compressRate":"10.7%","sourceSize":"384.2 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/service-dependencies-dashboard.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1060,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn37@2020_2/2022/03/14/14-56-24-322_8cb0ca38013b40a5.webp","sourceBytes":575177,"destBytes":38230,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":828,"convertSpendMs":137,"createdTime":"2022-03-14 22:56:23","host":"europe69*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"37.3 KB","compressRate":"6.6%","sourceSize":"561.7 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/jaeger-map-view-otel-demo.png","sourceStatusCode":200,"destWidth":1858,"destHeight":890,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn77@2020_1/2022/03/14/14-56-24-215_e706b72c68c28e6c.webp","sourceBytes":190046,"destBytes":21210,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":831,"convertSpendMs":37,"createdTime":"2022-03-14 22:56:23","host":"europe70*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"20.7 KB","compressRate":"11.2%","sourceSize":"185.6 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/request-durations-dashboard.png","sourceStatusCode":200,"destWidth":2000,"destHeight":1111,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn40@2020_6/2022/03/14/14-56-24-207_dc47b192576d1f14.webp","sourceBytes":496994,"destBytes":94164,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":917,"convertSpendMs":59,"createdTime":"2022-03-14 22:56:23","host":"europe70*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"92 KB","compressRate":"18.9%","sourceSize":"485.3 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/service-dependencies-table.png","sourceStatusCode":200,"destWidth":1928,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn26@2020_2/2022/03/14/14-56-25-966_7648496febefe3ef.webp","sourceBytes":148720,"destBytes":24074,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":405,"convertSpendMs":33,"createdTime":"2022-03-14 22:56:25","host":"us-54*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"23.5 KB","compressRate":"16.2%","sourceSize":"145.2 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/histogram-request-durations.png","sourceStatusCode":200,"destWidth":1928,"destHeight":942,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn61@2020_4/2022/03/14/14-56-25-996_d12543767df01cf4.webp","sourceBytes":583577,"destBytes":25588,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":550,"convertSpendMs":65,"createdTime":"2022-03-14 22:56:25","host":"us-54*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"25 KB","compressRate":"4.4%","sourceSize":"569.9 KB"},{"code":1,"isDone":false,"source":"https://www.timescale.com/blog/content/images/2022/02/error-rates-table.png","sourceStatusCode":200,"destWidth":1934,"destHeight":958,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn22@2020_3/2022/03/14/14-56-26-198_bf80dcee7b6c4f4d.webp","sourceBytes":389395,"destBytes":36460,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":690,"convertSpendMs":81,"createdTime":"2022-03-14 22:56:25","host":"europe66*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"35.6 KB","compressRate":"9.4%","sourceSize":"380.3 KB"},{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/Uhy9qLXROkdgaU_el7j2kKIU-bwgDBVitmsapyMQnKQnPMli7bZP6susmv52qSZDrCZMAKGxGZdQZ-5uFreVCkHz9KuuJHwe8yMmCBUga_zKAuKDl5mu1qdhew2BsfHKNs7xX9_p","sourceStatusCode":200,"destWidth":1600,"destHeight":987,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn58@2020_6/2022/03/14/14-56-26-427_87b4f017dba83878.webp","sourceBytes":212832,"destBytes":35698,"targetWebpQuality":75,"feedId":42941,"totalSpendMs":1185,"convertSpendMs":81,"createdTime":"2022-03-14 22:56:25","host":"europe66*","referer":"https://www.timescale.com/blog/learn-opentelemetry-tracing-with-this-lightweight-microservices-demo/","linkMd5ListStr":"69fd17512556765abaf5ebdcf98afbb4","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1Referer","extra23historyStatusCode":[200],"destSize":"34.9 KB","compressRate":"16.8%","sourceSize":"207.8 KB"}],"successGithubMap":{"myreaderx25":1,"myreaderx7":1,"myreaderx15":1,"myreaderx16":1,"myreaderx27":1,"myreaderx32":1,"myreaderx21":1,"myreaderx4":1,"myreaderx10":1,"myreaderx11":1,"myreaderx33":1,"myreaderx22":1,"myreaderx2":1,"myreaderx12":1,"myreaderx24":1,"myreaderx1":1,"myreaderx13":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx29":1,"myreaderx":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}}