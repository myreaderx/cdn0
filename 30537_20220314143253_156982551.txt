{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2022-03-14 22:30:21","updatedTime":"2022-03-14 22:30:21","title":"Digging into a MongoDB 3.6.20 concurrency bug","link":"https://venaeng.wpengine.com/?p=79","description":"<h1 id=\"whattodowhenmongowontreturnyourcalls\">What to do when Mongo won’t return your calls</h1> \n<p>Some time ago, one of our internal users reported a frustrating problem in our ETL job system. One of their jobs got stuck over the weekend, and all attempts to cancel it had no effect. It was the third weekend in a row that this had happened.</p> \n<p>We use MongoDB to store the metadata for these jobs, as its document structure makes it a good fit for describing heterogeneous jobs with varying numbers of (and types of) steps. The Java server running the job was still online and responding to requests, but a jstack showed that the job thread was hung waiting on a response from MongoDB. All it was trying to do was update a single field in the Mongo document, but it was waiting forever for a response. From the MongoDB side, the primary node was also aware of the same update: the connection showed up in&nbsp;<code>db.currentOp()</code>&nbsp;and was marked “active”. Terminating the query from the Mongo shell instantly caused the Java thread to throw an exception, so it wasn’t a communication problem. Both sides of the connection were alive, yet the update wasn’t making any progress!</p> \n<p>Not good.</p> \n<p><a href=\"https://engineering.vena.io/author/achoi/\">Albert</a>&nbsp;investigated and found that our weekly secondary rotation jobs are scheduled around the same time that the group of UPDATE commands fired. Comparing timestamps in the rotation job output strongly suggested that UPDATE commands issued during a replSetReconfigure will hang and never return.</p> \n<p>I started trying to reproduce the issue locally by mimicking the problematic UPDATEs and was surprised to find that my very simple Python script experienced the same problem:</p> \n<figure class=\"wp-block-image\"> \n <img src=\"https://venaeng.wpengine.com/wp-content/uploads/2021/12/the_bug.gif\" alt=\"\" /> \n</figure> \n<p><a></a></p> \n<p>I’ve pushed up scripts that I used to reproduce this issue&nbsp;<a href=\"https://github.com/jmpesp/mongo_3.6.20_concurrency_bug_repro\">here</a>.</p> \n<p>The fact that I could reproduce the issue with a simple Python script meant that it’s not something to do with any code we’ve written on top of the Java Mongo client. The reproduction test case was so straight forward I turned my sights on MongoDB itself.</p> \n<h1 id=\"background\">Background</h1> \n<p>This post assumes that you know a little about MongoDB: what a primary and secondary is, what replication is and how it works, what an operation is, and so on.</p> \n<p>MongoDB is written in C++ so it’s useful to have a little knowledge there.</p> \n<h1 id=\"mongodbtakesalongtimetobuild\">MongoDB takes a&nbsp;<em>long</em>&nbsp;time to build</h1> \n<p>My Vena-issued T480s has a 4 core i5-8350U (<a href=\"https://www.youtube.com/watch?v=jI3YE3Jlgw8\">with hyper-threading off</a>), 24GB of DDR4 RAM, a modern NVMe drive, and it still takes&nbsp;<a href=\"https://en.wikipedia.org/wiki/The_Soul_of_a_New_Machine\">gollum</a>&nbsp;33 minutes to build mongod and the mongo CLI binaries.</p> \n<pre class=\"wp-block-code\"><code><small><code>scons: done building targets.\n\nreal    33m42.499s  \nuser    119m10.756s  \n sys    7m56.016s\n</code></small></code></pre> \n<p><small></small></p> \n<p>Also, did you know that an optimized mongod binary (when statically linked) is around 1.3G? I didn’t either, and now you know.</p> \n<pre class=\"wp-block-code\"><code><small><code>jwm@gollum ~/opt/mongo [39c200878284912f19553901a6fea4b31531a899] [] $ ls -lh mongod  \n-rwxr-xr-x 1 jwm jwm 1.3G Oct 21 17:23 mongod\n</code></small></code></pre> \n<p><small></small></p> \n<p>Note: that SHA is for 3.6.20, our production cluster version.</p> \n<h1 id=\"hellogdbmyoldfriend\">Hello gdb, my old friend</h1> \n<p>After building the debug binaries, I created a docker container using the resulting&nbsp;<code>mongod</code>&nbsp;and&nbsp;<code>mongo</code>&nbsp;binaries, and confirmed that the issue was still reproducible.</p> \n<p>I reproduced the issue and used&nbsp;<code>db.currentOp()</code>&nbsp;to find the connection id (“conn15” in the example below):</p> \n<pre class=\"wp-block-code\"><code><small><code>        ...\n        {\n            \"host\" : \"ecb589a5d43b:27017\",\n            \"desc\" : \"conn15\",\n            \"connectionId\" : 15,\n            \"client\" : \"172.17.0.1:36012\",\n            \"clientMetadata\" : {\n                \"driver\" : {\n                    \"name\" : \"PyMongo\",\n                    \"version\" : \"3.11.0\"\n                },\n                ...\n            },\n            \"active\" : true,\n            ...\n</code></small></code></pre> \n<p><small></small></p> \n<p>Then I attached gdb to the primary mongod process and found the corresponding thread:</p> \n<pre class=\"wp-block-code\"><code><code>(gdb) info threads\n  ...\n  75   Thread 0x7ff5f8bfb700 (LWP 4427) \"conn15\"          futex_wait_cancelable (private=0, expected=0, futex_word=0x7ff5f8bf8a6c)\n    at ../sysdeps/unix/sysv/linux/futex-internal.h:88\n  ...\n(gdb) thread 75\n[Switching to thread 75 (Thread 0x7ff5f8bfb700 (LWP 4427))]\n#0  futex_wait_cancelable (...) at ../sysdeps/unix/sysv/linux/futex-internal.h:88\n88  in ../sysdeps/unix/sysv/linux/futex-internal.h  \n(gdb) bt\n#0  futex_wait_cancelable (...) at ../sysdeps/unix/sysv/linux/futex-internal.h:88\n#1  __pthread_cond_wait_common (...) at pthread_cond_wait.c:502\n#2  __pthread_cond_wait (...) at pthread_cond_wait.c:655\n#3  0x00007ff62ce983bc in std::condition_variable::wait(...) ()\n   from target:/usr/lib/x86_64-linux-gnu/libstdc++.so.6\n#4  0x00005620ed95c4f8 in mongo::OperationContext::&lt;lambda()&gt;::operator()(void) const (__closure=0x7ff5f8bf8890)\n    at src/mongo/db/operation_context.cpp:313\n#5  0x00005620ed95c86f in mongo::OperationContext::waitForConditionOrInterruptNoAssertUntil (...) at src/mongo/db/operation_context.cpp:317\n#6  0x00005620ec4cc511 in mongo::repl::ReplicationCoordinatorImpl::_awaitReplication_inlock (...)\n    at src/mongo/db/repl/replication_coordinator_impl.cpp:1689\n#7  0x00005620ec4cba58 in mongo::repl::ReplicationCoordinatorImpl::awaitReplication (...) at src/mongo/db/repl/replication_coordinator_impl.cpp:1581\n#8  0x00005620ec049e9f in mongo::waitForWriteConcern (...)\n    at src/mongo/db/write_concern.cpp:231\n#9  0x00005620ebf46fe1 in mongo::(anonymous namespace)::_waitForWriteConcernAndAddToCommandResponse (...) at src/mongo/db/service_entry_point_mongod.cpp:300\n#10 0x00005620ebf47847 in mongo::(anonymous namespace)::&lt;lambda()&gt;::operator()(void) const (__closure=0x7ff5f8bf9068)\n    at src/mongo/db/service_entry_point_mongod.cpp:507\n#11 0x00005620ebf4e750 in mongo::ScopeGuardImpl0&lt;mongo::(anonymous namespace)::runCommandImpl(...)::&lt;lambda()&gt; &gt;::Execute(void) (\n    this=0x7ff5f8bf9060) at src/mongo/util/scopeguard.h:142\n#12 0x00005620ebf4e594 in mongo::ScopeGuardImplBase::SafeExecute&lt;mongo::ScopeGuardImpl0&lt;mongo::(anonymous namespace)::runCommandImpl(...)::&lt;lambda()&gt; &gt; &gt;(mongo::ScopeGuardImpl0&lt;mongo::(anonymous namespace)::runCommandImpl(...)::&lt;lambda()&gt; &gt; &amp;) (j=...) at src/mongo/util/scopeguard.h:101\n#13 0x00005620ebf4e40e in mongo::ScopeGuardImpl0&lt;mongo::(anonymous namespace)::runCommandImpl(...)::&lt;lambda()&gt; &gt;::~ScopeGuardImpl0(void) (\n    this=0x7ff5f8bf9060, __in_chrg=&lt;optimized out&gt;) at src/mongo/util/scopeguard.h:138\n#14 0x00005620ebf4816f in mongo::(anonymous namespace)::runCommandImpl (...)\n    at src/mongo/db/service_entry_point_mongod.cpp:509\n#15 0x00005620ebf49fdc in mongo::(anonymous namespace)::execCommandDatabase (...)\n    at src/mongo/db/service_entry_point_mongod.cpp:768\n#16 0x00005620ebf4b049 in mongo::(anonymous namespace)::&lt;lambda()&gt;::operator()(void) const (__closure=0x7ff5f8bf9740)\n    at src/mongo/db/service_entry_point_mongod.cpp:882 \n#17 0x00005620ebf4b82d in mongo::(anonymous namespace)::runCommands (...)\n    at src/mongo/db/service_entry_point_mongod.cpp:895\n...</code></code></pre> \n<p>The&nbsp;<code>conn15</code>&nbsp;thread was stuck waiting on a condition variable at&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/operation_context.cpp#L313\"><code>src/mongo/db/operation_context.cpp</code>, line&nbsp;<code>313</code></a>, which is frame 4 in the above backtrace:</p> \n<pre class=\"wp-block-code\"><code>311     const auto waitStatus = [&amp;] {  \n312         if (Date_t::max() == deadline) {  \n313             cv.wait(m);  \n314             return stdx::cv_status::no_timeout;  \n315         }  \n316         return getServiceContext()-&gt;getPreciseClockSource()-&gt;waitForConditionUntil(cv, m, deadline);  \n317     }();  </code></pre> \n<p>Note this op doesn’t have write concern timeout, so the deadline is&nbsp;<code>Date_t::max()</code>. I interpreted this as “if our operation has no deadline, wait forever for the condition variable”.</p> \n<p>Frame 5 is the lambda execution, and frame 6 is the calling context:&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L1689\"><code>src/mongo/db/repl/replication_coordinator_impl.cpp</code>:1689</a>&nbsp;is from a loop in a function called&nbsp;<code>_awaitReplication_inlock</code>:</p> \n<pre class=\"wp-block-code\"><code>1679     // Must hold _mutex before constructing waitInfo as it will modify _replicationWaiterList  \n1680     stdx::condition_variable condVar;  \n1681     ThreadWaiter waiter(opTime, &amp;writeConcern, &amp;condVar);  \n1682     WaiterGuard guard(&amp;_replicationWaiterList, &amp;waiter);  \n1683     while (!_doneWaitingForReplication_inlock(opTime, minSnapshot, writeConcern)) {  \n...\n1689         auto status = opCtx-&gt;waitForConditionOrInterruptNoAssertUntil(condVar, *lock, wTimeoutDate);  \n1690         if (!status.isOK()) {  \n1691             return status.getStatus();  \n1692         }  \n...\n1710     }  </code></pre> \n<p>The condition variable created here (the one we’re waiting on in frame 4) is passed both to&nbsp;<code>ThreadWaiter waiter</code>&nbsp;and&nbsp;<code>opCtx-&gt;waitForConditionOrInterruptNoAssertUntil(...)</code>.</p> \n<p>Let’s look at&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.h#L486\"><code>ThreadWaiter</code></a>:<small><code> </code></small></p> \n<pre class=\"wp-block-code\"><code> 486     // When ThreadWaiter gets notified, it will signal the conditional variable.\n 487     //\n 488     // This is used when a thread wants to block inline until the opTime is reached with the given\n 489     // writeConcern.\n 490     struct ThreadWaiter : public Waiter {</code></pre> \n<p>The&nbsp;<code>ThreadWaiter</code>&nbsp;is passed to a&nbsp;<code>WaiterGuard</code>&nbsp;on line 1682, so let’s also look at&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L206\"><code>WaiterGuard</code></a>:<small><code> </code></small></p> \n<pre class=\"wp-block-code\"><code> 206 class ReplicationCoordinatorImpl::WaiterGuard {\n 207 public:\n 208     /**\n 209      * Constructor takes the list of waiters and enqueues itself on the list, removing itself\n 210      * in the destructor.\n 211      *\n 212      * Usually waiters will be signaled and removed when their criteria are satisfied, but\n 213      * wait_until() with timeout may signal waiters earlier and this guard will remove the waiter\n 214      * properly.\n 215      *\n 216      * _list is guarded by ReplicationCoordinatorImpl::_mutex, thus it is illegal to construct one\n 217      * of these without holding _mutex\n 218      */\n</code></pre> \n<p><a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L1527\"><code>_doneWaitingForReplication_inlock</code></a>&nbsp;does what you’d imagine – for an op with write concern majority, it waits for the majority of mongo instances to reach the op’s opTime:</p> \n<pre class=\"wp-block-code\"><code>1527 bool ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock(  \n1528     const OpTime&amp; opTime, Timestamp minSnapshot, const WriteConcernOptions&amp; writeConcern) {  \n...\n1569     StatusWith&lt;ReplSetTagPattern&gt; tagPattern = _rsConfig.findCustomWriteMode(patternName);  \n1570     if (!tagPattern.isOK()) {  \n1571         return true;  \n1572     }  \n1573     return _topCoord-&gt;haveTaggedNodesReachedOpTime(opTime, tagPattern.getValue(), useDurableOpTime);  \n1574 }  </code></pre> \n<p>Based on those snippets, here was the mental model I had so far:</p> \n<ul> \n <li>there’s a list of waiters in&nbsp;<code>_replicationWaiterList</code></li> \n <li>these waiters refer to condition variables</li> \n <li>if&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;returns false, then the thread waits on the condition variable</li> \n <li>once the condition variable is notified, the thread wakes up and checks if replication is done again.</li> \n</ul> \n<p>I assumed those condition variables would be notified as opTimes on other mongo instances change, but it looks like they are notified when the instance they’re running on has committed a new opTime or something called a committed snapshot. Both&nbsp;<code>_updateLastCommittedOpTime_inlock</code>&nbsp;and&nbsp;<code>_updateCommittedSnapshot_inlock</code>&nbsp;call&nbsp;<code>_wakeReadyWaiters_inlock</code>:</p> \n<pre class=\"wp-block-code\"><code>3385 void ReplicationCoordinatorImpl::_updateLastCommittedOpTime_inlock() {  \n...\n3389     // Wake up any threads waiting for replication that now have their replication  \n3390     // check satisfied.  We must do this regardless of whether we updated the lastCommittedOpTime,  \n3391     // as lastCommittedOpTime may be based on durable optimes whereas some waiters may be  \n3392     // waiting on applied (but not necessarily durable) optimes.  \n3393     _wakeReadyWaiters_inlock();  \n3394     _signalStepDownWaiterIfReady_inlock();  \n3395 }\n\n...\n\n3805 void ReplicationCoordinatorImpl::_updateCommittedSnapshot_inlock(  \n3806     const OpTime&amp; newCommittedSnapshot) {  \n...\n3834     // Wake up any threads waiting for read concern or write concern.  \n3835     _wakeReadyWaiters_inlock();  \n3836 }\n\n...\n\n3174 void ReplicationCoordinatorImpl::_wakeReadyWaiters_inlock() {  \n3175     _replicationWaiterList.signalAndRemoveIf_inlock([this](Waiter* waiter) {  \n3176         return _doneWaitingForReplication_inlock(  \n3177             waiter-&gt;opTime, Timestamp(), *waiter-&gt;writeConcern);  \n3178     });  \n3179 }  </code></pre> \n<p>Note that in&nbsp;<code>_wakeReadyWaiters_inlock</code>, the&nbsp;<code>ThreadWaiter</code>&nbsp;is removed from&nbsp;<code>_replicationWaiterList</code>&nbsp;if replication is done. This makes sense – that thread won’t need to wait ever again.</p> \n<p>I wanted to print out the ThreadWaiter object information, but:</p> \n<pre class=\"wp-block-code\"><code><small><code>(gdb) frame 6\n(gdb) print _replicationWaiterList._list.size()\n$13 = 0\n</code></small></code></pre> \n<p><small></small></p> \n<p>No more waiters! We’ve reached a state where there are no waiters in&nbsp;<code>_replicationWaiterList</code>&nbsp;but we’re still waiting on the condition variable to be notified. That means:</p> \n<ul> \n <li>after the thread woke up,&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;reported&nbsp;<code>false</code>, because we entered another iteration of the loop.</li> \n <li>nothing else will notify this thread’s condition variable because the reference to it was gone</li> \n</ul> \n<p>This explains what we’re seeing: operations that never return.</p> \n<p>So what removes waiters from this list?</p> \n<ul> \n <li><code>~WaiterGuard()</code></li> \n <li><a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L3164\"><code>ReplicationCoordinatorImpl::_wakeReadyWaiters_inlock</code></a></li> \n <li><a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L788\"><code>ReplicationCoordinatorImpl::shutdown</code></a></li> \n <li><a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L2661\"><code>ReplicationCoordinatorImpl::_updateMemberStateFromTopologyCoordinator_inlock</code></a></li> \n</ul> \n<p>The last two seemed related to primary stepdowns, and that’s not something we see in this case. This bug is triggered when secondaries are rotated out – nothing affects the primary.</p> \n<p>One hypothesis I had was that when a secondary goes away, corresponding waiters are removed from this list, but I scrapped it because replication shouldn’t rely on a designated secondary – any secondary should be able to report that its opTime is up to date and satisfy replication criteria.</p> \n<p>Our procedure for rotating secondaries is straight from Mongo’s documentation (<a href=\"https://docs.mongodb.com/manual/tutorial/remove-replica-set-member/\">https://docs.mongodb.com/manual/tutorial/remove-replica-set-member/</a>) where we:</p> \n<ol> \n <li>Provision a new secondary.</li> \n <li>Run&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/shell/utils.js#L1366\"><code>rs.add(new secondary)</code></a>&nbsp;on the primary.</li> \n <li>Shut down mongod on the instance to be removed.</li> \n <li>Run&nbsp;<code>rs.remove(old secondary)</code>&nbsp;on the primary.</li> \n</ol> \n<p><a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/shell/utils.js#L1423\"><code>rs.remove()</code></a>&nbsp;is an alias that calls the&nbsp;<code>replSetReconfig</code>&nbsp;admin command with a new configuration that no longer contains the instance to be removed.</p> \n<p>Issuing that admin command creates another thread that runs&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L2364\"><code>ReplicationCoordinatorImpl::processReplSetReconfig</code></a>. There’s a lot of machinery in there for validating the configuration update (among other things) and if all those are ok,&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L2481\"><code>ReplicationCoordinatorImpl::_finishReplSetReconfig</code></a>&nbsp;is called.</p> \n<h1 id=\"itsaconcurrencybug\">It’s a concurrency bug!</h1> \n<p>Looking at&nbsp;<code>_finishReplSetReconfig</code>, there’s a lot of code in there to deal with a potential leader election (any replSetReconfig can trigger a primary stepdown and subsequent leader election). If that is cut out, the code looks like the following:</p> \n<pre class=\"wp-block-code\"><code>2491 void ReplicationCoordinatorImpl::_finishReplSetReconfig(  \n2492     const executor::TaskExecutor::CallbackArgs&amp; cbData,  \n2493     const ReplSetConfig&amp; newConfig,  \n2494     const bool isForceReconfig,  \n2495     int myIndex,  \n2496     const executor::TaskExecutor::EventHandle&amp; finishedEvent) {  \n...\n2530     const ReplSetConfig oldConfig = _rsConfig;  \n2531     const PostMemberStateUpdateAction action =  \n2532         _setCurrentRSConfig_inlock(opCtx.get(), newConfig, myIndex);  \n2533  \n2534     // On a reconfig we drop all snapshots so we don't mistakenly read from the wrong one.  \n2535     // For example, if we change the meaning of the \"committed\" snapshot from applied -&gt; durable.  \n2536     _dropAllSnapshots_inlock();  \n...\n2549 }  </code></pre> \n<p>Following&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L2997\"><code>_setCurrentRSConfig_inlock</code></a>, there’s again a lot of validation logic in there, and some extra logic for dealing with protocol version changes. If we cut out that code again, the code looks like the following:</p> \n<pre class=\"wp-block-code\"><code>3007 ReplicationCoordinatorImpl::_setCurrentRSConfig_inlock(OperationContext* opCtx,  \n3008                                                        const ReplSetConfig&amp; newConfig,  \n3009                                                        int myIndex) {  \n...\n3164     // Update commit point for the primary. Called by every reconfig because the config  \n3165     // may change the definition of majority.  \n3166     //  \n3167     // On PV downgrade, commit point is probably still from PV1 but will advance to an OpTime with  \n3168     // term -1 once any write gets committed in PV0.  \n3169     _updateLastCommittedOpTime_inlock();  \n3170  \n3171     return action;  \n3172 }  </code></pre> \n<p><code>_updateLastCommittedOpTime_inlock</code>&nbsp;is one of the functions that calls&nbsp;<code>_wakeReadyWaiters_inlock</code>, which removes waiters from&nbsp;<code>_replicationWaiterList</code>&nbsp;if they are done waiting for replication. Here’s that code again:</p> \n<pre class=\"wp-block-code\"><code>3174 void ReplicationCoordinatorImpl::_wakeReadyWaiters_inlock() {  \n3175     _replicationWaiterList.signalAndRemoveIf_inlock([this](Waiter* waiter) {  \n3176         return _doneWaitingForReplication_inlock(  \n3177             waiter-&gt;opTime, Timestamp(), *waiter-&gt;writeConcern);  \n3178     });  \n3179 }  </code></pre> \n<p>The update operation’s waiter is only signaled and removed if&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;returned true for that particular waiter. In the update operation’s thread, execution would proceed after&nbsp;<code>cv.wait()</code>, and eventually reach another&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;in&nbsp;<code>_awaitReplication_inlock</code>:</p> \n<pre class=\"wp-block-code\"><code>1681     ThreadWaiter waiter(opTime, &amp;writeConcern, &amp;condVar);  \n1682     WaiterGuard guard(&amp;_replicationWaiterList, &amp;waiter);  \n1683     while (!_doneWaitingForReplication_inlock(opTime, minSnapshot, writeConcern)) {  </code></pre> \n<p>If the first&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;returned true and the second returned false the update thread will hang on that condition variable’s wait and (because&nbsp;<code>_wakeReadyWaiters_inlock</code>&nbsp;removes the waiter from the list) there’s nothing that will notify it.</p> \n<p>Both&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;calls above reference the same arguments except for the second one: it’s referring to Timestamp() and minSnapshot respectively. What could make it return true then false?</p> \n<p>Having another look at&nbsp;<a href=\"https://github.com/mongodb/mongo/blob/r3.6.20/src/mongo/db/repl/replication_coordinator_impl.cpp#L1527\"><code>_doneWaitingForReplication_inlock</code></a>, I spotted:</p> \n<pre class=\"wp-block-code\"><code>1544     if (writeConcern.wMode == WriteConcernOptions::kMajority) {  \n1545         if (_externalState-&gt;snapshotsEnabled() &amp;&amp; !testingSnapshotBehaviorInIsolation) {  \n1546             // Make sure we have a valid \"committed\" snapshot up to the needed optime.  \n1547             if (!_currentCommittedSnapshot) {  \n1548                 return false;  \n1549             }  </code></pre> \n<p>But back in&nbsp;<code>_finishReplSetReconfig</code>, snapshots are dropped!</p> \n<pre class=\"wp-block-code\"><code>2534     // On a reconfig we drop all snapshots so we don't mistakenly read from the wrong one.  \n2535     // For example, if we change the meaning of the \"committed\" snapshot from applied -&gt; durable.  \n2536     _dropAllSnapshots_inlock();\n\n3843 void ReplicationCoordinatorImpl::_dropAllSnapshots_inlock() {  \n3844     _currentCommittedSnapshot = boost::none;  \n3845     _externalState-&gt;dropAllSnapshots();  \n3846 }  </code></pre> \n<p>This looked like a concurrency bug!</p> \n<p>I confirmed it with a lot of extra Mongo log statements:</p> \n<pre class=\"wp-block-code\"><code>2020-10-21T15:07:36.344+0000 E REPL     [conn17] ReplicationCoordinatorImpl::WaiterList::add_inlock { opTime: { ts: Timestamp(1603292856, 3), t: 1 }, writeConcern: { w: \"majority\", j: true, wtimeout: 0 } }  \n2020-10-21T15:07:36.344+0000 E REPL     [conn17] ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock: [opTime { ts: Timestamp(1603292856, 3), t: 1 }, minSnapshot Timestamp(0, 0), writeConcern { w: \"majority\", j: true, wtimeout: 0 }] !_currentCommittedSnapshot, returning false  \n2020-10-21T15:07:36.344+0000 E REPL     [conn17] looping for 1551, waiters length 1  \n2020-10-21T15:07:36.344+0000 E REPL     [conn17] { opTime: { ts: Timestamp(1603292856, 3), t: 1 }, writeConcern: { w: \"majority\", j: true, wtimeout: 0 } }  \n2020-10-21T15:07:38.208+0000 I REPL     [replexec-7] New replica set config in use: { _id: \"rsvena0\", version: 3, protocolVersion: 1, members: [ { _id: 0, host: \"736f4cdb52d9:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: \"281b53d51585:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: \"acff78f7f7f1:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5f904dd902040f0b89191bc0') } }  \n2020-10-21T15:07:38.210+0000 E REPL     [replexec-7] ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock: [opTime { ts: Timestamp(1603292856, 3), t: 1 }, minSnapshot Timestamp(0, 0), writeConcern { w: \"majority\", j: true, wtimeout: 0 }] haveTaggedNodesReachedOpTime returned true  \n2020-10-21T15:07:38.210+0000 E REPL     [replexec-7] ReplicationCoordinatorImpl::WaiterList::signalAndRemoveIf_inlock { opTime: { ts: Timestamp(1603292856, 3), t: 1 }, writeConcern: { w: \"majority\", j: true, wtimeout: 0 } }  \n2020-10-21T15:07:38.210+0000 I CONTROL  [replexec-7] ReplicationCoordinatorImpl::WaiterList::signalAndRemoveIf_inlock  \n2020-10-21T15:07:38.317+0000 E REPL     [conn17] ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock: [opTime { ts: Timestamp(1603292856, 3), t: 1 }, minSnapshot Timestamp(0, 0), writeConcern { w: \"majority\", j: true, wtimeout: 0 }] !_currentCommittedSnapshot, returning false  \n2020-10-21T15:07:38.318+0000 E REPL     [conn17] looping for 1551, waiters length 0  </code></pre> \n<p>Note that&nbsp;<code>conn17</code>&nbsp;is the update operation from the python client, which in this case is operation 1551.</p> \n<p>Let’s go through this from the beginning:</p> \n<pre class=\"wp-block-code\"><code>[conn17] ReplicationCoordinatorImpl::WaiterList::add_inlock { opTime: { ts: Timestamp(1603292856, 3), t: 1 }, writeConcern: { w: \"majority\", j: true, wtimeout: 0 } } \n[conn17] ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock: [opTime { ts: Timestamp(1603292856, 3), t: 1 }, minSnapshot Timestamp(0, 0), writeConcern { w: \"majority\", j: true, wtimeout: 0 }] !_currentCommittedSnapshot, returning false\n[conn17] looping for 1551, waiters length 1\n[conn17] { opTime: { ts: Timestamp(1603292856, 3), t: 1 }, writeConcern: { w: \"majority\", j: true, wtimeout: 0 } }</code></pre> \n<p>These log messages were put in&nbsp;<code>_awaitReplication_inlock</code>:</p> \n<ol> \n <li>the&nbsp;<code>ThreadWaiter</code>&nbsp;is added to&nbsp;<code>_replicationWaiterList</code>&nbsp;(<code>add_inlock</code>)</li> \n <li>at the end of that function (where mongo loops waiting for&nbsp;<code>_doneWaitingForReplication_inlock</code>),&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;returns false.</li> \n <li>print out which operation we’re looping for (<em>looping for 1551, …</em>), and print out the waiters that are in the list.</li> \n</ol> \n<p>Next:</p> \n<pre class=\"wp-block-code\"><code>[replexec-7] New replica set config in use: { _id: \"rsvena0\", version: 3, protocolVersion: 1, members: [ { _id: 0, host: \"736f4cdb52d9:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: \"281b53d51585:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: \"acff78f7f7f1:27017\", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5f904dd902040f0b89191bc0') } }\n[replexec-7] ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock: [opTime { ts: Timestamp(1603292856, 3), t: 1 }, minSnapshot Timestamp(0, 0), writeConcern { w: \"majority\", j: true, wtimeout: 0 }] haveTaggedNodesReachedOpTime returned true\n[replexec-7] ReplicationCoordinatorImpl::WaiterList::signalAndRemoveIf_inlock { opTime: { ts: Timestamp(1603292856, 3), t: 1 }, writeConcern: { w: \"majority\", j: true, wtimeout: 0 } } \n[replexec-7] ReplicationCoordinatorImpl::WaiterList::signalAndRemoveIf_inlock</code></pre> \n<ol> \n <li>Replica set reconfiguration occurs.</li> \n <li><code>_wakeReadyWaiters_inlock</code>&nbsp;is called, which calls&nbsp;<code>signalAndRemoveIf_inlock</code></li> \n <li><code>_doneWaitingForReplication_inlock</code>&nbsp;returns true for our operation’s waiter (notice it reached the end, and called&nbsp;<code>haveTaggedNodesReachedOpTime</code>)</li> \n <li>that waiter is removed from&nbsp;<code>_replicationWaiterList</code></li> \n</ol> \n<p>Next:</p> \n<pre class=\"wp-block-code\"><code>[conn17] ReplicationCoordinatorImpl::_doneWaitingForReplication_inlock: [opTime { ts: Timestamp(1603292856, 3), t: 1 }, minSnapshot Timestamp(0, 0), writeConcern { w: \"majority\", j: true, wtimeout: 0 }] !_currentCommittedSnapshot, returning false\n[conn17] looping for 1551, waiters length 0</code></pre> \n<p>This comes again from&nbsp;<code>_awaitReplication_inlock</code>:</p> \n<ol> \n <li>The condition variable was signalled by&nbsp;<code>signalAndRemoveIf_inlock</code>.</li> \n <li>Execution continues to the top of the while loop, and&nbsp;<code>_doneWaitingForReplication_inlock</code>&nbsp;returns false.</li> \n <li>There are no more waiters in&nbsp;<code>_replicationWaiterList</code>.</li> \n <li>This thread will wait on the condition variable and never return.</li> \n</ol> \n<h1 id=\"thebug\">The bug</h1> \n<p>Visualized (where the left side is the operation thread, and the right side is the thread processing the replSetReconfig command):</p> \n<figure class=\"wp-block-image\"> \n <img src=\"https://venaeng.wpengine.com/wp-content/uploads/2021/12/mongo_concurrency_bug.png\" alt=\"\" /> \n</figure> \n<p><a></a></p> \n<p><a href=\"https://venaeng.wpengine.com/wp-content/uploads/2021/12/mongo_concurrency_bug.png\">link to image</a></p> \n<p>Below is the sequence that causes this bug, where blue represents the operation thread, and red represents the replSetReconfig thread.</p> \n<ol> \n <li><code>_doneWaitingForReplication_inlock</code>&nbsp;returns false</li> \n <li>wait on condition variable</li> \n <li><code>_doneWaitingForReplication_inlock</code>&nbsp;returns true</li> \n <li>set&nbsp;<code>_currentCommittedSnapshot = boost::none</code></li> \n <li>in&nbsp;<code>_doneWaitingForReplication_inlock</code>, check&nbsp;<code>_currentCommittedSnapshot</code>&nbsp;and return false because it’s&nbsp;<code>boost::none</code></li> \n <li>wait forever on condition variable</li> \n</ol> \n<h1 id=\"thefix\">The fix</h1> \n<p>The problem exists because&nbsp;<code>_currentCommittedSnapshot</code>&nbsp;was zeroed out temporarily – my proposal for a fix is to add a wait until&nbsp;<code>_currentCommittedSnapshot</code>&nbsp;is non-null again:<small><code> </code></small></p> \n<pre class=\"wp-block-code\"><code> 1680     stdx::condition_variable condVar;\n 1681     ThreadWaiter waiter(opTime, &amp;writeConcern, &amp;condVar);\n 1682     WaiterGuard guard(&amp;_replicationWaiterList, &amp;waiter);\n 1683     while (!_doneWaitingForReplication_inlock(opTime, minSnapshot, writeConcern)) {\n ...\n 1689         auto status = opCtx-&gt;waitForConditionOrInterruptNoAssertUntil(condVar, *lock, wTimeoutDate);\n 1690         if (!status.isOK()) {\n 1691             return status.getStatus();\n 1692         }\n ...\n+1711         // If a replSetReconfig occurred, then all snapshots will be dropped.\n+1712         // `_doneWaitingForReplication_inlock` will fail if there is no current snapshot, and\n+1713         // if this thread's waiter was signaled and removed from the wait list during\n+1714         // replSetReconfig we will enter waitForConditionOrInterruptNoAssertUntil above and\n+1715         // condVar will never be notified.\n+1716         //\n+1717         // If it's null, wait for newly committed snapshot here.\n+1718         while (!_currentCommittedSnapshot) {\n+1719             opCtx-&gt;waitForConditionOrInterrupt(_currentCommittedSnapshotCond, *lock);\n+1720         }\n 1721     }</code></pre> \n<p>I’ve tested this, and it successfully resumes after a replSetReconfig:</p> \n<figure class=\"wp-block-image\"> \n <img src=\"https://venaeng.wpengine.com/wp-content/uploads/2021/12/bug_fixed.gif\" alt=\"\" /> \n</figure> \n<p><a></a></p> \n<p>Note that there is still a pause during the reconfigure, but it eventually resumes.</p> \n<h1 id=\"filingaticket\">Filing a ticket</h1> \n<p>I was also able to reproduce this bug against versions&nbsp;<code>4.0.0</code>&nbsp;and&nbsp;<code>4.0.1</code>, but not&nbsp;<code>4.0.2</code>. I looked at all the commits between&nbsp;<code>4.0.1</code>&nbsp;and&nbsp;<code>4.0.2</code>&nbsp;and&nbsp;<a href=\"https://github.com/mongodb/mongo/commit/fe1b92cee5c133e82845ffbd31b25ab5b66084d3\">this one</a>&nbsp;looks like it would solve this problem by removing one of the necessary conditions: that the waiter was removed from the list by something other than the waiter guard. That looks like a much cleaner fix than my proposal.</p> \n<p>In any case, I&nbsp;<a href=\"https://jira.mongodb.org/browse/SERVER-54638\">opened an issue on Mongo’s Jira</a>, and the story continues there!</p>","descriptionType":"html","publishedDate":"Sat, 20 Feb 2021 02:22:00 +0000","feedId":30537,"bgimg":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/the_bug.gif","linkMd5":"3d789f5b5d0d6c997bdf7dd46258f2f7","destWidth":1280,"destHeight":720,"sourceBytes":686374,"destBytes":2489792,"author":"James MacMahon","articleImgCdnMap":{"https://venaeng.wpengine.com/wp-content/uploads/2021/12/the_bug.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn89@2020_3/2022/03/14/14-32-38-501_7d6d71b14acfdbc1.webp","https://venaeng.wpengine.com/wp-content/uploads/2021/12/mongo_concurrency_bug.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn88@2020_2/2022/03/14/14-32-23-416_44d2ed3d821b427a.webp","https://venaeng.wpengine.com/wp-content/uploads/2021/12/bug_fixed.gif":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn91@2020_5/2022/03/14/14-32-38-580_cde2166868a9f93b.webp"},"publishedOrCreatedDate":1647268221717}],"record":{"createdTime":"2022-03-14 22:30:21","updatedTime":"2022-03-14 22:30:21","feedId":30537,"fetchDate":"Mon, 14 Mar 2022 14:30:21 +0000","fetchMs":740,"handleMs":19,"totalMs":152880,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"f306b40366a93aa712f78b685bea0f01","hostName":"us-024*","requestId":"be3372961e1245ff8de1d68d9c35f5e9_30537","contentType":"application/rss+xml; charset=UTF-8","totalBytes":5433244,"bgimgsTotal":1,"bgimgsGithubTotal":0,"articlesImgsTotal":3,"articlesImgsGithubTotal":3,"successGithubMap":{"myreaderx8":1,"myreaderx15":1,"myreaderx6":1},"failGithubMap":{"myreaderx31":1}},"feed":{"createdTime":"2020-09-07 02:50:02","updatedTime":"2020-09-07 04:56:48","id":30537,"name":"Vena Engineering Blog","url":"https://engineering.vena.io/rss/","subscriber":106,"website":null,"icon":"https://engineering.vena.io/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn9@2020_4/2020/09/06/20-56-46-240_97d5e26907819d35.ico","description":"Engineering articles from the teams at Vena Solutions","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2022-03-14 22:32:53","updatedTime":"2022-03-14 22:32:53","id":null,"feedId":30537,"linkMd5":"3d789f5b5d0d6c997bdf7dd46258f2f7"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":5433244,"tmpBgImgCdnBytes":0,"extra4":{"start":1647268220928,"total":0,"statList":[{"spend":772,"msg":"获取xml内容"},{"spend":19,"msg":"解释文章"},{"spend":16699,"msg":"正文链接上传到cdn"},{"spend":31139,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"}]},"extra5":3,"extra6":3,"extra7ImgCdnFailResultVector":[null,null,{"code":1,"isDone":false,"source":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/the_bug.gif","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"sourceBytes":686374,"destBytes":2489792,"targetWebpQuality":75,"feedId":30537,"totalSpendMs":16662,"convertSpendMs":15692,"createdTime":"2022-03-14 22:32:22","host":"us-012*","referer":"https://venaeng.wpengine.com/?p=79","linkMd5ListStr":"3d789f5b5d0d6c997bdf7dd46258f2f7,3d789f5b5d0d6c997bdf7dd46258f2f7","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx31/cdn83/contents/2022/03/14/14-32-39-094_7d6d71b14acfdbc1.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69855631.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 14:32:39 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D9EC:5B6C:F6F3B7:2C88C9A:622F5207"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647271454"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx31/cdn83/contents/2022/03/14/14-32-39-094_7d6d71b14acfdbc1.webp","historyStatusCode":[],"spendMs":218},"base64UserPassword":null,"token":"da243******************************d9e47"},"githubUser":"myreaderx31","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"670.3 KB","destSize":"2.4 MB","compressRate":"362.7%"},{"code":1,"isDone":false,"source":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/the_bug.gif","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"sourceBytes":686374,"destBytes":2489792,"targetWebpQuality":75,"feedId":30537,"totalSpendMs":14460,"convertSpendMs":13716,"createdTime":"2022-03-14 22:32:39","host":"us-012*","referer":"https://venaeng.wpengine.com/?p=79","linkMd5ListStr":"3d789f5b5d0d6c997bdf7dd46258f2f7,3d789f5b5d0d6c997bdf7dd46258f2f7","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx31/cdn83/contents/2022/03/14/14-32-53-690_7d6d71b14acfdbc1.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69855631.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 14:32:53 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D9EC:5B6C:F6F4F9:2C8912E:622F5215"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647271454"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx31/cdn83/contents/2022/03/14/14-32-53-690_7d6d71b14acfdbc1.webp","historyStatusCode":[],"spendMs":91},"base64UserPassword":null,"token":"da243******************************d9e47"},"githubUser":"myreaderx31","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"670.3 KB","destSize":"2.4 MB","compressRate":"362.7%"}],"extra10_invalidATagHrefValue":{"https://venaeng.wpengine.com/?p=79_/wp-content/uploads/2021/12/mongo_concurrency_bug.png":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/mongo_concurrency_bug.png"},"extra111_proxyServerAndStatMap":{},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/mongo_concurrency_bug.png","sourceStatusCode":200,"destWidth":1470,"destHeight":976,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn88@2020_2/2022/03/14/14-32-23-416_44d2ed3d821b427a.webp","sourceBytes":203741,"destBytes":82374,"targetWebpQuality":75,"feedId":30537,"totalSpendMs":1078,"convertSpendMs":107,"createdTime":"2022-03-14 22:32:22","host":"us-51*","referer":"https://venaeng.wpengine.com/?p=79","linkMd5ListStr":"3d789f5b5d0d6c997bdf7dd46258f2f7","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"199 KB","destSize":"80.4 KB","compressRate":"40.4%"},{"code":1,"isDone":false,"source":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/the_bug.gif","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn89@2020_3/2022/03/14/14-32-38-501_7d6d71b14acfdbc1.webp","sourceBytes":686374,"destBytes":2489792,"targetWebpQuality":75,"feedId":30537,"totalSpendMs":16413,"convertSpendMs":15077,"createdTime":"2022-03-14 22:32:22","host":"us-039*","referer":"https://venaeng.wpengine.com/?p=79","linkMd5ListStr":"3d789f5b5d0d6c997bdf7dd46258f2f7,3d789f5b5d0d6c997bdf7dd46258f2f7","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"670.3 KB","destSize":"2.4 MB","compressRate":"362.7%"},{"code":1,"isDone":false,"source":"https://venaeng.wpengine.com/wp-content/uploads/2021/12/bug_fixed.gif","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn91@2020_5/2022/03/14/14-32-38-580_cde2166868a9f93b.webp","sourceBytes":826543,"destBytes":2861078,"targetWebpQuality":75,"feedId":30537,"totalSpendMs":16678,"convertSpendMs":15106,"createdTime":"2022-03-14 22:32:22","host":"us-040*","referer":"https://venaeng.wpengine.com/?p=79","linkMd5ListStr":"3d789f5b5d0d6c997bdf7dd46258f2f7","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"807.2 KB","destSize":"2.7 MB","compressRate":"346.1%"}],"successGithubMap":{"myreaderx8":1,"myreaderx15":1,"myreaderx6":1},"failGithubMap":{"myreaderx31":1}}