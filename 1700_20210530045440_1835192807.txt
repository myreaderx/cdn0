{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-05-30 12:51:08","updatedTime":"2021-05-30 12:51:08","title":"Transformer在计算机视觉领域走到哪了？","link":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","description":"<div><div><div id=\"media\" class=\"rich_media_thumb_wrp\">\n\n            <img class=\"rich_media_thumb\" src=\"http://content.sov5.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icH3NqZdxxqViaowrYGdrJRw9picNx3fpQZpOvNm8VMWdjDuRNAiaxiaCEiaQ?imageView2/1/w/600\">\n        </div>\n    \n\n    \n\n    <div class=\"rich_media_content\" id=\"js_content\">\n                    \n\n                    \n                    \n                    \n                    <section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;\" data-mpa-powered-by=\"yiban.io\"><img class=\"__bg_gif\" data-cropselx1=\"0\" data-cropselx2=\"578\" data-cropsely1=\"0\" data-cropsely2=\"116\" data-ratio=\"0.17896389324960754\" data-type=\"gif\" data-w=\"637\" style=\"box-sizing: border-box; color: rgb(62, 62, 62); max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin: 10px 8px;white-space: normal;\"><section style=\"border-width: 2px;border-style: dashed;border-color: rgb(160, 219, 239);\"><section style=\"margin: 15px;\"><p style=\"color: rgb(136, 136, 136);font-size: 14px;\">编者按：Transformer 模型在自然语言处理（NLP）领域已然成为一个新范式，如今越来越多的研究在尝试将 Transformer 模型强大的建模能力应用到计算机视觉（CV）领域。那么未来，Transformer 会不会如同在 NLP 领域的应用一样革新 CV 领域？今后的研究思路又有哪些？微软亚洲研究院多媒体搜索与挖掘组的研究员们基于 Vision Transformer 模型在图像和视频理解领域的最新工作，可能会带给你一些新的理解。</p></section></section></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">作为一个由自注意力机制组成的网络结构，Transformer一“出场”就以强大的缩放性、学习长距离的依赖等优势，替代卷积神经网络（CNN）、循环神经网络（RNN）等网络结构，“席卷”了自然语言处理（NLP）领域的理解、生成任务。</span><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"> </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">然而，Transformer 并未止步于此，2020年，Transformer 模型首次被应用到了图像分类任务中并得到了比 CNN 模型更好的结果。此后，不少研究都开始尝试将 Transformer 模型强大的建模能力应用到计算机视觉领域。目前，Transformer 已经在三大图像问题上——分类、检测和分割，都取得了不错的效果。视觉与语言预训练、图像超分、视频修复和视频目标追踪等任务也正在成为 Transformer “跨界”的热门方向，在 Transformer 结构基础上进行应用和设计，也都取得了不错的成绩。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;line-height: 1.75em;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"40\" data-cropsely1=\"0\" data-cropsely2=\"40\" data-ratio=\"1.118421052631579\" data-s=\"300,640\" data-type=\"png\" data-w=\"76\" style=\"width: 31px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png\"></section><section data-id=\"93765\" data-tools=\"135编辑器\" style=\"white-space: normal;\"><section data-width=\"100%\" style=\"width: 100%;text-align: center;\"><section style=\"display: inline-block;\"><section style=\"padding-right: 0.8em;padding-left: 0.8em;letter-spacing: 2px;font-size: 18px;font-weight: bold;box-sizing: border-box;\"><strong><span style=\"color: rgb(0, 0, 0);\">Transformer“跨界”图像任务</span></strong></section><section data-width=\"100%\" style=\"margin-top: -10px;background: rgb(160, 219, 239);border-radius: 20px;height: 10px;overflow: hidden;\"><br></section></section></section></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">最近几年，随着基于 Transformer 的预训练模型在 NLP 领域不断展现出惊人的能力，越来越多的工作将 Transformer 引入到了图像以及相关的跨模态领域，Transformer 的自注意力机制以其领域无关性和高效的计算，极大地推动了图像相关任务的发展。</span><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">端到端的视觉和语言跨模态预训练模型</span></strong></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">视觉-语言预训练任务属于图像领域，其目标是利用大规模图片和语言对应的数据集，通过设计预训练任务学习更加鲁棒且具有代表性的跨模态特征，从而提高下游视觉-语言任务的性能。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">现有的视觉-语言预训练工作大都沿用传统视觉-语言任务的视觉特征表示，即基于目标检测网络离线抽取的区域视觉特征，将研究重点放到了视觉-语言（vision-language，VL）的特征融合以及预训练上，却忽略了视觉特征的优化对于跨模态模型的重要性。这种传统的视觉特征对于 VL 任务的学习主要有两点问题：</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">1）视觉特征受限于原本视觉检测任务的目标类别</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">2）忽略了非目标区域中对于上下文理解的重要信息</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">为了在VL模型中优化视觉特征，微软亚洲研究院多媒体搜索与挖掘组的研究员们</span><span style=\"color: rgb(0, 0, 0);font-size: 15px;background-color: rgb(160, 219, 239);\">提出了一种端到端的 VL 预训练网络 SOHO</span><span style=\"font-size: 15px;\">，为 VL 训练模型提供了一条全新的探索路径。 该工作的相关论文“Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning”已收录于CVPR 2021 Oral。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">论文链接：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://arxiv.org/abs/2104.03135</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">GitHub地址：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://github.com/researchmm/soho</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"277\" data-galleryid=\"\" data-ratio=\"0.26563769293257516\" data-s=\"300,640\" data-type=\"png\" data-w=\"1231\" style=\"width: 578px; height: 154px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icvxrjB2owskGZ4Rt1wtubOV6akm8SiaRJeCLIwOlBlQHU7xVfPTcPXpA/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">SOHO 模型的主要思路是：</span><span style=\"font-size: 15px;\">将视觉编码器整合到 VL 的训练网络中，依靠 VL 预训练任务优化整个网络，从而简化训练流程，缓解依赖人工标注数据的问题，同时使得视觉编码器能够在 VL 预训练任务的指导下在线更新，提供更好的视觉表征。</span><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">经验证，</span><span style=\"color: rgb(0, 0, 0);font-size: 15px;background-color: rgb(160, 219, 239);\">SOHO 模型不仅降低了对人工标注数据的需求，而且在下游多个视觉-语言任务（包括视觉问答、图片语言检索、自然语言图像推理等）的公平比较下，都取得了 SOTA 的成绩</span><span style=\"font-size: 15px;\">。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"135\" data-galleryid=\"\" data-ratio=\"0.5389507154213037\" data-s=\"300,640\" data-type=\"png\" data-w=\"1258\" style=\"width: 562px; height: 303px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icrLWR7E9dibfpLZZdibQyDbVUYiaSiambdBTlniavUrfC3ciaNhGh3MIWnd2A/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;line-height: 1.75em;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">图1：端到端的视觉语言预训练网络 SOHO</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">如图1所示，SOHO 由三部分组成：1）基于卷积网络的视觉编码器（可在线更新）；2）基于视觉字典（Visual Dictionary）的视觉嵌入层；3）由多层 Transformer 组成的 VL 融合网络。三个部分“各司其职”，卷积网络负责将一张图像表征为一组向量，然后利用视觉字典对图像中相近的特征向量进行表征，最后利用 Transformer 组成的网络将基于字典嵌入的视觉特征与文本特征融合到一起。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">对于视觉编码器，研究员们采用了 ResNet-101 作为基础网络结构对输入图像进行编码，与基于目标检测模型的图像编码器相比，这种方式的好处是：可以简化操作。为了将图像中相近的特征用统一的特征表征，同时为 MVM（Masked vision Modeling）提供类别标签，研究员们利用了视觉字典。整个字典在网络学习的过程中都采用了动量更新的方式进行学习。基于 Transform 的特征融合网络则采用了和 BERT 相同的网络结构。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">为了优化整个网络，研究员们利用 MVM、MLM(Masked Language Modeling) 以及 ITM(Image-Text Matching) 三个预训练任务进行了模型训练，并将得到的参数应用到了四个相关的 VL 下游任务上，均取得了较好的结果（如表格1-4所示）。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"341\" data-galleryid=\"\" data-ratio=\"0.19777777777777777\" data-s=\"300,640\" data-type=\"png\" data-w=\"1350\" style=\"width: 578px; height: 114px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ick2vwgtNia5F3oFXRmxOwMRwsIvvVUl5kGw1ibYEMo2tjp8R57zIOSrZQ/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表格1：SOHO 在 MSCOCO 数据集上与其他方法的 text retrieval（TR）和 image retrieval（IR）的性能比较</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-croporisrc=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic072CFn36L9BsBticsu5fbuDpfTQocIn7fRpqcddNrgnDAGF555xEXGA/0?wx_fmt=png\" data-cropx1=\"0\" data-cropx2=\"578\" data-cropy1=\"7.199288256227759\" data-cropy2=\"324.99644128113886\" data-galleryid=\"\" data-ratio=\"0.5484429065743944\" data-s=\"300,640\" data-type=\"jpeg\" style=\"width: 562px;height: 309px;\" data-w=\"\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icRR6QEJ7KHDToKljeTjCwVx4QiaQ5KgYqU7aN1vjbetjCcWRfIAVO0iaQ/640?wx_fmt=jpeg\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表格2：SOHO 在 VQA 2.0 数据集上的 VQA 性能表现</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-croporisrc=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic84FNZdFYDMB3YRUuNibcl7bgprYf8ok25zZkajfyFTp8bxuxvUiaGyxA/0?wx_fmt=png\" data-cropx1=\"0\" data-cropx2=\"560\" data-cropy1=\"0\" data-cropy2=\"275\" data-galleryid=\"\" data-ratio=\"0.49107142857142855\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"560\" style=\"width: 560px; height: 275px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icXgViaic3b0DJP9tMlj6RrEeuUgQ2E6GUjIf321Wseqb3BAKuAF4vZCCA/640?wx_fmt=jpeg\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表格3：SOHO 在 NLVR2 数据集上的 Visual Reasoning 性能表现 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.2832369942196532\" data-s=\"300,640\" data-type=\"png\" data-w=\"519\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icpaa1Kgibju0dmTW9ZhhZo8WoBRGtrSCuUzbc3UwDCCuaK5VfZrLUVxw/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">格4：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">SOHO 在 SNLI-VE 数据集上的 Visual Entailment 性能表现</span><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">最后，通过对视觉字典中部分 ID 对应的图片内容进行可视化（如图2所示），研究员们发现即使没有强监督的视觉类别标注，SOHO 也可以将具有相似语义的视觉内容聚类到同一个字典项中。相对于使用基于目标检测的视觉语言模型，SOHO 摆脱了图片框的回归需求，推理时间（inference time）也加快了10倍，在真实场景应用中更加实际和便捷。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"295\" data-galleryid=\"\" data-ratio=\"0.5283018867924528\" data-s=\"300,640\" data-type=\"png\" data-w=\"1060\" style=\"width: 562px; height: 297px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icDPiad7Z6icHaSzfmsMib0uSeQ31J17uxpO1G3copeQelibXb7QFbHj145Q/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图2：Visual Dictionary 部分 ID 对应图片内容的可视化 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;text-align: center;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">基于纹理 Transformer 模型的图像超分辩率技术</span></strong></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">从古老的胶片照相机到今天的数码时代，人类拍摄和保存了大量的图片信息，但这些图片不可避免地存在各种不同程度的瑕疵。将图片变得更清晰、更鲜活，一直是计算机视觉领域的重要话题。针对于图像超分辨率的问题，微软亚洲研究院的研究员们创新性地将 Transformer 结构应用在了图像生成领域，提出了一种</span><span style=\"color: rgb(0, 0, 0);font-size: 15px;background-color: rgb(160, 219, 239);\">基于纹理 Transformer 模型的图像超分辩率方法 TTSR</span><span style=\"font-size: 15px;\">。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">该模型可以有效地搜索与迁移高清的纹理信息，最大程度地利用参考图像的信息，并可以正确地将高清纹理迁移到生成的超分辨率结果当中，从而解决了纹理模糊和纹理失真的问题。 该工作“Learning Texture Transformer Network for Image Super-Resolution”发表在 CVPR 2020。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">论文链接：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://arxiv.org/pdf/2006.04139.pdf</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">GitHub地址：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://github.com/researchmm/TTSR</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.2163375224416517\" data-s=\"300,640\" data-type=\"png\" data-w=\"1114\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icVaF2xC9WoU7A1hVDhicpvIQPMgj8wOcsZamWFibm0JSRHJs9KMvibo6GQ/640?wx_fmt=png\"></section><p><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">与先前盲猜图片细节的方法不同，研究员们通过引入一张高分辨率参考图像来指引整个超分辨率过程。高分辨率参考图像的引入，将图像超分辨率问题由较为困难的纹理恢复/生成转化为了相对简单的纹理搜索与迁移，使得超分辨率结果在指标以及视觉效果上有了显著的提升。如图3所示，TTSR 模型包括：可学习的纹理提取器模块（Learnable Texture Extractor）、相关性嵌入模块（Relevance Embedding）、硬注意力模块（Hard Attention）、软注意力模块（Soft Attention）。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"1.0121703853955375\" data-s=\"300,640\" data-type=\"png\" data-w=\"493\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic01U8AQTH6wKXf4E6eFBXaiakjpLqSx0O4CGnqnhuvcX27ovUH7T8ib9Q/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图3：纹理 Transformer 模型</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">传统 Transformer 通过堆叠使得模型具有更强的表达能力，然而在图像生成问题中，简单的堆叠很难产生很好的效果。为了进一步提升模型对参考图像信息的提取和利用，研究员们提出了</span><span style=\"color: rgb(0, 0, 0);font-size: 15px;background-color: rgb(160, 219, 239);\">跨层级的特征融合机制</span><span style=\"font-size: 15px;\">——将所提出的纹理 Transformer 应用于 x1、x2、x4 三个不同的层级，并将不同层级间的特征通过上采样或带步长的卷积进行交叉融合。因此，不同粒度的参考图像信息会渗透到不同的层级，使得网络的特征表达能力增强，提高生成图像的质量。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.44049459041731065\" data-s=\"300,640\" data-type=\"png\" data-w=\"647\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icAQtk9D2s2zsp7a7tkNyuYCXf6Ye8RtKbIqicD4ILcaXWJY6icTznibZvw/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图4：多个纹理 Transformer 跨层级堆叠模型</span><span style=\"font-size: 12px;\"> </span><br></p><p style=\"margin-top:0;margin-bottom:0;text-align:center;vertical-align:baseline;\"><span style=\"font-size:12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究员们在 CUFED5、Sun80、Urban100、Manga109 数据集上针对 TTSR 方法进行了量化比较，具体如表格5所示。图5展示了 TTSR 与现有的方法在不同数据集上的视觉比较结果，可以发现 TTSR 显著领先于其他方法的结果。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.5446009389671361\" data-s=\"300,640\" data-type=\"png\" data-w=\"852\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic7SCASbEGicKdwvkkztKKPQafnQ0lAf6Ng9nskM84nQ2vwh5I1EMyZ5Q/640?wx_fmt=png\"></section><p style=\"margin-top: 0px;margin-bottom: 0px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表格5：TTSR 与现有方法在不同数据集上的量化比较结果</span></p><p style=\"margin-top: 0px;margin-bottom: 0px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"1.1151358344113842\" data-s=\"300,640\" data-type=\"png\" data-w=\"773\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icaibGcZRNHAViaCA7ufeeQzGKWVGjckVqHyh0uxEmXucPxibicLFBs4t8uA/640?wx_fmt=png\"></section><p style=\"margin-top: 0px;margin-bottom: 0px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图5：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">TTSR 与现有方法在不同数据集上的视觉比较结果</span><br></p><p style=\"margin-top: 0px;margin-bottom: 0px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">了解更多 TTSR 细节，可点击查看此前的文章：<a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649452990&amp;idx=1&amp;sn=187f36f3e590f54f9f2f3a4f87d5dba4&amp;chksm=82c0983ab5b7112c1ee524343d83e611e93ffbab09fad97cd348b0e5c1b74c099a2aae5897d1&amp;scene=21#wechat_redirect\" textvalue=\"CVPR 2020丨图像超清化+老照片修复技术，拯救你所有的模糊、破损照片\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"2\">CVPR 2020丨图像超清化+老照片修复技术，拯救你所有的模糊、破损照片</a>。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;line-height: 1.75em;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"40\" data-cropsely1=\"0\" data-cropsely2=\"40\" data-ratio=\"1.118421052631579\" data-s=\"300,640\" data-type=\"png\" data-w=\"76\" style=\"width: 31px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png\"><br></section><section data-id=\"93765\" data-tools=\"135编辑器\" style=\"white-space: normal;\"><section data-width=\"100%\" style=\"width: 100%;text-align: center;\"><section style=\"display: inline-block;\"><section style=\"padding-right: 0.8em;padding-left: 0.8em;letter-spacing: 2px;font-size: 18px;font-weight: bold;\"><strong><span style=\"color: rgb(0, 0, 0);\">Transformer“跨界”视频任务</span></strong></section><section data-width=\"100%\" style=\"margin-top: -10px;background: rgb(160, 219, 239);border-radius: 20px;height: 10px;overflow: hidden;\"><br></section></section></section></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">相对于图像的空间信息，视频还增加了时序维度的信息。</span><span style=\"font-size: 15px;\">Transformer 可以很好地在空间-时序维度上进行建模，进而更好地学习图像与特征中的长距离依赖关系，有利于视频相关任务的增强与提高。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">视频修复：Transformer 初尝试 </span></strong></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">视频修复（video inpainting）是一个旨在通过视频中已知内容来推断并填补缺失内容的经典任务。它在老旧视频恢复、去除水印等视频编辑中有着广泛应用。尽管视频修复技术有很大的应用价值，然而在复杂变化的多个视频帧中找到相关信息，并生成在图像空间和时序上看起来和谐、一致的内容，仍然面临着巨大的挑战。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">为了解决这样的问题，微软亚洲研究院的研究员们利用并重新设计了Transformer结构，提出了 Spatial-Temporal Transformer Network （STTN）。 相关论文“Learning Joint Spatial-Temporal Transformations for Video Inpainting”发表在了 ECCV 2020。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">论文链接：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://arxiv.org/abs/2007.10247</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">GitHub地址：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://github.com/researchmm/STTN</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.3627450980392157\" data-s=\"300,640\" data-type=\"png\" data-w=\"714\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic3jjUg28Gib0GpnYSzUh7ssSLJUuruM5kty1V4j2GZWIqzoT90gyQcFw/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">STTN 模型的输入是带有缺失内容的视频帧以及每一帧的掩码，输出则是对应的修复好的视频帧。如图6所示，STTN 模型的输入是带有缺失内容的视频帧以及每一帧的掩码，输出则是对应的修复好的视频帧。如图6所示，STTN 模型采用了 CNN-Transformer 的混合结构。其中，frame-level encoder 以及 frame-level decoder 采用了 CNN，分别将每个视频帧从像素编码成特征以及将特征解码成视频帧。Transformer 则作为模型的主干，它将输入的视频帧特征切成块，并对块的序列进行建模，再通过多层时空 Transformer 层挖掘输入帧中的已知信息来推断缺失内容。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"289\" data-ratio=\"0.4742647058823529\" data-s=\"300,640\" data-type=\"png\" data-w=\"544\" style=\"height: 274px; width: 578px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icdJQz2DonNHUc4KhSiaFibdX8Dia0E3YTOibRwXhJ9QtTXCv3HDXEJkp6ng/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图6: Spatial-Temporal Transformer Network (STTN) 模型结构示意图</span></p><p style=\"margin-top: 0px;margin-bottom: 0px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">时空 Transformer 层继承了经典 Transformer 层强大的注意力机制，能聚焦于与缺失内容相关的信息上，通过多层的堆叠不断更新优化预测的内容。</span><span style=\"font-size: 15px;\">同时，不同于经典 Transformer 层中每个头部的是模型采用了固定的块大小，STTN 为了捕捉到尽可能多的上下文信息，在不同的头部上采用了不同大小的块切取方式。</span><span style=\"font-size: 15px;\">因此，当缺失区域的特征不够丰富时，基于大的块的注意力机制可以有效利用较多的已知信息；</span><span style=\"font-size: 15px;\">当缺失区域的特征丰富之后，基于小的块的注意力机制有助于模型聚焦更细微的变化。</span><span style=\"font-size: 15px;\">如图7所示，通过可视化 STTN 最后一层 Transformer 的注意力图，可以发现 STTN 为了填补目标帧中狗身上的缺失区域，能够 “精准追踪” 到其他帧里的信息，来修复缺失区域。</span><span style=\"font-size: 15px;\"> </span><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><img class=\"rich_pages\" data-cropselx1=\"0\" data-cropselx2=\"562\" data-cropsely1=\"0\" data-cropsely2=\"244\" data-galleryid=\"\" data-ratio=\"0.43419434194341944\" data-s=\"300,640\" data-type=\"png\" data-w=\"813\" style=\"width: 562px; height: 244px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icjhWL5ol680owSBricibVStiabL81QFOEjhqwThZ35BoC6PwyeQ4uT2aUw/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图7：Attention map 的可视化（attention 的部分用黄色高亮）。尽管视频里狗由于奔跑，在不同的帧里形态和位置差异较大，但为了填补目标帧（target frame）中狗身上缺失的部分，STTN 可以 “精准追踪” 到相关的帧里这只跑动的狗。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">除了 STTN 模型，该论文还提出了用动态和静态两种不同的视频掩码来模拟实际应用。</span><span style=\"font-size: 15px;\">动态掩码指视频每一帧的掩码是连续变化的，用来模拟移除运动物体的应用；</span><span style=\"font-size: 15px;\">而静态掩码不会随着视频变化，用来模拟水印移除。</span><span style=\"font-size: 15px;\">论文通过在 DAVIS 和 Youtube-VOS 数据集上定性和定量的分析，验证了 STTN 在视频修复任务上的优越性。</span><span style=\"font-size: 15px;\">如视频1所示，STTN 能够生成视觉上更真实的修复结果。</span><span style=\"font-size: 15px;\">同时得益于 STTN 强大的并行建模能力，它也加快了运行速度（24.10 fps VS. 3.84 fps）。</span><span style=\"font-size: 15px;\"> </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"text-align: center;\"><br></span></p><section style=\"margin-left: 8px;margin-right: 8px;\"><iframe class=\"video_iframe rich_pages\" data-vidtype=\"2\" data-mpvid=\"wxv_1877311761268211714\" data-cover=\"http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2FHkPvwCuFwNMicHREQliaPyRojFQQzmXj0icE18kNia4NV7iaqchHNEstrYWF6ic2cRkNfze9ib8q06hC4akwGjZJqbuQA%2F0%3Fwx_fmt%3Djpeg\" allowfullscreen=\"\" frameborder=\"0\" data-ratio=\"1.8\" data-w=\"864\" data-src=\"https://mp.weixin.qq.com/mp/readtemplate?t=pages/video_player_tmpl&amp;action=mpvideo&amp;auto=0&amp;vid=wxv_1877311761268211714\"></iframe></section><section style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">视频1：左上为输入的视频，其中黄色表示需要掩盖并重新填补的区域。右下为STTN的结果。 </span><br></section><p style=\"margin-top:0;margin-bottom:0;text-align:center;vertical-align:baseline;\"><span style=\"font-size:12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><strong><span style=\"font-size: 18px;\">目标跟踪新范式：基于时空 Transformer</span></strong></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">视频目标跟踪（Visual Object Tracking）是计算机视觉领域中的一项基础且颇具挑战性的任务。在过去几年中，基于卷积神经网络，目标跟踪迎来了快速的发展。然而卷积神经网络并不擅长建模图像与特征中的长距离依赖关系，同时现有的目标跟踪器或是仅利用了空间信息，亦或是并未考虑到时间与空间之间的联系，造成跟踪器在复杂场景下性能的下降。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">如何解决以上问题？微软亚洲研究院的研究员们</span><span style=\"color: rgb(0, 0, 0);font-size: 15px;background-color: rgb(160, 219, 239);\">提出了一种名为 STARK 的基于时空 Transformer 的目标跟踪器新范式</span><span style=\"font-size: 15px;\">，将目标跟踪建模为一种端到端的边界框预测问题，从而彻底摆脱以往跟踪器使用的超参敏感的后处理，该方法在多个短时与长时跟踪数据集上都取得了当前最优的性能。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">相关论文“Learning Spatio-Temporal Transformer for Visual Tracking”</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">链接：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://arxiv.org/abs/2103.17154</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">GitHub地址：</span><span style=\"color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 0.544px;background-color: rgb(254, 254, 254);\">https://github.com/researchmm/stark</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.1885245901639344\" data-s=\"300,640\" data-type=\"png\" data-w=\"976\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icz6yUq9sauM5UescFm66ic2EyR2MDL6J8PGBceebLXp3mDicWd6F2ibSZQ/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">STARK 包括 Spatial-Only 和 Spatio-Temporal 两个版本，其中 Spatial-Only 版本仅使用空间信息，Spatio-Temporal 版本则同时利用了时间和空间信息。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Spatial-Only 版本的框架图如图8所示。首先，第一帧的模板和当前帧的搜索区域会一同送入骨干网络提取视觉特征，然后特征图沿空间维度展开并拼接，进而得到一个特征序列。之后，Transformer 编码器会建模序列元素之间的全局关联，并利用学习到的全局信息来强化原始特征，使得新的特征序列对目标具有更强的判别力。受 DETR 的启发，研究员们使用了一个解码器以及一个目标查询（Target Query）来对编码器的输出进行译码。目标查询与前面提到的编码器输出的特征序列进行交互，从而学习到和目标相关的重要信息。最后，编码器输出的特征序列以及译码器输出的新的目标查询特征再一同送入边界框预测模块，得到最终的边界框坐标。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.8704453441295547\" data-s=\"300,640\" data-type=\"png\" data-w=\"494\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icje6LJN5FoZmiam0vdVH8AFwfATcBN2XythicDzrsA3nnJVjkbKd8YXFA/640?wx_fmt=png\"><span style=\"font-size: 15px;text-align: justify;\"></span></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图8：Spatial-Only 版本的框架图 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">边界框预测模块的结构如图9所示，首先从编码器的输出序列中取出搜索区域相关的特征，用该特征序列与译码器输出的目标查询特征计算一次注意力机制，强化目标所在区域的特征，削弱非目标区域的特征。然后，经注意力机制强化后的搜索区域特征序列的空间结构被还原，并通过简单的全卷积网络预测目标左上角和右下角一对角点(corners)的热力图，最终的角点坐标则通过计算角点坐标的数学期望得到。不同于之前的Siamese和DCF方法，该框架将目标跟踪建模为一个直接的边界框预测问题，每一帧上都可直接预测一个边界框坐标，无需使用任何超参敏感的后处理。</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.5434782608695652\" data-s=\"300,640\" data-type=\"png\" data-w=\"552\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichGwaaRDVLEO58TO5bWx1Y6NCvBQYQNghfIP8M8MmoTwhQevqOAsepA/640?wx_fmt=png\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"></span></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图9：边界框预测模块的结构</span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">Spatio-Temporal 版本的框架图如图10所示，粉色区域展示了为了利用时序信息而新加入的结构。新框架额外加入了一个 “动态模板” 作为新输入。动态模板是根据中间帧跟踪结果裁剪得到的，并随着跟踪的进行动态更新，为整个框架补充了之前缺少的时序信息。利用第一帧模板、当前帧搜索区域、动态模板同时作为 Transformer 编码器的输入，编码器能够从全局视角提取时空信息，学习到鲁棒的时空联合表示。除动态模板之外，研究员们还引入了由多层感知机实现的更新控制器来更新动态模板，它与边界框预测头并联，以预测当前帧可靠程度的置信度分数。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.7472256473489519\" data-s=\"300,640\" data-type=\"png\" data-w=\"811\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icribBjADib4Az09Yu9vnQjibaic0aRAZqb5nSaX1wFY0jWbiagicZYgSwVpcw/640?wx_fmt=png\"><span style=\"font-size: 15px;text-align: justify;\"></span></section><p style=\"margin-top:0;margin-bottom:0;text-indent:48px;text-align:center;vertical-align:baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图10：Spatio-Temporal 版本框架图</span><span style=\"font-size:12px;\"> </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">STARK 在多个短时跟踪与长时跟踪数据集上都取得了目前最先进的性能，并且运行速度可达 30FPS 到 40FPS。其中，在 LaSOT, GOT-10K, TrackingNet 三个大规模目标跟踪数据集上的结果如下所示。 </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.9821029082774049\" data-s=\"300,640\" data-type=\"png\" data-w=\"447\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icZPqqGNhF7iaR9UYsiba7odhnse9lOIGJnxQQMo1NQOQrAnMCVOMnHb0A/640?wx_fmt=png\"></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></p><p style=\"margin-top: 0px;margin-bottom: 0px;text-indent: 48px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">图11：LaSOT 数据集上的结果比较</span></p><p style=\"margin-top: 0px;margin-bottom: 0px;text-indent: 48px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.10271703114645461\" data-s=\"300,640\" data-type=\"png\" data-w=\"1509\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichBM2iakmqLD0sWLaNeYBq7SFcOWaPqep8Gice9VkCfc3AowLrIaYqjMQ/640?wx_fmt=png\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;text-indent: 48px;\"></span></section><p style=\"margin-top: 0px;margin-bottom: 0px;text-indent: 48px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表格6：GOT-10K 数据集上的结果比较</span></p><p style=\"margin-top: 0px;margin-bottom: 0px;text-indent: 48px;text-align: center;vertical-align: baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><br></span></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.07412309728656519\" data-s=\"300,640\" data-type=\"png\" data-w=\"1511\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic1kMwx86KSoiaQABs0ooJa4oMYhOicJBR4c5EMdJrI1hF6vFmwIkxjtXw/640?wx_fmt=png\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;text-indent: 48px;\"></span></section><p style=\"margin-top:0;margin-bottom:0;text-align:center;vertical-align:baseline;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\">表格7：TrackingNet 数据集上的结果比较</span><span style=\"font-size:12px;\"> </span></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"font-size: 15px;\">上述四个工作将 Transformer 结构成功地应用于图像内容增强和视频内容分析， 充分地展现了 Transformer 的优势和潜力。目前研究员们已经看到，无论是在图像分类、物体检测与分割等基础视觉任务上，还是在 3D 点云分析、图像视频内容生成等新兴课题中，Transformer 都大放异彩。未来，视觉 Transformer 结构的设计和自动化搜索将会是一个非常具有前景的研究课题。相信 Transformer 结构在计算机视觉领域会继续展现其强大的模型潜力。</span></p><section style=\"margin-top: 10px;margin-bottom: 10px;white-space: normal;text-align: center;max-width: 100%;box-sizing: border-box;overflow-wrap: break-word !important;\"><section style=\"margin-bottom: -3px;padding-top: 3px;width: 677px;border-bottom: 1px dotted rgb(160, 160, 160);max-width: 100%;box-sizing: border-box;overflow-wrap: break-word !important;\"><br></section><section style=\"height: 5px;line-height: 5px;max-width: 100%;box-sizing: border-box;overflow-wrap: break-word !important;\"><section style=\"vertical-align: top;display: inline-block;max-width: 100%;box-sizing: border-box;overflow-wrap: break-word !important;\"><span style=\"margin-left: 5px;max-width: 100%;box-sizing: border-box;width: 5px;height: 5px;float: left;border-radius: 50%;border-width: 1px;border-style: solid;border-color: rgb(160, 160, 160);overflow-wrap: break-word !important;\"><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></span></section><section style=\"clear: both;max-width: 100%;box-sizing: border-box;overflow-wrap: break-word !important;\"><br></section></section></section><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;min-height: 1em;max-width: 100%;box-sizing: border-box;background-color: rgb(255, 255, 255);overflow-wrap: break-word !important;\"><br></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;min-height: 1em;max-width: 100%;box-sizing: border-box;overflow-wrap: break-word;background-color: rgb(255, 255, 255);\"><strong style=\"box-sizing: border-box;color: rgb(46, 87, 151);font-size: 15px;letter-spacing: 1px;max-width: 100%;overflow-wrap: break-word;\">你也许还想看</strong><strong style=\"box-sizing: border-box;color: rgb(46, 87, 151);font-size: 15px;letter-spacing: 1px;max-width: 100%;overflow-wrap: break-word;\">：</strong></p><p style=\"margin-right: 8px;margin-left: 8px;white-space: normal;text-align: center;\"><br></p><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649464370&amp;idx=1&amp;sn=e7e2caefcfd8806eab9aaec2bc937f57&amp;chksm=82c077b6b5b7fea038891214e44d43848c683f602bf881ececcc82f5b7a260451011a569fd8e&amp;scene=21#wechat_redirect\" textvalue=\"你已选中了添加链接的内容\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"1\"><span class=\"js_jump_icon h5_image_link\" data-positionback=\"static\" style=\"inset: auto;margin: 0px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.3322222222222222\" data-s=\"300,640\" data-type=\"png\" data-w=\"900\" style=\"margin: 0px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icf6Xh8ibtQ6rz4lSVZHic06xfaITH0I08l4hDO9n8hkLlIPqK7icLn2mBQ/640?wx_fmt=png\"></span></a></section><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649452990&amp;idx=1&amp;sn=187f36f3e590f54f9f2f3a4f87d5dba4&amp;chksm=82c0983ab5b7112c1ee524343d83e611e93ffbab09fad97cd348b0e5c1b74c099a2aae5897d1&amp;scene=21#wechat_redirect\" textvalue=\"你已选中了添加链接的内容\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"1\"><span class=\"js_jump_icon h5_image_link\" data-positionback=\"static\" style=\"inset: auto;margin: 0px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.3322222222222222\" data-s=\"300,640\" data-type=\"png\" data-w=\"900\" style=\"margin: 0px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icHgdGWLs84U5zMPpmeL4kjKVW9S0dFCz4jDpGdic3lrzIAMHibmUD7jAw/640?wx_fmt=png\"></span></a></section><section style=\"text-align: center;margin-left: 8px;margin-right: 8px;\"><a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649450528&amp;idx=1&amp;sn=f770a8b4fc2927c0be206310d700a612&amp;chksm=82c081a4b5b708b2a8cd0134d095be5196422ba6adf677bc077cc0a0e8ec6047ba09979e27a1&amp;scene=21#wechat_redirect\" textvalue=\"你已选中了添加链接的内容\" data-itemshowtype=\"0\" tab=\"innerlink\" data-linktype=\"1\"><span class=\"js_jump_icon h5_image_link\" data-positionback=\"static\" style=\"inset: auto;margin: 0px;\"><img class=\"rich_pages\" data-galleryid=\"\" data-ratio=\"0.3322222222222222\" data-s=\"300,640\" data-type=\"png\" data-w=\"900\" style=\"margin: 0px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icsfNHXstRHeRUxd8QRe4I6yhTiaAZbnb0lsgctrHX5vcU924MwqCRPRw/640?wx_fmt=png\"></span></a></section><section style=\"margin: 20px 8px;white-space: normal;display: flex;justify-content: center;align-items: center;\"><section data-width=\"90%\" style=\"background: rgb(137, 137, 137);width: 562px;height: 2px;\"><br></section><section data-width=\"90%\" style=\"background: rgb(137, 137, 137);width: 562px;height: 2px;\"><br></section></section><p style=\"margin-left: 8px;white-space: normal;line-height: 1.75em;\"><span style=\"color: rgb(0, 0, 0);font-size: 15px;letter-spacing: 1px;\"><img class=\"__bg_gif\" data-copyright=\"0\" data-ratio=\"0.5\" data-type=\"gif\" data-w=\"750\" style=\"box-sizing: border-box; color: rgb(51, 51, 51); max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNLsud1BZ6dFjDQiaG01NGT1z7llzgFU2U91l2cXywRJmR0FZ8SrqEpzuibSgI078eLPTHIlPElysjQ/640?wx_fmt=gif\"></span></p><p style=\"white-space: normal;\"><img class=\"rich_pages __bg_gif\" data-ratio=\"0.41379310344827586\" data-type=\"gif\" data-w=\"638\" style=\"letter-spacing: 0.544px; text-align: center; background-color: rgb(255, 255, 255); box-sizing: border-box !important; width: 638px !important; visibility: visible !important; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNMMrKApwKS4eEP6EC9NIpYiaLvqtet8icWZqrHsFqWkWrN99RVkkGGEOOCCj9XPXW5H1ZhwOQulrxZg/640?wx_fmt=gif\"></p><p><br></p>\n                </div>\n\n    \n    <br>\n\n    \n        <a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649464566&amp;idx=1&amp;sn=04c63181831b101ea1613529f3c17e1c&amp;chksm=82c07772b5b7fe64725559d0fc00dbb00830ed2414f53c724324f713fff36d4f7ee379547aa2&amp;scene=0#rd\" style=\"color: blue\" class=\"media_tool_meta meta_primary\">原文</a>\n        <br>\n    \n\n    \n\n    <img alt=\"\" width=\"1px\" height=\"1px\" class=\"\" style=\"width:1px;height:1px;display:none\" src=\"http://www.jintiankansha.me/rss_static/5416/fijETjubt4\"></div></div>","descriptionType":"html","publishedDate":"Thu, 20 May 2021 10:49:00 +0000","feedId":1700,"bgimg":"http://content.sov5.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icH3NqZdxxqViaowrYGdrJRw9picNx3fpQZpOvNm8VMWdjDuRNAiaxiaCEiaQ?imageView2/1/w/600","linkMd5":"20805817771fca5008d48917f5207389","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn36@2020_3/2021/05/30/04-52-35-968_f66123cea4d4ea35.webp","destWidth":600,"destHeight":382,"sourceBytes":29131,"destBytes":23902,"author":"","articleImgCdnMap":{"http://content.sov5.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icH3NqZdxxqViaowrYGdrJRw9picNx3fpQZpOvNm8VMWdjDuRNAiaxiaCEiaQ?imageView2/1/w/600":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn36@2020_3/2021/05/30/04-52-35-968_f66123cea4d4ea35.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn43@2020_4/2021/05/30/04-52-39-484_d8f757d4d88b52d6.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn48@2020_6/2021/05/30/04-52-36-876_f61698ba2e51c308.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icvxrjB2owskGZ4Rt1wtubOV6akm8SiaRJeCLIwOlBlQHU7xVfPTcPXpA/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn65@2020_5/2021/05/30/04-52-46-925_9fd66a506837fabe.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icrLWR7E9dibfpLZZdibQyDbVUYiaSiambdBTlniavUrfC3ciaNhGh3MIWnd2A/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ick2vwgtNia5F3oFXRmxOwMRwsIvvVUl5kGw1ibYEMo2tjp8R57zIOSrZQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn72@2020_2/2021/05/30/04-52-54-802_f124b94af7fddfc5.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icRR6QEJ7KHDToKljeTjCwVx4QiaQ5KgYqU7aN1vjbetjCcWRfIAVO0iaQ/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn7@2020_3/2021/05/30/04-52-44-681_841b98e9f0a031b4.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icXgViaic3b0DJP9tMlj6RrEeuUgQ2E6GUjIf321Wseqb3BAKuAF4vZCCA/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn67@2020_6/2021/05/30/04-52-44-709_71504039068f2843.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icpaa1Kgibju0dmTW9ZhhZo8WoBRGtrSCuUzbc3UwDCCuaK5VfZrLUVxw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn62@2020_6/2021/05/30/04-52-40-484_7943c164535886f5.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icDPiad7Z6icHaSzfmsMib0uSeQ31J17uxpO1G3copeQelibXb7QFbHj145Q/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icVaF2xC9WoU7A1hVDhicpvIQPMgj8wOcsZamWFibm0JSRHJs9KMvibo6GQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn36@2020_3/2021/05/30/04-53-52-631_ecf63520176f8c10.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic01U8AQTH6wKXf4E6eFBXaiakjpLqSx0O4CGnqnhuvcX27ovUH7T8ib9Q/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn56@2020_1/2021/05/30/04-53-00-179_1c9c724c1d0f279f.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icAQtk9D2s2zsp7a7tkNyuYCXf6Ye8RtKbIqicD4ILcaXWJY6icTznibZvw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn81@2020_5/2021/05/30/04-52-46-827_2303bacb83b4f2a0.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic7SCASbEGicKdwvkkztKKPQafnQ0lAf6Ng9nskM84nQ2vwh5I1EMyZ5Q/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn31@2020_6/2021/05/30/04-53-31-370_cb073582eeaf0beb.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icaibGcZRNHAViaCA7ufeeQzGKWVGjckVqHyh0uxEmXucPxibicLFBs4t8uA/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic3jjUg28Gib0GpnYSzUh7ssSLJUuruM5kty1V4j2GZWIqzoT90gyQcFw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn99@2020_2/2021/05/30/04-52-40-210_411473453c8c757a.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icdJQz2DonNHUc4KhSiaFibdX8Dia0E3YTOibRwXhJ9QtTXCv3HDXEJkp6ng/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn93@2020_4/2021/05/30/04-53-45-164_5c08738f0e78754f.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icjhWL5ol680owSBricibVStiabL81QFOEjhqwThZ35BoC6PwyeQ4uT2aUw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn16@2020_5/2021/05/30/04-54-33-517_6ed5df64220263c1.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icz6yUq9sauM5UescFm66ic2EyR2MDL6J8PGBceebLXp3mDicWd6F2ibSZQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn87@2020_2/2021/05/30/04-52-43-010_74af3907cf217794.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icje6LJN5FoZmiam0vdVH8AFwfATcBN2XythicDzrsA3nnJVjkbKd8YXFA/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichGwaaRDVLEO58TO5bWx1Y6NCvBQYQNghfIP8M8MmoTwhQevqOAsepA/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icribBjADib4Az09Yu9vnQjibaic0aRAZqb5nSaX1wFY0jWbiagicZYgSwVpcw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn40@2020_3/2021/05/30/04-53-05-627_c8f6054c988291c7.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icZPqqGNhF7iaR9UYsiba7odhnse9lOIGJnxQQMo1NQOQrAnMCVOMnHb0A/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn52@2020_6/2021/05/30/04-52-44-231_7f742741dada07cc.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichBM2iakmqLD0sWLaNeYBq7SFcOWaPqep8Gice9VkCfc3AowLrIaYqjMQ/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic1kMwx86KSoiaQABs0ooJa4oMYhOicJBR4c5EMdJrI1hF6vFmwIkxjtXw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn12@2020_6/2021/05/30/04-52-42-575_5807ead3d0fe273b.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icf6Xh8ibtQ6rz4lSVZHic06xfaITH0I08l4hDO9n8hkLlIPqK7icLn2mBQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn69@2020_5/2021/05/30/04-53-00-554_e73f7df4782d9828.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icHgdGWLs84U5zMPpmeL4kjKVW9S0dFCz4jDpGdic3lrzIAMHibmUD7jAw/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icsfNHXstRHeRUxd8QRe4I6yhTiaAZbnb0lsgctrHX5vcU924MwqCRPRw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn20@2020_5/2021/05/30/04-52-53-574_c600f89a87db332f.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNLsud1BZ6dFjDQiaG01NGT1z7llzgFU2U91l2cXywRJmR0FZ8SrqEpzuibSgI078eLPTHIlPElysjQ/640?wx_fmt=gif":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNMMrKApwKS4eEP6EC9NIpYiaLvqtet8icWZqrHsFqWkWrN99RVkkGGEOOCCj9XPXW5H1ZhwOQulrxZg/640?wx_fmt=gif":null,"http://www.jintiankansha.me/rss_static/5416/fijETjubt4":null},"publishedOrCreatedDate":1622350268612}],"record":{"createdTime":"2021-05-30 12:51:08","updatedTime":"2021-05-30 12:51:08","feedId":1700,"fetchDate":"Sun, 30 May 2021 04:51:08 +0000","fetchMs":350,"handleMs":9,"totalMs":212498,"newArticles":0,"totalArticles":5,"status":1,"type":0,"ip":"bc1eecef1292254c09de6c1e66b750f3","hostName":"europe63*","requestId":"5b541214f6f64be5b465d13b4626d1bd_1700","contentType":"application/rss+xml","totalBytes":644596,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":31,"articlesImgsGithubTotal":21,"successGithubMap":{"myreaderx8":1,"myreaderx25":1,"myreaderx15":1,"myreaderx27":1,"myreaderx21":1,"myreaderx32":1,"myreaderx4":1,"myreaderx11":2,"myreaderx22":1,"myreaderx3":1,"myreaderx33":1,"myreaderx2":1,"myreaderx12":1,"myreaderx1":1,"myreaderx24":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx29":1,"myreaderx":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1,"myreaderx31":1}},"feed":{"createdTime":"2020-08-24 21:31:32","updatedTime":"2020-09-01 09:51:25","id":1700,"name":"微软研究院AI头条","url":"http://feedmaker.kindle4rss.com/feeds/MSRAsia.weixin.xml","subscriber":null,"website":null,"icon":"http://www.sogou.com/images/logo/new/favicon.ico?v=4","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn60@2020_1/2020/09/01/01-51-26-423_d24121c9beed1de6.ico","description":"专注科研18年，盛产黑科技","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-05-30 12:54:40","updatedTime":"2021-05-30 12:54:40","id":null,"feedId":1700,"linkMd5":"20805817771fca5008d48917f5207389"}],"tmpCommonImgCdnBytes":23902,"tmpBodyImgCdnBytes":620694,"tmpBgImgCdnBytes":0,"extra4":{"start":1622350267929,"total":0,"statList":[{"spend":674,"msg":"获取xml内容"},{"spend":9,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":124078,"msg":"正文链接上传到cdn"}]},"extra5":31,"extra6":25,"extra7ImgCdnFailResultVector":[null,{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5416/fijETjubt4","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":680,"convertSpendMs":0,"createdTime":"2021-05-30 12:52:36","host":"us-027*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5416/fijETjubt4","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1700,"totalSpendMs":670,"convertSpendMs":0,"createdTime":"2021-05-30 12:52:37","host":"europe-24*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichGwaaRDVLEO58TO5bWx1Y6NCvBQYQNghfIP8M8MmoTwhQevqOAsepA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":552,"destHeight":300,"sourceBytes":22563,"destBytes":17414,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":9743,"convertSpendMs":9,"createdTime":"2021-05-30 12:52:36","host":"us-008*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn23/contents/2021/05/30/04-52-46-174_35aff9a3fc9c2969.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sun, 30 May 2021 04:52:46 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D482:44B2:B80A3A:1AEF9AE:60B31A1E"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1622352171"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn23/contents/2021/05/30/04-52-46-174_35aff9a3fc9c2969.webp","historyStatusCode":[],"spendMs":35},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"22 KB","destSize":"17 KB","compressRate":"77.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichGwaaRDVLEO58TO5bWx1Y6NCvBQYQNghfIP8M8MmoTwhQevqOAsepA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":552,"destHeight":300,"sourceBytes":22563,"destBytes":17414,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":1025,"convertSpendMs":10,"createdTime":"2021-05-30 12:52:46","host":"europe62*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn23/contents/2021/05/30/04-52-47-064_35aff9a3fc9c2969.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sun, 30 May 2021 04:52:47 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["C6EE:1244:24B3597:25DA1B0:60B31A1F"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1622352171"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn23/contents/2021/05/30/04-52-47-064_35aff9a3fc9c2969.webp","historyStatusCode":[],"spendMs":208},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"22 KB","destSize":"17 KB","compressRate":"77.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichBM2iakmqLD0sWLaNeYBq7SFcOWaPqep8Gice9VkCfc3AowLrIaYqjMQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":111,"sourceBytes":67447,"destBytes":21432,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":12511,"convertSpendMs":8,"createdTime":"2021-05-30 12:52:36","host":"europe-24*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn79/contents/2021/05/30/04-52-48-572_14a407453c2f11f7.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sun, 30 May 2021 04:52:48 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["9DE4:4D5E:4728D2A:48959E2:60B31A20"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1622352163"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn79/contents/2021/05/30/04-52-48-572_14a407453c2f11f7.webp","historyStatusCode":[],"spendMs":304},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"65.9 KB","destSize":"20.9 KB","compressRate":"31.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icje6LJN5FoZmiam0vdVH8AFwfATcBN2XythicDzrsA3nnJVjkbKd8YXFA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":494,"destHeight":430,"sourceBytes":67827,"destBytes":25440,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":12449,"convertSpendMs":12,"createdTime":"2021-05-30 12:52:36","host":"us-024*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx31/cdn75/contents/2021/05/30/04-52-48-886_491e97655ece3a67.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69855631.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sun, 30 May 2021 04:52:48 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["E262:245A:AA4785:13A8490:60B31A20"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1622352147"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx31/cdn75/contents/2021/05/30/04-52-48-886_491e97655ece3a67.webp","historyStatusCode":[],"spendMs":27},"base64UserPassword":null,"token":"da243******************************d9e47"},"githubUser":"myreaderx31","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"66.2 KB","destSize":"24.8 KB","compressRate":"37.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ichBM2iakmqLD0sWLaNeYBq7SFcOWaPqep8Gice9VkCfc3AowLrIaYqjMQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":111,"sourceBytes":67447,"destBytes":21432,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":14566,"convertSpendMs":8,"createdTime":"2021-05-30 12:52:48","host":"europe-58*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn79/contents/2021/05/30/04-53-03-074_14a407453c2f11f7.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sun, 30 May 2021 04:53:03 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["9430:1245:45AE213:470C627:60B31A2F"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1622352163"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn79/contents/2021/05/30/04-53-03-074_14a407453c2f11f7.webp","historyStatusCode":[],"spendMs":373},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"65.9 KB","destSize":"20.9 KB","compressRate":"31.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icje6LJN5FoZmiam0vdVH8AFwfATcBN2XythicDzrsA3nnJVjkbKd8YXFA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":494,"destHeight":430,"sourceBytes":67827,"destBytes":25440,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":16512,"convertSpendMs":12,"createdTime":"2021-05-30 12:52:48","host":"europe67*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx31/cdn75/contents/2021/05/30/04-53-05-307_491e97655ece3a67.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69855631.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sun, 30 May 2021 04:53:05 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["94A0:B997:4D176A5:4E91D92:60B31A31"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1622352147"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx31/cdn75/contents/2021/05/30/04-53-05-307_491e97655ece3a67.webp","historyStatusCode":[],"spendMs":155},"base64UserPassword":null,"token":"da243******************************d9e47"},"githubUser":"myreaderx31","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"66.2 KB","destSize":"24.8 KB","compressRate":"37.5%"},null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe-24.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[405,200]},"http://us-53.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-028.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-015.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-52.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe70.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-25.herokuapp.com/":{"failCount":1,"successCount":2,"resultList":[200,200,null]},"http://us-54.herokuapp.com/":{"failCount":1,"successCount":2,"resultList":[200,200,null]},"http://europe62.herokuapp.com/":{"failCount":1,"successCount":3,"resultList":[200,200,200,null]},"http://europe21.herokuapp.com/":{"failCount":2,"successCount":0,"resultList":[null,null]},"http://us-036.herokuapp.com/":{"failCount":2,"successCount":0,"resultList":[null,null]},"http://us-004.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe67.herokuapp.com/":{"failCount":1,"successCount":2,"resultList":[200,200,null]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-008.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-027.herokuapp.com/":{"failCount":2,"successCount":0,"resultList":[405,null]},"http://us-012.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://content.sov5.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icH3NqZdxxqViaowrYGdrJRw9picNx3fpQZpOvNm8VMWdjDuRNAiaxiaCEiaQ?imageView2/1/w/600","sourceStatusCode":200,"destWidth":600,"destHeight":382,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn36@2020_3/2021/05/30/04-52-35-968_f66123cea4d4ea35.webp","sourceBytes":29131,"destBytes":23902,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":26427,"convertSpendMs":9,"createdTime":"2021-05-30 12:52:09","host":"europe68*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389,20805817771fca5008d48917f5207389","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"28.4 KB","destSize":"23.3 KB","compressRate":"82.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNPMhaNuospc1FrHVRVv0cGu9TicyLQdiaAAET8gXokXUcaQ3JGpyrZpaj7ajY2HRYGW3v4icv66VKcYg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":76,"destHeight":85,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn48@2020_6/2021/05/30/04-52-36-876_f61698ba2e51c308.webp","sourceBytes":1776,"destBytes":2060,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":1018,"convertSpendMs":3,"createdTime":"2021-05-30 12:52:36","host":"europe-25*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389,20805817771fca5008d48917f5207389","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 KB","destSize":"2 KB","compressRate":"116%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/HkPvwCuFwNNEt7x0uibwVJAbgnJV0icdoRML1sUKS9L9SpjIvZyC2vgKbiboFDiaup5tONBH57X824fS9nHjEcDLcg/640?wx_fmt=gif","sourceStatusCode":200,"destWidth":637,"destHeight":114,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn43@2020_4/2021/05/30/04-52-39-484_d8f757d4d88b52d6.webp","sourceBytes":21989,"destBytes":14032,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":3210,"convertSpendMs":26,"createdTime":"2021-05-30 12:52:36","host":"us-012*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.5 KB","destSize":"13.7 KB","compressRate":"63.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic3jjUg28Gib0GpnYSzUh7ssSLJUuruM5kty1V4j2GZWIqzoT90gyQcFw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":714,"destHeight":259,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn99@2020_2/2021/05/30/04-52-40-210_411473453c8c757a.webp","sourceBytes":61680,"destBytes":29960,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":4057,"convertSpendMs":11,"createdTime":"2021-05-30 12:52:36","host":"us-016*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"60.2 KB","destSize":"29.3 KB","compressRate":"48.6%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icpaa1Kgibju0dmTW9ZhhZo8WoBRGtrSCuUzbc3UwDCCuaK5VfZrLUVxw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":519,"destHeight":147,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn62@2020_6/2021/05/30/04-52-40-484_7943c164535886f5.webp","sourceBytes":24788,"destBytes":11924,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":4243,"convertSpendMs":6,"createdTime":"2021-05-30 12:52:36","host":"us-015*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"24.2 KB","destSize":"11.6 KB","compressRate":"48.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic1kMwx86KSoiaQABs0ooJa4oMYhOicJBR4c5EMdJrI1hF6vFmwIkxjtXw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":80,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn12@2020_6/2021/05/30/04-52-42-575_5807ead3d0fe273b.webp","sourceBytes":49732,"destBytes":16280,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":6445,"convertSpendMs":6,"createdTime":"2021-05-30 12:52:36","host":"us-54*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"48.6 KB","destSize":"15.9 KB","compressRate":"32.7%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icz6yUq9sauM5UescFm66ic2EyR2MDL6J8PGBceebLXp3mDicWd6F2ibSZQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":976,"destHeight":184,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn87@2020_2/2021/05/30/04-52-43-010_74af3907cf217794.webp","sourceBytes":50136,"destBytes":28606,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":6968,"convertSpendMs":11,"createdTime":"2021-05-30 12:52:36","host":"europe62*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"49 KB","destSize":"27.9 KB","compressRate":"57.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icZPqqGNhF7iaR9UYsiba7odhnse9lOIGJnxQQMo1NQOQrAnMCVOMnHb0A/640?wx_fmt=png","sourceStatusCode":200,"destWidth":447,"destHeight":439,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn52@2020_6/2021/05/30/04-52-44-231_7f742741dada07cc.webp","sourceBytes":91747,"destBytes":33514,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":8380,"convertSpendMs":15,"createdTime":"2021-05-30 12:52:36","host":"europe62*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"89.6 KB","destSize":"32.7 KB","compressRate":"36.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icXgViaic3b0DJP9tMlj6RrEeuUgQ2E6GUjIf321Wseqb3BAKuAF4vZCCA/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":560,"destHeight":275,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn67@2020_6/2021/05/30/04-52-44-709_71504039068f2843.webp","sourceBytes":37372,"destBytes":26454,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":8701,"convertSpendMs":8,"createdTime":"2021-05-30 12:52:36","host":"europe-58*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"36.5 KB","destSize":"25.8 KB","compressRate":"70.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icRR6QEJ7KHDToKljeTjCwVx4QiaQ5KgYqU7aN1vjbetjCcWRfIAVO0iaQ/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":578,"destHeight":317,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn7@2020_3/2021/05/30/04-52-44-681_841b98e9f0a031b4.webp","sourceBytes":39996,"destBytes":27640,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":8889,"convertSpendMs":9,"createdTime":"2021-05-30 12:52:36","host":"europe67*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"39.1 KB","destSize":"27 KB","compressRate":"69.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icvxrjB2owskGZ4Rt1wtubOV6akm8SiaRJeCLIwOlBlQHU7xVfPTcPXpA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":287,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn65@2020_5/2021/05/30/04-52-46-925_9fd66a506837fabe.webp","sourceBytes":164708,"destBytes":46048,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":10729,"convertSpendMs":19,"createdTime":"2021-05-30 12:52:36","host":"us-003*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"160.8 KB","destSize":"45 KB","compressRate":"28%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icAQtk9D2s2zsp7a7tkNyuYCXf6Ye8RtKbIqicD4ILcaXWJY6icTznibZvw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":647,"destHeight":285,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn81@2020_5/2021/05/30/04-52-46-827_2303bacb83b4f2a0.webp","sourceBytes":67852,"destBytes":24570,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":10876,"convertSpendMs":12,"createdTime":"2021-05-30 12:52:36","host":"europe-25*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"66.3 KB","destSize":"24 KB","compressRate":"36.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icsfNHXstRHeRUxd8QRe4I6yhTiaAZbnb0lsgctrHX5vcU924MwqCRPRw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":900,"destHeight":299,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn20@2020_5/2021/05/30/04-52-53-574_c600f89a87db332f.webp","sourceBytes":257128,"destBytes":26298,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":17325,"convertSpendMs":20,"createdTime":"2021-05-30 12:52:36","host":"us-020*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"251.1 KB","destSize":"25.7 KB","compressRate":"10.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ick2vwgtNia5F3oFXRmxOwMRwsIvvVUl5kGw1ibYEMo2tjp8R57zIOSrZQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":214,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn72@2020_2/2021/05/30/04-52-54-802_f124b94af7fddfc5.webp","sourceBytes":100618,"destBytes":31746,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":18647,"convertSpendMs":13,"createdTime":"2021-05-30 12:52:36","host":"us-53*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"98.3 KB","destSize":"31 KB","compressRate":"31.6%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic01U8AQTH6wKXf4E6eFBXaiakjpLqSx0O4CGnqnhuvcX27ovUH7T8ib9Q/640?wx_fmt=png","sourceStatusCode":200,"destWidth":493,"destHeight":499,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn56@2020_1/2021/05/30/04-53-00-179_1c9c724c1d0f279f.webp","sourceBytes":114506,"destBytes":26604,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":23964,"convertSpendMs":13,"createdTime":"2021-05-30 12:52:36","host":"us-54*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"111.8 KB","destSize":"26 KB","compressRate":"23.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icf6Xh8ibtQ6rz4lSVZHic06xfaITH0I08l4hDO9n8hkLlIPqK7icLn2mBQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":900,"destHeight":299,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn69@2020_5/2021/05/30/04-53-00-554_e73f7df4782d9828.webp","sourceBytes":355624,"destBytes":37256,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":24353,"convertSpendMs":20,"createdTime":"2021-05-30 12:52:36","host":"us-52*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"347.3 KB","destSize":"36.4 KB","compressRate":"10.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icribBjADib4Az09Yu9vnQjibaic0aRAZqb5nSaX1wFY0jWbiagicZYgSwVpcw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":811,"destHeight":606,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn40@2020_3/2021/05/30/04-53-05-627_c8f6054c988291c7.webp","sourceBytes":188360,"destBytes":38218,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":29400,"convertSpendMs":23,"createdTime":"2021-05-30 12:52:36","host":"us-024*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"183.9 KB","destSize":"37.3 KB","compressRate":"20.3%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0ic7SCASbEGicKdwvkkztKKPQafnQ0lAf6Ng9nskM84nQ2vwh5I1EMyZ5Q/640?wx_fmt=png","sourceStatusCode":200,"destWidth":852,"destHeight":464,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn31@2020_6/2021/05/30/04-53-31-370_cb073582eeaf0beb.webp","sourceBytes":148922,"destBytes":85910,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":55541,"convertSpendMs":23,"createdTime":"2021-05-30 12:52:36","host":"europe-58*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"145.4 KB","destSize":"83.9 KB","compressRate":"57.7%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icdJQz2DonNHUc4KhSiaFibdX8Dia0E3YTOibRwXhJ9QtTXCv3HDXEJkp6ng/640?wx_fmt=png","sourceStatusCode":200,"destWidth":544,"destHeight":258,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn93@2020_4/2021/05/30/04-53-45-164_5c08738f0e78754f.webp","sourceBytes":79119,"destBytes":33746,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":6790,"convertSpendMs":14,"createdTime":"2021-05-30 12:53:38","host":"us-53*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"77.3 KB","destSize":"33 KB","compressRate":"42.7%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icVaF2xC9WoU7A1hVDhicpvIQPMgj8wOcsZamWFibm0JSRHJs9KMvibo6GQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":234,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn36@2020_3/2021/05/30/04-53-52-631_ecf63520176f8c10.webp","sourceBytes":130218,"destBytes":35482,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":14289,"convertSpendMs":15,"createdTime":"2021-05-30 12:53:38","host":"us-003*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"127.2 KB","destSize":"34.7 KB","compressRate":"27.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/HkPvwCuFwNMicHREQliaPyRojFQQzmXj0icjhWL5ol680owSBricibVStiabL81QFOEjhqwThZ35BoC6PwyeQ4uT2aUw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":813,"destHeight":353,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn16@2020_5/2021/05/30/04-54-33-517_6ed5df64220263c1.webp","sourceBytes":424910,"destBytes":44346,"targetWebpQuality":75,"feedId":1700,"totalSpendMs":55132,"convertSpendMs":23,"createdTime":"2021-05-30 12:53:38","host":"us-016*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E5%BE%AE%E8%BD%AF%E7%A0%94%E7%A9%B6%E9%99%A2AI%E5%A4%B4%E6%9D%A1+Transformer%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E8%B5%B0%E5%88%B0%E5%93%AA%E4%BA%86%EF%BC%9F","linkMd5ListStr":"20805817771fca5008d48917f5207389","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"415 KB","destSize":"43.3 KB","compressRate":"10.4%"}],"successGithubMap":{"myreaderx8":1,"myreaderx25":1,"myreaderx15":1,"myreaderx27":1,"myreaderx21":1,"myreaderx32":1,"myreaderx4":1,"myreaderx11":2,"myreaderx22":1,"myreaderx3":1,"myreaderx33":1,"myreaderx2":1,"myreaderx12":1,"myreaderx1":1,"myreaderx24":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx29":1,"myreaderx":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1,"myreaderx31":1}}