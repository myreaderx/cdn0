{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-09-07 12:13:24","updatedTime":"2020-09-07 12:13:24","title":"常识知识确能被捕获，西湖大学博士探究BERT如何做常识问答","link":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","description":"<div><div><div id=\"media\" class=\"rich_media_thumb_wrp\">\n\n            <img class=\"rich_media_thumb\" src=\"http://img.100weidu.com/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dYgl2mu48uicjRePRXckeChOTehNg882Xg2DxnVFaEia53WHC0u3m1cIw?imageView2/1/w/600\">\n        </div>\n    \n\n    \n\n    <div class=\"rich_media_content\" id=\"js_content\">\n                    \n\n                    \n\n                    \n                    \n                    <section data-mpa-powered-by=\"yiban.io\" data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-style=\"max-width: 100%; font-variant-numeric: normal; font-variant-east-asian: normal; letter-spacing: 0.544px; line-height: 27.2px; white-space: normal; widows: 1; font-family: \" helvetica neue sans gb yahei arial sans-serif background-color: rgb box-sizing: border-box overflow-wrap: break-word class=\"js_darkmode__0\" style=\"font-variant-numeric: normal;font-variant-east-asian: normal;white-space: normal;max-width: 100%;letter-spacing: 0.544px;line-height: 27.2px;widows: 1;font-family: \" visible rgba><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-id=\"85660\" data-custom=\"rgb(117, 117, 118)\" data-color=\"rgb(117, 117, 118)\" data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-id=\"85660\" data-custom=\"rgb(117, 117, 118)\" data-color=\"rgb(117, 117, 118)\" data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><p style=\"margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;color: rgb(255, 255, 255);visibility: visible;background-color: rgb(117, 117, 118);font-size: 15px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">选自arXiv</span></p><p style=\"max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.5em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;color: rgb(136, 136, 136);visibility: visible;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgb(136, 136, 136)\" data-darkmode-original-color-15899528520055=\"rgb(136, 136, 136)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\">作者：Leyang Cui等</strong></span><span style=\"max-width: 100%;color: rgb(136, 136, 136);visibility: visible;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgb(136, 136, 136)\" data-darkmode-original-color-15899528520055=\"rgb(136, 136, 136)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></strong></span></p><p style=\"max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.5em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;color: rgb(136, 136, 136);visibility: visible;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgb(136, 136, 136)\" data-darkmode-original-color-15899528520055=\"rgb(136, 136, 136)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\">编辑：小舟、杜伟</strong></span></p></section></section></section></section></section></section></section></section></section></section></section><blockquote class=\"js_blockquote_wrap\" data-type=\"2\" data-url=\"\" data-author-name=\"\" data-content-utf8-length=\"21\" data-source-title=\"\"><section class=\"js_blockquote_digest\"><p style=\"line-height: 1.75em;\">BERT 是通过常识知识来解决常识任务的吗？</p></section></blockquote><p><br></p><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.5625\" data-s=\"300,640\" data-type=\"png\" data-w=\"896\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1doQlrEevlUwov4VLoVf4LJEYH5FUJcPq9puoB65ChoyUJZ2iaib9nCZOw/640?wx_fmt=png\"><br></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">预训练上下文化语言模型（例如 BERT）的成功激发了研究人员探索此类模型中的语言知识，以解释下游任务的巨大改进。尽管先前的研究工作展示了 BERT 中的句法、语义和词义知识，但在研究 BERT 如何解决常识问答（CommonsenseQA）任务方面做的工作还很少。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">尤其是，BERT 是依靠浅层句法模式还是较深层常识知识来消除歧义是一个有趣的研究课题。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">近日，来自西湖大学、复旦大学和微软亚洲研究院的研究者提出了两种基于注意力的方法来分析 BERT 内部的常识知识，以及这些知识对模型预测的贡献。论文一作 Leyang Cui 为西湖大学文本智能实验室（Text Intelligence Lab）的在读博士生。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">该研究发现，注意力头（attention head）成功捕获了以 ConceptNet 编码的结构化常识知识，从而对 BERT 直接解决常识任务提供帮助。此外，微调进一步使 BERT 学习在更高层次上使用常识知识。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.3078125\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dy7y5kfsCDHgjw6oeAoQib6urxwHm44oQ7CU6X342gb7ZblmppM759dQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span><br></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 15px;\">论文地址：</span><span style=\"font-size: 15px;color: rgb(123, 12, 0);\">https://arxiv.org/pdf/2008.03945.pdf</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>任务和模型</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在讲解 BERT 的应用之前，研究者首先简要介绍了 CommonsenseQA 的相关知识。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">CommonsenseQA</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">CommonsenseQA（Talmor 等人，2019 年）是一个基于 ConceptNet 知识图谱（Speer 等人，2017 年）构建的多项选择问答数据集，它由关系对的大规模三元集合，即源概念、关系和目标概念组成，「鸟、栖息和乡村」就是一个典型示例。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">如下图 2 所示，给定源概念「鸟」和关系类型「栖息」，则存在 3 个目标概念「笼子」、「窗台」和「乡村」。在 CommonsenseQA 数据集的开发过程中，要求参与者分别基于源概念和 3 个目标概念来生成问题和候选答案。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.45687203791469194\" data-s=\"300,640\" data-type=\"png\" data-w=\"1055\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1deZKsCmRXcuicGkAXPT4SKPicZT9hw2d9xfj7hAhYUcZRQ7P6dj9hUV7g/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\"><em>图 2：从 ConceptNet 到 CommonsenseQA。</em></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">基于 Talmor 等人（2019 年）的研究，研究者将问题中的源概念称为问题概念（question concept），将答案中的目标概念称为答案概念（answer concept）。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">为了使任务更加困难，研究者还添加了两个不正确的答案。研究者将 commonsene 链接定义为从答案概念到问题概念的链接。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">此外，为了分析基于从答案概念到问题概念的链接的隐式结构常识知识，研究者选择过滤掉了一些问题，并且过滤掉的这些问题不包含 ConceptNet 形式的问题概念（如释义）。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">下表 1 汇总了数据集 CommonsenseQA 和 CommonsenseQA * 的详细数据：</span></section><section style=\"line-height: 1.75em;\"><br></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.39644970414201186\" data-s=\"300,640\" data-type=\"png\" data-w=\"1014\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dNCJ5QOK15x2sFKTQvPtiaUKcIXicKAXAQQC7bNRbiarB4a9s0t1zPEyGg/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">将 BERT 应用于 CommonsenseQA</span></strong></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者采用 Talmor 等人在 2019 年提出的方法，在 CommonsenseQA 上使用 BERT（Devlin 等人，2019 年）。结构如下图 3 所示：</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.3505654281098546\" data-s=\"300,640\" data-type=\"png\" data-w=\"1238\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dXLNn5b4BiaibHS5nVJrQYoOqLQkO2pwlrVISbcZLQr6pd2BywI1mic8iaQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">具体来说，给定一个问题 q 以及 5 个候选答案（a_1, ..., a+5），研究者将这个问题与每个答案连接起来，以分别获得 5 个链接序列（即句子）s_1, ..., s_5。在表示上，每个句子的开头使用特殊符号 [CLS]，问题和候选答案之间使用符号 [SEP]，句子末尾使用符号 [SEP]。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">BERT 由 L 个 stacked Transformer 层（Vaswani 等人，2017 年）组成，以对每个句子进行编码。所以，[CLS] token 最后一层的隐状态用于带有 softmax 的线性分类，并且 s_1, ... , s_5 中得分最高的候选对象被选为预测答案。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>分析方法</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">该研究使用注意力权重和相应的归因得分（attribution score）来分析常识链接。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">注意力权重</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">给定一个句子，我们可以将 Transformer 中的注意力权重视为生成下一层表示过程中，每个 token 与其他 token 之间的相对重要性权重（Kovaleva 等人，2019 年；Vashishth 等人，2020 年）。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">注意力权重α通过 Q = W^QH 中查询向量和 K = W^KH 中核心向量的缩放点积（scaled dot-product）来计算，然后得到 softmax 归一化：</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.17782909930715934\" data-s=\"300,640\" data-type=\"png\" data-w=\"866\" style=\"width: 215px; height: 38px; max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dgpJ2q1ibNpVhN9J28NeCn45gbkbSic6iaLJQ2Rt4tauCIhIXMVMqziaL7Q/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">归因得分</span></strong><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">Kobayashi 等人指出，仅分析注意力权重可能不足以调查注意头的行为，因为注意力权重忽略了隐藏向量 H 的值。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">作为注意力权重的补充，已经研究了基于梯度的特征归因方法来解释反向传播中每个输入特征对模型预测的贡献。对注意力权重和相应的归因得分的分析有助于更全面地理解 BERT 中的常识链接。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者使用一种名为集成梯度（Integrated Gradient，Sundararajan 等人 2017 年提出）的归因方法来解释 BERT 中的常识链接。直观地讲，集成梯度方法模拟剪枝特定注意力头的过程（从初始注意力权重α到零向量α'），并计算反向传播中的集成梯度值。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">归因得分直接反映出了注意力权重的变化会对模型输出造成多大程度的改变。通常来说，归因得分越高表示单个注意力权重越重要。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>BERT 是否包含结构化常识知识？</strong></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者首先进行了一组实验来探究常识链接权重，从而可以反映出常识知识是否会被句子的 BERT 表示捕获。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">直观地讲，如果答案概念到问题概念的链接权重高于答案概念到其他疑问词的链接权重，则 ConceptNet 中的常识知识是通过经验表示捕获的。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">值得注意的是，[CLS] token 的表示不是问题概念，而是直接连接至输出层以进行候选评分。因此，在预训练和微调阶段，对于输出层以及答案概念 token 到问题概念 token 的链接权重，都没有直接的监督信号。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">探测任务（probing task）</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者通过计算最相关的词（most associated word, MAW）来评估链接权重，其中 MAW 是从所有疑问词中的答案概念中获得最大链接权重的问题概念词。研究者计算了每层中每个注意力头的 MAW。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">实验结果</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">下表 2 展示了对于不同的常识关系，原始归一化 BERT 和在 CommonsenseQA 上微调的 BERT 模型的平均和最大准确率结果：</span></section><section style=\"line-height: 1.75em;\"><br></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.8676948051948052\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"1232\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dMQPHPuAbWtUuLNZkat8gXN2qBcvqYtickKmPic8ziceWcBxpdqG86JiaAQ/640?wx_fmt=jpeg\"></p><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">首先观察未经微调的原始 BERT，每一层的最大 MAW 准确率明显优于随机基准。这表明 BERT 确实捕获了常识知识。此外，BERT 的平均 MAW 也明显优于随机基准（p 值 &lt; 0.01），这表明相关的问题概念无需微调即可在 BERT 编码中发挥非常重要的作用。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">其次，就平均 MAW 准确率和最大 MAW 准确率而言，BERT-FT 均优于 BERT。这表明对常识任务的监督训练可以增强结构化的常识知识。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>BERT 如何将常识知识用于常识任务？</strong></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者进一步进行了一组实验，来描述常识链接与模型预测之间的相关性。目的是为了研究不同候选答案概念到问题概念的链接权重是否会对这些候选答案之间的模型决策造成影响。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">具体而言，研究者比较了 5 个候选答案对于同一问题的链接权重，并找出了与相关问题概念最相关的候选答案。这个候选答案被称为最相关候选对象（most associated candidate, MAC）。MAC 和每个问题的模型预测之间也存在着相关性。直观地讲，如果 MAC 与模型预测呈现相关性，则证明模型在预测过程中运用到了常识知识。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者进行实验来评估 MAC 对模型决策的贡献，以及 MAC 依赖与输出准确率之间的相关性。实验中使用注意力权重和归因得分来测量链接，这是因为在考虑模型预测时梯度会发挥作用。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">此外，对于所有试验来说，归因得分的趋势与使用注意力权重测量的结果保持一致。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">探测任务</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">从形式上，给定一个问题 q 和 5 个候选答案 a1, ..., a5，研究者对相应的 5 个候选句子 s1, ..., s5 进行比较。在每个候选句子中，研究者根据 ConceptNet 计算了答案概念到问题概念的链接权重。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者通过测量答案概念到 [CLS]token 的链接权重，进一步定义了最相关句子（most associated sentence, MAS）。这是因为梯度是从 [CLS]token 后向传播，而不是从问题概念或答案概念。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">此外，通过比较 MAC 和 MAS，研究者可以获得 MAC 是否对模型决策造成影响的有用信息。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">常识链接的重要性</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">研究者测量了 BERT-FT 和 BERT-Probing（这是一个仅针对输出层进行微调的 BERT 变体）的 MAC 性能，其中 BERT-Probing 是一个线性探测（linear probing）模型。直观地讲，如果线性分类器可以预测常识任务，则未经微调的原始模型可能会编码丰富的常识知识。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">下表 3 为 top Transformer 层中 12 个注意力头条件下，MAC 和 MAS 的重叠率（overlapping rate）：</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.8820960698689956\" data-s=\"300,640\" data-type=\"png\" data-w=\"687\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dPGO4yicuUJINMP7CFjbDPCbsHTEkozCObZEZicxkIicGo8PnVXFs699XA/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">常识链接与模型预测之间的相关性</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">为了进一步探究常识知识对模型预测的贡献，下图 4 展示了每个 Transformer 层上 MAC 和模型预测之间的重叠率：</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.654014598540146\" data-s=\"300,640\" data-type=\"png\" data-w=\"685\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dpViaEYYElPPUDHfQGqWEDS0AATNqrylmwt6wcykY3cHyDWGK6clzibwQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">下表 4 则展示了 MAC 准确率和预测准确率之间的相关性：</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.6508422664624809\" data-s=\"300,640\" data-type=\"png\" data-w=\"653\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dZbov0Js9iahGmrHiaAfMSdicTyLaM9TWNACOicYG6Vt6c1PlibIUF45C7xA/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">不同层的贡献</span></strong><span style=\"font-size: 15px;\"></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">最后，研究者进一步探究了常识知识使用上的两个具体问题。其一，在决策过程中，BERT 最依赖哪个层？其二，BERT 使用的常识知识来自预训练或微调吗？为此，研究者通过连接每个 Transformer 层上的输出层，对 12 个模型变体进行了比较。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">下表 5 展示了模型准确率和 MAC 重叠率的数据：</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.8103607770582794\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"1081\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dQBCoWgYWIdLGUlTuSq8NAGEFTZqBjuUedN3K19ktLufJsJaibMMkJ9g/640?wx_fmt=jpeg\"></p><p style=\"text-align: center;\"><br></p><section mpa-from-tpl=\"t\" style=\"white-space: normal;\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\"><section data-id=\"90835\" mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\" style=\"padding: 10px;\"><section mpa-from-tpl=\"t\" style=\"padding: 15px;box-shadow: rgb(204, 204, 204) 0px 0px 5px;\"><section mpa-from-tpl=\"t\"><section mpa-from-tpl=\"t\" style=\"padding-top: 0.8em;padding-right: 0.5em;padding-left: 0.5em;\"><section data-autoskip=\"1\" mpa-from-tpl=\"t\" style=\"line-height: 1.5em;\"><p style=\"margin-top: 15px;max-width: 100%;min-height: 1em;font-family: -apple-system-font, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif left break-word border-box><span style=\"color: rgb(102, 102, 102);font-size: 14px;\">Amazon SageMaker 是一项完全托管的服务，可以帮助开发人员和数据科学家快速构建、训练和部署机器学习 模型。SageMaker完全消除了机器学习过程中每个步骤的繁重工作，让开发高质量模型变得更加轻松。</span></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif left break-word border-box><br></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif left break-word border-box><span style=\"color: rgb(102, 102, 102);font-size: 14px;\">现在，企业开发者可以免费领取1000元服务抵扣券，轻松上手Amazon SageMaker，快速体验5个人工智能应用实例。</span></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif left break-word border-box><br></p><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"2.7853333333333334\" data-s=\"300,640\" data-type=\"png\" data-w=\"750\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicW11x3icsnzyBibRtLHxmY7HGYibGvz4IWo3zNeVFX4PtraWnWf1o5RRwqqfewLVqhh0eO1cJ1vfmuw/640?wx_fmt=png\" style=\"max-width: 600px\"></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, system-ui, \" helvetica neue sc sans gb yahei ui arial sans-serif center break-word border-box><br></p></section></section></section></section></section></section></section></section></section></section></section><p style=\"margin-top: 5px;white-space: normal;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">© THE END </span></p><p style=\"margin-top: 5px;white-space: normal;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">转载请联系本公众号获得授权</span></p><p style=\"margin-top: 5px;white-space: normal;text-align: center;\"><span style=\"font-size: 12px;color: rgb(136, 136, 136);\">投稿或寻求报道：content@jiqizhixin.com</span><span style=\"font-size: 15px;text-align: justify;\"></span></p>\n                </div>\n\n    \n    <br>\n\n    \n        <a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650796953&amp;idx=3&amp;sn=2b5953df3167a7db3cfe82c1330f1684&amp;chksm=871a37e7b06dbef16ec3e6a47afcef37afdcd27e653cacad2232352d5b6baf20e4c5729c94be&amp;scene=0#rd\" style=\"color: blue\" class=\"media_tool_meta meta_primary\">原文</a>\n        <br>\n    \n\n    \n\n    <img alt=\"\" width=\"1px\" height=\"1px\" class=\"\" style=\"width:1px;height:1px;display:none\" src=\"http://www.jintiankansha.me/rss_static/5409/ZVAoxuD7i2\"></div></div>","descriptionType":"html","publishedDate":"Sun, 06 Sep 2020 04:19:00 +0000","feedId":1837,"bgimg":"http://img.100weidu.com/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dYgl2mu48uicjRePRXckeChOTehNg882Xg2DxnVFaEia53WHC0u3m1cIw?imageView2/1/w/600","linkMd5":"0f7e7999b0fa73a6fc56fdd4aeec0320","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn3@2020_6/2020/09/07/04-13-25-094_c94c6493836b7b86.webp","destWidth":504,"destHeight":504,"sourceBytes":38221,"destBytes":32940,"author":"","articleImgCdnMap":{"http://img.100weidu.com/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dYgl2mu48uicjRePRXckeChOTehNg882Xg2DxnVFaEia53WHC0u3m1cIw?imageView2/1/w/600":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn3@2020_6/2020/09/07/04-13-25-094_c94c6493836b7b86.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1doQlrEevlUwov4VLoVf4LJEYH5FUJcPq9puoB65ChoyUJZ2iaib9nCZOw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn35@2020_4/2020/09/07/04-14-38-188_daab437257e465be.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dy7y5kfsCDHgjw6oeAoQib6urxwHm44oQ7CU6X342gb7ZblmppM759dQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn28@2020_6/2020/09/07/04-13-27-714_267d9f09f7426b8f.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1deZKsCmRXcuicGkAXPT4SKPicZT9hw2d9xfj7hAhYUcZRQ7P6dj9hUV7g/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn11@2020_5/2020/09/07/04-13-38-688_bd2b495e290e94a0.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dNCJ5QOK15x2sFKTQvPtiaUKcIXicKAXAQQC7bNRbiarB4a9s0t1zPEyGg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn23@2020_2/2020/09/07/04-13-35-332_65ea907b04f69097.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dXLNn5b4BiaibHS5nVJrQYoOqLQkO2pwlrVISbcZLQr6pd2BywI1mic8iaQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn55@2020_6/2020/09/07/04-13-27-027_858c4a12ae8dec8c.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dgpJ2q1ibNpVhN9J28NeCn45gbkbSic6iaLJQ2Rt4tauCIhIXMVMqziaL7Q/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn43@2020_4/2020/09/07/04-13-26-924_8ac5703e730aff66.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dMQPHPuAbWtUuLNZkat8gXN2qBcvqYtickKmPic8ziceWcBxpdqG86JiaAQ/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn19@2020_4/2020/09/07/04-13-30-346_e766979b63bc28b5.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dPGO4yicuUJINMP7CFjbDPCbsHTEkozCObZEZicxkIicGo8PnVXFs699XA/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn29@2020_4/2020/09/07/04-13-37-281_757756b2d1edeef2.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dpViaEYYElPPUDHfQGqWEDS0AATNqrylmwt6wcykY3cHyDWGK6clzibwQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn40@2020_2/2020/09/07/04-13-30-958_f09b382439489b32.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dZbov0Js9iahGmrHiaAfMSdicTyLaM9TWNACOicYG6Vt6c1PlibIUF45C7xA/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn16@2020_2/2020/09/07/04-13-27-938_da96db249f1009ef.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dQBCoWgYWIdLGUlTuSq8NAGEFTZqBjuUedN3K19ktLufJsJaibMMkJ9g/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn8@2020_4/2020/09/07/04-13-38-355_c2436c56935e0fad.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicW11x3icsnzyBibRtLHxmY7HGYibGvz4IWo3zNeVFX4PtraWnWf1o5RRwqqfewLVqhh0eO1cJ1vfmuw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn48@2020_6/2020/09/07/04-13-39-623_6e090a02b4f164c4.webp","http://www.jintiankansha.me/rss_static/5409/ZVAoxuD7i2":null},"publishedOrCreatedDate":1599452004625}],"record":{"createdTime":"2020-09-07 12:13:24","updatedTime":"2020-09-07 12:13:24","feedId":1837,"fetchDate":"Mon, 07 Sep 2020 04:13:24 +0000","fetchMs":655,"handleMs":27,"totalMs":75651,"newArticles":0,"totalArticles":5,"status":1,"type":0,"ip":"ba032c6aeba6e9df40465e65d6669584","hostName":"europe70*","requestId":"32d2d6840584446f9160c2d2b7239a47_1837","contentType":"application/rss+xml","totalBytes":696328,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":14,"articlesImgsGithubTotal":13,"successGithubMap":{"myreaderx8":1,"myreaderx25":1,"myreaderx15":1,"myreaderx7":1,"myreaderx16":1,"myreaderx27":1,"myreaderx4":1,"myreaderx21":1,"myreaderx24":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-24 21:31:33","updatedTime":"2020-09-01 09:54:29","id":1837,"name":"机器之心","url":"http://feedmaker.kindle4rss.com/feeds/almosthuman2014.weixin.xml","subscriber":null,"website":null,"icon":"http://www.sogou.com/images/logo/new/favicon.ico?v=4","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn87@2020_3/2020/09/01/01-54-30-263_d24121c9beed1de6.ico","description":"专业的人工智能媒体和产业服务平台","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2020-09-07 12:14:39","updatedTime":"2020-09-07 12:14:39","id":null,"feedId":1837,"linkMd5":"0f7e7999b0fa73a6fc56fdd4aeec0320"}],"tmpCommonImgCdnBytes":32940,"tmpBodyImgCdnBytes":663388,"tmpBgImgCdnBytes":0,"extra4":{"start":1599452003711,"total":0,"statList":[{"spend":888,"msg":"获取xml内容"},{"spend":27,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":73363,"msg":"正文链接上传到cdn"}]},"extra5":14,"extra6":14,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicW11x3icsnzyBibRtLHxmY7HGYibGvz4IWo3zNeVFX4PtraWnWf1o5RRwqqfewLVqhh0eO1cJ1vfmuw/640?wx_fmt=png","sourceStatusCode":403,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":851,"convertSpendMs":0,"createdTime":"2020-09-07 12:13:26","host":"europe-25*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","extra22GetBytesInfo":"2、Referer字段 ： http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","extra23historyStatusCode":[403,403],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5409/ZVAoxuD7i2","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":2002,"convertSpendMs":0,"createdTime":"2020-09-07 12:13:26","host":"europe62*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/5409/ZVAoxuD7i2","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":735,"convertSpendMs":0,"createdTime":"2020-09-07 12:13:28","host":"us-040*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-25.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[403]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe62.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]},"http://us-036.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe-57.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-010.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,405]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://img.100weidu.com/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dYgl2mu48uicjRePRXckeChOTehNg882Xg2DxnVFaEia53WHC0u3m1cIw?imageView2/1/w/600","sourceStatusCode":200,"destWidth":504,"destHeight":504,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn3@2020_6/2020/09/07/04-13-25-094_c94c6493836b7b86.webp","sourceBytes":38221,"destBytes":32940,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":1199,"convertSpendMs":13,"createdTime":"2020-09-07 12:13:24","host":"us-004*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320,0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"37.3 KB","destSize":"32.2 KB","compressRate":"86.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dgpJ2q1ibNpVhN9J28NeCn45gbkbSic6iaLJQ2Rt4tauCIhIXMVMqziaL7Q/640?wx_fmt=png","sourceStatusCode":200,"destWidth":866,"destHeight":154,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn43@2020_4/2020/09/07/04-13-26-924_8ac5703e730aff66.webp","sourceBytes":21572,"destBytes":10652,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":1609,"convertSpendMs":19,"createdTime":"2020-09-07 12:13:26","host":"us-012*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.1 KB","destSize":"10.4 KB","compressRate":"49.4%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dXLNn5b4BiaibHS5nVJrQYoOqLQkO2pwlrVISbcZLQr6pd2BywI1mic8iaQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":379,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn55@2020_6/2020/09/07/04-13-27-027_858c4a12ae8dec8c.webp","sourceBytes":101550,"destBytes":33860,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":1824,"convertSpendMs":22,"createdTime":"2020-09-07 12:13:26","host":"us-040*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"99.2 KB","destSize":"33.1 KB","compressRate":"33.3%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dy7y5kfsCDHgjw6oeAoQib6urxwHm44oQ7CU6X342gb7ZblmppM759dQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":332,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn28@2020_6/2020/09/07/04-13-27-714_267d9f09f7426b8f.webp","sourceBytes":122140,"destBytes":26856,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":2827,"convertSpendMs":20,"createdTime":"2020-09-07 12:13:26","host":"europe-57*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"119.3 KB","destSize":"26.2 KB","compressRate":"22%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dZbov0Js9iahGmrHiaAfMSdicTyLaM9TWNACOicYG6Vt6c1PlibIUF45C7xA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":653,"destHeight":425,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn16@2020_2/2020/09/07/04-13-27-938_da96db249f1009ef.webp","sourceBytes":133695,"destBytes":38650,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":2626,"convertSpendMs":18,"createdTime":"2020-09-07 12:13:26","host":"us-032*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"130.6 KB","destSize":"37.7 KB","compressRate":"28.9%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dMQPHPuAbWtUuLNZkat8gXN2qBcvqYtickKmPic8ziceWcBxpdqG86JiaAQ/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":1080,"destHeight":937,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn19@2020_4/2020/09/07/04-13-30-346_e766979b63bc28b5.webp","sourceBytes":143303,"destBytes":98704,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":5155,"convertSpendMs":87,"createdTime":"2020-09-07 12:13:26","host":"us-020*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"139.9 KB","destSize":"96.4 KB","compressRate":"68.9%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dpViaEYYElPPUDHfQGqWEDS0AATNqrylmwt6wcykY3cHyDWGK6clzibwQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":685,"destHeight":448,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn40@2020_2/2020/09/07/04-13-30-958_f09b382439489b32.webp","sourceBytes":84757,"destBytes":27288,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":5685,"convertSpendMs":30,"createdTime":"2020-09-07 12:13:26","host":"us-024*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"82.8 KB","destSize":"26.6 KB","compressRate":"32.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dNCJ5QOK15x2sFKTQvPtiaUKcIXicKAXAQQC7bNRbiarB4a9s0t1zPEyGg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1014,"destHeight":402,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn23@2020_2/2020/09/07/04-13-35-332_65ea907b04f69097.webp","sourceBytes":79338,"destBytes":36256,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":10054,"convertSpendMs":21,"createdTime":"2020-09-07 12:13:26","host":"us-008*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"77.5 KB","destSize":"35.4 KB","compressRate":"45.7%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dPGO4yicuUJINMP7CFjbDPCbsHTEkozCObZEZicxkIicGo8PnVXFs699XA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":687,"destHeight":606,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn29@2020_4/2020/09/07/04-13-37-281_757756b2d1edeef2.webp","sourceBytes":241129,"destBytes":70532,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":12556,"convertSpendMs":28,"createdTime":"2020-09-07 12:13:26","host":"europe-58*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"235.5 KB","destSize":"68.9 KB","compressRate":"29.3%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1deZKsCmRXcuicGkAXPT4SKPicZT9hw2d9xfj7hAhYUcZRQ7P6dj9hUV7g/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1055,"destHeight":482,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn11@2020_5/2020/09/07/04-13-38-688_bd2b495e290e94a0.webp","sourceBytes":110347,"destBytes":42844,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":13473,"convertSpendMs":33,"createdTime":"2020-09-07 12:13:26","host":"us-54*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"107.8 KB","destSize":"41.8 KB","compressRate":"38.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1dQBCoWgYWIdLGUlTuSq8NAGEFTZqBjuUedN3K19ktLufJsJaibMMkJ9g/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":1080,"destHeight":875,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn8@2020_4/2020/09/07/04-13-38-355_c2436c56935e0fad.webp","sourceBytes":149406,"destBytes":102200,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":11751,"convertSpendMs":34,"createdTime":"2020-09-07 12:13:28","host":"europe66*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"145.9 KB","destSize":"99.8 KB","compressRate":"68.4%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicW11x3icsnzyBibRtLHxmY7HGYibGvz4IWo3zNeVFX4PtraWnWf1o5RRwqqfewLVqhh0eO1cJ1vfmuw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":750,"destHeight":2089,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn48@2020_6/2020/09/07/04-13-39-623_6e090a02b4f164c4.webp","sourceBytes":642026,"destBytes":137034,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":13562,"convertSpendMs":109,"createdTime":"2020-09-07 12:13:27","host":"us-010*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"627 KB","destSize":"133.8 KB","compressRate":"21.3%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibjbNWQJj9WauATUibE0Ga1doQlrEevlUwov4VLoVf4LJEYH5FUJcPq9puoB65ChoyUJZ2iaib9nCZOw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":896,"destHeight":504,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn35@2020_4/2020/09/07/04-14-38-188_daab437257e465be.webp","sourceBytes":350677,"destBytes":38512,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":11745,"convertSpendMs":31,"createdTime":"2020-09-07 12:14:27","host":"europe66*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86%E7%A1%AE%E8%83%BD%E8%A2%AB%E6%8D%95%E8%8E%B7%EF%BC%8C%E8%A5%BF%E6%B9%96%E5%A4%A7%E5%AD%A6%E5%8D%9A%E5%A3%AB%E6%8E%A2%E7%A9%B6BERT%E5%A6%82%E4%BD%95%E5%81%9A%E5%B8%B8%E8%AF%86%E9%97%AE%E7%AD%94","linkMd5ListStr":"0f7e7999b0fa73a6fc56fdd4aeec0320","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"342.5 KB","destSize":"37.6 KB","compressRate":"11%"}],"successGithubMap":{"myreaderx8":1,"myreaderx25":1,"myreaderx15":1,"myreaderx7":1,"myreaderx16":1,"myreaderx27":1,"myreaderx4":1,"myreaderx21":1,"myreaderx24":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{}}