{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-09-30 22:51:45","updatedTime":"2021-09-30 22:51:45","title":"HTTP/2 in infrastructure: Ambry network stack refactoring","link":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","description":"<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p><i>Co-authors: <a href=\"https://www.linkedin.com/in/newkernel/\" target=\"_blank\">Ze Mao</a>, <a href=\"https://www.linkedin.com/in/matthewrswise/\" target=\"_blank\">Matt Wise</a>, <a href=\"https://www.linkedin.com/in/caseygetz/\" target=\"_blank\">Casey Getz</a>, <a href=\"https://www.linkedin.com/in/justin-lin-64a29049/\" target=\"_blank\">Justin Lin</a>, <a href=\"https://www.linkedin.com/in/asinghai/\" target=\"_blank\">Ashish Singhai</a>, and <a href=\"https://www.linkedin.com/in/lightningrob/\" target=\"_blank\">Rob Block</a></i></p> \n   <h2>Introduction</h2> \n   <p><a href=\"https://engineering.linkedin.com/blog/2016/05/introducing-and-open-sourcing-ambry---linkedins-new-distributed-\" target=\"_blank\">Ambry</a> is LinkedIn's scalable geo-distributed object store. Developed in-house and open sourced in 2016, Ambry stores tens of petabytes of data.&#160;</p> \n   <p>At LinkedIn, Ambry is used to store objects like photos, videos, and resume uploads, as well as internal binary data. After years of production use, Ambry has proven to be stable and easy to operate. But there have been challenges along the way, the most recent being around the network bottleneck between the Ambry frontend and storage nodes. In this post, we will explain how we successfully adopted Netty-based HTTP/2 in our infrastructure service to solve the network bottleneck and examine the performance of the new stack.</p> \n   <h2>Ambry architecture review</h2> \n   <p>Before we discuss the network bottleneck, let’s first review the current Ambry architecture. Ambry includes frontends, storage nodes, and a cluster manager. The frontend nodes are stateless and provide HTTP interfaces for clients and route requests to the storage tier. The storage nodes perform data persistence, compaction, and replication, while the cluster manager manages cluster membership.</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor2.png\" alt=\"ambry-architecture-overview\" height=\"542\" width=\"900\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_1983575844\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p>Ambry provides a handle store API. Ambry clients send requests to an Ambry frontend chosen via a load balancing layer. On receiving a PUT request, the frontend generates a Blob ID, routes the request to storage servers for persistence, and then returns the Blob ID to the client. With this Blob ID, the client can request the data by sending a GET request.&#160;</p> \n   <p>There are several benefits of this design:</p> \n   <ol> \n    <li><p>The frontend tier is stateless and can be expanded easily. This affords cheaper frontend hosts that don’t need special hardware to support multiple disks.</p> </li> \n    <li><p>Multiple features can be implemented in frontends, including: encryption, compression, signed URL, ACL, and so on.&#160;</p> </li> \n    <li><p>Storage node design is simplified, as it only needs to take care of data persistence and not business logic.</p> </li> \n   </ol> \n   <p>Here is how PUT and GET requests are processed inside the Ambry cluster. For PUT, (1) the client sends a request to the frontend, (2) the frontend chooses a partition, (3) generates the Blob ID, and then (4) sends parallel requests to a configured number of replicas on storage nodes. (5) Once a quorum of successful responses are received, (6) the frontend returns a HTTP response to the client, along with the Blob ID. See diagram below:</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_800098939\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor3.png\" alt=\"put-request-diagram\" height=\"476\" width=\"800\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_1299895684\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p>For GET, (1) the client sends a request to the frontend, (2) the frontend determines partition based on Blob ID and then (3) sends requests to a configured number of replicas on storage nodes. (Note: with <a href=\"https://github.com/linkedin/ambry/blob/c8ca02acbbdcd8c44206f8ff2a1058d14d09c97b/ambry-router/src/main/java/com/github/ambry/router/AdaptiveOperationTracker.java\" target=\"_blank\">AdaptiveOperationTracker</a>, we can defer sending the second request until the first one takes longer than the 95th percentile of observed latency). As soon as one response is (4) returned successfully to the frontend, (5) the frontend returns the blob data to the client. See flow diagram below:</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_1614722001\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor4.png\" alt=\"get-request-diagram\" height=\"461\" width=\"800\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_1369383786\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p>The protocol between frontends and storage nodes was a TCP socket based custom protocol prior to the HTTP/2 refactoring described in this post. It comprised a TCP client in the frontend and a TCP server in the storage node.&#160;</p> \n   <p><a href=\"https://github.com/linkedin/ambry/blob/c8ca02acbbdcd8c44206f8ff2a1058d14d09c97b/ambry-network/src/main/java/com/github/ambry/network/SocketNetworkClient.java\" target=\"_blank\">The client</a> is in the frontend's router library, which is a non-blocking, multi-connection client based on the Java NIO <a href=\"https://docs.oracle.com/javase/7/docs/api/java/nio/channels/Selector.html\" target=\"_blank\">selector</a>. The client processes all requests and responses on a single thread asynchronously, and the number of clients is scalable. In practice, we create multiple router instances based on CPU cores.</p> \n   <p><a href=\"https://github.com/linkedin/ambry/blob/44a1a2eea62a5095bef570c55470ba7f5b3b04dd/ambry-network/src/main/java/com/github/ambry/network/SocketServer.java\" target=\"_blank\">The server</a> is a threading model based TCP server on storage nodes.</p> \n   <ul> \n    <li><p>1 Acceptor thread that handles new connections</p> </li> \n    <li><p>N Processor threads that each have their own selector and read requests from sockets</p> </li> \n    <li><p>M Handler threads that handle requests and produce responses back to the Processor threads for writing.</p> </li> \n   </ul> \n   <p>In addition, each Ambry frontend holds a pool of live connections to all storage nodes.&#160;</p> \n   <h2>Network stack bottleneck</h2> \n   <p>The design above worked well when there were hundreds of storage nodes and total storage capacity was less than 5 PB. However, with the growth of the cluster and SSL communication requirements, frontends became a bottleneck in our system. We encountered the following issues:</p> \n   <ul> \n    <li><p>Ran out of file descriptors because more connections were required.</p> \n     <ul> \n      <li><p>We solved this by changing the max allowed file descriptors from 200K to 500K.&#160;</p> </li> \n     </ul> </li> \n    <li><p>Ran out of connections.</p> \n     <ul> \n      <li><p>With our old stack, multiplexing was not supported. A connection would be fully occupied until a response was fully received. There were no issues when the connections were plain text based; however, with SSL, the connections were easily exhausted because of the SSL setup. We addressed this to some degree with 30 pre-warmed connections between each frontend and storage node. Subsequently, the increased connection occupancy due to SSL encryption/decryption continued to cause connection pool exhaustion, leading to latency deterioration during regular traffic and large spikes under heavy load.&#160;&#160;</p> </li> \n     </ul> </li> \n    <li><p>Experienced bad SSL performance from JDK.</p> \n     <ul> \n      <li><p>The Java SSLEngine is not CPU efficient. We also noticed it uses a small SSL packet (16 KB) compared to OpenSSL (64 KB), which likely impacts throughput. Here are <a href=\"https://www.infoq.com/presentations/apple-netty/\" target=\"_blank\">some comparisons presented by Netty’s author, Norman Maurer</a>.&#160;</p> </li> \n     </ul> </li> \n    <li><p>Needed network related performance tuning, especially after we enabled SSL.</p> \n     <ul> \n      <li><p>We had high SSL handshake latency in the beginning and added SSL connection warmup to mitigate the handshake latency.&#160;</p> </li> \n      <li><p>We ran out of memory because SSL encryption and decryption needed an extra memory copy, and solved this by increasing JVM memory size.</p> </li> \n      <li><p>Our SSLTransmission implementation issued small IOs, which impacted the overall performance. We batched these <a href=\"https://github.com/linkedin/ambry/pull/1105\" target=\"_blank\">small IOs</a> to address this issue.&#160;</p> </li> \n      <li><p>To compensate for SSL-induced latency, we <a href=\"https://github.com/linkedin/ambry/pull/1227\" target=\"_blank\">added a thread pool</a> for encryption and decryption to unblock the selector's polling thread.</p> </li> \n     </ul> </li> \n   </ul> \n   <p>We solved issues for 1 and 4, but the others were still fundamental bottlenecks that prevented us from expanding Ambry’s cluster size, supporting very large blobs, and providing optimal performance. Considering the above issues, we decided to refactor the Ambry network stack between the frontend and storage nodes.&#160;</p> \n   <h2>Ambry network stack refactor goals</h2> \n   <p>For a single frontend, the fundamental issues could be summarized as:</p> \n   <ol> \n    <li><p>Each frontend needs to maintain tens of live connections to each storage node, which will eventually prevent us from adding more storage nodes.</p> </li> \n    <li><p>SSL encryption and decryption are time consuming and bottleneck the overall throughput. A high performance SSL library would be expected to save resources and reduce latency.</p> </li> \n   </ol> \n   <p>Therefore, for the next-generation Ambry network stack, the primary design goals were:</p> \n   <ol> \n    <li><p>Improved SSL performance.</p> </li> \n    <li><p>Connection multiplexing for cluster scalability and performance.</p> </li> \n    <li><p>A high performance single-client-multiple-servers implementation in the frontend.&#160;</p> </li> \n    <li><p>A good network framework to reduce tuning time.&#160;</p> </li> \n   </ol> \n   <p><b>Our technology choice: Netty-based HTTP/2<br> </b>Netty was a natural candidate, as it’s a full-fledged framework and multiple products at LinkedIn (e.g., <a href=\"https://engineering.linkedin.com/blog/2017/02/building-venice-with-apache-helix\" target=\"_blank\">Venice</a>, <a href=\"https://engineering.linkedin.com/blog/2019/06/espresso-new-netty-framework\" target=\"_blank\">Espresso</a>, and <a href=\"https://linkedin.github.io/rest.li/Rest_li-with-Netty\" target=\"_blank\">Rest.li</a>) have used it for a long time. Netty also provides great performance due to its event-driven design and JNI-based SSLEngine, which met our goals well.&#160;</p> \n   <p>In order to reduce total connections, we looked into connection multiplexing. Instead of extending our current protocol to support multiplexing, we explored HTTP/2. HTTP/2 is a major revision of the <a href=\"https://en.wikipedia.org/wiki/HTTP\" target=\"_blank\">HTTP</a> network protocol. It enables the use of <a href=\"https://developers.google.com/web/fundamentals/performance/http2#request_and_response_multiplexing\" target=\"_blank\">connection multiplexing</a> and focuses on performance—specifically, latency, network, and server resource usage, which are all desired by Ambry as an infrastructure service. In addition, HTTP/2 has a binary framing layer for data encapsulation and transfer. This allows us to encapsulate our previous customized protocol into an HTTP/2 data frame without extensive refactoring.</p> \n   <p>Netty started to embed HTTP/2 implementation years ago, and now it (<a href=\"https://netty.io/news/2020/11/11/4-1-54-Final.html\" target=\"_blank\">netty 4.1.54-Final</a>) provides a robust HTTP/2 implementation that is easy to use.&#160;</p> \n   <p>Based on the above, we decided to use Netty-based HTTP/2 for our network stack refactoring.&#160;</p> \n   <h2>Design and implementation</h2> \n   <p>In the new stack, we refactored both the frontend and storage node network implementations.&#160;</p> \n   <p>On the storage nodes, we used <a href=\"https://livebook.manning.com/book/netty-in-action/chapter-3/1\" target=\"_blank\">Netty’s classic design pattern</a>. We added handlers to the Netty pipeline to handle inbound messages. At the end of the inbound pipeline, we pass requests to our existing IO threads for data fetch or persistence. Responses are eventually passed back via Netty outbound pipelines. The implementation can be found <a href=\"https://github.com/linkedin/ambry/blob/c8ca02acbbdcd8c44206f8ff2a1058d14d09c97b/ambry-server/src/main/java/com/github/ambry/server/StorageServerNettyFactory.java\" target=\"_blank\">on GitHub</a>.</p> \n   <p>On the frontend side, we created <a href=\"https://github.com/linkedin/ambry/blob/d42f78bf3a282f22ac01530451664720e84ea045/ambry-network/src/main/java/com/github/ambry/network/http2/Http2NetworkClient.java\" target=\"_blank\">Http2NetworkClient</a> and its related components to leverage Netty HTTP/2. Http2NetworkClient is a multi-connection, multiplexing, asynchronous client. It’s the core of this refactoring. The diagram below is a high level picture of it. </p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_871481071\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor5.png\" alt=\"architecture-of-http2-network-client\" height=\"620\" width=\"1200\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_1485853853\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p><b>Multi-connection<br> </b>In our design, each frontend establishes a fixed number of connections to a storage node. These connections are managed by <a href=\"https://github.com/linkedin/ambry/blob/c914b3befbbda6ec7ea4a2eeb2b47cdd84a59aab/ambry-network/src/main/java/com/github/ambry/network/http2/Http2MultiplexedChannelPool.java#L56\" target=\"_blank\">Http2MultiplexedChannelPool</a>. The number of connections per storage node is configurable and connections are selected in a round robin order.&#160;</p> \n   <p>Usually, a single connection from client to server is enough in the context of HTTP/2. However, Ambry is not a simple web app. As an infrastructure storage system, Ambry needs to provide:</p> \n   <ol> \n    <li><p><b>Availability:</b> we need to avoid single connection failure. Re-establishing a connection is time consuming and impacts availability.</p> </li> \n    <li><p><b>Throughput:</b> Ambry network throughput is huge. However, HTTP/2 is based on TCP, and a single TCP connection’s throughput is fundamentally limited by TCP flow control, TCP buffers, and a <a href=\"https://kinsta.com/blog/http3/#quic-http3\" target=\"_blank\">TCP-based head of line block issue</a>. Therefore, we create multiple connections to load-balance the overall throughput and mitigate any delay caused by the TCP layer.</p> </li> \n   </ol> \n   <p>Below is an experiment we did to compare throughput with different numbers of connections. (Experiment setup: same workload (moderate load) to a single frontend and the single frontend is connected to 300 storage nodes. All hosts’ NIC are 10 Gbps).</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceTable section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourcetable\"></a>\n </div> \n <div class=\"resource-table-section header-row\"> \n  <table> \n   <tbody>\n    <tr>\n     <td>Number of connections</td> \n     <td>Throughput (MB/s)</td> \n    </tr>\n    <tr>\n     <td>1</td> \n     <td>732</td> \n    </tr>\n    <tr>\n     <td>2</td> \n     <td>891</td> \n    </tr>\n    <tr>\n     <td>4</td> \n     <td>963</td> \n    </tr>\n    <tr>\n     <td>8</td> \n     <td>971</td> \n    </tr>\n   </tbody>\n  </table> \n </div> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_2101670220\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p><b>Multiplexing</b> <br> When Http2NetworkClient sends a request, it acquires a HTTP/2 stream channel from Http2MultiplexedChannelPool. Http2MultiplexedChannelPool manages the underlying multiplexing logic based on the Netty HTTP/2 implementation.&#160;</p> \n   <p><b>Asynchronous<br> </b>Http2NetworkClient has an asynchronous single thread. The thread sends all requests to the Netty pipeline and returns all the ready responses to the original dispatchers. Thanks to Netty’s eventloop and Promise, most of the operations (connection acquire, stream acquire, writeAndFlush, and so on) are done asynchronously without blocking the thread.&#160;</p> \n   <p><b>Customized protocol encapsulation <br> </b>Remember that Ambry previously used a custom protocol for TCP communication. For the refactor, we reused existing binary message formats and wrapped them in an HTTP/2 data frame. This simplified the migration because we were able to reuse the request parsing logic. In addition, we:</p> \n   <ol> \n    <li><p>Changed the protocol’s data holder from ByteBuffer to Netty’s ByteBuf. This allowed us to migrate to Netty’s memory management and send requests/responses via the Netty pipeline.&#160;</p> </li> \n    <li><p>Introduced <a href=\"https://github.com/linkedin/ambry/blob/b25e9077fc5b97798b06d6b4c521b86838c97cad/ambry-network/src/main/java/com/github/ambry/network/http2/AmbrySendToHttp2Adaptor.java\" target=\"_blank\">AmbrySendToHttp2Adaptor</a> to slice the protocol’s data to HTTP/2 data frames.</p> </li> \n   </ol> \n   <p><b>Leveraging open source<br> </b>For the frontend’s implementation, we leveraged open-source code from <a href=\"https://github.com/aws/aws-sdk-java-v2/tree/master/http-clients/netty-nio-client/src/main/java/software/amazon/awssdk/http/nio/netty/internal/http2\" target=\"_blank\">here</a>, which provided a good starting point for us to implement multiple connection and multiplexing HTTP/2 clients.&#160;</p> \n   <h2>Rollout</h2> \n   <p>To avoid cluster-level failure and data inconsistency issues, we used dual-protocol mode on storage nodes to smooth the rollout.</p> \n   <p>On servers, our plan was to serve old socket-based requests and new HTTP/2 requests at the same time with different ports, so storage nodes could simultaneously handle both types of requests. On the frontend, we canaried hosts by enabling HTTP/2-based requests and rolled back defective changes (if needed) by switching back to socket-based requests.&#160;&#160;</p> \n   <p>With this rollout plan, we gradually validated critical changes and successfully enabled HTTP/2 stack in all of our production clusters.&#160;</p> \n   <h2>Performance</h2> \n   <p><b>Latency with regular load<br> </b>In production hosts, we observed lower latency after switching to the HTTP/2 stack (red curve). Memory and CPU utilization were also reduced.</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_1642671266\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor6.png\" alt=\"graph-showing-lower-latency-after-switching-to-the-http2-stack\" height=\"352\" width=\"900\"> </li> \n </ul> \n</div> \n<div class=\"resourceTable section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourcetable_2022915034\"></a>\n </div> \n <div class=\"resource-table-section header-row\"> \n  <table> \n   <tbody>\n    <tr>\n     <td>&#160;</td> \n     <td>Memory (heap and off-heap)</td> \n     <td>CPU</td> \n     <td>Router to server latency P95</td> \n    </tr>\n    <tr>\n     <td>Socket SSL</td> \n     <td>6.6 GB</td> \n     <td>7-10%</td> \n     <td>100 ms</td> \n    </tr>\n    <tr>\n     <td>HTTP/2 SSL</td> \n     <td>4.9 GB</td> \n     <td>4%</td> \n     <td>38 ms</td> \n    </tr>\n   </tbody>\n  </table> \n </div> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_897961709\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p>The improvement is expected. After all, one of the reasons we use Netty is its JNI-based SSLEngine. Its performance is much better than Java’s default SSLEngine (see <a href=\"https://www.infoq.com/presentations/apple-netty/\" target=\"_blank\">here</a> for Norman’s presentation on this topic). In this case, Netty's SSLEngine reduces SSL encryption/decryption cost and lowers latency and hardware resource usage.</p> \n   <p><b>Latency with load test <br> </b>We performed multiple tests with different loads (We send traffic to frontends and trigger different GET rates between frontend and storage nodes) to compare the performance between the socket stack and HTTP/2 stack. The result is shown in Figure 1. With the HTTP/2 stack, the latency is much lower compared to the socket SSL implementation. The improvement comes from both Netty’s efficient SSLEngine and HTTP/2 multiplexing.</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_99457237\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor7.png\" alt=\"frontend-to-storage-nodes-get-latency-graph\" height=\"624\" width=\"900\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_694012182\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p style=\"text-align: center;\"><i>Figure 1</i></p> \n   <p><b>Stability/Connection multiplexing<br> </b>With the socket SSL stack, a big issue was connection exhaustion between the frontend and storage nodes. Once connections are exhausted, new requests need to wait for available connections from the connections pool. We increased the pool size multiple times, but that also required increasing the Java heap, which resulted in more GC work.&#160;</p> \n   <p>Connection exhaustion caused latency spikes in our production cluster. See Figure 2 and Figure 3 below. Figure 2 shows connection exhaustion occurrences. The blue curve in Figure 3 is the latency between the frontend and storage nodes. As you can see, latency spikes are highly correlated to connection exhaustion.</p> \n   <p>With HTTP/2 multiplexing, the number of connections is no longer a problem and we see significant improvement on latency spikes. See the red curve in Figure 3.</p> \n   <p>In the socket stack, 30 connections were used between each frontend and storage node.&#160;</p> \n   <p>In the HTTP/2 stack, only 4 HTTP/2 connections are used.</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_1213634036\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor8.png\" alt=\"chart-showing-connection-exhaustion-occurrences\" height=\"459\" width=\"900\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_89344340\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p style=\"text-align: center;\"><i>Figure 2</i></p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_1975233672\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor9.png\" alt=\"graph-showing-latency-spikes-are-highly-correlated-to-connection-exhaustion\" height=\"458\" width=\"900\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_1317306198\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p style=\"text-align: center;\"><i>Figure 3</i></p> \n   <p><b>TCP buffer size tuning<br> </b>On the TCP level, each TCP connection has a buffer at the OS level for the sender to write and the receiver to read. The default buffer size is usually between 128 KB and 512 KB, depending on the operating system. However, Ambry blobs are big (4 MB) in some of our use cases and HTTP/2 multiplexing shares TCP connections. It’s good to use a bigger TCP buffer size to avoid data getting stuck at the application level.&#160;</p> \n   <p>In production, we use a 4 MB TCP buffer size.&#160;</p> \n   <p><b>HTTP/2 tuning<br> </b>We tuned two HTTP/2 parameters: <a href=\"https://netty.io/4.1/api/io/netty/handler/codec/http2/Http2Settings.html#initialWindowSize-int-\" target=\"_blank\">initial window size</a> in HTTP/2 stream flow control and HTTP/2 frame size.&#160;</p> \n   <p>The default initial value for the flow control window is 64 KB and default frame size is 16 KB. In Ambry, blob sizes are in the MB range. Data may be split to multiple frames and need multiple round trips to transfer. To avoid multiple round trips for a single request, we increased the initial window size and max frame size to 4 MB.</p> \n   <p>However, when we made this change, it didn’t seem to take effect, because we still saw small <a href=\"https://github.com/netty/netty/issues/10193#issuecomment-616345883\" target=\"_blank\">frames</a>. We looked into it and found that we needed to address another important setting: WriteBufferHighWaterMark, which is a threshold to decide if a channel is writable or not. The Netty HTTP/2 implementation uses min(window size, WriteBufferHighWaterMark) to decide actual frame size. With the change to WriteBufferHighWaterMark, we were finally able to see large frames.</p> \n   <p>Figure 4 is a performance comparison between the 64 KB initial window size and 4 MB initial window size. Both are with 4 MB WriteBufferHighWaterMark and 4 MB TCP buffer size.</p> \n  </div> \n </div> \n</div> \n<div class=\"resourceImageBlock section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceimageblock_594487340\"></a>\n </div> \n <ul class=\"resource-image-block single\"> \n  <li class=\"resource-image\"> <img src=\"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor10.png\" alt=\"performance-comparison-between-the-64-kb-initial-window-size-and-4-mb-initial-window-size\" height=\"499\" width=\"900\"> </li> \n </ul> \n</div> \n<div class=\"resourceParagraph section\"> \n <div class=\"component-anchor-container\">\n  <a class=\"component-anchor\" name=\"post_par_resourceparagraph_992473039\"></a>\n </div> \n <div class=\"resource-text-section\"> \n  <div class=\"resource-paragraph rich-text\"> \n   <p style=\"text-align: center;\"><i>Figure 4</i></p> \n   <h2>Netty best practices</h2> \n   <p>The <a href=\"https://www.youtube.com/watch?v=_GRIyCMNGGI\" target=\"_blank\">Netty best practices presentation</a> made by Norman Maurer points out many good practices in Netty-based development, and may be useful for others who want to avoid pitfalls in implementations.&#160;</p> \n   <h2>Summary</h2> \n   <p>With HTTP/2, we successfully solved Ambry’s network bottlenecks between Ambry frontends and Ambry storage nodes. The new stack saves CPU and memory, as well as reducing latency. The benefits come from both Netty (efficient eventloop, OpenSSL integration, and memory management) and HTTP/2 multiplexing. This allows us to scale Ambry to a much bigger cluster size and enables throughput-oriented (as opposed to latency-oriented) use cases. Now, the HTTP/2 stack has been fully deployed in our production clusters for months and has been operating stably.</p> \n   <h2>Acknowledgements</h2> \n   <p>This took an immense amount of work across the Ambry team. Specific shout outs to <a href=\"https://www.linkedin.com/in/caseygetz/\" target=\"_blank\">Casey Getz</a> and <a href=\"https://www.linkedin.com/in/justin-lin-64a29049/\" target=\"_blank\">Justin Lin</a> for their knowledge and help in Ambry socket stack and Netty; <a href=\"https://www.linkedin.com/in/matthewrswise/\" target=\"_blank\">Matthew Wise</a>, <a href=\"https://www.linkedin.com/in/asinghai/\" target=\"_blank\">Ashish Singhai</a>, and <a href=\"https://www.linkedin.com/in/lightningrob/\" target=\"_blank\">Rob Block</a> for providing suggestions on design and feedback to this blog post; and all Ambry Team members: <a href=\"https://www.linkedin.com/in/matthewrswise/\" target=\"_blank\">Matthew Wise</a>, <a href=\"https://www.linkedin.com/in/lightningrob/\" target=\"_blank\">Rob Block</a>, <a href=\"https://www.linkedin.com/in/caseygetz/\" target=\"_blank\">Casey Getz</a>, <a href=\"https://www.linkedin.com/in/justin-lin-64a29049/\" target=\"_blank\">Justin Lin</a>, <a href=\"https://www.linkedin.com/in/yingyi-zhang-a0a164111/\" target=\"_blank\">Yingyi Zhang</a>, <a href=\"https://www.linkedin.com/in/xiang-sophie-guo-3b316743/\" target=\"_blank\">Sophie Guo</a>, <a href=\"https://www.linkedin.com/in/erankuragr/\" target=\"_blank\">Ankur Agrawal</a>, and <a href=\"https://www.linkedin.com/in/arun-sai-0447911a/\" target=\"_blank\">Arun Sai</a> for brainstorming and ideas. </p> \n  </div> \n </div> \n</div>","descriptionType":"html","publishedDate":"Tue, 24 Aug 2021 17:00:00 +0000","feedId":13630,"bgimg":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor1.png","linkMd5":"49426fb4e4596e9da310e14ca1fbebe1","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn89@2020_3/2021/09/30/14-51-45-857_bad822e9f57092f1.webp","destWidth":700,"destHeight":363,"sourceBytes":73841,"destBytes":15552,"author":"Ze Mao","articleImgCdnMap":{"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor2.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn97@2020_1/2021/09/30/14-51-46-003_a8b0296ceca8cd59.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor3.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn5@2020_5/2021/09/30/14-51-45-868_fea9057443b1ce17.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor4.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn2@2020_4/2021/09/30/14-51-46-027_eab356e6c452eff8.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor5.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn92@2020_5/2021/09/30/14-51-45-950_413e90bba92fb449.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor6.png":null,"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor7.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn13@2020_1/2021/09/30/14-51-46-126_a65e108301997e45.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor8.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn25@2020_2/2021/09/30/14-51-45-876_c9f17de21167c59a.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor9.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn21@2020_5/2021/09/30/14-51-46-007_ada8317538e5cdf9.webp","https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor10.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn10@2020_4/2021/09/30/14-52-46-123_d2a4ccfe49bf6f61.webp"},"publishedOrCreatedDate":1633013505756}],"record":{"createdTime":"2021-09-30 22:51:45","updatedTime":"2021-09-30 22:51:45","feedId":13630,"fetchDate":"Thu, 30 Sep 2021 14:51:45 +0000","fetchMs":76,"handleMs":18,"totalMs":60952,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"245d88d7b2d8f26704713c23b090d029","hostName":"us-038*","requestId":"1888d7b9e0f54e51bf07fd47e08ea5ed_13630","contentType":"text/xml;charset=utf-8","totalBytes":215638,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":9,"articlesImgsGithubTotal":8,"successGithubMap":{"myreaderx8":1,"myreaderx7":1,"myreaderx15":1,"myreaderx6":1,"myreaderx32":1,"myreaderx22":1,"myreaderx2":1,"myreaderx30":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1}},"feed":{"createdTime":"2020-08-25 04:38:45","updatedTime":"2020-09-05 16:55:07","id":13630,"name":"LinkedIn Engineering","url":"https://engineering.linkedin.com/blog.rss.html","subscriber":227,"website":null,"icon":"https://static.licdn.com/scds/common/u/images/logos/favicons/v1/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx61/cdn75@2020_6/2020/09/05/08-55-05-775_2a156552121ff50f.ico","description":"The official blog of the Engineering team at LinkedIn","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-09-30 22:52:46","updatedTime":"2021-09-30 22:52:46","id":null,"feedId":13630,"linkMd5":"49426fb4e4596e9da310e14ca1fbebe1"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":215638,"tmpBgImgCdnBytes":0,"extra4":{"start":1633013505653,"total":0,"statList":[{"spend":85,"msg":"获取xml内容"},{"spend":18,"msg":"解释文章"},{"spend":346,"msg":"上传封面图到cdn"},{"spend":1,"msg":"修正封面图上传失败重新上传"},{"spend":60847,"msg":"正文链接上传到cdn"}]},"extra5":9,"extra6":9,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor6.png","sourceStatusCode":200,"destWidth":900,"destHeight":352,"sourceBytes":152052,"destBytes":40982,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":508,"convertSpendMs":15,"createdTime":"2021-09-30 22:51:45","host":"europe-22*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn18/contents/2021/09/30/14-51-46-020_dfdbb4229299a0da.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Thu, 30 Sep 2021 14:51:46 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A0B4:8483:7306C2:75AEAC:6155CF02"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1633016427"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn18/contents/2021/09/30/14-51-46-020_dfdbb4229299a0da.webp","historyStatusCode":[],"spendMs":352},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"148.5 KB","destSize":"40 KB","compressRate":"27%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor6.png","sourceStatusCode":200,"destWidth":900,"destHeight":352,"sourceBytes":152052,"destBytes":40982,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":151,"convertSpendMs":19,"createdTime":"2021-09-30 22:52:16","host":"us-009*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn18/contents/2021/09/30/14-52-16-535_dfdbb4229299a0da.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Thu, 30 Sep 2021 14:52:16 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["8686:13E3:310D88:BF4D3E:6155CF20"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1633016427"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn18/contents/2021/09/30/14-52-16-535_dfdbb4229299a0da.webp","historyStatusCode":[],"spendMs":33},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"148.5 KB","destSize":"40 KB","compressRate":"27%"},null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://europe-56.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-037.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe68.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-021.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe-60.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor1.png","sourceStatusCode":200,"destWidth":700,"destHeight":363,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn89@2020_3/2021/09/30/14-51-45-857_bad822e9f57092f1.webp","sourceBytes":73841,"destBytes":15552,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":337,"convertSpendMs":22,"createdTime":"2021-09-30 22:51:45","host":"us-004*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"72.1 KB","destSize":"15.2 KB","compressRate":"21.1%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor8.png","sourceStatusCode":200,"destWidth":900,"destHeight":459,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn25@2020_2/2021/09/30/14-51-45-876_c9f17de21167c59a.webp","sourceBytes":71937,"destBytes":23592,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":386,"convertSpendMs":14,"createdTime":"2021-09-30 22:51:45","host":"us-037*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"70.3 KB","destSize":"23 KB","compressRate":"32.8%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor3.png","sourceStatusCode":200,"destWidth":800,"destHeight":476,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn5@2020_5/2021/09/30/14-51-45-868_fea9057443b1ce17.webp","sourceBytes":81718,"destBytes":25228,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":393,"convertSpendMs":14,"createdTime":"2021-09-30 22:51:45","host":"us-033*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"79.8 KB","destSize":"24.6 KB","compressRate":"30.9%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor5.png","sourceStatusCode":200,"destWidth":1200,"destHeight":620,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn92@2020_5/2021/09/30/14-51-45-950_413e90bba92fb449.webp","sourceBytes":126841,"destBytes":33554,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":452,"convertSpendMs":66,"createdTime":"2021-09-30 22:51:45","host":"us-005*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"123.9 KB","destSize":"32.8 KB","compressRate":"26.5%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor7.png","sourceStatusCode":200,"destWidth":900,"destHeight":624,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn13@2020_1/2021/09/30/14-51-46-126_a65e108301997e45.webp","sourceBytes":91441,"destBytes":18478,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":605,"convertSpendMs":20,"createdTime":"2021-09-30 22:51:45","host":"us-009*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"89.3 KB","destSize":"18 KB","compressRate":"20.2%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor9.png","sourceStatusCode":200,"destWidth":900,"destHeight":458,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn21@2020_5/2021/09/30/14-51-46-007_ada8317538e5cdf9.webp","sourceBytes":101387,"destBytes":35088,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":609,"convertSpendMs":17,"createdTime":"2021-09-30 22:51:45","host":"europe-60*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"99 KB","destSize":"34.3 KB","compressRate":"34.6%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor2.png","sourceStatusCode":200,"destWidth":900,"destHeight":542,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn97@2020_1/2021/09/30/14-51-46-003_a8b0296ceca8cd59.webp","sourceBytes":154075,"destBytes":38760,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":625,"convertSpendMs":19,"createdTime":"2021-09-30 22:51:45","host":"europe68*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"150.5 KB","destSize":"37.9 KB","compressRate":"25.2%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor4.png","sourceStatusCode":200,"destWidth":800,"destHeight":461,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn2@2020_4/2021/09/30/14-51-46-027_eab356e6c452eff8.webp","sourceBytes":103070,"destBytes":25072,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":733,"convertSpendMs":15,"createdTime":"2021-09-30 22:51:45","host":"europe-56*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"100.7 KB","destSize":"24.5 KB","compressRate":"24.3%"},{"code":1,"isDone":false,"source":"https://content.linkedin.com/content/dam/engineering/site-assets/images/blog/posts/2021/08/ambryrefactor10.png","sourceStatusCode":200,"destWidth":900,"destHeight":499,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn10@2020_4/2021/09/30/14-52-46-123_d2a4ccfe49bf6f61.webp","sourceBytes":83932,"destBytes":15866,"targetWebpQuality":75,"feedId":13630,"totalSpendMs":591,"convertSpendMs":18,"createdTime":"2021-09-30 22:52:45","host":"europe-56*","referer":"https://engineering.linkedin.com/blog/2021/http-2-in-infrastructure--ambry-network-stack-refactoring","linkMd5ListStr":"49426fb4e4596e9da310e14ca1fbebe1","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"82 KB","destSize":"15.5 KB","compressRate":"18.9%"}],"successGithubMap":{"myreaderx8":1,"myreaderx7":1,"myreaderx15":1,"myreaderx6":1,"myreaderx32":1,"myreaderx22":1,"myreaderx2":1,"myreaderx30":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1}}