{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-04-05 15:36:43","updatedTime":"2021-04-05 15:36:43","title":"A Visual History of Interpretation for Image Recognition","link":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","description":"<figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://lh3.googleusercontent.com/6eWc600ZsRUesxmAmk20FT0vMjrv0qh6wC_o2CNIewTAsag_NFwLlswuVwh8oOsGlWJTbaavRKjBDSJ8kIygYE1Xy7_fTy82Egpl0hHMCtKvklp24IjaZIttQduMpPCMQtxu1fBd\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><img src=\"https://thegradient.pub/content/images/2021/01/main.png\" alt=\"A Visual History of Interpretation for Image Recognition\"><p>Image recognition (i.e. classifying what object is shown in an image) is a core task in computer vision, as it enables various downstream applications (automatically tagging photos, assisting visually impaired people, etc.), and has become a standard task on which to benchmark machine learning (ML) algorithms. Deep learning (DL) algorithms have, over the past decade, emerged as the most competitive image recognition algorithms; however, they are by default “black box” algorithms: it is difficult to explain <em>why </em>they make a specific prediction.</p><p>Why is that an issue? Users of ML models often want the ability to interpret which parts of the image led to the algorithm’s prediction for many reasons:</p><ul><li>Machine learning developers can analyze interpretations to <strong>debug models</strong>, <strong>identify biases</strong>, and predict whether the model is likely to <strong>generalize </strong>to new images</li><li>Users of machine learning models may <strong>trust a model</strong> more if provided explanations for why a specific prediction was made</li><li><strong>Regulations </strong>around ML such as GDPR require some algorithmic decisions to be explainable in human terms</li></ul><p>Motivated by these use cases, during the last decade, researchers developed many different methods to open the “black box” of deep learning, aiming to make underlying models more explainable. Some methods are specific for certain kinds of algorithms, while some are general. Some are fast, and some are slow.</p><p>In this piece, we provide an overview of the interpretation methods invented for image recognition, discuss their tradeoffs and provide examples and code to try them out yourself using <a href=\"http://www.gradio.app\">Gradio</a>.</p><p><strong>Leave-One-Out</strong></p><p>Before we dig into the research, let’s start with a very basic algorithm that works for <em>any</em> type of image classification: leave-one-out (LOO).</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/zRDuOSnF29pcOjBzVD1_AYYuUYtC_9diaBv_wv0KiJ21edQ2b-2SH9tmMXyYNTp4kRQ8PYEs-Qf1jIgmkpbCEme3lLZtyNSv3XbhGhJr5_gczhC8OrsmxDmqqIdEJ4x6-1o4tN4x\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>LOO is an easy method to understand; it’s the first algorithm you might come up with if you were to design an interpretation method from scratch. The idea is to first segment the input image into a bunch of smaller subregions. Then, you run a series of predictions, each time masking (i.e. setting the pixel values to zero of) one of the subregions. Each region is assigned an importance score based on how much its “being masked” affected the prediction relative to the original image. Intuitively, these scores quantify which regions are most responsible for the prediction.</p><p>So if we segment our image into 9 subregions in a 3x3 grid, here’s what LOO would look like:</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://lh5.googleusercontent.com/8iCNWTgKlLhMzuH8wJmIRGkJJjX4v_b8N_86uDta2DFLuqqnk1Bqba7dRjod6Ek1YkTz3UDZttggA17aB42Fs9EYQhWLIXhhtq1qNhDwbK1Y-AenyAbXDqOByawVZSBbZ6t7CyPf\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>The darkest red squares are the ones that most changed the output, whereas the lightest had the least effect. In this case, when the top-center region was masked, the prediction confidence dropped the most, from an initial 95% to 67%.</p><p>If we segment in a better way (for example, using superpixels instead of grids), we get a pretty reasonable saliency map, which highlights the face, ears and tail of the doberman.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/cXkB8FYxeaXmovUwJ3-rdkgI2vyfxnBX4JQOknji5da9moTGwGkZs3jS4shGUfKSLKVPn8gMZlCyQ6Pkg2bfa3oe4BXA-YIQXe5E7Fha6KufNj9qXNVDDSif4rWXrVWsNp30EowD\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>LOO is a simple yet powerful method. Depending on the image resolution and how the segmentation is done, it can generate very accurate and useful results. Here is LOO applied to a 1100 × 825 image of a Golden Retriever as predicted using InceptionNet.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/KQvbWEkCQKrgjNw4F1p_eUczLUBscjQ7edpIbtCJJ93mzBsFmyfQPLX79jHYB8Oh1h1wKGsMNQVq-2IfjVJtopUBL0uGAaKZ9A2V9FiOP5fmVgGh6g7Su0uDlImfCeN68-9PMTS_\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>One huge advantage of LOO in practice is that it doesn’t need any access to the internals of the model and can even work on other computer vision tasks besides recognition, making it a flexible all-purpose-tool.</p><p>So what are the drawbacks? First, it’s slow. Every time a region is masked, we run inference on the image. To get a saliency map with reasonable resolution, your mask size might have to be small. So, if you segment the image into 100 regions, it will take 100x the inference time to get the heat map. On the other hand, if you have too many subregions, masking any one of them will not necessarily produce a big difference in the prediction. This LOO’s second limitation, which is that it does not take into account interdependencies between regions.</p><p>So, let’s look at a much faster and slightly more involved technique: <em>(vanilla) gradient ascent.</em></p><p><strong>Vanilla Gradient Ascent [2013]</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/dvBTxWEYHpQw_eqk9DV5ohf_H-IOmfRhZ8gzZvNhbHMVSeb_LuQ_KtQ9mU29kOe9x3H5NpdgGC7r2VkvKyxkJ-HXLymWgRJY84974QactaDnY6JH-RPTZybzuA0jP4nxbO6QtbuJ\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>(Vanilla) gradient ascent was presented in the <a href=\"https://arxiv.org/abs/1312.6034\">Visualizing Image Classification Models and Saliency Maps [2013]</a> paper.  There is a conceptual relationship between LOO and gradient ascent. With LOO, we considered how the output changed when we masked each region in the image, one by one. With gradient ascent, we calculate how the output is affected by each individual pixel, all at once. How do we do this? With a modified version of backpropagation.</p><p>With standard back-propagation, we compute the gradient of the model’s loss with respect to the weights. The gradient is a vector that contains a value for each weight, reflecting how much a small change in that weight will affect the output, essentially telling us which weights are most important for the loss. By taking the negative of this gradient, we minimize the loss during training. For gradient ascent, we instead take the gradient of the <em>class score </em>with respect to the <em>input pixels</em>, telling us which input pixels are most important in classifying the image. This single step through the network gives us an importance value for each pixel, which we display in the form of a heatmap, like shown below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh3.googleusercontent.com/YZg2IK3On5szm2NtuvXpbAfVlwVx-5B1c88okt5lHBc0OYuvpLvUsewkoiOS0XQSJC3vlLoxDqR9CYCT9mTdKmzfdzCv6yMQUomhHG5eRQneP4BkDVGQn6dK135LqLxexwkQUn2I\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"><figcaption>Examples of saliency maps from <a href=\"https://arxiv.org/abs/1312.6034\">Simonyan et al</a>, computed with a single back-propagation pass</figcaption></figure><p>Here’s what that would look like for our Doberman image:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh6.googleusercontent.com/lxZUn3mRjWd4vZkVGj9bP2XNZwaoJi7n_5D_3ge6QhWolcKmo8i7QYbbBAJkuDsgJirHdkbIB0YEaWHiBkd7cHKQ9keQs1CQzpSh-OY2i3ZJ5-ulvUbcpnVDT7oWGmIXwWK_0DAo\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>The main advantage here is speed; because we only need to make one pass through the network, gradient ascent is much faster than LOO, although the resulting heatmap is a little grainy.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://thegradient.pub/content/images/2021/01/image.png\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\" srcset=\"https://thegradient.pub/content/images/size/w600/2021/01/image.png 600w, https://thegradient.pub/content/images/size/w1000/2021/01/image.png 1000w, https://thegradient.pub/content/images/2021/01/image.png 1114w\" sizes=\"(min-width: 720px) 720px\"><figcaption>LOO (left) compared to Vanilla Gradient Ascent (right) on an image of a doberman. Here the model is InceptionNet.</figcaption></figure><p>Although gradient ascent works, it was discovered that this original formulation, called <em>vanilla </em>gradient ascent, has a significant shortcoming: it propagates negative gradients, which end up causing interference and a noisy output. A new method, “guided backpropagation,” was proposed to fix these issues.</p><p><strong>Guided Back-Propogation [2014]</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/8L11NqXXV5g9_GZsyEJmbGMC8wb3cgaIIkAgrfuc5cxh6d4YV9GMMIiS03nU4U-wMaTFaf4IUvy8FpheY7dYdfR6d1s12bOHPw-Z-V6NHDakoex7sIW5LIHa0XPiseDWuOyGtXmp\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>Guided back-propagation was published in <a href=\"https://arxiv.org/abs/1412.6806\">Striving for Simplicity: The All Convolutional Net [2014]</a>, where the authors proposed adding an additional guidance signal from the higher layers to the usual step of back-propagation. In essence, the method blocks the backward flow of gradients from neurons whenever the output is negative, leaving only those gradients that result in increased output, which ultimately results in a less noisy interpretation.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh4.googleusercontent.com/JBAabUm4M2008pak7rrIMY6P-LRso41jQdfX01v6ohGp4aQ2wj1znzJKBkU8w6_wPSIpQA3sLFF0IbGikF68NWAs-GRQfRENgzH3HdDBmpV4HRdUVjZPi3h5lgLvY84c5hdJWDf1\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"><figcaption>In this image, we show the back-propagated gradients (left) for a given layer whose outputs are shown on the right. In the top layer, we show the regular gradients. In the bottom layer, we show the guided back-propagation, which zeros out the gradients whenever the output is negative. (figure taken from <a href=\"https://arxiv.org/abs/1412.6806\">Springenberg et al</a>)</figcaption></figure><p>Guided back-propagation works about as fast as vanilla gradient ascent, since it only requires one pass through the network, but usually produces a cleaner output, especially around the edges of the object. The method works particularly well relative to other methods in neural architectures that don’t have max-pooling layers.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://thegradient.pub/content/images/2021/01/image-1.png\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\" srcset=\"https://thegradient.pub/content/images/size/w600/2021/01/image-1.png 600w, https://thegradient.pub/content/images/size/w1000/2021/01/image-1.png 1000w, https://thegradient.pub/content/images/2021/01/image-1.png 1132w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Vanilla Gradient Ascent (left) compared to Guided Back-Propagation (right) on an image of a doberman. Here the model is InceptionNet.</figcaption></figure><p>However, it was discovered that there is still a major issue with vanilla gradient ascent and guided back-propagation: they don’t work so well when there are two or more classes present in the image, which often occurs with natural images.<br></p><p><strong>Grad-CAM [2016]</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/HrMjWr8KBZqpH7zTPIPug-hrWM6oPyTirFfuLcmV4-SKxNzT_UzGlo-h4IwbAew5MOvKUuJxewDc3cZx2iOYoUjjECnkoNheyZlg5qpClpRJJAnkM16z32aYsFcF_oSdQHACdrqB\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>Enter Grad-CAM, or Gradient-Weighted Class Activation Mapping, presented in <a href=\"https://arxiv.org/abs/1610.02391\">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</a> [2016]. Here the authors found that the quality of interpretations improved when the gradients were taken at each filter of the last convolutional layer, instead of at the class score (but still with respect to the input pixels). To get a class-specific interpretation, Grad-CAM does a weighted average of these gradients, with the weight based on a filter’s contribution to the class score. The result, as shown below, is far better than guided back-propagation alone.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh4.googleusercontent.com/vaSiaHSaODodKmHjorhnKtDUpt6dmFpW9h_NW18v33uwfFwgzt0AgsjT8xBaYL4kIQSXx8GEyVGcb1YCv_YHVZV1P3f1tEL4TkYSFMNqDzw_3AmG4Jghm0MNUFfZ58OHAuUeHOdX\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"><figcaption>The original image which has two classes (‘Cat’ and ‘Dog’) is passed through guided back-propagation, but the resulting heatmap highlights both classes. Once Grad-CAM is applied as a filter, guided Grad-CAM produces a high resolution, class-discriminative heatmap. (figure taken from <a href=\"https://arxiv.org/abs/1610.02391\">Selvaraju et al</a>)</figcaption></figure><p>The authors further generalized Grad-CAM to work not just with a target class, but with any target “concept.” This meant that Grad-CAM could be used to interpret why an image captioning model predicted a particular caption, or even handle models that take several inputs, like a visual question-answering model. As a result of this flexibility, Grad-CAM has become quite popular. Below is an overview of its architecture.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://lh5.googleusercontent.com/sQqLKA5fqJwAdsWuOb7ui0VTqZ5ECUNqhF-z-QYl0rf5sIu43GBhY8EFeMIgWxQxGdX8DMCY96ljg1jwMTPPsioi7twL_QGn7k7Tplwej6a5_HmN2qedsukMT1V44XrMg1UMAJ7G\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"><figcaption>Grad-CAM overview: First, we forward propagate the image. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then back-propagated to the rectified convolutional feature maps of interest, which we combine to compute the coarse Grad-CAM localization (blue heatmap) which represents where the model has to look to make the particular decision. Finally, we pointwise multiply the heatmap with guided backpropagation to get Guided Grad-CAM visualizations which are both high-resolution and concept-specific. (figure and description taken from <a href=\"https://arxiv.org/abs/1610.02391\">Selvaraju et al</a>)</figcaption></figure><p><strong>SmoothGrad [2017]</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh4.googleusercontent.com/MRQpyhs8c1A3ftX7M549wbYsy278UOFmFs536pIMj_rJOuf3rE_XRNxCHAH41fz4ABqN88tufIGpm1Ol_GFRdwnx2Ifk-n8kVQe9pFmVku2tU2OWsXDjmlXOv5C-_ba531kVPyw_\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>Yet, you may have noticed that with all the previous methods, the results are still not very sharp. SmoothGrad, presented in <a href=\"https://arxiv.org/abs/1706.03825\">SmoothGrad: removing noise by adding noise [2017]</a>], is a modification of previous methods. The idea is quite simple: the authors noted that if the input image is first perturbed with noise, you can compute gradients once for each version of the perturbed input and then average the sensitivity maps together. This leads to a much sharper result, albeit with a longer runtime.</p><p>Here’s how Guided Back-Propagation looks next to SmoothGrad:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://thegradient.pub/content/images/2021/01/image-2.png\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\" srcset=\"https://thegradient.pub/content/images/size/w600/2021/01/image-2.png 600w, https://thegradient.pub/content/images/size/w1000/2021/01/image-2.png 1000w, https://thegradient.pub/content/images/2021/01/image-2.png 1160w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Standard Guided Back-Propagation (left) vs. SmoothGrad (right) on an image of a doberman. Here the model is InceptionNet.</figcaption></figure><p>When you’re faced with all of these interpretation methods, which one do you choose? Or when methods conflict, is there one method that can be proven theoretically to be better than the others? Let’s look at integrated gradients.</p><p><strong>Integrated Gradients [2017]</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/Qyri3unenVF2mCqFwGGk2MQAVHFcI1laNRU4NHJzpHlqiCOfkNMnrNKzzPOsqYlPbwlh0Ky4-dUaqByLsHEiGDUqYbfxbwePxlRsFe4wlCebGMvUwsIxdODsdoEXVc5yG0LmFKdb\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>Unlike previous papers, the authors of <a href=\"https://arxiv.org/abs/1703.01365\">Axiomatic Attribution for Deep Networks</a> [2017] start from a theoretical basis of interpretation. They focus on two axioms: sensitivity and implementation invariance, that they posit a good interpretation method should satisfy.</p><p>The Axiom of Sensitivity means that if two images differ in exactly one pixel (yet have all of the other pixels in common), and produce different predictions, the interpretation algorithm should give non-zero attribution to that different pixel. The Axiom of Implementation Invariance means that the underlying implementation of the algorithm should not affect the result of the interpretation method. They use these principles to guide the design of a new attribution method called Integrated Gradients (IG).</p><p>IG begins with a baseline image (usually a completely darkened version of the input image), and increases the brightness until the original image is recovered. The gradients of the class scores with respect to the input pixels are computed for each image and are averaged to get a global importance value for each pixel. Beyond the theoretical properties, IG thereby also solves another problem with vanilla gradient ascent: saturated gradients. Because gradients are <em>local</em>, they don’t capture the global importance of pixels, only the sensitivity at a particular input point. By varying the brightness of the image and computing the gradients at different points, IG is able to get a more complete picture of the importance of each pixel.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://thegradient.pub/content/images/2021/01/image-3.png\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\" srcset=\"https://thegradient.pub/content/images/size/w600/2021/01/image-3.png 600w, https://thegradient.pub/content/images/size/w1000/2021/01/image-3.png 1000w, https://thegradient.pub/content/images/2021/01/image-3.png 1152w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Standard Guided Back-Propagation (left) vs. Integrated Gradients (right) on an image of a doberman, both smoothed with SmoothGrad. Here the model is InceptionNet.</figcaption></figure><p>Although this usually produces more accurate sensitivity maps, the method is slower and introduces two new additional hyperparameters: the choice of baseline image and the number of steps over which to produce the integrated gradients. Can we do without these?</p><p><strong>Blur Integrated Gradients [2020]</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh3.googleusercontent.com/KXqB_BPsq2gvunD43WkFUwBw5PXuAF-Kmg-efSqQH6VlMphyZdOHaG9A1BNn_7iVYKlUmnsyDXFhkheU2SB6GCx95tgZxpiAJqXZFqjJeFiBWUE4x7aq6h7r-XfHSG0dX4_rP3Ir\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p>That’s what our final interpretation method, blur integrated gradients seeks to do. Presented in <a href=\"https://arxiv.org/pdf/2004.03383\">Attribution in Scale and Space</a> [2020], the method was proposed to solve specific issues with integrated gradients, including the elimination of the ‘baseline’ parameter, and removing certain visual artifacts that tend to appear in interpretations.</p><p>The blur integrated gradients method works by measuring gradients along a series of increasingly blurry versions of the original input image (rather than dimmed versions of the image, as integrated gradients does). Although this may seem like a minor difference, the authors argue that this choice is more theoretically justified, as blurring an image cannot introduce new artifacts into the interpretation, in the way that choosing a baseline image can.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://thegradient.pub/content/images/2021/01/image-4.png\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\" srcset=\"https://thegradient.pub/content/images/size/w600/2021/01/image-4.png 600w, https://thegradient.pub/content/images/size/w1000/2021/01/image-4.png 1000w, https://thegradient.pub/content/images/2021/01/image-4.png 1168w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Standard Integrated Gradients (left) vs. Blur Integrated Gradients (right) on an image of a doberman, both smoothed with SmoothGrad. Here the model is InceptionNet.</figcaption></figure><p><strong>Final Words</strong></p><p>The 2010s were a fruitful decade for interpretation methods for machine learning, and a rich suite of methods now exist that explain neural network behavior. We’ve compared them in this blog post, and we are indebted to several awesome libraries, particularly <a href=\"http://github.com/gradio-app/gradio\"><strong>Gradio</strong></a> to create the interfaces that you see in the GIFs and <a href=\"https://github.com/PAIR-code/saliency\"><strong>PAIR-code’s</strong></a> TensorFlow implementation of the papers. The model used for all of the interfaces is the <a href=\"https://arxiv.org/abs/1512.00567\">Inception Net image classifier</a>.</p><p>Complete code to reproduce this blog post can be found on this <a href=\"https://github.com/gradio-app/history-of-interpretation/blob/master/History-of-Interpretation.ipynb\">Jupyter notebook</a> and on <a href=\"https://colab.research.google.com/drive/1IxhImCFknNMctIonSo98nkco2ufKmfdj?usp=sharing\">Google Colab</a>. Try out the live interface (with guided back-propagation) <a href=\"https://gradio.app/g/history-of-interpretation\">here</a>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://lh5.googleusercontent.com/u0IBlgw34ABVpYbm60GLW_6lHKy8joiqTRtYc1-BFzyzcN1xKLq0b1tWUa-yiz8Nd6tCEHAnQDicLwx8anLdNzrXi19vdXMMaQ9FiR3lK5YTzMituS18CrK3kGfQFzCI3Q9sBX6N\" class=\"kg-image\" alt=\"A Visual History of Interpretation for Image Recognition\"></figure><p><strong>Author Bio</strong> <br>Ali is a co-founder of <a href=\"https://gradio.app\">Gradio</a>, where he works as a machine learning engineer. Before that, he spent time at Tesla, iRobot and MIT. He published several <a href=\"https://scholar.google.com/citations?user=-XiYI4wAAAAJ&#38;hl=en\">academic papers</a>, and has contributed to many <a href=\"https://github.com/aliabd\">open-source projects</a>. You can find him on twitter <a href=\"https://twitter.com/si3luwa\">@si3luwa</a>. </p><p><strong>Acknowledgements</strong><br>Thanks to <a href=\"https://twitter.com/andrey_kurenkov\">Andrey Kurenkov</a>, <a href=\"https://twitter.com/abidlabs\">Abubakar Abid</a> and <a href=\"https://twitter.com/jessicadai_?lang=en\">Jessica Dai</a> for their immense help in putting this piece together. </p><p><strong>Citation</strong><br><em>For attribution in academic contexts or books, please cite this work as</em></p><blockquote>Ali Abdalla, \"A Visual History of Interpretation for Image Recognition\", The Gradient, 2021.</blockquote><p><em>BibTeX citation:</em></p><blockquote>@article{abid2021visual,<br>author = {Abdalla, Ali},<br>title = {A Visual History of Interpretation for Image Recognition},<br>journal = {The Gradient},<br>year = {2021},<br>howpublished = {\\url{<a href=\"https://thegradient.pub/the-gap-where-machine-learning-education-falls-short/\">https://thegradient.pub/</a>a-visual-history-of-interpretation-for-image-recognition<a href=\"https://thegradient.pub/the-gap-where-machine-learning-education-falls-short/\">/</a>} },<br>}</blockquote><p>If you enjoyed this piece and want to hear more, <a href=\"https://thegradient.pub/subscribe/\">subscribe</a> to the Gradient and follow us on <a href=\"https://twitter.com/gradientpub\">Twitter</a>.</p>","descriptionType":"html","publishedDate":"Sun, 17 Jan 2021 02:35:26 +0000","feedId":23313,"bgimg":"https://lh3.googleusercontent.com/6eWc600ZsRUesxmAmk20FT0vMjrv0qh6wC_o2CNIewTAsag_NFwLlswuVwh8oOsGlWJTbaavRKjBDSJ8kIygYE1Xy7_fTy82Egpl0hHMCtKvklp24IjaZIttQduMpPCMQtxu1fBd","linkMd5":"6b6a5a1cd9a065cbd69f5573c97ea7ed","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn88@2020_1/2021/04/05/07-38-38-539_444d480a450e54b1.webp","destWidth":1600,"destHeight":894,"sourceBytes":20826208,"destBytes":1467878,"author":"Ali Abdalla","articleImgCdnMap":{"https://lh3.googleusercontent.com/6eWc600ZsRUesxmAmk20FT0vMjrv0qh6wC_o2CNIewTAsag_NFwLlswuVwh8oOsGlWJTbaavRKjBDSJ8kIygYE1Xy7_fTy82Egpl0hHMCtKvklp24IjaZIttQduMpPCMQtxu1fBd":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn88@2020_1/2021/04/05/07-38-38-539_444d480a450e54b1.webp","https://thegradient.pub/content/images/2021/01/main.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn77@2020_4/2021/04/05/07-38-40-936_eabf0a5bf18e2732.webp","https://lh4.googleusercontent.com/zRDuOSnF29pcOjBzVD1_AYYuUYtC_9diaBv_wv0KiJ21edQ2b-2SH9tmMXyYNTp4kRQ8PYEs-Qf1jIgmkpbCEme3lLZtyNSv3XbhGhJr5_gczhC8OrsmxDmqqIdEJ4x6-1o4tN4x":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn24@2020_6/2021/04/05/07-38-49-013_492d062304cd1bb9.webp","https://lh5.googleusercontent.com/8iCNWTgKlLhMzuH8wJmIRGkJJjX4v_b8N_86uDta2DFLuqqnk1Bqba7dRjod6Ek1YkTz3UDZttggA17aB42Fs9EYQhWLIXhhtq1qNhDwbK1Y-AenyAbXDqOByawVZSBbZ6t7CyPf":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn47@2020_6/2021/04/05/07-38-40-930_ba8656a9005515c6.webp","https://lh3.googleusercontent.com/cXkB8FYxeaXmovUwJ3-rdkgI2vyfxnBX4JQOknji5da9moTGwGkZs3jS4shGUfKSLKVPn8gMZlCyQ6Pkg2bfa3oe4BXA-YIQXe5E7Fha6KufNj9qXNVDDSif4rWXrVWsNp30EowD":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn16@2020_1/2021/04/05/07-38-40-550_8b364e1bda33a45c.webp","https://lh3.googleusercontent.com/KQvbWEkCQKrgjNw4F1p_eUczLUBscjQ7edpIbtCJJ93mzBsFmyfQPLX79jHYB8Oh1h1wKGsMNQVq-2IfjVJtopUBL0uGAaKZ9A2V9FiOP5fmVgGh6g7Su0uDlImfCeN68-9PMTS_":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_1/2021/04/05/07-38-44-345_6804c1424cad7cdf.webp","https://lh3.googleusercontent.com/dvBTxWEYHpQw_eqk9DV5ohf_H-IOmfRhZ8gzZvNhbHMVSeb_LuQ_KtQ9mU29kOe9x3H5NpdgGC7r2VkvKyxkJ-HXLymWgRJY84974QactaDnY6JH-RPTZybzuA0jP4nxbO6QtbuJ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn88@2020_3/2021/04/05/07-38-46-525_adf2c70d7dcf0a69.webp","https://lh3.googleusercontent.com/YZg2IK3On5szm2NtuvXpbAfVlwVx-5B1c88okt5lHBc0OYuvpLvUsewkoiOS0XQSJC3vlLoxDqR9CYCT9mTdKmzfdzCv6yMQUomhHG5eRQneP4BkDVGQn6dK135LqLxexwkQUn2I":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn65@2020_5/2021/04/05/07-38-40-910_e8802a9cfc639e96.webp","https://lh6.googleusercontent.com/lxZUn3mRjWd4vZkVGj9bP2XNZwaoJi7n_5D_3ge6QhWolcKmo8i7QYbbBAJkuDsgJirHdkbIB0YEaWHiBkd7cHKQ9keQs1CQzpSh-OY2i3ZJ5-ulvUbcpnVDT7oWGmIXwWK_0DAo":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn59@2020_5/2021/04/05/07-39-03-561_99a5ccc353523fb5.webp","https://thegradient.pub/content/images/2021/01/image.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn51@2020_2/2021/04/05/07-38-40-674_d0de194626867422.webp","https://lh4.googleusercontent.com/8L11NqXXV5g9_GZsyEJmbGMC8wb3cgaIIkAgrfuc5cxh6d4YV9GMMIiS03nU4U-wMaTFaf4IUvy8FpheY7dYdfR6d1s12bOHPw-Z-V6NHDakoex7sIW5LIHa0XPiseDWuOyGtXmp":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn70@2020_4/2021/04/05/07-38-50-727_76bad8b8ce7ab7fe.webp","https://lh4.googleusercontent.com/JBAabUm4M2008pak7rrIMY6P-LRso41jQdfX01v6ohGp4aQ2wj1znzJKBkU8w6_wPSIpQA3sLFF0IbGikF68NWAs-GRQfRENgzH3HdDBmpV4HRdUVjZPi3h5lgLvY84c5hdJWDf1":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn20@2020_2/2021/04/05/07-38-40-531_a0d18b5028481176.webp","https://thegradient.pub/content/images/2021/01/image-1.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn4@2020_3/2021/04/05/07-38-40-558_42a9ed94420ab1cf.webp","https://lh5.googleusercontent.com/HrMjWr8KBZqpH7zTPIPug-hrWM6oPyTirFfuLcmV4-SKxNzT_UzGlo-h4IwbAew5MOvKUuJxewDc3cZx2iOYoUjjECnkoNheyZlg5qpClpRJJAnkM16z32aYsFcF_oSdQHACdrqB":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn62@2020_6/2021/04/05/07-38-46-480_a669acaed663b19a.webp","https://lh4.googleusercontent.com/vaSiaHSaODodKmHjorhnKtDUpt6dmFpW9h_NW18v33uwfFwgzt0AgsjT8xBaYL4kIQSXx8GEyVGcb1YCv_YHVZV1P3f1tEL4TkYSFMNqDzw_3AmG4Jghm0MNUFfZ58OHAuUeHOdX":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn43@2020_6/2021/04/05/07-38-40-676_173856ed55b7db32.webp","https://lh5.googleusercontent.com/sQqLKA5fqJwAdsWuOb7ui0VTqZ5ECUNqhF-z-QYl0rf5sIu43GBhY8EFeMIgWxQxGdX8DMCY96ljg1jwMTPPsioi7twL_QGn7k7Tplwej6a5_HmN2qedsukMT1V44XrMg1UMAJ7G":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn91@2020_3/2021/04/05/07-38-41-086_321819a74ac5d293.webp","https://lh4.googleusercontent.com/MRQpyhs8c1A3ftX7M549wbYsy278UOFmFs536pIMj_rJOuf3rE_XRNxCHAH41fz4ABqN88tufIGpm1Ol_GFRdwnx2Ifk-n8kVQe9pFmVku2tU2OWsXDjmlXOv5C-_ba531kVPyw_":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn83@2020_3/2021/04/05/07-38-46-787_5be81593a1fcdb2f.webp","https://thegradient.pub/content/images/2021/01/image-2.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn100@2020_5/2021/04/05/07-38-40-742_c7e7bedaae41ed5c.webp","https://lh5.googleusercontent.com/Qyri3unenVF2mCqFwGGk2MQAVHFcI1laNRU4NHJzpHlqiCOfkNMnrNKzzPOsqYlPbwlh0Ky4-dUaqByLsHEiGDUqYbfxbwePxlRsFe4wlCebGMvUwsIxdODsdoEXVc5yG0LmFKdb":null,"https://thegradient.pub/content/images/2021/01/image-3.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn11@2020_2/2021/04/05/07-38-40-638_3787120d51174206.webp","https://lh3.googleusercontent.com/KXqB_BPsq2gvunD43WkFUwBw5PXuAF-Kmg-efSqQH6VlMphyZdOHaG9A1BNn_7iVYKlUmnsyDXFhkheU2SB6GCx95tgZxpiAJqXZFqjJeFiBWUE4x7aq6h7r-XfHSG0dX4_rP3Ir":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn96@2020_5/2021/04/05/07-38-46-383_89117aa8593dd123.webp","https://thegradient.pub/content/images/2021/01/image-4.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn7@2020_4/2021/04/05/07-38-40-683_a4953532ceeb0734.webp","https://lh5.googleusercontent.com/u0IBlgw34ABVpYbm60GLW_6lHKy8joiqTRtYc1-BFzyzcN1xKLq0b1tWUa-yiz8Nd6tCEHAnQDicLwx8anLdNzrXi19vdXMMaQ9FiR3lK5YTzMituS18CrK3kGfQFzCI3Q9sBX6N":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn55@2020_6/2021/04/05/07-38-40-804_f490bc3d607f4f35.webp"},"publishedOrCreatedDate":1617608203617}],"record":{"createdTime":"2021-04-05 15:36:43","updatedTime":"2021-04-05 15:36:43","feedId":23313,"fetchDate":"Mon, 05 Apr 2021 07:36:43 +0000","fetchMs":843,"handleMs":84,"totalMs":141695,"newArticles":0,"totalArticles":15,"status":1,"type":0,"ip":"0c100e5b1ca1404feeceff01695dc4bf","hostName":"us-015*","requestId":"342c2da5881e467491a3d322f549aad1_23313","contentType":"text/xml; charset=utf-8","totalBytes":13706084,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":23,"articlesImgsGithubTotal":22,"successGithubMap":{"myreaderx8":1,"myreaderx7":1,"myreaderx15":1,"myreaderx16":1,"myreaderx4":1,"myreaderx32":1,"myreaderx10":1,"myreaderx21":1,"myreaderx33":1,"myreaderx22":1,"myreaderx11":1,"myreaderx12":1,"myreaderx2":1,"myreaderx24":1,"myreaderx1":1,"myreaderx13":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx29":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{"myreaderx23":1}},"feed":{"createdTime":"2020-09-07 02:30:04","updatedTime":"2020-09-07 04:22:30","id":23313,"name":"The Gradient","url":"https://thegradient.pub/rss/","subscriber":141,"website":null,"icon":"https://thegradient.pub/favicon.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx62/cdn74@2020_1/2020/09/06/20-22-26-304_d01d55c4f5e19c3b.png","description":"A digital publication about artificial intelligence and the future.","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-04-05 15:39:04","updatedTime":"2021-04-05 15:39:04","id":null,"feedId":23313,"linkMd5":"6b6a5a1cd9a065cbd69f5573c97ea7ed"}],"tmpCommonImgCdnBytes":1467878,"tmpBodyImgCdnBytes":12238206,"tmpBgImgCdnBytes":0,"extra4":{"start":1617608202395,"total":0,"statList":[{"spend":1140,"msg":"获取xml内容"},{"spend":84,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":23762,"msg":"正文链接上传到cdn"}]},"extra5":23,"extra6":23,"extra7ImgCdnFailResultVector":[null,{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/Qyri3unenVF2mCqFwGGk2MQAVHFcI1laNRU4NHJzpHlqiCOfkNMnrNKzzPOsqYlPbwlh0Ky4-dUaqByLsHEiGDUqYbfxbwePxlRsFe4wlCebGMvUwsIxdODsdoEXVc5yG0LmFKdb","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"sourceBytes":4505180,"destBytes":1246962,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":10868,"convertSpendMs":9124,"createdTime":"2021-04-05 15:38:40","host":"us-036*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn73/contents/2021/04/05/07-38-51-090_2d2d7d124281314d.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 05 Apr 2021 07:38:51 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A708:7277:2E19ECF:4ABE926:606ABE8B"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1617608686"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn73/contents/2021/04/05/07-38-51-090_2d2d7d124281314d.webp","historyStatusCode":[],"spendMs":101},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.3 MB","destSize":"1.2 MB","compressRate":"27.7%"},{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/Qyri3unenVF2mCqFwGGk2MQAVHFcI1laNRU4NHJzpHlqiCOfkNMnrNKzzPOsqYlPbwlh0Ky4-dUaqByLsHEiGDUqYbfxbwePxlRsFe4wlCebGMvUwsIxdODsdoEXVc5yG0LmFKdb","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"sourceBytes":4505180,"destBytes":1246962,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":6573,"convertSpendMs":6142,"createdTime":"2021-04-05 15:38:51","host":"us-007*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn73/contents/2021/04/05/07-38-57-736_2d2d7d124281314d.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 05 Apr 2021 07:38:57 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["AEE6:2C30:25C16E9:402955F:606ABE91"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1617608686"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn73/contents/2021/04/05/07-38-57-736_2d2d7d124281314d.webp","historyStatusCode":[],"spendMs":67},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.3 MB","destSize":"1.2 MB","compressRate":"27.7%"}],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-55.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-007.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe70.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-017.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-51.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-029.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/6eWc600ZsRUesxmAmk20FT0vMjrv0qh6wC_o2CNIewTAsag_NFwLlswuVwh8oOsGlWJTbaavRKjBDSJ8kIygYE1Xy7_fTy82Egpl0hHMCtKvklp24IjaZIttQduMpPCMQtxu1fBd","sourceStatusCode":200,"destWidth":1600,"destHeight":894,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn88@2020_1/2021/04/05/07-38-38-539_444d480a450e54b1.webp","sourceBytes":20826208,"destBytes":1467878,"targetWebpQuality":4,"feedId":23313,"totalSpendMs":55078,"convertSpendMs":37846,"createdTime":"2021-04-05 15:37:43","host":"us-036*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed,6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"19.9 MB","destSize":"1.4 MB","compressRate":"7%"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/JBAabUm4M2008pak7rrIMY6P-LRso41jQdfX01v6ohGp4aQ2wj1znzJKBkU8w6_wPSIpQA3sLFF0IbGikF68NWAs-GRQfRENgzH3HdDBmpV4HRdUVjZPi3h5lgLvY84c5hdJWDf1","sourceStatusCode":200,"destWidth":1600,"destHeight":842,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn20@2020_2/2021/04/05/07-38-40-531_a0d18b5028481176.webp","sourceBytes":177809,"destBytes":49196,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":401,"convertSpendMs":70,"createdTime":"2021-04-05 15:38:40","host":"us-033*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"173.6 KB","destSize":"48 KB","compressRate":"27.7%"},{"code":1,"isDone":false,"source":"https://thegradient.pub/content/images/2021/01/image-1.png","sourceStatusCode":200,"destWidth":1132,"destHeight":498,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn4@2020_3/2021/04/05/07-38-40-558_42a9ed94420ab1cf.webp","sourceBytes":431752,"destBytes":19078,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":465,"convertSpendMs":83,"createdTime":"2021-04-05 15:38:40","host":"us-017*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"421.6 KB","destSize":"18.6 KB","compressRate":"4.4%"},{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/cXkB8FYxeaXmovUwJ3-rdkgI2vyfxnBX4JQOknji5da9moTGwGkZs3jS4shGUfKSLKVPn8gMZlCyQ6Pkg2bfa3oe4BXA-YIQXe5E7Fha6KufNj9qXNVDDSif4rWXrVWsNp30EowD","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn16@2020_1/2021/04/05/07-38-40-550_8b364e1bda33a45c.webp","sourceBytes":408652,"destBytes":31796,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":502,"convertSpendMs":53,"createdTime":"2021-04-05 15:38:40","host":"us-55*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"399.1 KB","destSize":"31.1 KB","compressRate":"7.8%"},{"code":1,"isDone":false,"source":"https://thegradient.pub/content/images/2021/01/image.png","sourceStatusCode":200,"destWidth":1114,"destHeight":488,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn51@2020_2/2021/04/05/07-38-40-674_d0de194626867422.webp","sourceBytes":495422,"destBytes":17938,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":564,"convertSpendMs":42,"createdTime":"2021-04-05 15:38:40","host":"us-55*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"483.8 KB","destSize":"17.5 KB","compressRate":"3.6%"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/vaSiaHSaODodKmHjorhnKtDUpt6dmFpW9h_NW18v33uwfFwgzt0AgsjT8xBaYL4kIQSXx8GEyVGcb1YCv_YHVZV1P3f1tEL4TkYSFMNqDzw_3AmG4Jghm0MNUFfZ58OHAuUeHOdX","sourceStatusCode":200,"destWidth":1600,"destHeight":595,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn43@2020_6/2021/04/05/07-38-40-676_173856ed55b7db32.webp","sourceBytes":883882,"destBytes":95792,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":559,"convertSpendMs":125,"createdTime":"2021-04-05 15:38:40","host":"us-004*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"863.2 KB","destSize":"93.5 KB","compressRate":"10.8%"},{"code":1,"isDone":false,"source":"https://thegradient.pub/content/images/2021/01/image-2.png","sourceStatusCode":200,"destWidth":1160,"destHeight":496,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn100@2020_5/2021/04/05/07-38-40-742_c7e7bedaae41ed5c.webp","sourceBytes":378518,"destBytes":16112,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":608,"convertSpendMs":122,"createdTime":"2021-04-05 15:38:40","host":"us-029*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"369.6 KB","destSize":"15.7 KB","compressRate":"4.3%"},{"code":1,"isDone":false,"source":"https://thegradient.pub/content/images/2021/01/image-4.png","sourceStatusCode":200,"destWidth":1168,"destHeight":494,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn7@2020_4/2021/04/05/07-38-40-683_a4953532ceeb0734.webp","sourceBytes":361434,"destBytes":13802,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":623,"convertSpendMs":63,"createdTime":"2021-04-05 15:38:40","host":"us-004*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"353 KB","destSize":"13.5 KB","compressRate":"3.8%"},{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/u0IBlgw34ABVpYbm60GLW_6lHKy8joiqTRtYc1-BFzyzcN1xKLq0b1tWUa-yiz8Nd6tCEHAnQDicLwx8anLdNzrXi19vdXMMaQ9FiR3lK5YTzMituS18CrK3kGfQFzCI3Q9sBX6N","sourceStatusCode":200,"destWidth":1514,"destHeight":1600,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn55@2020_6/2021/04/05/07-38-40-804_f490bc3d607f4f35.webp","sourceBytes":1649547,"destBytes":97308,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":655,"convertSpendMs":205,"createdTime":"2021-04-05 15:38:40","host":"us-033*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.6 MB","destSize":"95 KB","compressRate":"5.9%"},{"code":1,"isDone":false,"source":"https://thegradient.pub/content/images/2021/01/image-3.png","sourceStatusCode":200,"destWidth":1152,"destHeight":500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn11@2020_2/2021/04/05/07-38-40-638_3787120d51174206.webp","sourceBytes":341346,"destBytes":11764,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":610,"convertSpendMs":32,"createdTime":"2021-04-05 15:38:40","host":"europe67*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"333.3 KB","destSize":"11.5 KB","compressRate":"3.4%"},{"code":1,"isDone":false,"source":"https://thegradient.pub/content/images/2021/01/main.png","sourceStatusCode":200,"destWidth":1282,"destHeight":898,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn77@2020_4/2021/04/05/07-38-40-936_eabf0a5bf18e2732.webp","sourceBytes":741174,"destBytes":46898,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":880,"convertSpendMs":260,"createdTime":"2021-04-05 15:38:40","host":"us-024*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"723.8 KB","destSize":"45.8 KB","compressRate":"6.3%"},{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/8iCNWTgKlLhMzuH8wJmIRGkJJjX4v_b8N_86uDta2DFLuqqnk1Bqba7dRjod6Ek1YkTz3UDZttggA17aB42Fs9EYQhWLIXhhtq1qNhDwbK1Y-AenyAbXDqOByawVZSBbZ6t7CyPf","sourceStatusCode":200,"destWidth":1556,"destHeight":830,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn47@2020_6/2021/04/05/07-38-40-930_ba8656a9005515c6.webp","sourceBytes":367917,"destBytes":54342,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":875,"convertSpendMs":59,"createdTime":"2021-04-05 15:38:40","host":"europe67*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"359.3 KB","destSize":"53.1 KB","compressRate":"14.8%"},{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/YZg2IK3On5szm2NtuvXpbAfVlwVx-5B1c88okt5lHBc0OYuvpLvUsewkoiOS0XQSJC3vlLoxDqR9CYCT9mTdKmzfdzCv6yMQUomhHG5eRQneP4BkDVGQn6dK135LqLxexwkQUn2I","sourceStatusCode":200,"destWidth":1148,"destHeight":546,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn65@2020_5/2021/04/05/07-38-40-910_e8802a9cfc639e96.webp","sourceBytes":447053,"destBytes":35478,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":911,"convertSpendMs":85,"createdTime":"2021-04-05 15:38:40","host":"europe70*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"436.6 KB","destSize":"34.6 KB","compressRate":"7.9%"},{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/sQqLKA5fqJwAdsWuOb7ui0VTqZ5ECUNqhF-z-QYl0rf5sIu43GBhY8EFeMIgWxQxGdX8DMCY96ljg1jwMTPPsioi7twL_QGn7k7Tplwej6a5_HmN2qedsukMT1V44XrMg1UMAJ7G","sourceStatusCode":200,"destWidth":1600,"destHeight":591,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn91@2020_3/2021/04/05/07-38-41-086_321819a74ac5d293.webp","sourceBytes":430677,"destBytes":66756,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":999,"convertSpendMs":129,"createdTime":"2021-04-05 15:38:40","host":"europe63*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"420.6 KB","destSize":"65.2 KB","compressRate":"15.5%"},{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/KQvbWEkCQKrgjNw4F1p_eUczLUBscjQ7edpIbtCJJ93mzBsFmyfQPLX79jHYB8Oh1h1wKGsMNQVq-2IfjVJtopUBL0uGAaKZ9A2V9FiOP5fmVgGh6g7Su0uDlImfCeN68-9PMTS_","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn27@2020_1/2021/04/05/07-38-44-345_6804c1424cad7cdf.webp","sourceBytes":5668574,"destBytes":1503760,"targetWebpQuality":67,"feedId":23313,"totalSpendMs":4490,"convertSpendMs":3608,"createdTime":"2021-04-05 15:38:40","host":"us-008*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.4 MB","destSize":"1.4 MB","compressRate":"26.5%"},{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/KXqB_BPsq2gvunD43WkFUwBw5PXuAF-Kmg-efSqQH6VlMphyZdOHaG9A1BNn_7iVYKlUmnsyDXFhkheU2SB6GCx95tgZxpiAJqXZFqjJeFiBWUE4x7aq6h7r-XfHSG0dX4_rP3Ir","sourceStatusCode":200,"destWidth":1600,"destHeight":779,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn96@2020_5/2021/04/05/07-38-46-383_89117aa8593dd123.webp","sourceBytes":5013927,"destBytes":1449324,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":6447,"convertSpendMs":5376,"createdTime":"2021-04-05 15:38:40","host":"us-51*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.8 MB","destSize":"1.4 MB","compressRate":"28.9%"},{"code":1,"isDone":false,"source":"https://lh5.googleusercontent.com/HrMjWr8KBZqpH7zTPIPug-hrWM6oPyTirFfuLcmV4-SKxNzT_UzGlo-h4IwbAew5MOvKUuJxewDc3cZx2iOYoUjjECnkoNheyZlg5qpClpRJJAnkM16z32aYsFcF_oSdQHACdrqB","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn62@2020_6/2021/04/05/07-38-46-480_a669acaed663b19a.webp","sourceBytes":5336943,"destBytes":1469514,"targetWebpQuality":67,"feedId":23313,"totalSpendMs":6506,"convertSpendMs":5500,"createdTime":"2021-04-05 15:38:40","host":"us-007*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.1 MB","destSize":"1.4 MB","compressRate":"27.5%"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/MRQpyhs8c1A3ftX7M549wbYsy278UOFmFs536pIMj_rJOuf3rE_XRNxCHAH41fz4ABqN88tufIGpm1Ol_GFRdwnx2Ifk-n8kVQe9pFmVku2tU2OWsXDjmlXOv5C-_ba531kVPyw_","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn83@2020_3/2021/04/05/07-38-46-787_5be81593a1fcdb2f.webp","sourceBytes":4500617,"destBytes":1289736,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":6829,"convertSpendMs":5906,"createdTime":"2021-04-05 15:38:40","host":"us-012*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.3 MB","destSize":"1.2 MB","compressRate":"28.7%"},{"code":1,"isDone":false,"source":"https://lh3.googleusercontent.com/dvBTxWEYHpQw_eqk9DV5ohf_H-IOmfRhZ8gzZvNhbHMVSeb_LuQ_KtQ9mU29kOe9x3H5NpdgGC7r2VkvKyxkJ-HXLymWgRJY84974QactaDnY6JH-RPTZybzuA0jP4nxbO6QtbuJ","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn88@2020_3/2021/04/05/07-38-46-525_adf2c70d7dcf0a69.webp","sourceBytes":5388058,"destBytes":1469502,"targetWebpQuality":67,"feedId":23313,"totalSpendMs":6937,"convertSpendMs":2596,"createdTime":"2021-04-05 15:38:40","host":"europe-25*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.1 MB","destSize":"1.4 MB","compressRate":"27.3%"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/zRDuOSnF29pcOjBzVD1_AYYuUYtC_9diaBv_wv0KiJ21edQ2b-2SH9tmMXyYNTp4kRQ8PYEs-Qf1jIgmkpbCEme3lLZtyNSv3XbhGhJr5_gczhC8OrsmxDmqqIdEJ4x6-1o4tN4x","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn24@2020_6/2021/04/05/07-38-49-013_492d062304cd1bb9.webp","sourceBytes":4599510,"destBytes":1468890,"targetWebpQuality":75,"feedId":23313,"totalSpendMs":9072,"convertSpendMs":5312,"createdTime":"2021-04-05 15:38:40","host":"us-021*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.4 MB","destSize":"1.4 MB","compressRate":"31.9%"},{"code":1,"isDone":false,"source":"https://lh4.googleusercontent.com/8L11NqXXV5g9_GZsyEJmbGMC8wb3cgaIIkAgrfuc5cxh6d4YV9GMMIiS03nU4U-wMaTFaf4IUvy8FpheY7dYdfR6d1s12bOHPw-Z-V6NHDakoex7sIW5LIHa0XPiseDWuOyGtXmp","sourceStatusCode":200,"destWidth":1600,"destHeight":780,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn70@2020_4/2021/04/05/07-38-50-727_76bad8b8ce7ab7fe.webp","sourceBytes":6522495,"destBytes":1538890,"targetWebpQuality":60,"feedId":23313,"totalSpendMs":11089,"convertSpendMs":6645,"createdTime":"2021-04-05 15:38:40","host":"europe-58*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.2 MB","destSize":"1.5 MB","compressRate":"23.6%"},{"code":1,"isDone":false,"source":"https://lh6.googleusercontent.com/lxZUn3mRjWd4vZkVGj9bP2XNZwaoJi7n_5D_3ge6QhWolcKmo8i7QYbbBAJkuDsgJirHdkbIB0YEaWHiBkd7cHKQ9keQs1CQzpSh-OY2i3ZJ5-ulvUbcpnVDT7oWGmIXwWK_0DAo","sourceStatusCode":200,"destWidth":1600,"destHeight":803,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn59@2020_5/2021/04/05/07-39-03-561_99a5ccc353523fb5.webp","sourceBytes":16254577,"destBytes":1492330,"targetWebpQuality":4,"feedId":23313,"totalSpendMs":23653,"convertSpendMs":12789,"createdTime":"2021-04-05 15:38:40","host":"us-025*","referer":"https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/","linkMd5ListStr":"6b6a5a1cd9a065cbd69f5573c97ea7ed","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"15.5 MB","destSize":"1.4 MB","compressRate":"9.2%"}],"successGithubMap":{"myreaderx8":1,"myreaderx7":1,"myreaderx15":1,"myreaderx16":1,"myreaderx4":1,"myreaderx32":1,"myreaderx10":1,"myreaderx21":1,"myreaderx33":1,"myreaderx22":1,"myreaderx11":1,"myreaderx12":1,"myreaderx2":1,"myreaderx24":1,"myreaderx1":1,"myreaderx13":1,"myreaderx30":1,"myreaderx5oss":1,"myreaderx18":1,"myreaderx29":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{"myreaderx23":1}}