{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-11-18 16:24:10","updatedTime":"2020-11-18 16:24:10","title":"An intro to Reinforcement Learning (with otters)","link":"https://meowni.ca/posts/rl-with-otters","description":"<style> img.otter { max-height: 220px !important; } iframe.otter { height: 800px; width: 100%; margin: 0 auto; border: 5px solid #E0F7FA; border-radius: 3px; } </style> \n<p>Before I wrote the JavaScripts, I got a master‚Äôs in AI (almost a decade ago üôÄ), and wrote a <a href=\"https://meowni.ca/includes/mdinculescu_thesis.pdf\">thesis</a> on a weird and new area in Reinforcement Learning. Or at least it was new then. It‚Äôs definitely still weird now. Anyway, I loved it. With all the hype around Machine Learning and Deep Learning, I thought it would be neat if I wrote a little primer on what Reinforcement Learning really means, and why it‚Äôs different than just another neural net.</p> \n<p>Richard Sutton and Andrew Barto wrote an <em>amazing</em> book called ‚ÄúReinforcement Learning: an introduction‚Äù; it‚Äôs my favourite non-fiction book I have ever read in my life, and it‚Äôs why I fell in love with RL. The complete draft is available for free <a href=\"http://incompleteideas.net/book/bookdraft2017nov5.pdf\">here</a>, and if you‚Äôre into math, and want to explore this topic further, I can‚Äôt recommend it enough.</p> \n<p>If you‚Äôre not into math, I have otters.</p> \n<p><img class=\"otter\" alt=\"otter says hi\" src=\"https://meowni.ca/images/2018-02-26/0.png\" /></p> \n<h2 id=\"what-is-it\">What is it?</h2> \n<p>Reinforcement learning (or RL) solves a very specific problem: figuring out how to act over time, so that you get the most long term reward. Both these sequences of actions and the reward bit are important components that make RL a ‚Äúgood‚Äù approach to solve a problem.</p> \n<p>For example, this is perfect if you‚Äôre a Roomba who is trying to get home (the only reward you get is if you actually get home, so while you‚Äôre roaming around aimlessly and get no üí∞, you have a feeeeeeeling you‚Äôre not doing it right).</p> \n<p>On the other hand, this is terrible if you‚Äôre trying to figure out if a photo has an otter in it; there are no sequences of actions that matter here, other than doing the decision of saying ‚Äúyes iz otter‚Äù. You‚Äôre just trapped in a room where people slip Polaroids of animals under the door and you have to tell them what it is. Nightmares aren‚Äôt really a good area for RL.</p> \n<p><img class=\"otter\" alt=\"i'm doing RL\" src=\"https://meowni.ca/images/2018-02-26/1.png\" /></p> \n<h2 id=\"what-isnt-it\">What isn‚Äôt it?</h2> \n<p>There are many things with the word ‚Äúlearning‚Äù in them that <em>aren‚Äôt</em> Reinforcement Learning.</p> \n<ul> \n <li><em>supervised learning</em>. This is a kind of Machine Learning where someone gave you a training set that has everything labelled correctly, you learn from it, and hope that at exam time what you‚Äôve learnt is correct. This is the ‚ÄúI have 1000 images of cats, now I can tell you if this new image is a cat‚Äù problem.</li> \n <li><em>unsupervised learning</em>. This is another kind of Machine Learning where someone gave you a bunch of data with no labels, and just by staring at it you try to find structure in it and make up labels. This is the ‚ÄúI have 1000 images of cats and dogs, but nobody told me what a cat or a dog looks like; now I can tell you if this new image is like what I call a cat or a dog‚Äù.</li> \n</ul> \n<p><em>Classification</em> is a very common problem that can be solved with both of these Machine Learning approaches (but can‚Äôt be solved very well with RL, which isn‚Äôt really suited for one-off actions).</p> \n<p><a href=\"https://en.wikipedia.org/wiki/Artificial_neural_network\">Neural nets</a> are very good at solving these 2 kinds of Machine Learning problems. For example, the secrets straight out of your <a href=\"https://secure.i.telegraph.co.uk/multimedia/archive/03370/doge_3370416k.jpg\">nightmares</a> are created by a ‚Äúdeep‚Äù neural net, a neural net that has several layers in between its input and output layers.</p> \n<p>If you add neural nets on top of some Reinforcement Learning algorithms, you get something called <em>Deep Reinforcement Learning</em>, which is a brand new area of research that brought you supercomputers that <a href=\"https://en.wikipedia.org/wiki/AlphaGo\">win at Go</a>.</p> \n<h2 id=\"the-world\">The world</h2> \n<p>RL problems are usually set up in an environment that is built out of <strong>states</strong>, and you can move between them by taking <strong>actions</strong>. Once you take an action, you‚Äôre given a <strong>reward</strong>, and you keep doing this until someone tells you to stop.</p> \n<p>In the Roomba example, the states could be the <code class=\"language-plaintext highlighter-rouge\">(x,y)</code> positions on the map, and you move between two states (i.e. locations) by moving the motors in a particular direction. The reward might be set up in such a way that you only get <code class=\"language-plaintext highlighter-rouge\">1</code> point if you reach the home base, and <code class=\"language-plaintext highlighter-rouge\">0</code> otherwise. If there‚Äôs particularly dangerous spots in the world you want to make sure the Roomba learns to avoid (like cliffs or a cat), you can make sure any actions that end up in those states get a reward of <code class=\"language-plaintext highlighter-rouge\">-1</code>.</p> \n<p>Some environments are less like real worlds and more like abstract worlds: when you‚Äôre playing Texas Hold‚Äôem poker, the state that you‚Äôre in could be the hand that you have, and what cards are on the table, and the actions could be <code class=\"language-plaintext highlighter-rouge\">folding</code>, <code class=\"language-plaintext highlighter-rouge\">raising</code>, <code class=\"language-plaintext highlighter-rouge\">checking</code>. If you only give a reward at the end of the game (eg. ‚ÄúI won this hand or I didn‚Äôt‚Äù), it‚Äôs very hard to know how you‚Äôre actually doing. These problems have much more complicated reward signals (and tbh, states): how players are doing, which are staying, how they‚Äôre playing, need to be considered.</p> \n<p><img class=\"otter\" alt=\"this is otterly rewarding\" src=\"https://meowni.ca/images/2018-02-26/2.png\" /></p> \n<p><strong>Nerd alert</strong>: If you‚Äôre interested in the math behind this, the environments are usually represented by a <a href=\"https://en.wikipedia.org/wiki/Markov_decision_process\">Markov Decision Process</a> (MDP), or a <a href=\"https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process\">Partially Observable Markov Decision Process</a> (POMDP). The difference between the two is that in the latter case you‚Äôre not told exactly what your state in the world is (you‚Äôre a GPS-less Roomba). You still know what actions you took, and what reward you‚Äôre accumulating, but since you don‚Äôt know what they <em>actually</em> mean in the world, you have to make up your own representation of it. These are typically harder and weirder problems, and these were the ones I was focusing my research on, btw!</p> \n<h2 id=\"learning-how-to-act\">Learning how to act</h2> \n<p>Ok, so: we‚Äôre a Roomba, we‚Äôve been placed somewhere in a world, and we have a goal: to get home (I think this technically makes us ET, but hey). The thing that tells us which action to take in a state is our <strong>policy</strong>. If we can figure out the best action to take in every state in the world, then we have an <strong>optimal policy</strong>.</p> \n<p><img class=\"otter\" alt=\"clear eyes, optimal policy, can't lose\" src=\"https://meowni.ca/images/2018-02-26/3.png\" /></p> \n<p>In order to figure out if a policy is better than another, we need to figure out how ‚Äúgood‚Äù it is to be in a certain state according to that policy (because then you get to compare them: from this state, one policy leads me to a pot of gold, and one to sudden death. One is clearly superior). We call this the <strong>value of a state</strong>, and it‚Äôs basically the reward we <em>expect we‚Äôre going to get</em> from that state if we follow what the policy tells us to do.</p> \n<p>The <strong>expected reward</strong> bit is subtle but hella important: if you just care about immediate reward, a state that doesn‚Äôt lead you to instant death sounds pretty good! However, if you keep taking these seemingly-ok-because-they-didn‚Äôt-kill-us actions, you might still end up at the edge of the cliff, one step away from instant death. By considering reward a number of steps away, you don‚Äôt get trapped in shitty trajectories like this.</p> \n<p>Most basic RL algorithms try to learn one of these functions:</p> \n<ul> \n <li>the <strong>state-value function</strong>, which is the value of every state in the world. This is usually called <code class=\"language-plaintext highlighter-rouge\">V</code> (for value)</li> \n <li>the <strong>action-value function</strong>, which is the value of taking an action from a state, for all actions and states in the world. This is usually called <code class=\"language-plaintext highlighter-rouge\">Q</code> (for qaction? lolmath.)</li> \n</ul> \n<p>The difference between the two is potentially religious. The <strong>state-value</strong> function basically says ‚Äúwhere you are in the world is important, so figure out the sequence of good states and follow that‚Äù. The <strong>action-value</strong> function says ‚Äúwe‚Äôre in a state, and some of the actions we can take are awesome, and some are terribad, figure out the awesome ones‚Äù.</p> \n<p>The point of an RL algorithm is to basically learn these functions, and then pick the one with the highest value: that‚Äôs your optimal policy!</p> \n<h2 id=\"how-do-we-learn\">How do we learn?</h2> \n<p>We learn things about the world by exploring the world. You can think about it as roaming the world in ‚Äúpractice mode‚Äù, which gives you experience, which helps you learn what your policy is (what to do in a particular state). When it‚Äôs ‚Äúexam time mode‚Äù, you use the policy you‚Äôve learnt and act according to that. The more data you have, the better you learn.</p> \n<p>If we think about our <strong>practice policy</strong> as the way we decided to act while in practice mode, and our <strong>optimal policy</strong> as the way we will act during ‚Äúexam time‚Äù (always be the very best you at exams), then there are two fundamentally different ways in which you can learn:</p> \n<ul> \n <li><strong>on-policy learning</strong>: in practice mode, you are following the <strong>practice policy</strong> to explore the environment, and learning how well it works. the more you learn, the better it gets. in ‚Äúexam time mode‚Äù, you still use this <strong>practice policy</strong> you‚Äôve perfected.</li> \n <li><strong>off-policy learning</strong>: in practice mode, you are following the <strong>practice policy</strong> to explore the environment, and learning what the <strong>optimal</strong> policy should look like, based on what you‚Äôre discovering. in ‚Äúexam time mode‚Äù, you would use the <strong>optimal policy</strong> you‚Äôve been learning.</li> \n</ul> \n<p><img class=\"otter\" alt=\"i'm an on policy otter, my policy is to always say yes to food\" src=\"https://meowni.ca/images/2018-02-26/4.png\" /></p> \n<h2 id=\"and-now-a-code\">And now, a code!</h2> \n<p>My favourite favourite FAVOURITE thing about AI is that if you do a simple thing, you can get a very satisfying demo. There are tons of Reinforcement Learning algorithms out there: some are very complicated and take a lot of math. But some are very simple, and that‚Äôs the one I <a href=\"https://glitch.com/edit/#!/q-learning\">implemented</a> for you.</p> \n<p>It‚Äôs called <strong>Q-Learning</strong>, because it learns the <code class=\"language-plaintext highlighter-rouge\">Q</code> function (if you forgot: this is the action-value function, i.e. the value of all of the actions, from all of the states). It works like this:</p> \n<ol> \n <li>Initialize your <code class=\"language-plaintext highlighter-rouge\">Q</code> function randomly (so the value of any action from any state is a random number). This bit is important so that you don‚Äôt accidentally bias your policy with lies</li> \n <li>Start in a random state (call it <code class=\"language-plaintext highlighter-rouge\">S</code>).</li> \n <li>From this state, we need to figure out how to move in the world. We‚Äôre gonna do something slightly fancy called <code class=\"language-plaintext highlighter-rouge\">epsilon-greedy</code>: most of the time, we‚Äôre going to move according to what the policy says (‚Äúgreedily‚Äù). However, <code class=\"language-plaintext highlighter-rouge\">epsilon</code> percent of the time, we‚Äôre going to move randomly. This means that we still get to do some random exploration, which is important to make sure we see new states we might not otherwise. <code class=\"language-plaintext highlighter-rouge\">epsilon-greedy</code> is loooooved by RL people because it balances ‚Äúexploration‚Äù (doing things randomly) with ‚Äúexploitation‚Äù (doing things correctly) and you‚Äôll find it in like literally every RL paper out there.</li> \n <li>And‚Ä¶take that action! Once you take it, you‚Äôll end up in a state <code class=\"language-plaintext highlighter-rouge\">S_2</code>, and the world tells you what reward you got. Call it <code class=\"language-plaintext highlighter-rouge\">R</code>. We‚Äôre going to use this reward to update our <code class=\"language-plaintext highlighter-rouge\">Q</code> function for the state we were in, and the action we took; more precisely: we‚Äôre going to update our <code class=\"language-plaintext highlighter-rouge\">Q(S,A)</code> value. Note how you basically always update the <em>previous</em> state-action pair, by seeing the results of that action in the world.</li> \n <li>The update step is a bit mathy, so I‚Äôll spare you it (here‚Äôs the <a href=\"https://glitch.com/edit/#!/q-learning?path=q-learner.js:73:32\">relevant code</a> if you want to check it out), but the TL;DR is: if this action was a good action, then the state that we ended up in should be a better state than the one we were currently in (closer to the goal). If we got a bad reward, then we reduce the value of <code class=\"language-plaintext highlighter-rouge\">Q(S,A)</code>; if we didn‚Äôt, then we increase it.</li> \n <li>boring note incoming: this is an <code class=\"language-plaintext highlighter-rouge\">off-policy</code> algorithm. How we calculate the <code class=\"language-plaintext highlighter-rouge\">Q(S,A)</code> values isn‚Äôt affected by how we actually moved in the world; we assume we followed the <code class=\"language-plaintext highlighter-rouge\">greedy</code> (aka best) policy, even if we didn‚Äôt.</li> \n <li>Anyway, now, we‚Äôre in a new state, so back at Step 2. Repeat Steps 2-6 until you end up in a goal state. Once you do (yay!), you can go back to Step 1 and start in a new random state (this is important so that you see new parts of the world!).</li> \n</ol> \n<p>If you do this enough times, you eventually experience enough of the world that your <code class=\"language-plaintext highlighter-rouge\">Q</code> function will tell you what to do!</p> \n<p><img class=\"otter\" alt=\"get otter here, we otter see a demo!\" src=\"https://meowni.ca/images/2018-02-26/5.png\" /></p> \n<h2 id=\"demo\">Demo</h2> \n<p>This is a gridworld! It has a goal state, and a blob can move in any direction from any state. If you press play before doing any learning, the blob will just walk around randomly. If you press the learn button, the blob will take <a href=\"https://glitch.com/edit/#!/q-learning?path=index.html:64:6\">10000 steps</a> around the world and learn the optimal policy. I also plotted a heatmap of the Q function (the greener the square, the higher its value is). States close to the goal are more important, and this makes sense!</p> \n<p>You can check out that glitch, clone it, and play with that value. If you take far fewer steps (like 5000), you‚Äôll see that your policy isn‚Äôt perfect everywhere around the world (you might see the blob get stuck in circles a lot, far away from the goal, because it hasn‚Äôt explored that area well enough yet).</p> \n<iframe class=\"otter\" src=\"https://q-learning.glitch.me/\" frameborder=\"0\"></iframe> \n<hr /> \n<p>Hope this was useful! I wanted to write this post because I read this awesome <a href=\"https://www.alexirpan.com/2018/02/14/rl-hard.html\">article</a> by Alex Irpan about the problems with Deep Learning, but I didn‚Äôt know who to share it with, because I don‚Äôt really hang out with RL researchers anymore. So instead, I decided to teach you (YES, YOU!) some Reinforcement Learning, so that you can now read that article and not be lost in it. Yay? Yay!</p> \n<p>Thanks to <a href=\"https://twitter.com/danlizotte\">Dan Lizotte</a> for reading this, even though he really didn‚Äôt have to.</p>","descriptionType":"html","feedId":27745,"bgimg":"https://meowni.ca/images/2018-02-26/0.png","linkMd5":"948d7300c618738851557b4861b2fc30","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn8@2020_4/2020/11/18/08-24-10-564_1dd65de18eb28b9a.webp","destWidth":800,"destHeight":669,"sourceBytes":30618,"destBytes":24292,"author":"","articleImgCdnMap":{"https://meowni.ca/images/2018-02-26/0.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn8@2020_4/2020/11/18/08-24-10-564_1dd65de18eb28b9a.webp","https://meowni.ca/images/2018-02-26/1.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn24@2020_5/2020/11/18/08-24-11-570_64d211548f614ff5.webp","https://meowni.ca/images/2018-02-26/2.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn16@2020_5/2020/11/18/08-25-12-877_91fc71d08e64bba7.webp","https://meowni.ca/images/2018-02-26/3.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn28@2020_5/2020/11/18/08-24-11-548_eb46d348600c34d1.webp","https://meowni.ca/images/2018-02-26/4.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn19@2020_5/2020/11/18/08-24-11-543_c2d38b6984e78761.webp","https://meowni.ca/images/2018-02-26/5.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn11@2020_1/2020/11/18/08-24-11-491_aaf599df75ff40f8.webp"},"publishedOrCreatedDate":1605687850233}],"record":{"createdTime":"2020-11-18 16:24:10","updatedTime":"2020-11-18 16:24:10","feedId":27745,"fetchDate":"Wed, 18 Nov 2020 08:24:10 +0000","fetchMs":118,"handleMs":79,"totalMs":63716,"newArticles":0,"totalArticles":42,"status":1,"type":0,"ip":"11ba7afd62496d78a715fbed802c0b13","hostName":"us-001*","requestId":"aad96352d07a414692d7cc40163ac124_27745","contentType":"application/xml","totalBytes":201362,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":6,"articlesImgsGithubTotal":6,"successGithubMap":{"myreaderx33":1,"myreaderx3":1,"myreaderx24":1,"myreaderx31":1,"myreaderx5oss":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 02:42:11","updatedTime":"2020-09-07 04:44:32","id":27745,"name":"Monica Dinculescu","url":"http://meowni.ca/atom.xml","subscriber":117,"website":null,"icon":"https://meowni.ca/icons/icon-192x192.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn31@2020_4/2020/09/06/20-44-31-851_4acd0b34ab00d351.png","description":"","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":24292,"tmpBodyImgCdnBytes":177070,"tmpBgImgCdnBytes":0,"extra4":{"start":1605687850025,"total":0,"statList":[{"spend":129,"msg":"Ëé∑ÂèñxmlÂÜÖÂÆπ"},{"spend":79,"msg":"Ëß£ÈáäÊñáÁ´†"},{"spend":0,"msg":"‰∏ä‰º†Â∞ÅÈù¢ÂõæÂà∞cdn"},{"spend":0,"msg":"‰øÆÊ≠£Â∞ÅÈù¢Âõæ‰∏ä‰º†Â§±Ë¥•ÈáçÊñ∞‰∏ä‰º†"},{"spend":62325,"msg":"Ê≠£ÊñáÈìæÊé•‰∏ä‰º†Âà∞cdn"}]},"extra5":6,"extra6":6,"extra7ImgCdnFailResultVector":[null],"extra10_invalidATagHrefValue":{"https://meowni.ca/posts/rl-with-otters_/includes/mdinculescu_thesis.pdf":"https://meowni.ca/includes/mdinculescu_thesis.pdf"},"extra111_proxyServerAndStatMap":{"http://us-55.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe67.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://meowni.ca/images/2018-02-26/0.png","sourceStatusCode":200,"destWidth":800,"destHeight":669,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn8@2020_4/2020/11/18/08-24-10-564_1dd65de18eb28b9a.webp","sourceBytes":30618,"destBytes":24292,"targetWebpQuality":75,"feedId":27745,"totalSpendMs":968,"convertSpendMs":98,"createdTime":"2020-11-18 16:24:10","host":"us-017*","referer":"https://meowni.ca/posts/rl-with-otters","linkMd5ListStr":"948d7300c618738851557b4861b2fc30,948d7300c618738851557b4861b2fc30","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"sourceSize":"29.9 KB","destSize":"23.7 KB","compressRate":"79.3%"},{"code":1,"isDone":false,"source":"https://meowni.ca/images/2018-02-26/5.png","sourceStatusCode":200,"destWidth":1000,"destHeight":427,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn11@2020_1/2020/11/18/08-24-11-491_aaf599df75ff40f8.webp","sourceBytes":41832,"destBytes":33936,"targetWebpQuality":75,"feedId":27745,"totalSpendMs":876,"convertSpendMs":22,"createdTime":"2020-11-18 16:24:11","host":"us-005*","referer":"https://meowni.ca/posts/rl-with-otters","linkMd5ListStr":"948d7300c618738851557b4861b2fc30","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"sourceSize":"40.9 KB","destSize":"33.1 KB","compressRate":"81.1%"},{"code":1,"isDone":false,"source":"https://meowni.ca/images/2018-02-26/4.png","sourceStatusCode":200,"destWidth":1000,"destHeight":454,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn19@2020_5/2020/11/18/08-24-11-543_c2d38b6984e78761.webp","sourceBytes":39884,"destBytes":33100,"targetWebpQuality":75,"feedId":27745,"totalSpendMs":943,"convertSpendMs":35,"createdTime":"2020-11-18 16:24:11","host":"us-55*","referer":"https://meowni.ca/posts/rl-with-otters","linkMd5ListStr":"948d7300c618738851557b4861b2fc30","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"sourceSize":"38.9 KB","destSize":"32.3 KB","compressRate":"83%"},{"code":1,"isDone":false,"source":"https://meowni.ca/images/2018-02-26/3.png","sourceStatusCode":200,"destWidth":1200,"destHeight":435,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn28@2020_5/2020/11/18/08-24-11-548_eb46d348600c34d1.webp","sourceBytes":95039,"destBytes":40640,"targetWebpQuality":75,"feedId":27745,"totalSpendMs":979,"convertSpendMs":30,"createdTime":"2020-11-18 16:24:11","host":"us-021*","referer":"https://meowni.ca/posts/rl-with-otters","linkMd5ListStr":"948d7300c618738851557b4861b2fc30","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"sourceSize":"92.8 KB","destSize":"39.7 KB","compressRate":"42.8%"},{"code":1,"isDone":false,"source":"https://meowni.ca/images/2018-02-26/1.png","sourceStatusCode":200,"destWidth":1200,"destHeight":368,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn24@2020_5/2020/11/18/08-24-11-570_64d211548f614ff5.webp","sourceBytes":49835,"destBytes":41236,"targetWebpQuality":75,"feedId":27745,"totalSpendMs":1176,"convertSpendMs":59,"createdTime":"2020-11-18 16:24:11","host":"us-033*","referer":"https://meowni.ca/posts/rl-with-otters","linkMd5ListStr":"948d7300c618738851557b4861b2fc30","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"sourceSize":"48.7 KB","destSize":"40.3 KB","compressRate":"82.7%"},{"code":1,"isDone":false,"source":"https://meowni.ca/images/2018-02-26/2.png","sourceStatusCode":200,"destWidth":800,"destHeight":572,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn16@2020_5/2020/11/18/08-25-12-877_91fc71d08e64bba7.webp","sourceBytes":34539,"destBytes":28158,"targetWebpQuality":75,"feedId":27745,"totalSpendMs":921,"convertSpendMs":27,"createdTime":"2020-11-18 16:25:12","host":"us-021*","referer":"https://meowni.ca/posts/rl-with-otters","linkMd5ListStr":"948d7300c618738851557b4861b2fc30","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1„ÄÅÊ≤°ÊúâRefererÂ≠óÊÆµ","extra23historyStatusCode":[200],"sourceSize":"33.7 KB","destSize":"27.5 KB","compressRate":"81.5%"}],"successGithubMap":{"myreaderx33":1,"myreaderx3":1,"myreaderx24":1,"myreaderx31":1,"myreaderx5oss":1,"myreaderx":1},"failGithubMap":{}}