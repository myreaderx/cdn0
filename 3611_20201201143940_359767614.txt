{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-12-01 22:38:15","updatedTime":"2020-12-01 22:38:15","title":"Project InnerEye evaluation shows how AI can augment and accelerate clinicians’ ability  to perform radiotherapy planning 13 times faster","link":"https://www.microsoft.com/en-us/research/?p=707932","description":"</p>\n<figure class=\"wp-block-image size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/1400x788_Innereye_no_logo.gif\" alt=\"\"/></figure>\n</p>\n<p>Up to half of the population <a href=\"https://www.cancer.org/cancer/cancer-basics/lifetime-probability-of-developing-or-dying-from-cancer.html\">in the United States</a> and <a href=\"https://www.cancerresearchuk.org/health-professional/cancer-statistics/risk/lifetime-risk\">United Kingdom</a> will be diagnosed with cancer at some point in their lives. Of those, half will be treated with radiotherapy (RT), often in combination with other treatments such as surgery, chemotherapy, and increasingly immunotherapy. Radiotherapy involves focusing high-intensity radiation beams to damage the DNA of deep-seated cancerous tumors while avoiding surrounding healthy organs (known as organs at risk or OARs). Around 40% of successfully treated cancer patients undergo some form of radiotherapy, which shows how critical this tool is to cancer treatment regimens.</p>\n</p>\n<p><a href=\"https://www.microsoft.com/en-us/research/project/medical-image-analysis/\">Project InnerEye</a> is research conducted in the <a href=\"https://www.microsoft.com/en-us/research/theme/health-intelligence/\">Health Intelligence</a> team at <a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/\">Microsoft Research Cambridge (UK)</a> that is exploring ways in which machine learning (ML) has the potential to assist clinicians in planning their radiotherapy treatments so that they can spend more time with their patients. In September 2020, we released <a href=\"https://www.microsoft.com/en-us/research/blog/project-innereye-open-source-deep-learning-toolkit-democratizing-medical-imaging-ai/\">the InnerEye open-source deep learning toolkit</a>. Our latest peer-reviewed findings, published in the JAMA Network Open article titled “<a href=\"https://www.microsoft.com/en-us/research/publication/evaluation-of-deep-learning-to-augment-image-guided-radiotherapy-for-head-and-neck-and-prostate-cancers/\">Evaluation of Deep Learning to Augment Image Guided Radiotherapy for Head and Neck and Prostate Cancers</a>,” address the question:</p>\n</p>\n<p><strong>Can machine learning (ML) models achieve clinically acceptable image segmentation in radiotherapy planning and reduce overall contouring time?</strong></p>\n</p>\n<p>Planning radiotherapy treatment can be a lengthy process. It starts with a 3D CT (Computed Tomography) imaging scan of the part of the body to be targeted. These CT images come in the form of stacks of 2D images, dozens of images deep, each of which must be examined and marked up by a radiation oncologist or specialist technician. This process is called contouring. In each image, an expert must manually draw a contour line around the tumors and OARs in the target area using dedicated computer software. </p>\n</p>\n<p>For complex cases, this can take several hours in the planning of a single patient’s treatment. This image segmentation tasks consumes significant time and resources in the cancer treatment pathway for radiotherapy, which increases the burden on clinicians and the final cost to hospitals. As this task is subjective, there can be significant variability across experts and institutions where protocols and patient demographics vary. This is a limitation to the use of imaging in clinical trials and can introduce variability in patient care.</p>\n</p>\n<p>Our work shows that clinicians using ML assistance can segment images up to 13 times faster than doing it manually, with an accuracy that is within the bounds of human expert variability.</p>\n</p>\n<h2>Sourcing information from 8 clinical centers to build robust and generalizable machine learning models</h2>\n</p>\n<p>One of the barriers to the uptake of ML in clinical use across different hospitals is that most models are only trained on a dataset from a single institution and focus on a single task. This lack of generalizability can reduce the potential utility of ML in the real world, due to the model robustness across different institutions. To overcome this, we have developed generic models, trained on anonymized data from eight different clinical centers across Australia, Europe, New Zealand, North America, and South America. Models were trained on a dataset of 519 pelvic 3D CT planning scans and 242 of the head and neck, acquired as part of the treatment dose planning process. All identifying information was removed from the data by the clinical sites prior to the transfer to Microsoft Research<sup>1</sup>.</p>\n</p>\n<p>The de-identified images were then manually annotated by two clinically trained expert readers and two radiation oncologists. Two datasets were used for different purposes in this research: a main dataset and an external dataset. In the <em>main </em>dataset, the ML segmentation models were trained on data from five clinical sites to automatically delineate 15 different target structures. The <em>external </em>dataset comprised data not used for training from three of the clinical sites, which allowed for blind testing of the ML models.</p>\n</p>\n<p class=\"has-small-font-size\"><sup>1</sup> A deﬁned list of tags was used indicating which are retained, hashed, or randomized in the case of the dates and times. Any tags that were not in the list were removed. Tags were only retained where related to image geometry and scanner settings and other non-identiﬁable tags (such as gender) that were necessary to process the data.</p>\n</p>\n<h2>Working toward the end goal of saving clinicians time and resources</h2>\n</p>\n<p>We performed an evaluation of the potential clinical utility of ML in the radiotherapy planning pathway by comparing the time taken for the <em>end-to-end</em> image segmentation task performed manually by clinicians, with the time taken when clinicians use the ML model to assist them in marking up images. This provides key insights into how ML might be applied to reduce clinician workload, overall planning time, and hospital costs.</p>\n</p>\n<p>The image segmentation model is a state-of-the-art convolutional neural network based on a 3D U-Net architecture, with approximately 39 million trainable parameters.</p>\n</p>\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"837\" height=\"789\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog.jpg\" alt=\"\" class=\"wp-image-707968\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog.jpg 837w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog-300x283.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog-768x724.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog-13x12.jpg 13w\" sizes=\"(max-width: 837px) 100vw, 837px\" /><figcaption>Figure 1: The 3D U-Net model shown on top encodes a given input 3D CT scan in multiple image scales to extract the necessary semantic information for the segmentation end task. Individual components of the segmentation model are shown in detail: encoder, decoder, and aggregation blocks.</figcaption></figure>\n</p>\n<p>We used Microsoft <a href=\"https://azure.microsoft.com/en-us/services/machine-learning/\">Azure Machine Learning</a> to allow us to easily develop and train our models across 20 NVIDIA Tesla V100 GPUs. See <a href=\"https://www.microsoft.com/en-us/research/publication/evaluation-of-deep-learning-to-augment-image-guided-radiotherapy-for-head-and-neck-and-prostate-cancers/\">our paper </a>for how we used mixed-precision representations and pipeline parallelism to reduce memory requirements, lower time to solution, and improve inference speed of the model.</p>\n</p>\n<h2>Is the ML model usable in radiotherapy practice?</h2>\n</p>\n<p>One way of thinking about this is to check how the model compares with the difference of interpretation between expert clinicians performing the same task. To test this, we performed an inter-observer variability (IOV) study using 10 test images each for pelvis and head-and-neck cases, comparing the variability between three expert clinicians.</p>\n</p>\n<p>To create ground truth segmentations, we collected manually generated contours from multiple experts and aggregated them using a majority voting rule. For instance, in the case of three experts, if two of the experts indicated that a particular voxel belonged to the prostate, then a prostate label was assigned regardless of the third labeler&#8217;s opinion. For the IOV study we used a volumetric measure called <a href=\"https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\">the Dice score</a> that compared the overlap between segmented structures in pairs of images. Perfectly overlapping structures have a Dice score of 100%, while a Dice score of 0% corresponds to no overlap. We also measured the distance between the surface contour of the manual and automatic segmentations as additional metrics: Hausdorff distance and mean surface-to-surface distance.</p>\n</p>\n<p>\n</p>\n</p>\n<p>We used two widely used statistical measurements, Cohen’s Kappa and Fleiss’ Kappa for single and multiple annotators respectively, to assess the agreement between contours generated by the model and expert readers. After plotting these results visually, we learned that similarity scores when compared with ground truth are on par with expert IOV in contouring, giving us some confidence that errors in the ML model fall within the variability of the three experts involved in the IOV study. See <a href=\"https://www.microsoft.com/en-us/research/publication/evaluation-of-deep-learning-to-augment-image-guided-radiotherapy-for-head-and-neck-and-prostate-cancers/\">our paper</a> for the statistical agreement results, such as Bland-Altman plots.</p>\n</p>\n<p>We also wanted to see if the ML model might be usable on datasets not used in training and applicable for more widespread use. So we tested it on <em>external </em>datasets from three clinical sites, separate from those used for training, each of which had different CT imaging protocols, scanner hardware, and patient groups. This would give us confidence in the generalizability of the model beyond the sites supplying training data. We used the Mann-Whitney U test to measure the model performance difference across datasets. Our observations of the segmentation errors tended to occur in the superior and inferior extent of tubular structures and in the interface between adjacent organs. However, we have not observed any inconsistencies that, if not corrected, could lead to significant errors in a treatment plan, as evidenced by the surface distance results. This is because the proposed postprocessing method does not allow inconsistencies at a distance from the anatomical structure by design.</p>\n</p>\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"1024\" height=\"402\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-1024x402.jpg\" alt=\"Six example images of head-and-neck and pelvis CT scans (three of each). Darker colored outlines and lighter colored outlines of similar areas in the scans show roughly the same area outlined. \" class=\"wp-image-707974\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-1024x402.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-300x118.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-768x302.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-16x6.jpg 16w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1.jpg 1029w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 2: The algorithm predictions are shown in darker colors in comparison to the ground-truth OAR annotations. The corresponding expert reader annotations are shown in lighter colors. From left to right (columns displaying mid-axial and mid-coronal slices): for comparison purposes the first scan on the left is retrieved from the main dataset and the remaining two are from the external dataset. Here we see differences between the datasets in terms of patient anatomy and through-plane scan resolution.</figcaption></figure>\n</p>\n<p>Our results show that the ML model greatly reduces the time it takes for end-to-end image segmentation and annotation in radiotherapy. This included both the time to draw segmentation contours on the image and time to correct inaccuracies in the automated (or semi-automated) system. Figure 4 shows the time taken for a radiation oncologist to perform image contouring manually, compared with using the ML model, including the time for the expert to inspect and update the contours to ensure clinical accuracy. The time taken for the ML model to perform inference was only 23 ± 3 <em>seconds </em>in a full input CT scan. For the radiation oncologists in this specific research study, the time taken is shown to be reduced by over 90% for head and neck image segmentation.</p>\n</p>\n<figure class=\"wp-block-image size-large is-resized\"><img loading=\"lazy\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-1024x656.png\" alt=\"Figure 3: Time taken to perform image segmentation task (contouring) manually compared with time taken using the InnerEye ML model to read head-and-neck CT scans, including time for an expert to check and update contours for clinical accuracy (all timings in minutes). The 10 images from the head-and-neck IOV dataset used for this study varied in imaging quality. In-house image annotation software was used for both contouring and correction tasks, which include assistive contouring and interactive contour refinement tools.\" class=\"wp-image-707977\" width=\"900\" height=\"576\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-1024x656.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-300x192.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-768x492.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-16x10.png 16w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3.png 1379w\" sizes=\"(max-width: 900px) 100vw, 900px\" /><figcaption>Figure 3: Time taken to perform image segmentation task (contouring) manually compared with time taken using the InnerEye ML model to read head-and-neck CT scans, including time for an expert to check and update contours for clinical accuracy (all timings in minutes). The 10 images from the head-and-neck IOV dataset used for this study varied in imaging quality. In-house image annotation software was used for both contouring and correction tasks, which include assistive contouring and interactive contour refinement tools.</figcaption></figure>\n</p>\n<h2>Creating an end-to-end deployment framework for use in clinics</h2>\n</p>\n<p>Our reproducible ML model and work creates an opportunity for easy and widespread adoption of auto-segmentation models into existing radiotherapy workflows. However, creating an ML model that performs well enough to be clinically useful does not necessarily mean that it can be deployed successfully in the clinic. The additional engineering and infrastructure required to integrate it into a clinical setting is significant. To help bridge this gap between research and application deployment, the InnerEye team has been working with Microsoft Azure over the last three years to create an end-to-end framework for both edge and cloud deployment using the industry-standard Digital Imaging and Communications in Medicine (DICOM) image format.</p>\n</p>\n<p>In the proposed workflow shown in Figure 4, CT scans are acquired from patients as they attend preparations for radiotherapy treatment. These scans are initially stored at the hospital’s image database and later securely transferred via the gateway to the auto-segmentation platform in the cloud after anonymizing them. Once the segmentation process is completed, resultant files are uploaded back to the hospital’s image database, creating a seamless clinical workflow where clinicians can review and refine contours in their existing contouring and planning tools. </p>\n</p>\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"1024\" height=\"412\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-1024x412.jpg\" alt=\"Radiotherapy planning workflow. Image aquisition followed by Image storage/PACs. The images are sent through a refinement tool and then back to image storage. These images are then sent through the InnerEye Gateway and then through InnerEye inference, finally moving through InnerEye training with Azure Machine Learning.\" class=\"wp-image-707983\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-1024x412.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-300x121.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-768x309.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-16x6.jpg 16w, https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4.jpg 1335w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /><figcaption>Figure 4: Integration of the proposed segmentation models into radiotherapy planning workflow. 3D CT scans acquired from patients are anonymized and passed through the gateway after receiving an informed consent form from patients. The gateway technology establishes a secure and scalable connection between clinical sites and the auto-segmentation platform in the cloud. It provides both model training and deployment services using the compute resources in Azure. Once OAR contours are automatically generated, the gateway uploads files back to the hospital’s image database and seamlessly integrates them into DICOM viewer software. In the last stage, the contours can be reviewed and further refined, if required, by radiation oncologists prior to generating dose plans.</figcaption></figure>\n</p>\n<h2>Clinical utility of ML for radiotherapy</h2>\n</p>\n<p>This most recent work demonstrates the potential for our InnerEye research in the clinical world. We have shown how we:</p>\n</p>\n<p>• <strong>trained ML models </strong>that can be easily integrated into current radiotherapy practices (with approval from the appropriate regulatory agencies) and have accuracy within the bounds of human expert variability;<br />• <strong>tested the robustness of the model</strong> when applied to images from clinical sites with different protocols and imaging hardware;<br />• <strong>indicated potential time savings</strong> for complex radiotherapy planning clinical workflows of over 90%; and<br />• <strong>developed underlying cloud platform technology</strong> that could be used for seamless integration into existing clinical workflows.</p>\n</p>\n<p>While the ML models have been shown to perform well enough to be relevant for clinical practice, it is imperative that clinicians and experts remain in the loop to assess accuracy and clinical significance. The ability for experts to manually correct the model outputs is a necessary component of the ML-augmented radiotherapy workflow.</p>\n</p>\n<p>We hope our latest work contributes to addressing the practical challenges of scalable adoption of ML across healthcare systems and opens possibilities for new radiotherapy treatments to become mainstream. By making the source code used in this study publicly available as open-source software in the <a href=\"https://github.com/microsoft/InnerEye-DeepLearning/\">InnerEye Deep Learning Toolkit on GitHub</a>, we are making our research more reproducible and empowering researchers and organizations to build on this work by training and deploying their own ML models, using their own datasets.</p>\n</p>\n<div class=\"annotations\" data-bi-area=\"citation\">\n<ul class=\"annotations__list \">\n<li class=\"annotations__list-item\">\n\t\t\t\t\t\t<span class=\"annotations__type\">VIDEO</span><br />\n\t\t\t<a href=\"https://aka.ms/tmie\" data-bi-type=\"annotated-link\" data-bi-area=\"citation\" data-bi-name=\"Tech Minutes: Inner Eye\"><br />\n\t\t\t\tTech Minutes: Inner Eye\t\t\t</a><br />\n\t\t\t<span class=\"svg-icon icon-external-link\"></span><br />\n\t\t\t\t\t\t\t<span class=\"annotations__caption\">Javier Alverez explains how we’re using state of the art machine learning technology to build innovative tools for the automatic, quantitative analysis of three-dimensional medical images.</span>\n\t\t\t\t\t</li>\n</ul>\n</div>\n</p>\n<p>We are excited to see how our work will be built upon to improve the experience of clinicians planning radiotherapy and enhance cancer treatment for patients at centers around the world.</p>\n</p></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/project-innereye-evaluation-shows-how-ai-can-augment-and-accelerate-clinicians-ability-to-perform-radiotherapy-planning-13-times-faster/\">Project InnerEye evaluation shows how AI can augment and accelerate clinicians’ ability  to perform radiotherapy planning 13 times faster</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Mon, 30 Nov 2020 16:14:53 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/1400x788_Innereye_no_logo.gif","linkMd5":"c63b1bc2db28a1641dd266e18ff4b1fe","destWidth":1400,"destHeight":788,"sourceBytes":1341218,"destBytes":610242,"author":"Alexis Hagen","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/1400x788_Innereye_no_logo.gif":null,"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog.jpg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn60@2020_1/2020/12/01/14-39-39-260_76c6e3fba365e048.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-1024x402.jpg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn49@2020_2/2020/12/01/14-39-39-245_cc09c566bb44fad7.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-1024x656.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn48@2020_2/2020/12/01/14-39-39-358_2477cee33be3eb00.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-1024x412.jpg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_3/2020/12/01/14-39-39-238_6f150f9b15735883.webp"},"publishedOrCreatedDate":1606833495290}],"record":{"createdTime":"2020-12-01 22:38:15","updatedTime":"2020-12-01 22:38:15","feedId":3611,"fetchDate":"Tue, 01 Dec 2020 14:38:15 +0000","fetchMs":71,"handleMs":38,"totalMs":85034,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"0c808457295d30b82472f0d65e841800","hostName":"us-034*","requestId":"a77b49fa175746cca283abb758a85495_3611","contentType":"application/rss+xml; charset=UTF-8","totalBytes":132024,"bgimgsTotal":1,"bgimgsGithubTotal":0,"articlesImgsTotal":5,"articlesImgsGithubTotal":4,"successGithubMap":{"myreaderx10":1,"myreaderx3":1,"myreaderx2":1,"myreaderx18":1},"failGithubMap":{"myreaderx23":1}},"feed":{"createdTime":"2020-08-25 04:29:28","updatedTime":"2020-09-01 10:32:05","id":3611,"name":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/feed/","subscriber":null,"website":null,"icon":"https://www.microsoft.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx62/cdn38@2020_5/2020/09/01/02-32-02-775_ddd9473b17dec87a.ico","description":"","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2020-12-01 22:39:40","updatedTime":"2020-12-01 22:39:40","id":null,"feedId":3611,"linkMd5":"c63b1bc2db28a1641dd266e18ff4b1fe"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":132024,"tmpBgImgCdnBytes":0,"extra4":{"start":1606833495172,"total":0,"statList":[{"spend":81,"msg":"获取xml内容"},{"spend":38,"msg":"解释文章"},{"spend":1,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":1050,"msg":"正文链接上传到cdn"}]},"extra5":5,"extra6":5,"extra7ImgCdnFailResultVector":[null,{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/1400x788_Innereye_no_logo.gif","sourceStatusCode":200,"destWidth":1400,"destHeight":788,"sourceBytes":1341218,"destBytes":610242,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":23666,"convertSpendMs":23392,"createdTime":"2020-12-01 22:39:15","host":"us-51*","referer":"https://www.microsoft.com/en-us/research/?p=707932","linkMd5ListStr":"c63b1bc2db28a1641dd266e18ff4b1fe,c63b1bc2db28a1641dd266e18ff4b1fe","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn5/contents/2020/12/01/14-39-39-078_2d3e707fbd3df820.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Tue, 01 Dec 2020 14:39:39 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["EC8E:3CD0:476DC3:9ED16E:5FC655A5"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1606836451"],"x-ratelimit-used":["60"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn5/contents/2020/12/01/14-39-39-078_2d3e707fbd3df820.webp","historyStatusCode":[],"spendMs":66},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"595.9 KB","compressRate":"45.5%","sourceSize":"1.3 MB"}],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-038.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-014.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-026.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSR-Blog-InnerEye-Figure-4-1024x412.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":412,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn54@2020_3/2020/12/01/14-39-39-238_6f150f9b15735883.webp","sourceBytes":35933,"destBytes":16412,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":951,"convertSpendMs":13,"createdTime":"2020-12-01 22:39:39","host":"us-026*","referer":"https://www.microsoft.com/en-us/research/?p=707932","linkMd5ListStr":"c63b1bc2db28a1641dd266e18ff4b1fe","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"16 KB","compressRate":"45.7%","sourceSize":"35.1 KB"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/MSRblog-InnerEye_fig1-1024x402.jpg","sourceStatusCode":200,"destWidth":1024,"destHeight":402,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn49@2020_2/2020/12/01/14-39-39-245_cc09c566bb44fad7.webp","sourceBytes":70246,"destBytes":38060,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":992,"convertSpendMs":23,"createdTime":"2020-12-01 22:39:39","host":"us-025*","referer":"https://www.microsoft.com/en-us/research/?p=707932","linkMd5ListStr":"c63b1bc2db28a1641dd266e18ff4b1fe","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"37.2 KB","compressRate":"54.2%","sourceSize":"68.6 KB"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/Figure-1_InnerEyeBlog.jpg","sourceStatusCode":200,"destWidth":837,"destHeight":789,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn60@2020_1/2020/12/01/14-39-39-260_76c6e3fba365e048.webp","sourceBytes":84186,"destBytes":53524,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1029,"convertSpendMs":24,"createdTime":"2020-12-01 22:39:39","host":"us-014*","referer":"https://www.microsoft.com/en-us/research/?p=707932","linkMd5ListStr":"c63b1bc2db28a1641dd266e18ff4b1fe","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"52.3 KB","compressRate":"63.6%","sourceSize":"82.2 KB"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/11/InnerEye-Blog-Figure-3-1024x656.png","sourceStatusCode":200,"destWidth":1024,"destHeight":656,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn48@2020_2/2020/12/01/14-39-39-358_2477cee33be3eb00.webp","sourceBytes":45941,"destBytes":24028,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1029,"convertSpendMs":102,"createdTime":"2020-12-01 22:39:39","host":"us-038*","referer":"https://www.microsoft.com/en-us/research/?p=707932","linkMd5ListStr":"c63b1bc2db28a1641dd266e18ff4b1fe","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"destSize":"23.5 KB","compressRate":"52.3%","sourceSize":"44.9 KB"}],"successGithubMap":{"myreaderx10":1,"myreaderx3":1,"myreaderx2":1,"myreaderx18":1},"failGithubMap":{"myreaderx23":1}}