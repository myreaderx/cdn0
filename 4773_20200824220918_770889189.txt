{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Improving Verifiability\nin AI Development","link":"https://openai.com/blog/improving-verifiability/","description":"<!--kg-card-begin: markdown--> \n<div class=\"medium-copy color-fg-80 mt-n0.5\"> \n <p>We’ve contributed to a multi-stakeholder report by <a href=\"https://openai.com/blog/improving-verifiability/#authors\">58 co-authors</a> at 30 organizations, including the <a href=\"http://lcfi.ac.uk/\">Centre for the Future of Intelligence</a>, <a href=\"https://mila.quebec/en/\">Mila</a>, <a href=\"https://sr-institute.utoronto.ca/\">Schwartz Reisman Institute for Technology and Society</a>, <a href=\"https://casbs.stanford.edu/\">Center for Advanced Study in the Behavioral Sciences</a>, and <a href=\"https://cset.georgetown.edu/\">Center for Security and Emerging Technologies</a>. This report describes 10 mechanisms to improve the verifiability of claims made about AI systems. Developers can use these tools to provide evidence that AI systems are safe, secure, fair, or privacy-preserving. Users, policymakers, and civil society can use these tools to evaluate AI development processes.</p> \n</div> \n<section class=\"btns\"> \n <a href=\"https://arxiv.org/abs/2004.07213\" class=\"btn btn-padded icon-paper\">Read Report</a> \n</section> \n<p>While a growing number of organizations have articulated ethics principles to guide their AI development process, it can be difficult for those outside of an organization to verify whether the organization's AI systems reflect those principles in practice. This ambiguity makes it harder for stakeholders such as users, policymakers, and civil society to scrutinize AI developers' claims about properties of AI systems and could fuel competitive corner-cutting, increasing social risks and harms. The report describes existing and potential mechanisms that can help stakeholders grapple with questions like:</p> \n<ul> \n <li>Can I (as a user) verify the claims made about the level of privacy protection guaranteed by a new AI system I’d like to use for machine translation of sensitive documents?</li> \n <li>Can I (as a regulator) trace the steps that led to an accident caused by an autonomous vehicle? Against what standards should an autonomous vehicle company’s safety claims be compared?</li> \n <li>Can I (as an academic) conduct impartial research on the risks associated with large-scale AI systems when I lack the computing resources of industry?</li> \n <li>Can I (as an AI developer) verify that my competitors in a given area of AI development will follow best practices rather than cut corners to gain an advantage?</li> \n</ul> \n<p>The 10 mechanisms highlighted in the report are listed below, along with recommendations aimed at advancing each one. (See the <a href=\"https://arxiv.org/abs/2004.07213\">report</a> for discussion of how these mechanisms support verifiable claims as well as relevant caveats about our findings.)</p> \n<div id=\"recommendations\" class=\"my-1.5 py-0.25 rounded shadowed bg-white\"> \n <div class=\"px-1\"> \n  <h4 class=\"mt-0.75 mb-0.75\">Institutional Mechanisms and Recommendations</h4> \n  <ol class=\"list-indented\"> \n   <li><strong>Third party auditing</strong>. A coalition of stakeholders should create a task force to research options for conducting and funding third party auditing of AI systems.</li> \n   <li><strong>Red teaming exercises</strong>. Organizations developing AI should run red teaming exercises to explore risks associated with systems they develop, and should share best practices and tools.</li> \n   <li><strong>Bias and safety bounties</strong>. AI developers should pilot bias and safety bounties for AI systems to strengthen incentives and processes for broad-based scrutiny of AI systems.</li> \n   <li><strong>Sharing of AI incidents</strong>. AI developers should share more information about AI incidents, including through collaborative channels.</li> \n  </ol> \n </div> \n <hr class=\"my-0\"> \n  <div class=\"px-1\"> \n   <h4 class=\"mt-1 mb-0.75\">Software Mechanisms and Recommendations</h4> \n   <ol class=\"list-indented\" style=\"counter-reset:decimal-counter 4\"> \n    <li><strong>Audit trails</strong>. Standard setting bodies should work with academia and industry to develop audit trail requirements for safety-critical applications of AI systems.</li> \n    <li><strong>Interpretability</strong>. Organizations developing AI and funding bodies should support research into the interpretability of AI systems, with a focus on supporting risk assessment and auditing.</li> \n    <li><strong>Privacy-preserving machine learning</strong>. AI developers should develop, share, and use suites of tools for privacy-preserving machine learning that include measures of performance against common standards.</li> \n   </ol> \n  </div> \n  <hr class=\"my-0\"> \n   <div class=\"px-1\"> \n    <h4 class=\"mt-1 mb-0.75\">Hardware Mechanisms and Recommendations</h4> \n    <ol class=\"list-indented\" style=\"counter-reset:decimal-counter 7\"> \n     <li><strong>Secure hardware for machine learning</strong>. Industry and academia should work together to develop hardware security features for AI accelerators or otherwise establish best practices for the use of secure hardware (including secure enclaves on commodity hardware) in machine learning contexts. </li> \n     <li><strong>High-precision compute measurement</strong>. One or more AI labs should estimate the computing power involved in a single project in great detail and report on lessons learned regarding the potential for wider adoption of such methods.</li> \n     <li><strong>Compute support for academia</strong>. Government funding bodies should substantially increase funding for computing power resources for researchers in academia, in order to improve the ability of those researchers to verify claims made by industry.</li> \n    </ol> \n   </div> \n  </hr> \n </hr> \n</div> \n<!-- end #recommendations --> \n<p>We and our co-authors will be doing further research on these mechanisms and OpenAI will be looking to adopt several of these mechanisms in the future. We hope that this report inspires meaningful dialogue, and we are eager to discuss additional institutional, software, and hardware mechanisms that could be useful in enabling trustworthy AI development. We encourage anyone interested in collaborating on these issues to connect with the corresponding authors and visit the <a href=\"http://www.towardtrustworthyai.com/\">report website</a>.</p> \n<section class=\"btns mt-1.5\"> \n <a href=\"https://arxiv.org/abs/2004.07213\" class=\"btn btn-padded icon-paper\">Read Report</a> \n</section> \n<div class=\"wide mt-3 mb-1\" id=\"authors\"> \n <hr class=\"my-0\"> \n  <div class=\"small-copy mt-2/3\"> \n   <div class=\"row\"> \n    <div class=\"col-12 col-xl-3\"> \n     <div class=\"color-fg-50 mb-1\">\n       Report Authors \n     </div> \n    </div> \n    <div class=\"col-12 col-xl-9\"> \n     <ul class=\"list-unstyled multicol-sm-2 multicol-lg-3 mb-0.25\"> \n      <li><a href=\"mailto:miles@openai.com\">Miles Brundage</a> <span class=\"affiliation\">OpenAI</span></li> \n      <li><a href=\"mailto:sa478@cam.ac.uk\">Shahar Avin</a> <span class=\"affiliation\">Centre for the Study of Existential Risk, Leverhulme Centre for the Future of Intelligence</span></li> \n      <li><a href=\"mailto:jasminewang76@gmail.com\">Jasmine Wang</a> <span class=\"affiliation\">Mila, University of Montreal</span></li> \n      <li><a href=\"mailto:hb492@cam.ac.uk\">Haydn Belfield</a> <span class=\"affiliation\">Centre for the Study of Existential Risk, Leverhulme Centre for the Future of Intelligence</span></li> \n      <li><a href=\"mailto:gretchen@openai.com\">Gretchen Krueger</a> <span class=\"affiliation\">OpenAI</span></li> \n     </ul> \n     <div class=\"small-caps color-fg-30 mt-0.25\">\n       (Equal contribution) \n     </div> \n     <ul class=\"list-unstyled multicol-sm-2 multicol-lg-3 mb-0.25 mt-1\"> \n      <li>Gillian Hadfield <span class=\"affiliation\">OpenAI, University of Toronto, Schwartz Reisman Institute for Technology and Society</span></li> \n      <li>Heidy Khlaaf <span class=\"affiliation\">Adelard</span></li> \n      <li>Jingying Yang <span class=\"affiliation\">Partnership on AI</span></li> \n      <li>Helen Toner <span class=\"affiliation\">Center for Security and Emerging Technology</span></li> \n      <li>Ruth Fong <span class=\"affiliation\">University of Oxford</span></li> \n      <li>Tegan Maharaj <span class=\"affiliation\">Mila, Montreal Polytechnic</span></li> \n      <li>Pang Wei Koh <span class=\"affiliation\">Stanford University</span></li> \n      <li>Sara Hooker <span class=\"affiliation\">Google Brain</span></li> \n      <li>Jade Leung <span class=\"affiliation\">Future of Humanity Institute</span></li> \n      <li>Andrew Trask <span class=\"affiliation\">University of Oxford</span></li> \n      <li>Emma Bluemke <span class=\"affiliation\">University of Oxford</span></li> \n      <li>Jonathan Lebensold <span class=\"affiliation\">Mila, McGill University</span></li> \n      <li>Cullen O’Keefe <span class=\"affiliation\">OpenAI</span></li> \n      <li>Mark Koren <span class=\"affiliation\">Stanford Centre for AI Safety</span></li> \n      <li>Théo Ryffel <span class=\"affiliation\">École Normale Supérieure (Paris)</span></li> \n      <li>JB Rubinovitz <span class=\"affiliation\">Remedy.AI</span></li> \n      <li>Tamay Besiroglu <span class=\"affiliation\">University of Cambridge</span></li> \n      <li>Federica Carugati <span class=\"affiliation\">Center for Advanced Study in the Behavioral Sciences</span></li> \n      <li>Jack Clark <span class=\"affiliation\">OpenAI</span></li> \n      <li>Peter Eckersley <span class=\"affiliation\">Partnership on AI</span></li> \n      <li>Sarah de Haas <span class=\"affiliation\">Google Research</span></li> \n      <li>Maritza Johnson <span class=\"affiliation\">Google Research</span></li> \n      <li>Ben Laurie <span class=\"affiliation\">Google Research</span></li> \n      <li>Alex Ingerman <span class=\"affiliation\">Google Research</span></li> \n      <li>Igor Krawczuk <span class=\"affiliation\">École Polytechnique Fédérale de Lausanne</span></li> \n      <li>Amanda Askell <span class=\"affiliation\">OpenAI</span></li> \n      <li>Rosario Cammarota <span class=\"affiliation\">Intel</span></li> \n      <li>Andrew Lohn <span class=\"affiliation\">RAND Corporation</span></li> \n      <li>David Krueger <span class=\"affiliation\">Mila, Montreal Polytechnic</span></li> \n      <li>Charlotte Stix <span class=\"affiliation\">Eindhoven University of Technology</span></li> \n      <li>Peter Henderson <span class=\"affiliation\">Stanford University</span></li> \n      <li>Logan Graham <span class=\"affiliation\">University of Oxford</span></li> \n      <li>Carina Prunkl <span class=\"affiliation\">Future of Humanity Institute</span></li> \n      <li>Bianca Martin <span class=\"affiliation\">OpenAI</span></li> \n      <li>Elizabeth Seger <span class=\"affiliation\">University of Cambridge</span></li> \n      <li>Noa Zilberman <span class=\"affiliation\">University of Oxford</span></li> \n      <li>Seán Ó hÉigeartaigh <span class=\"affiliation\">Leverhulme Centre for the Future of Intelligence, Centre for the Study of Existential Risk</span></li> \n      <li>Frens Kroeger <span class=\"affiliation\">Coventry University</span></li> \n      <li>Girish Sastry <span class=\"affiliation\">OpenAI</span></li> \n      <li>Rebecca Kagan <span class=\"affiliation\">Center for Security and Emerging Technology</span></li> \n      <li>Adrian Weller <span class=\"affiliation\">University of Cambridge, Alan Turing Institute</span></li> \n      <li>Brian Tse <span class=\"affiliation\">Future of Humanity Institute, Partnership on AI</span></li> \n      <li>Elizabeth Barnes <span class=\"affiliation\">OpenAI</span></li> \n      <li>Allan Dafoe <span class=\"affiliation\">Future of Humanity Institute</span></li> \n      <li>Paul Scharre <span class=\"affiliation\">Center for a New American Security</span></li> \n      <li>Ariel Herbert-Voss <span class=\"affiliation\">OpenAI</span></li> \n      <li>Martijn Rasser <span class=\"affiliation\">Center for a New American Security</span></li> \n      <li>Shagun Sodhani <span class=\"affiliation\">Mila, University of Montreal</span></li> \n      <li>Carrick Flynn <span class=\"affiliation\">Center for Security and Emerging Technology</span></li> \n      <li>Thomas Gilbert <span class=\"affiliation\">University of California, Berkeley</span></li> \n      <li>Lisa Dyer <span class=\"affiliation\">Partnership on AI</span></li> \n      <li>Saif Khan <span class=\"affiliation\">Center for Security and Emerging Technology</span></li> \n      <li>Yoshua Bengio <span class=\"affiliation\">Mila, University of Montreal</span></li> \n      <li>Markus Anderljung <span class=\"affiliation\">Future of Humanity Institute</span></li> \n     </ul> \n     <div class=\"small-caps color-fg-30 mt-0.25\">\n       (Descending contribution) \n     </div> \n    </div> \n   </div> \n   <!-- end .row --> \n  </div> \n </hr> \n</div> \n<!--kg-card-end: markdown-->","descriptionType":"html","publishedDate":"Thu, 16 Apr 2020 15:59:39 +0000","feedId":4773,"bgimg":"","linkMd5":"24e4109d28ea659b98e7834394193ca0","bgimgJsdelivr":"","metaImg":"","author":"OpenAI","publishedOrCreatedDate":1598306916761},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"OpenAI Five","link":"https://openai.com/projects/five/","description":"<!--kg-card-begin: markdown-->\n<div class=\"medium-copy\"> \n <img src=\"https://openai.com/content/images/2019/12/OPEN_AI_DOTA_FINALS_3_DSC1231.JPG\" alt=\"OpenAI Five\"><p>At OpenAI, we’ve used the multiplayer video game <a href=\"https://www.dota2.com/play/\">Dota 2</a> as a research platform for general-purpose AI systems. Our Dota 2 AI, called OpenAI Five, learned by playing over 10,000 years of games against itself. It demonstrated the ability to achieve <a href=\"https://openai.com/blog/openai-five-defeats-dota-2-world-champions\">expert-level performance</a>, learn <a href=\"https://openai.com/blog/openai-five-defeats-dota-2-world-champions/#cooperativemode\">human–AI cooperation</a>, and <a href=\"https://openai.com/blog/openai-five-defeats-dota-2-world-champions/#arena\">operate at internet scale</a>.</p> </img>\n</div> \n<section class=\"btns\">\n <a href=\"https://arxiv.org/abs/1912.06680\" class=\"btn btn-padded icon-paper\">Read Paper</a>\n <a href=\"https://openai.com/blog/tags/dota-2/\" class=\"btn btn-padded icon-papers\">Read Blog Posts</a>\n</section> \n<!--kg-card-end: markdown-->","descriptionType":"html","publishedDate":"Fri, 13 Dec 2019 17:15:41 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2019/12/OPEN_AI_DOTA_FINALS_3_DSC1231.JPG","linkMd5":"006992e573a7d2cc169071f39b466593","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn46@2020_6/2020/08/24/22-09-05-306_8a441a626ecf2d01.webp","destWidth":2000,"destHeight":1333,"sourceBytes":366714,"destBytes":204738,"author":"OpenAI","articleImgCdnMap":{"https://openai.com/content/images/2019/12/OPEN_AI_DOTA_FINALS_3_DSC1231.JPG":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn46@2020_6/2020/08/24/22-09-05-306_8a441a626ecf2d01.webp"},"publishedOrCreatedDate":1598306916761},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Deep Double Descent","link":"https://openai.com/blog/deep-double-descent/","description":"<!--kg-card-begin: markdown-->\n<div class=\"js-excerpt\"> \n <img src=\"https://openai.com/content/images/2019/12/Frame-1--3-.png\" alt=\"Deep Double Descent\"><p>We show that the <a href=\"https://arxiv.org/abs/1812.11118\">double</a> <a href=\"https://arxiv.org/abs/1710.03667\">descent</a> <a href=\"https://arxiv.org/abs/1809.09349\">phenomenon</a> occurs in CNNs, ResNets, and transformers: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly universal, we don't yet fully understand why it happens, and view further study of this phenomenon as an important research direction.</p> </img>\n</div> \n<section class=\"btns\">\n <a href=\"https://arxiv.org/abs/1912.02292\" class=\"btn btn-padded icon-paper\">Read Paper</a>\n</section> \n<p>Many classes of modern deep learning models, including CNNs, ResNets, and transformers, exhibit the previously-observed <a href=\"https://arxiv.org/abs/1812.11118\">double</a> <a href=\"https://arxiv.org/abs/1710.03667\">descent</a> <a href=\"https://arxiv.org/abs/1809.09349\">phenomenon</a> when not using early stopping or regularization. The peak occurs predictably at a \"critical regime,\" where the models are barely able to fit the training set. As we increase the number of parameters in a neural network, the test error initially decreases, increases, and, just as the model is able to fit the train set, undergoes a second descent.</p> \n<p>Neither classical statisticians’ conventional wisdom that <em>too large models are worse</em> nor the modern ML paradigm that <em>bigger models are better</em> uphold. We find that double descent also occurs over train epochs. Surprisingly, we show these phenomena can lead to a regime where more data hurts, and training a deep network on a larger train set actually performs worse.</p> \n<h4 class=\"mb-1/12\" id=\"modelwise\">Model-wise double descent</h4> \n<div class=\"medium-copy mb-1\">\n 1. There is a regime where bigger models are worse.\n</div> \n<figure class=\"mx-n0.5\"> \n <p><img src=\"https://openai.com/content/images/2019/12/modeldd.svg\" alt=\"Deep Double Descent\" /></p> \n</figure> \n<p>The model-wise double descent phenomenon can lead to a regime where training on more data hurts. In the chart above, the peak in test error occurs around the interpolation threshold, when the models are just barely large enough to fit the train set.</p> \n<p>In all cases we've observed, changes which affect the interpolation threshold (such as changing the optimization algorithm, the number of train samples, or the amount of label noise) also affect the location of the test error peak correspondingly. The double descent phenomena is most prominent in settings with added label noise; without it, the peak is smaller and easy to miss. Adding label noise amplifies this general behavior and allows us to easily investigate.</p> \n<h4 class=\"mb-1/12\" id=\"samplewise\">Sample-wise non-monotonicity</h4> \n<div class=\"medium-copy mb-1\">\n 2. There is a regime where more samples hurts.\n</div> \n<figure class=\"mx-n0.5\"> \n <p><img src=\"https://openai.com/content/images/2019/12/fig_data_hurts.svg\" alt=\"Deep Double Descent\" /></p> \n</figure> \n<p>The above chart shows transformers trained on a language-translation task with no added label noise. As expected, increasing the number of samples shifts the curve downwards towards lower test error. However, since more samples require larger models to fit, increasing the number of samples also shifts the interpolation threshold (and peak in test error) to the right.</p> \n<p>For intermediate model sizes (red arrows), these two effects combine, and we see that training on 4.5x more samples actually hurts test performance.</p> \n<h4 class=\"mb-1/12\" id=\"epochwise\">Epoch-wise double descent</h4> \n<div class=\"medium-copy mb-1\">\n 3. There is a regime where training longer reverses overfitting.\n</div> \n<div class=\"wide my-0\"> \n <div class=\"row\"> \n  <div class=\"col-12 col-md-6\"> \n   <figure class=\"mx-n0.5\"> \n    <p><img src=\"https://openai.com/content/images/2019/12/epoch_train.png\" alt=\"Deep Double Descent\" /></p> \n   </figure> \n  </div> \n  <div class=\"col-12 col-md-6 mt-n1 mt-md-0\"> \n   <figure class=\"mx-n0.5\"> \n    <p><img src=\"https://openai.com/content/images/2019/12/epoch_test.png\" alt=\"Deep Double Descent\" /></p> \n   </figure> \n  </div> \n </div> \n</div> \n<p>The charts above show test and train error as a function of both model size and number of optimization steps. For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent. For a given model size (fixed x-coordinate), as training proceeds, test and train error decreases, increases, and decreases again; we call this phenomenon epoch-wise double descent.</p> \n<p><em>In general, the peak of test error appears systematically when models are just barely able to fit the train set.</em></p> \n<p>Our intuition is that, for models at the interpolation threshold, there is effectively only one model that fits the train data, and forcing it to fit even slightly noisy or misspecified labels will destroy its global structure. That is, there are no \"good models\" which both interpolate the train set and perform well on the test set. However, in the over-parameterized regime, there are many models that fit the train set and there exist such good models. Moreover, the implicit bias of stochastic gradient descent (SGD) leads it to such good models, for reasons we don't yet understand.</p> \n<p>We leave fully understanding the mechanisms behind double descent in deep neural networks as an important open question.</p> \n<footer class=\"post-footer js-post-footer\"> \n <!-- footer item --> \n <div>\n  <hr>\n   <div class=\"row\"> \n    <div class=\"col\">\n     Acknowledgments\n    </div> \n    <div class=\"col\"> \n     <p>Thanks to Mikhail Belkin and Chris Olah for helpful discussions and feedback throughout this work. An expanded version of this post can also be found on Boaz Barak's blog, <a href=\"https://windowsontheory.org/2019/12/05/deep-double-descent/\">Windows on Theory</a>.</p> \n    </div> \n   </div>\n  </hr>\n </div> \n</footer> \n<!--kg-card-end: markdown-->","descriptionType":"html","publishedDate":"Thu, 05 Dec 2019 17:24:26 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2019/12/Frame-1--3-.png","linkMd5":"309259fcaa8289b1c6ef46dffd6a76f9","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn49@2020_1/2020/08/24/22-09-05-479_2ba7b23a226055b6.webp","destWidth":2270,"destHeight":1130,"sourceBytes":107872,"destBytes":237238,"author":"Preetum Nakkiran","articleImgCdnMap":{"https://openai.com/content/images/2019/12/Frame-1--3-.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn49@2020_1/2020/08/24/22-09-05-479_2ba7b23a226055b6.webp","https://openai.com/content/images/2019/12/modeldd.svg":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn22@2020_2/2020/08/24/22-09-07-566_6a9ed98f6074ec0d.svg","https://openai.com/content/images/2019/12/fig_data_hurts.svg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn86@2020_4/2020/08/24/22-09-07-514_5054b0b04ac75889.svg","https://openai.com/content/images/2019/12/epoch_train.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn30@2020_6/2020/08/24/22-09-08-228_12c68bf716e27a4b.webp","https://openai.com/content/images/2019/12/epoch_test.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn49@2020_6/2020/08/24/22-09-08-112_5bd77856d3258ead.webp"},"publishedOrCreatedDate":1598306916761},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Procgen Benchmark","link":"https://openai.com/blog/procgen-benchmark/","description":"<!--kg-card-begin: markdown-->\n<div class=\"medium-copy color-fg-80 mt-n0.5\"> \n <img src=\"https://openai.com/content/images/2019/12/og-image.jpg\" alt=\"Procgen Benchmark\"><p>We’re releasing Procgen Benchmark, 16 simple-to-use <a href=\"https://en.wikipedia.org/wiki/Procedural_generation\">procedurally-generated</a> environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills.</p> </img>\n</div> \n<section class=\"btns\"> \n <a href=\"https://arxiv.org/abs/1912.01588\" class=\"btn btn-padded icon-paper\">Paper</a>\n <a href=\"https://github.com/openai/procgen\" class=\"btn btn-padded icon-code\">Environment Code</a>\n <a href=\"https://github.com/openai/train-procgen\" class=\"btn btn-padded icon-code\">Training Code</a> \n</section> \n<!-- carousel --> \n<div class=\"full my-0 pt-0.5 pb-2 position-relative overflow-hidden\"> \n <div class=\"container\"> \n  <div class=\"js-carousel-videos carousel-videos\"> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     CoinRun\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_coinrun.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_coinrun.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     StarPilot\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_starpilot.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_starpilot.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     CaveFlyer\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_caveflyer.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_caveflyer.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Dodgeball\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_dodgeball.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_dodgeball.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     FruitBot\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_fruitbot.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_fruitbot.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Chaser\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_chaser.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_chaser.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Miner\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_miner.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_miner.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Jumper\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_jumper.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_jumper.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Leaper\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_leaper.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_leaper.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Maze\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_maze.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_maze.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     BigFish\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_bigfish.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_bigfish.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Heist\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_heist.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_heist.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Climber\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_climber.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_climber.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Plunder\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_plunder.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_plunder.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     Ninja\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_ninja.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_ninja.jpg\"></video> \n   </div> \n   <div class=\"js-carousel-video carousel-video\"> \n    <div class=\"small-copy mb-0.2\">\n     BossFight\n    </div> \n    <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/procgen-benchmark/assets/env_bossfight.mp4\" poster=\"https://cdn.openai.com/procgen-benchmark/assets/env_bossfight.jpg\"></video> \n   </div> \n  </div>\n  <!-- end .js-carousel-videos --> \n </div>\n <!-- end .container --> \n</div>\n<!-- end .full --> \n<h3 id=\"gettingstarted\">Getting started</h3> \n<p>Using the environment is easy whether you’re a human or AI:</p> \n<pre><code class=\"language-bash\">$ pip install procgen # install\n$ python -m procgen.interactive --env-name starpilot # human\n$ python &lt;&lt;EOF # random AI agent\nimport gym\nenv = gym.make('procgen:procgen-coinrun-v0')\nobs = env.reset()\nwhile True:\n    obs, rew, done, info = env.step(env.action_space.sample())\n    env.render()\n    if done:\n        break\nEOF\n</code></pre> \n<p>We’ve found that all of the Procgen environments require training on 500–1000 different levels before they can generalize to new levels, which suggests that standard RL benchmarks need much more diversity within each environment. Procgen Benchmark has become the standard research platform used by the OpenAI RL team, and we hope that it accelerates the community in creating better RL algorithms.</p> \n<h3 id=\"environmentdiversityiskey\">Environment diversity is key</h3> \n<p><a href=\"https://arxiv.org/abs/1804.06893\">In</a> <a href=\"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/\">several</a> <a href=\"https://arxiv.org/abs/1806.10729\">environments</a>, it has been observed that agents can overfit to remarkably large training sets. This evidence raises the possibility that overfitting pervades classic benchmarks like the <a href=\"https://arxiv.org/abs/1207.4708\">Arcade Learning Environment</a>, which has long served as a gold standard in reinforcement learning (RL). While the diversity between different games in the ALE is one of the benchmark's greatest strengths, the low emphasis on generalization presents a significant drawback. In each game the question must be asked: are agents robustly learning a relevant skill, or are they approximately memorizing specific trajectories?</p> \n<p><a href=\"https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/\">CoinRun</a> was designed to address precisely this issue, by using procedural generation to construct distinct sets of training levels and test levels. While CoinRun has helped us better quantify generalization in RL, it is still only a single environment. It’s likely that CoinRun is not fully representative of the many challenges RL agents must face. We want the best of both worlds: a benchmark comprised of many diverse environments, each of which fundamentally requires generalization. To fulfill this need, we have created Procgen Benchmark. CoinRun now serves as the inaugural environment in Procgen Benchmark, contributing its diversity to a greater whole.</p> \n<p>Previous work, including the <a href=\"https://arxiv.org/abs/1902.01378\">Obstacle Tower Challenge</a> and the <a href=\"https://arxiv.org/abs/1802.10363\">General Video Game AI framework</a>, has also encouraged using procedural generation to better evaluate generalization in RL. We've designed environments in a similar spirit, with two Procgen environments drawing direct inspiration from <a href=\"https://arxiv.org/abs/1806.10729\">GVGAI-based work</a>. Other environments like Dota and StarCraft also provide lots of per-environment complexity, but these environments are hard to rapidly iterate with (and it's even harder to use more than one such environment at a time). With Procgen Benchmark, we strive for all of the following: experimental convenience, high diversity within environments, and high diversity across environments.</p> \n<h3 id=\"procgenbenchmark\">Procgen Benchmark</h3> \n<p>Procgen Benchmark consists of 16 unique environments designed to measure both sample efficiency and generalization in reinforcement learning. This benchmark is ideal for evaluating generalization since distinct training and test sets can be generated in each environment. This benchmark is also well-suited to evaluate sample efficiency, since all environments pose diverse and compelling challenges for RL agents. The environments' intrinsic diversity demands that agents learn robust policies; overfitting to narrow regions in state space will not suffice. Put differently, the ability to generalize becomes an integral component of success when agents are faced with ever-changing levels.</p> \n<h3 id=\"designprinciples\">Design principles</h3> \n<p>We’ve designed all Procgen environments to satisfy the following criteria:</p> \n<ul> \n <li> <p><strong>High Diversity</strong>: Environment generation logic is given maximal freedom, subject to basic design constraints. The diversity in the resulting level distributions presents agents with meaningful generalization challenges.</p> </li> \n <li> <p><strong>Fast Evaluation</strong>: Environment difficulty is calibrated such that baseline agents make significant progress after training for 200M timesteps. Moreover, the environments are optimized to perform thousands of steps per second on a single CPU core, enabling a fast experimental pipeline.</p> </li> \n <li> <p><strong>Tunable Difficulty</strong>: All environments support two well-calibrated difficulty settings: easy and hard. While we report results using the hard difficulty setting, we make the easy difficulty setting available for those with limited access to compute power. Easy environments require approximately an eighth of the resources to train.</p> </li> \n <li> <p><strong>Emphasis on Visual Recognition and Motor Control</strong>: In keeping with precedent, environments mimic the style of many Atari and Gym Retro games. Performing well primarily depends on identifying key assets in the observation space and enacting appropriate low level motor responses.</p> </li> \n</ul> \n<h3 id=\"evaluatinggeneralization\">Evaluating generalization</h3> \n<p>We came to appreciate how hard RL generalization can be while conducting the <a href=\"https://openai.com/blog/first-retro-contest-retrospective/\">Retro Contest</a>, as agents continually failed to generalize from the limited data in the training set. Later, our CoinRun experiments painted an even clearer picture of our agents' struggle to generalize. We've now expanded on those results, conducting our most thorough study of RL generalization to date using all 16 environments in Procgen Benchmark.</p> \n<p>We first measured how the size of the training set impacts generalization. In each environment, we generated training sets ranging in size from 100 to 100,000 levels. We trained agents for 200M timesteps on these levels using <a href=\"https://openai.com/blog/openai-baselines-ppo/\">Proximal Policy Optimization</a>, and we measured performance on unseen test levels.</p> \n<h5 class=\"mb-0\" id=\"generalization\">Generalization performance</h5> \n<div class=\"medium-xsmall-copy\">\n Score over 100k levels, log scale\n</div> \n<div id=\"levels-legend\" class=\"mt-0.5\"></div> \n<div class=\"wide mt-1.5 mb-1\"> \n <div class=\"row\">\n  <div class=\"col-12 col-xl-10 offset-xl-1\"> \n   <div class=\"row align-items-end\"> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      CoinRun\n     </div> \n     <div id=\"levels-coinrun\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      StarPilot\n     </div> \n     <div id=\"levels-starpilot\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      CaveFlyer\n     </div> \n     <div id=\"levels-caveflyer\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Dodgeball\n     </div> \n     <div id=\"levels-dodgeball\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      FruitBot\n     </div> \n     <div id=\"levels-fruitbot\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Chaser\n     </div> \n     <div id=\"levels-chaser\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Miner\n     </div> \n     <div id=\"levels-miner\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Jumper\n     </div> \n     <div id=\"levels-jumper\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Leaper\n     </div> \n     <div id=\"levels-leaper\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Maze\n     </div> \n     <div id=\"levels-maze\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      BigFish\n     </div> \n     <div id=\"levels-bigfish\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Heist\n     </div> \n     <div id=\"levels-heist\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Climber\n     </div> \n     <div id=\"levels-climber\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Plunder\n     </div> \n     <div id=\"levels-plunder\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Ninja\n     </div> \n     <div id=\"levels-ninja\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      BossFight\n     </div> \n     <div id=\"levels-bossfight\"></div> \n    </div> \n   </div>\n   <!-- end.row --> \n  </div>\n </div> \n</div>\n<!-- end .wide--> \n<p>We found that agents strongly overfit to small training sets in almost all environments. In some cases, agents need access to as many as 10,000 levels to close the generalization gap. We also saw a peculiar trend emerge in many environments: past a certain threshold, training performance improves as the training sets grows! This runs counter to trends found in supervised learning, where training performance commonly decreases with the size of the training set. We believe this increase in training performance comes from an implicit curriculum provided by a diverse set of levels. A larger training set can improve training performance if the agent learns to generalize <em>even across levels in the training set</em>. We previously noticed this effect with CoinRun, and have found it often occurs in many Procgen environments as well.</p> \n<h3 id=\"anablationwithdeterministiclevels\">An ablation with deterministic levels</h3> \n<p>We also conducted a simple ablation study to emphasize the importance of procedural generation. Instead of using a new level at the start of every episode, we trained agents on a fixed sequence of levels. The agent begins each episode on the first level, and when it successfully completes a level, it progresses to the next one. If the agent fails at any point, the episode terminates. The agent can reach arbitrarily many levels, though in practice it rarely progresses beyond the 20th level in any environment.</p> \n<h5 class=\"mb-0\" id=\"ablation\">Train and test performance</h5> \n<div class=\"medium-xsmall-copy\">\n Score over 200M timesteps\n</div> \n<div id=\"ablation-legend\" class=\"mt-0.5\"></div> \n<div class=\"wide mt-1.5 mb-1\"> \n <div class=\"row\">\n  <div class=\"col-12 col-xl-10 offset-xl-1\"> \n   <div class=\"row align-items-end\"> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      CoinRun\n     </div> \n     <div id=\"ablation-coinrun\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      StarPilot\n     </div> \n     <div id=\"ablation-starpilot\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      CaveFlyer\n     </div> \n     <div id=\"ablation-caveflyer\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Dodgeball\n     </div> \n     <div id=\"ablation-dodgeball\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      FruitBot\n     </div> \n     <div id=\"ablation-fruitbot\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Chaser\n     </div> \n     <div id=\"ablation-chaser\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Miner\n     </div> \n     <div id=\"ablation-miner\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Jumper\n     </div> \n     <div id=\"ablation-jumper\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Leaper\n     </div> \n     <div id=\"ablation-leaper\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Maze\n     </div> \n     <div id=\"ablation-maze\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      BigFish\n     </div> \n     <div id=\"ablation-bigfish\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Heist\n     </div> \n     <div id=\"ablation-heist\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Climber\n     </div> \n     <div id=\"ablation-climber\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Plunder\n     </div> \n     <div id=\"ablation-plunder\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      Ninja\n     </div> \n     <div id=\"ablation-ninja\"></div> \n    </div> \n    <div class=\"col-6 col-sm-4 col-md-3 mb-0.75 mb-md-1\"> \n     <div class=\"small-copy\">\n      BossFight\n     </div> \n     <div id=\"ablation-bossfight\"></div> \n    </div> \n   </div>\n   <!-- end.row --> \n  </div>\n </div> \n</div>\n<!-- end .wide--> \n<p>At test time, we remove the determinism in the sequence of levels, instead choosing level sequences at random. We find that agents become competent over the first several training levels in most games, giving an illusion of meaningful progress. However, test performance demonstrates that the agents have in fact learned almost nothing about the underlying level distribution. We believe this vast gap between training and test performance is worth highlighting. It reveals a crucial hidden flaw in training on environments that follow a fixed sequence of levels. These results show just how essential it is to use diverse environment distributions when training and evaluating RL agents.</p> \n<h3 id=\"nextsteps\">Next steps</h3> \n<p>We expect many insights gleaned from this benchmark to apply in more complex settings, and we’re excited to use these new environments to design more capable and efficient agents.</p> \n<p><em>If you’re interested in helping develop diverse environments, <a href=\"https://openai.com/jobs/\">we’re hiring</a>!</em></p> \n<footer class=\"post-footer js-post-footer\"> \n <!-- footer item --> \n <div>\n  <hr>\n   <div class=\"row\"> \n    <div class=\"col\">\n     Acknowledgments\n    </div> \n    <div class=\"col\"> \n     <p>Thanks to Marc Bellemare, Julian Togelius, Carles Gelada, Jacob Jackson, Alex Ray, Lilian Weng, and Joshua Achiam for their feedback on the paper.</p> \n     <p>Thanks to Mira Murati, Brooke Chan, Justin Jay Wang, Greg Brockman, Ashley Pilipiszyn and Jack Clark for their work supporting, designing, writing, and providing feedback on this post.</p> \n     <p>Special thanks to <a href=\"https://www.kenney.nl\">Kenney</a> for the many high quality game assets used throughout these environments.</p> \n     <p>Additional thanks to <a href=\"https://craftpix.net\">CraftPix.net</a> for several game backgrounds, as well as to <a href=\"https://www.gameartguppy.com\">GameArtGuppy</a>, and <a href=\"https://ansimuz.itch.io\">ansimuz</a>. All asset licenses can be found <a href=\"https://github.com/openai/procgen/blob/master/ASSET_LICENSES.md\">here</a>.</p> \n    </div> \n   </div>\n  </hr>\n </div> \n</footer> \n<!--kg-card-end: markdown-->","descriptionType":"html","publishedDate":"Tue, 03 Dec 2019 17:00:16 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2019/12/og-image.jpg","linkMd5":"7b1f49144e8f08b38626c454b9087deb","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn86@2020_4/2020/08/24/22-09-05-644_0c7dba03dbb35a3c.webp","destWidth":1804,"destHeight":1082,"sourceBytes":434743,"destBytes":307400,"author":"Karl Cobbe","articleImgCdnMap":{"https://openai.com/content/images/2019/12/og-image.jpg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn86@2020_4/2020/08/24/22-09-05-644_0c7dba03dbb35a3c.webp"},"publishedOrCreatedDate":1598306916762},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Procgen and MineRL Competitions","link":"https://openai.com/blog/procgen-minerl-competitions/","description":"<!--kg-card-begin: markdown--> \n<div class=\"color-fg-80 medium-copy mt-n0.5\"> \n <p>We’re excited to announce that OpenAI is co-organizing two NeurIPS 2020 competitions with <a href=\"https://www.aicrowd.com\">AIcrowd</a>, <a href=\"https://www.cmu.edu/\">Carnegie Mellon University</a>, and <a href=\"https://deepmind.com/\">DeepMind</a>, using <a href=\"https://openai.com/blog/procgen-benchmark/\">Procgen Benchmark</a> and <a href=\"http://minerl.io\">MineRL</a>. We rely heavily on these environments internally for research on reinforcement learning, and we look forward to seeing the progress the community makes in these challenging competitions.</p> \n</div> \n<h3 id=\"procgencompetition\">Procgen Competition</h3> \n<p><a href=\"https://www.aicrowd.com/challenges/neurips-2020-procgen-competition\" class=\"btn btn-padded icon-external right\">Sign up for Procgen</a></p> \n<aside class=\"aside\"> \n <div style=\"max-width:360px\"> \n  <video autoplay=\"\" muted=\"\" loop=\"\" playsinline=\"\" poster=\"https://cdn.openai.com/procgen-minerl-competitions/procgen.jpg\" src=\"https://cdn.openai.com/procgen-minerl-competitions/procgen.mp4\"></video> \n </div> \n</aside> \n<p>The <a href=\"https://www.aicrowd.com/challenges/neurips-2020-procgen-competition\">Procgen Competition</a> focuses on improving sample efficiency and generalization in reinforcement learning. Participants will attempt to maximize agents' performance using a fixed number of environment interactions. Agents will be evaluated in each of the 16 environments already publicly released in <a href=\"https://arxiv.org/abs/1912.01588\">Procgen Benchmark</a>, as well as in four secret test environments created specifically for this competition. By aggregating performance across so many diverse environments, we obtain high quality metrics to judge the underlying algorithms. More information about the details of each round can be found <a href=\"https://www.aicrowd.com/challenges/neurips-2020-procgen-competition\">here</a>.</p> \n<p>Since all content is procedurally generated, each Procgen environment intrinsically requires agents to generalize to never-before-seen situations. These environments therefore provide a robust test of an agent's ability to learn in many diverse settings. Moreover, we designed Procgen environments to be fast and simple to use. Participants with limited computational resources will be able to easily reproduce our baseline results and run new experiments. We hope that this will empower participants to iterate quickly on new methods to improve sample efficiency and generalization in RL.</p> \n<h3 id=\"minerlcompetition\">MineRL Competition</h3> \n<p><a href=\"https://www.aicrowd.com/challenges/neurips-2020-minerl-challenge\" class=\"btn btn-padded icon-external right\">Sign up for MineRL</a></p> \n<aside class=\"aside mb-1\"> \n <div style=\"max-width:360px\"> \n  <div class=\"row no-gutters\"> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate1.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate2.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate3.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate4.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed1.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed2.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed3.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed4.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat1.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat2.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat3.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat4.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival1.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival2.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival3.mp4.gif\" /> \n   </div> \n   <div class=\"col-3\"> \n    <img class=\"w-100 mb-0\" src=\"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival4.mp4.gif\" /> \n   </div> \n  </div> \n </div> \n</aside> \n<p>Many of the recent, celebrated successes of artificial intelligence, such as AlphaStar, AlphaGo, and our own <a href=\"https://openai.com/projects/five/\">OpenAI Five</a>, utilize deep reinforcement learning to achieve human or super-human level performance in sequential decision-making tasks. These improvements to the state-of-the-art have thus far required an <a href=\"https://openai.com/blog/ai-and-compute/\">exponentially increasing</a> amount of compute and simulator samples, and therefore it is difficult<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/procgen-minerl-competitions/#fn1\" id=\"fnref1\">[1]</a></sup> to apply many of these systems directly to real-world problems where environment samples are expensive. One well-known way to reduce the environment sample complexity is to leverage human priors and demonstrations of the desired behavior.</p> \n<iframe id=\"ytplayer\" type=\"text/html\" width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/GHo8B4JMC38?controls=1&amp;fs=0&amp;modestbranding=1\" frameborder=\"0\" allowfullscreen=\"\"></iframe> \n<div class=\"caption mt-0\">\n  A rendering of the 1st place submission from the MineRL 2019 competition getting an iron pickaxe. \n</div> \n<p>To further catalyze research in this direction, we are co-organizing the <a href=\"https://www.aicrowd.com/challenges/neurips-2020-minerl-challenge\">MineRL 2020 Competition</a> which aims to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments. To that end, participants will compete to develop systems which can obtain a diamond in <a href=\"http://minercraft.net/\">Minecraft</a> from raw pixels using only 8,000,000 samples from the <a href=\"http://minerl.io/docs\">MineRL simulator</a> and 4 days of training on a single GPU machine. Participants will be provided the MineRL-v0 dataset (<a href=\"http://minerl.io/dataset/\">website</a>, <a href=\"https://arxiv.org/abs/1907.13440\">paper</a>), a large-scale collection of over 60 million frames of human demonstrations, enabling them to utilize expert trajectories to minimize their algorithm’s interactions with the Minecraft simulator.</p> \n<p>This competition is a follow-up to the <a href=\"https://www.aicrowd.com/challenges/neurips-2019-minerl-competition\">MineRL 2019 Competition</a> in which the <a href=\"https://arxiv.org/pdf/1912.08664v2.pdf\">top team’s agent</a> was able to <a href=\"https://www.youtube.com/watch?v=GHo8B4JMC38&amp;feature=youtu.be\">obtain an iron pickaxe</a> (the penultimate goal of the competition) under this extremely limited compute and simulator-interaction budget. Put in perspective, state-of-the-art standard reinforcement learning systems require hundreds of millions of environment interactions on large multi-GPU systems to achieve the same goal. This year, we anticipate competitors will push the state-of-the-art even further.</p> \n<p>To guarantee that competitors develop truly sample efficient algorithms, the MineRL competition organizers train the top team’s final round models from scratch with strict constraints on the hardware, compute, and simulator-interaction available. The MineRL 2020 Competition also features a novel measure to avoid hand engineering features and overfitting solutions to the domain. More details on the competition structure can be found <a href=\"https://www.aicrowd.com/challenges/neurips-2020-minerl-challenge\">here</a>.</p> \n<footer class=\"post-footer js-post-footer\"> \n <!-- footer item --> \n <div> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Acknowledgments \n    </div> \n    <div class=\"col\"> \n     <p>Our partners at <a href=\"https://www.aicrowd.com/\">AIcrowd</a> have been instrumental in the development of these competitions, by creating much of the competition infrastructure, securing computational resources, and providing valuable technical support. Additionally we’d like to thank our partners at Preferred Networks for being instrumental in developing baselines for the MineRL competition. The MineRL competition extends its gratitude to our sponsors and co-organizers at DeepMind, Microsoft, and NVIDIA.</p> \n     <p>The Procgen Competition is a collaboration between OpenAI and AIcrowd. The organizing team consists of Sharada Mohanty, Karl Cobbe, Jyotish Poonganam, Shivam Khandelwal, Christopher Hesse, Jacob Hilton, John Schulman, and William H. Guss.</p> \n     <p>The MineRL Competition is a collaboration between OpenAI, Carnegie Mellon University, MineRL Labs, Google DeepMind, Preferred Networks, Microsoft, and AIcrowd. The lead organizer is William H. Guss, and the organizing team consists of Brandon Houghton, Stephanie Milani, Nicholay Topin, John Schulman, Oriol Vinyals, Ruslan Salakhutdinov, Noboru Sean Kuno, Sam Devlin, Crissman Loomis, Keisuke Nakata, Shinya Shiroshita, Avinash Ummadisingu, and Mario Ynocente Castro.</p> \n    </div> \n   </div> \n  </hr> \n </div> \n <!-- special footer item for footnotes --> \n <div data-order=\"-1\"> \n  <hr> \n   <div class=\"row\" id=\"footnotes\"> \n    <div class=\"col\">\n      Footnotes \n    </div> \n    <div class=\"col\"> \n     <hr class=\"footnotes-sep\"> \n      <section class=\"footnotes\"> \n       <ol class=\"footnotes-list\"> \n        <li id=\"fn1\" class=\"footnote-item\"><p>While direct application is not possible due to the sheer number of samples required, Sim2Real and data augmentation techniques can mitigate the need to sample real-world dynamics directly. <a href=\"https://openai.com/blog/procgen-minerl-competitions/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n       </ol> \n      </section> \n      <!--kg-card-end: markdown--> \n     </hr> \n    </div> \n   </div> \n  </hr> \n </div> \n</footer>","descriptionType":"html","publishedDate":"Tue, 09 Jun 2020 16:00:20 +0000","feedId":4773,"bgimg":"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate1.mp4.gif","linkMd5":"352ecd9e9376ddbc4cf6744344f96336","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn82@2020_1/2020/08/24/22-09-05-802_94a67421a27b5482.webp","destWidth":64,"destHeight":64,"sourceBytes":231127,"destBytes":225140,"author":"OpenAI","articleImgCdnMap":{"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate1.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn82@2020_1/2020/08/24/22-09-05-802_94a67421a27b5482.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate2.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn4@2020_6/2020/08/24/22-09-09-803_b3d448de75365716.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate3.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn41@2020_2/2020/08/24/22-09-08-985_2bff4faf6ebb1e88.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate4.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn74@2020_3/2020/08/24/22-09-08-437_4e79865946d7be21.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/obed1.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn89@2020_4/2020/08/24/22-09-09-457_5e1d1164288f53b5.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/obed2.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn75@2020_2/2020/08/24/22-09-15-448_04f7cc5fe7549bac.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/obed3.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn9@2020_6/2020/08/24/22-09-13-410_3fbb1aae443af78e.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/obed4.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn23@2020_4/2020/08/24/22-09-10-388_6a6b719ca01b44fc.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat1.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn100@2020_3/2020/08/24/22-09-10-915_d1afe67763540e0a.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat2.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn94@2020_4/2020/08/24/22-09-12-875_ad6ecf1ca5184893.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat3.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn49@2020_2/2020/08/24/22-09-07-829_aeec7daa62f1bfc7.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat4.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn58@2020_3/2020/08/24/22-09-09-364_2d31b4f27551eee7.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/survival1.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn87@2020_1/2020/08/24/22-09-09-761_d652cd14204ee7d8.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/survival2.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn9@2020_4/2020/08/24/22-09-08-532_201162c17c179b27.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/survival3.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn95@2020_5/2020/08/24/22-09-09-227_f2c9ec47ec9058d9.webp","https://cdn.openai.com/procgen-minerl-competitions/minerl/survival4.mp4.gif":"https://cdn.jsdelivr.net/gh/myreaderx/cdn54@2020_6/2020/08/24/22-09-08-350_edf0bfa580384ae0.webp"},"publishedOrCreatedDate":1598306916756},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Image GPT","link":"https://openai.com/blog/image-gpt/","description":"<!--kg-card-begin: markdown--> \n<div class=\"js-excerpt d-none\"> \n <img src=\"https://openai.com/content/images/2020/06/completions-3x5-clean-1.png\" alt=\"Image GPT\"><p>We find that, just as a large transformer model trained on language can generate coherent text, the same exact model trained on pixel sequences can generate coherent image <a href=\"https://openai.com/blog/image-gpt/#completions\">completions</a> and <a href=\"https://openai.com/blog/image-gpt/#samples\">samples</a>. By establishing a correlation between sample quality and image classification accuracy, we show that our best generative model also contains features competitive with top convolutional nets in the unsupervised&nbsp;setting.</p> \n  <section class=\"post-header-btns btns my-1\"> \n   <a href=\"https://github.com/openai/image-gpt\" class=\"btn btn-padded icon-code\">Code</a> \n   <a href=\"https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V1_ICML.pdf\" class=\"btn btn-padded icon-paper\">ICML 2020 Paper (v1)</a> \n   <a href=\"https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf\" class=\"btn btn-padded icon-paper\">Paper (v2)</a> \n  </section> </img> \n</div> \n<aside class=\"aside color-fg-50 small-copy\"> \n <div class=\"pt-0 pt-md-0.125 mt-0 mt-md-1 mb-0.125\">\n   Contents \n </div> \n <ol class=\"list-indented list-compact\"> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#introduction\">Introduction</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#completions\">Completions</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#samples\">Samples</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#fromlanguagegpttoimagegpt\">From language GPT to image GPT</a></li> \n  <li class=\"no-widow\"><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#towardsgeneralunsupervisedlearning\">Towards general unsupervised<span class=\"d-none d-lg-inline\">&nbsp;</span><span class=\"d-inline d-lg-none\"> </span>learning</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#approach\">Approach</a></li> \n  <li class=\"no-widow\"><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#experimentalresults\">Experimental results</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#limitations\">Limitations</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/image-gpt/#conclusion\">Conclusion</a></li> \n </ol> \n</aside> \n<hr class=\"d-md-none\"> \n <h2 id=\"introduction\" class=\"mt-1\">Introduction</h2> \n <p>Unsupervised and self-supervised learning,<span class=\"js-rfref\" data-id=\"lecun-2017-predictive\"></span> or learning without human-labeled data, is a longstanding challenge of machine learning. Recently, it has seen incredible success in language, as transformer<span class=\"js-rfref\" data-id=\"vaswani-2017-attention\"></span> models like BERT,<span class=\"js-rfref\" data-id=\"devlin-2018-bert\"></span> GPT-2,<span class=\"js-rfref\" data-id=\"radford-2019-language\"></span> RoBERTa,<span class=\"js-rfref\" data-id=\"liu-2019-roberta\"></span> T5,<span class=\"js-rfref\" data-id=\"raffel-2019-exploring\"></span> and other variants<span class=\"js-rfref\" data-id=\"dai-2015-semi\"></span><span class=\"js-rfref\" data-id=\"peters-2018-deep\"></span><span class=\"js-rfref\" data-id=\"howard-2018-universal\"></span><span class=\"js-rfref\" data-id=\"radford-2018-improving\"></span> have achieved top performance on a wide array of language tasks. However, the same broad class of models has not been successful in producing strong features for image classification.<span class=\"js-rfref\" data-id=\"ke-2018-sparse\"></span> Our work aims to understand and bridge this gap.</p> \n <p>Transformer models like BERT and GPT-2 are domain agnostic, meaning that they can be directly applied to 1-D sequences of any form. When we train GPT-2 on images unrolled into long sequences of pixels, which we call iGPT, we find that the model appears to understand 2-D image characteristics such as object appearance and category. This is evidenced by the diverse range of coherent image samples it generates, even without the guidance of human provided labels. As further proof, features from the model achieve state-of-the-art performance on a number of classification datasets and near state-of-the-art unsupervised accuracy<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn1\" id=\"fnref1\">[1]</a></sup> on ImageNet.</p> \n <!-- table: results summary --> \n <table class=\"table-compact\"> \n  <thead> \n   <tr> \n    <th style=\"width:20%;min-width:6rem\">Evaluation</th> \n    <th style=\"width:20%;min-width:6rem\">Dataset</th> \n    <th style=\"width:30%;min-width:8rem\">Our Result</th> \n    <th style=\"width:30%;min-width:8rem\">Best non-<span style=\"text-transform:none\">iGPT</span> Result</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td rowspan=\"4\">Logistic regression on learned features (linear&nbsp;probe)</td> \n    <td>CIFAR-10</td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>96.3</strong> \n       </div>iGPT-L 32x32 w/ 1536 features \n      </div> \n     </div></td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\">\n         95.3 \n       </div>SimCLR \n       <span class=\"js-rfref\" data-id=\"chen-2020-simple\"></span> w/ 8192 features \n      </div> \n     </div></td> \n   </tr> \n   <tr> \n    <td>CIFAR-100</td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>82.8</strong> \n       </div>iGPT-L 32x32 w/ 1536 features \n      </div> \n     </div></td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\">\n         80.2 \n       </div>SimCLR w/ 8192 features \n      </div> \n     </div></td> \n   </tr> \n   <tr> \n    <td>STL-10</td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>95.5</strong> \n       </div>iGPT-L 32x32 w/ 1536 features \n      </div> \n     </div></td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\">\n         94.2 \n       </div>AMDIM \n       <span class=\"js-rfref\" data-id=\"bachman-2019-learning\"></span> w/ 8192 features \n      </div> \n     </div></td> \n   </tr> \n   <tr> \n    <td>ImageNet</td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\">\n         72.0 \n       </div>iGPT-XL \n       <sup><a href=\"https://openai.com/blog/image-gpt/#igpt-xl\">a</a></sup> 64x64 w/ 15360 features \n      </div> \n     </div></td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>76.5</strong> \n       </div>SimCLR w/ 8192 features \n      </div> \n     </div></td> \n   </tr> \n   <tr> \n    <td rowspan=\"3\">Full fine-tune</td> \n    <td>CIFAR-10</td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>99.0</strong> \n       </div>iGPT-L 32x32, trained on ImageNet \n      </div> \n     </div></td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>99.0</strong> \n        <sup><a href=\"https://openai.com/blog/image-gpt/#bit-l\">b</a></sup> \n       </div>GPipe, \n       <span class=\"js-rfref\" data-id=\"huang-2019-gpipe\"></span> trained on ImageNet \n      </div> \n     </div></td> \n   </tr> \n   <tr> \n    <td>ImageNet 32x32</td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\">\n         66.3 \n       </div>iGPT-L 32x32 \n      </div> \n     </div></td> \n    <td> \n     <div class=\"position-relative\"> \n      <div class=\"pl-1.5 ml-0.125\"> \n       <div class=\"position-absolute color-fg\" style=\"left:0\"> \n        <strong>70.2</strong> \n       </div>Isometric Nets \n       <span class=\"js-rfref\" data-id=\"sandler-2019-nondiscriminative\"></span> \n      </div> \n     </div></td> \n   </tr> \n  </tbody> \n </table> \n <div class=\"caption mb-1.5\"> \n  <ol class=\"list-indented list-lower-latin\"> \n   <li id=\"igpt-xl\" class=\"footnote-item\">We only show ImageNet linear probe accuracy for iGPT-XL since other experiments did not finish before we needed to transition to different supercomputing facilities.</li> \n   <li id=\"bit-l\" class=\"footnote-item\">Bit-L,<span class=\"js-rfref\" data-id=\"kolesnikov-2019-big\"></span> trained on JFT (300M images with 18K classes), achieved a result of <span class=\"font-tnum\">99.3</span>.</li> \n  </ol> \n </div> \n <!-- end table: results summary --> \n <p>To highlight the potential of generative<span class=\"js-rfref\" data-id=\"lasserre-2006-principled\"></span><span class=\"js-rfref\" data-id=\"erhan-2010-why\"></span> sequence modeling<span class=\"js-rfref\" data-id=\"elman-1990-finding\"></span><span class=\"js-rfref\" data-id=\"mikolov-2010-recurrent\"></span><span class=\"js-rfref\" data-id=\"larochelle-2011-neural\"></span><span class=\"js-rfref\" data-id=\"graves-2013-generating\"></span> as a general purpose unsupervised learning algorithm, we deliberately use the same transformer architecture as GPT-2 in language. As a consequence, we require significantly more compute in order to produce features competitive with those from top unsupervised convolutional nets.<span class=\"js-rfref\" data-id=\"bachman-2019-learning\"></span><span class=\"js-rfref\" data-id=\"tian-2019-contrastive\"></span><span class=\"js-rfref\" data-id=\"he-2019-momentum\"></span><span class=\"js-rfref\" data-id=\"henaff-2019-data\"></span><span class=\"js-rfref\" data-id=\"chen-2020-simple\"></span> However, our results suggest that when faced with a new domain where the correct model priors are unknown, a large GPT-2 can learn excellent features without the need for domain-specific<span class=\"js-rfref\" data-id=\"oord-2016-pixel\"></span><span class=\"js-rfref\" data-id=\"parmar-2018-image\"></span><span class=\"js-rfref\" data-id=\"menick-2018-generating\"></span> architectural design choices.</p> \n <!-- start #samples-wrap --> \n <div id=\"samples-wrap\" class=\"samples-wrap js-theme-container full py-0.5\" data-theme=\"dark\"> \n  <div class=\"container\"> \n   <div class=\"row\"> \n    <div class=\"content py-0.5\"> \n     <h2 id=\"completions\">Completions</h2> \n     <div class=\"samples mt-0.75 mb-2.5\"> \n      <!-- categories --> \n      <div class=\"overflow-hidden small-copy mb-1 completion-categories js-completion-categories position-relative py-0.125\" style=\"visibility:hidden;padding-left:1px;padding-right:1px\"> \n       <div class=\"overflow-x-auto mb-n2 pb-2 js-hscroll\" style=\"scroll-behavior:smooth\"> \n        <div class=\"d-flex flex-nowrap\"> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category is-active\" data-id=\"miscellaneous\" onclick=\"showCompletions('miscellaneous')\">\n           Favorites \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"animals\" onclick=\"showCompletions('animals')\">\n           Animals \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"painted-landscapes\" onclick=\"showCompletions('painted-landscapes')\">\n           Painted Landscapes \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"sports\" onclick=\"showCompletions('sports')\">\n           Sports \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"interior-design\" onclick=\"showCompletions('interior-design')\">\n           Architecture \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"imagenet-r\" onclick=\"showCompletions('imagenet-r')\">\n           ImageNet-R \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"movie-posters\" onclick=\"showCompletions('movie-posters')\">\n           Movie Posters \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"famous-artworks\" onclick=\"showCompletions('famous-artworks')\">\n           Famous Artworks \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"popular-memes\" onclick=\"showCompletions('popular-memes')\">\n           Popular Memes \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"landscapes\" onclick=\"showCompletions('landscapes')\">\n           Landscapes \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"album-covers\" onclick=\"showCompletions('album-covers')\">\n           Album Covers \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"common-english-words\" onclick=\"showCompletions('common-english-words')\">\n           Common English Words \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"us-state-flags\" onclick=\"showCompletions('us-state-flags')\">\n           US &amp; State Flags \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"openai-research-covers\" onclick=\"showCompletions('openai-research-covers')\">\n           OpenAI Research Covers \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"openai-pets\" onclick=\"showCompletions('openai-pets')\">\n           OpenAI Pets \n         </div> \n         <div class=\"flex-shrink-0 d-inline-block py-0.125 mr-0.5 completion-category js-completion-category\" data-id=\"openai-cooking\" onclick=\"showCompletions('openai-cooking')\">\n           OpenAI Cooking \n         </div> \n        </div> \n        <div class=\"js-hscroll-prev position-absolute h-100\" style=\"cursor:pointer;user-select:none;width:3rem;top:0;left:-0.25rem\" onclick=\"hScroll(0)\"> \n         <span class=\"icon font-xlarge color-fg-50 position-relative\" style=\"top:0.1rem\">navigateleft</span> \n        </div> \n        <div class=\"js-hscroll-next position-absolute h-100 text-right\" style=\"cursor:pointer;user-select:none;width:3rem;top:0;right:-0.25rem\" onclick=\"hScroll(1)\"> \n         <span class=\"icon font-xlarge color-fg-50 position-relative\" style=\"top:0.1rem\">navigateright</span> \n        </div> \n       </div> \n      </div> \n      <!-- legend --> \n      <div class=\"samples-legend js-sticky sticky row xsmall-copy align-items-end\"> \n       <div class=\"col-2\"> \n        <div class=\"pr-1/12\">\n          Model Input \n        </div> \n       </div> \n       <div class=\"col-8\"> \n        <div class=\"pr-1/12\">\n          Completions \n        </div> \n       </div> \n       <div class=\"col-2\"> \n        <div class=\"pr-1/12\">\n          Original \n        </div> \n       </div> \n      </div> \n      <div class=\"row\" style=\"margin-bottom:0.3rem\"> \n       <div class=\"col-2\"> \n        <hr class=\"my-0 hr-strong\"> \n        </hr> \n       </div> \n       <div class=\"col-8\"> \n        <hr class=\"my-0 hr-strong\"> \n        </hr> \n       </div> \n       <div class=\"col-2\"> \n        <hr class=\"my-0 hr-strong\"> \n        </hr> \n       </div> \n      </div> \n      <!-- aside caption --> \n      <aside class=\"aside-right small-copy color-fg-50 d-none d-md-block\" style=\"margin-top:-0.2rem\">\n        Model-generated completions of human-provided half-images. We sample the remaining halves with temperature 1 and without tricks like beam search or nucleus sampling. While we showcase our favorite completions in the first panel, we do not cherry-pick images or completions in all following panels. \n      </aside> \n      <!-- completions --> \n      <div id=\"samples-completions\"> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n       <div class=\"row\"> \n        <div class=\"col-2\"> \n         <div class=\"mask position-relative\"> \n          <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-orig.png\" alt=\"Image GPT\" /> \n         </div> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-0.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-1.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-2.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-3.png\" alt=\"Image GPT\" /> \n        </div> \n        <div class=\"col-2\"> \n         <img src=\"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-orig.png\" alt=\"Image GPT\" /> \n        </div> \n       </div> \n      </div> \n      <!-- inline caption --> \n      <div class=\"mt-0.5 small-copy color-fg-50 d-block d-md-none\">\n        Model-generated completions of human-provided half-images. We sample the remaining halves with temperature 1 and without tricks like beam search or nucleus sampling. While we showcase our favorite completions in the first panel, we do not cherry-pick images or completions in all following panels. \n      </div> \n     </div> \n     <h2 id=\"samples\">Samples</h2> \n     <div class=\"samples mt-0.75\"> \n      <!-- aside caption --> \n      <aside class=\"aside-right small-copy color-fg-50 d-none d-md-block\" style=\"margin-top:-0.2rem\">\n        Model-generated image samples. We sample these images with temperature 1 and without tricks like beam search or nucleus sampling. All of our samples are shown, with no cherry-picking. Nearly all generated images contain clearly recognizable objects. \n      </aside> \n      <!-- samples --> \n      <div id=\"samples-samples\" class=\"row\"> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n       <div class=\"col-2\"> \n        <img src=\"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=\" alt=\"Image GPT\" /> \n       </div> \n      </div> \n      <!-- view more samples --> \n      <div class=\"mt-1/3 mb-0.75 mb-md-1.5 color-fg-50\"> \n       <a class=\"js-refresh-sample btn icon-refresh\" onclick=\"refreshSamples()\" style=\"visibility:hidden;margin-left:2px\">View more samples</a> \n      </div> \n      <!-- inline caption --> \n      <div class=\"mb-1.5 small-copy color-fg-50 d-block d-md-none\">\n        Model-generated image samples. We sample these images with temperature 1 and without tricks like beam search or nucleus sampling. All of our samples are shown, with no cherry-picking. Nearly all generated images contain clearly recognizable objects. \n      </div> \n     </div> \n    </div> \n   </div> \n   <!-- end .content .row --> \n   <!-- scale and theme switcher --> \n   <div class=\"sticky js-sticky small-copy color-fg-50\" style=\"bottom:0\"> \n    <div class=\"d-flex justify-content-end py-0.25\"> \n     <div class=\"js-image-scale font-tnum mr-1/3\" data-base=\"64\" data-id=\"samples-completions\"></div> \n     <div class=\"position-relative\" style=\"top:0.1rem\"> \n      <button class=\"button-unstyled theme-switch theme-switch-dark js-theme-switch\" data-theme=\"dark\" title=\"Switch to Dark Theme\" onclick=\"switchTheme('dark')\"></button> \n      <button class=\"button-unstyled theme-switch theme-switch-medium js-theme-switch\" data-theme=\"medium\" title=\"Switch to Medium Theme\" onclick=\"switchTheme('medium')\"></button> \n      <button class=\"button-unstyled theme-switch theme-switch-light js-theme-switch\" title=\"Switch to Light Theme\" onclick=\"switchTheme('light')\"></button> \n     </div> \n    </div> \n   </div> \n  </div> \n  <!-- end .container --> \n </div> \n <!-- end #samples-wrap --> \n <h2 id=\"fromlanguagegpttoimagegpt\">From language GPT to image GPT</h2> \n <p>In language, unsupervised learning algorithms that rely on word prediction (like GPT-2 and BERT) have been extremely successful, achieving top performance on a wide array of language tasks. One possible reason for this success is that instances of downstream language tasks appear naturally in text: questions are often followed by answers (which could help with question-answering) and passages are often followed by summaries (which could help with summarization). In contrast, sequences of pixels do not clearly contain labels for the images they belong to.</p> \n <p>Even without this explicit supervision, there is still a reason why GPT-2 on images might work: a sufficiently large transformer trained on next pixel prediction might eventually learn to generate diverse<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn2\" id=\"fnref2\">[2]</a></sup> samples with clearly recognizable objects. Once it learns to do so, an idea known as “Analysis by Synthesis”<span class=\"js-rfref\" data-id=\"mumford-1992-computational\"></span><span class=\"js-rfref\" data-id=\"rao-1999-predictive\"></span><sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn3\" id=\"fnref3\">[3]</a></sup> suggests that the model will also know about object categories. Many early generative models<span class=\"js-rfref\" data-id=\"smolensky-1986-information\"></span><span class=\"js-rfref\" data-id=\"hinton-2002-training\"></span><span class=\"js-rfref\" data-id=\"hinton-2006-fast\"></span><span class=\"js-rfref\" data-id=\"vincent-2008-extracting\"></span><span class=\"js-rfref\" data-id=\"coates-2011-analysis\"></span><span class=\"js-rfref\" data-id=\"le-2012-building\"></span> were motivated by this idea, and more recently, BigBiGAN<span class=\"js-rfref\" data-id=\"donahue-2019-large\"></span> was an example which produced encouraging samples and features. In our work, we first show that better generative models achieve stronger classification performance. Then, through optimizing GPT-2 for generative capabilities, we achieve top-level classification performance in many settings, providing further evidence for analysis by synthesis.</p> \n <h2 id=\"towardsgeneralunsupervisedlearning\">Towards general unsupervised learning</h2> \n <p>Generative sequence modeling is a universal unsupervised learning algorithm: since all data types can be represented as sequences of bytes, a transformer can be directly applied to any data type without additional engineering. Our work tests the power of this generality by directly applying the architecture used to train GPT-2 on natural language to image generation. We deliberately chose to forgo hand coding any image specific knowledge in the form of convolutions<span class=\"js-rfref\" data-id=\"ciresan-2010-deep\"></span> or techniques like relative attention,<span class=\"js-rfref\" data-id=\"shaw-2018-self\"></span> sparse attention,<span class=\"js-rfref\" data-id=\"child-2019-generating\"></span> and 2-D position embeddings.<span class=\"js-rfref\" data-id=\"parmar-2018-image\"></span></p> \n <p>As a consequence of its generality, our method requires significantly more compute to achieve competitive performance in the unsupervised setting. Indeed, contrastive methods<span class=\"js-rfref\" data-id=\"becker-1991-self\"></span><span class=\"js-rfref\" data-id=\"bromley-1994-signature\"></span><span class=\"js-rfref\" data-id=\"mikolov-2013-distributed\"></span><span class=\"js-rfref\" data-id=\"oord-2018-representation\"></span><span class=\"js-rfref\" data-id=\"hjelm-2018-learning\"></span><span class=\"js-rfref\" data-id=\"bachman-2019-learning\"></span><span class=\"js-rfref\" data-id=\"tian-2019-contrastive\"></span><span class=\"js-rfref\" data-id=\"he-2019-momentum\"></span><span class=\"js-rfref\" data-id=\"henaff-2019-data\"></span><span class=\"js-rfref\" data-id=\"chen-2020-simple\"></span> are still the most computationally efficient methods for producing high quality features from images. However, in showing that an unsupervised transformer model is competitive with the best unsupervised convolutional nets,<span class=\"js-rfref\" data-id=\"he-2019-momentum\"></span><span class=\"js-rfref\" data-id=\"henaff-2019-data\"></span><span class=\"js-rfref\" data-id=\"chen-2020-simple\"></span> we provide evidence that it is possible to trade off hand coded domain knowledge for compute. In new domains,<span class=\"js-rfref\" data-id=\"alley-2019-unified\"></span><span class=\"js-rfref\" data-id=\"rives-2019-biological\"></span> where there isn’t much knowledge to hand code, scaling compute seems an appropriate technique to test.</p> \n <h2 id=\"approach\">Approach</h2> \n <p>We train iGPT-S, iGPT-M, and iGPT-L, transformers containing 76M, 455M, and 1.4B parameters respectively, on ImageNet. We also train iGPT-XL<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn4\" id=\"fnref4\">[4]</a></sup>, a 6.8 billion parameter transformer, on a mix of ImageNet and images from the web. Due to the large computational cost of modeling long sequences with dense attention, we train at the low resolutions of 32x32, 48x48, and 64x64.</p> \n <p>While it is tempting to work at even lower resolutions to further reduce compute cost, prior work has demonstrated that human performance on image classification begins to drop rapidly below these sizes.<span class=\"js-rfref\" data-id=\"torralba-2008-80\"></span> Instead, motivated by early color display palettes,<span class=\"js-rfref\" data-id=\"wiki-8bit\"></span> we create our own 9-bit color palette to represent pixels. Using this palette yields an input sequence length 3 times shorter than the standard (R, G, B) palette, while still encoding color faithfully.</p> \n <h2 id=\"experimentalresults\">Experimental results</h2> \n <p>There are two methods we use to assess model performance, both of which involve a downstream classification task. The first, which we refer to as a linear probe, uses the trained model to extract features<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn5\" id=\"fnref5\">[5]</a></sup> from the images in the downstream dataset, and then fits a logistic regression to the labels. The second method fine-tunes<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn6\" id=\"fnref6\">[6]</a></sup> the entire model on the downstream dataset.</p> \n <p>Since next pixel prediction is not obviously relevant to image classification, features from the final layer may not be the most predictive of the object category. Our first result shows that feature quality is a sharply increasing, then mildly decreasing function of depth. This behavior suggests that a transformer generative model operates in two phases: in the first phase, each position gathers information from its surrounding context in order to build a contextualized image feature. In the second phase, this contextualized feature is used to solve the conditional next pixel prediction task. The observed two stage performance of our linear probes is reminiscent of another unsupervised neural net, the bottleneck autoencoder, which is manually designed so that features in the middle are used.</p> \n <div class=\"my-1.5\"> \n  <div id=\"chart-depth\" class=\"mb-1\"></div> \n  <div class=\"caption\">\n    Feature quality depends heavily on the layer we choose to evaluate. In contrast with supervised models, the best features for these generative models lie in the middle of the network. \n  </div> \n </div> \n <p>Our next result establishes the link between generative performance and feature quality. We find that both increasing the scale of our models and training for more iterations result in better generative performance, which directly translates into better feature quality.</p> \n <div class=\"my-1.5\"> \n  <div id=\"chart-val-styles\"></div> \n  <div id=\"chart-val\" class=\"mb-1\"></div> \n  <div class=\"mt-n1.25 mb-1.25 xsmall-copy\">\n    Hover to see sample images \n   <span class=\"icon position-relative\" style=\"top:0.15em\">up</span> \n  </div> \n  <div class=\"caption\">\n    Each line tracks a model throughout generative pre-training: the dotted markers denote checkpoints at steps 131K, 262K, 524K, and 1000K. The positive slopes suggest a link between improved generative performance and improved feature quality. Larger models also produce better features than smaller models. iGPT-XL is not included because it was trained on a different dataset. \n  </div> \n </div> \n <p>When we evaluate our features using linear probes on CIFAR-10, CIFAR-100, and STL-10, we outperform features from all supervised and unsupervised transfer algorithms. Our results are also compelling in the full fine-tuning setting.</p> \n <!-- table: eval --> \n <table class=\"table-compact mt-1\"> \n  <thead> \n   <tr> \n    <th style=\"border-bottom:none\"></th> \n    <th style=\"border-bottom:none\"></th> \n    <th style=\"border-bottom:none\"></th> \n    <th style=\"border-bottom:none\" colspan=\"2\" class=\"text-right\">Pre-trained on ImageNet</th> \n   </tr> \n   <tr> \n    <th style=\"width:33.333%;min-width:6rem\">Evaluation</th> \n    <th style=\"width:33.333%;min-width:6rem\">Model</th> \n    <th style=\"width:11.111%;min-width:6rem\" class=\"text-right\">Accuracy</th> \n    <th style=\"width:11.111%\" class=\"text-right\">w/o labels</th> \n    <th style=\"width:11.111%\" class=\"text-right\">w/ labels</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td rowspan=\"3\">CIFAR-10<br>Linear Probe</br></td> \n    <td>ResNet-152<span class=\"js-rfref\" data-id=\"kornblith-2019-better\"></span></td> \n    <td class=\"text-right\">94.0</td> \n    <td class=\"text-right\"></td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n   </tr> \n   <tr> \n    <td>SimCLR<span class=\"js-rfref\" data-id=\"chen-2020-simple\"></span></td> \n    <td class=\"text-right\">95.3</td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td style=\"background-color:var(--td-highlight) !important\">iGPT-L 32x32</td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><strong>96.3</strong></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td rowspan=\"3\">CIFAR-100<br>Linear Probe</br></td> \n    <td>ResNet-152</td> \n    <td class=\"text-right\">78.0</td> \n    <td class=\"text-right\"></td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n   </tr> \n   <tr> \n    <td>SimCLR</td> \n    <td class=\"text-right\">80.2</td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td style=\"background-color:var(--td-highlight) !important\">iGPT-L 32x32</td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><strong>82.8</strong></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td rowspan=\"2\" class=\"td-strong\">STL-10<br>Linear Probe</br></td> \n    <td>AMDIM-L<span class=\"js-rfref\" data-id=\"bachman-2019-learning\"></span></td> \n    <td class=\"text-right\">94.2</td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"td-strong\">iGPT-L 32x32</td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"td-strong text-right\"><strong>95.5</strong></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"td-strong text-right\"><span class=\"icon\">check</span></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"td-strong text-right\"></td> \n   </tr> \n   <tr> \n    <td rowspan=\"4\">CIFAR-10<br>Fine-tune</br></td> \n    <td>AutoAugment<span class=\"js-rfref\" data-id=\"cubuk-2019-autoaugment\"></span></td> \n    <td class=\"text-right\">98.5</td> \n    <td class=\"text-right\"></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td>SimCLR</td> \n    <td class=\"text-right\">98.6</td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td>GPipe<span class=\"js-rfref\" data-id=\"huang-2019-gpipe\"></span></td> \n    <td class=\"text-right\"><strong>99.0</strong></td> \n    <td class=\"text-right\"></td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n   </tr> \n   <tr> \n    <td style=\"background-color:var(--td-highlight) !important\">iGPT-L</td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><strong>99.0</strong></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td rowspan=\"4\">CIFAR-100<br>Fine-tune</br></td> \n    <td style=\"background-color:var(--td-highlight) !important\">iGPT-L</td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\">88.5</td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td style=\"background-color:var(--td-highlight) !important\" class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td>SimCLR</td> \n    <td class=\"text-right\">89.0</td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td>AutoAugment</td> \n    <td class=\"text-right\">89.3</td> \n    <td class=\"text-right\"></td> \n    <td class=\"text-right\"></td> \n   </tr> \n   <tr> \n    <td>EfficientNet<span class=\"js-rfref\" data-id=\"tan-2019-efficientnet\"></span></td> \n    <td class=\"text-right\"><strong>91.7</strong></td> \n    <td class=\"text-right\"></td> \n    <td class=\"text-right\"><span class=\"icon\">check</span></td> \n   </tr> \n  </tbody> \n </table> \n <div class=\"caption mb-1.5\">\n   A comparison of linear probe and fine-tune accuracies between our models and top performing models which utilize either unsupervised or supervised ImageNet transfer. We also include AutoAugment, the best performing model trained end-to-end on CIFAR. \n </div> \n <p>Given the resurgence of interest in unsupervised and self-supervised learning on ImageNet, we also evaluate the performance of our models using linear probes on ImageNet. This is an especially difficult setting, as we do not train at the standard ImageNet input resolution. Nevertheless, a linear probe on the 1536 features from the best layer of iGPT-L trained on 48x48 images yields 65.2% top-1 accuracy, outperforming AlexNet.</p> \n <p>Contrastive methods typically report their best results on 8192 features, so we would ideally evaluate iGPT with an embedding dimension of 8192 for comparison. However, training such a model is prohibitively expensive, so we instead concatenate features from multiple layers as an approximation. Unfortunately, our features tend to be correlated across layers, so we need more of them to be competitive. Taking 15360 features from 5 layers in iGPT-XL yields 72.0% top-1 accuracy, outperforming AMDIM, MoCo, and CPC v2, but still underperforming SimCLR by a decent margin.</p> \n <!-- table: features, parameters, accuracy --> \n <table class=\"table-compact\"> \n  <thead> \n   <tr> \n    <th style=\"width:20%;min-width:6rem\">Method</th> \n    <th style=\"width:20%;min-width:6rem\">Input Resolution</th> \n    <th style=\"width:20%;min-width:6rem\" class=\"text-right\">Features</th> \n    <th style=\"width:20%;min-width:6rem\" class=\"text-right\">Parameters</th> \n    <th style=\"width:20%;min-width:6rem\" class=\"text-right\">Accuracy</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Rotation<span class=\"js-rfref\" data-id=\"gidaris-2018-unsupervised\"></span></td> \n    <td>original</td> \n    <td class=\"text-right\">8192</td> \n    <td class=\"text-right\">86M</td> \n    <td class=\"text-right\">55.4</td> \n   </tr> \n   <tr style=\"background-color:var(--td-highlight)\"> \n    <td>iGPT-L</td> \n    <td>32x32</td> \n    <td class=\"text-right\">1536</td> \n    <td class=\"text-right\">1362M</td> \n    <td class=\"text-right\">60.3</td> \n   </tr> \n   <tr> \n    <td>BigBiGAN<span class=\"js-rfref\" data-id=\"donahue-2019-large\"></span></td> \n    <td>original</td> \n    <td class=\"text-right\">16384</td> \n    <td class=\"text-right\">86M</td> \n    <td class=\"text-right\">61.3</td> \n   </tr> \n   <tr style=\"background-color:var(--td-highlight)\"> \n    <td>iGPT-L</td> \n    <td>48x48</td> \n    <td class=\"text-right\">1536</td> \n    <td class=\"text-right\">1362M</td> \n    <td class=\"text-right\">65.2</td> \n   </tr> \n   <tr> \n    <td>AMDIM<span class=\"js-rfref\" data-id=\"bachman-2019-learning\"></span></td> \n    <td>original</td> \n    <td class=\"text-right\">8192</td> \n    <td class=\"text-right\">626M</td> \n    <td class=\"text-right\">68.1</td> \n   </tr> \n   <tr> \n    <td>MoCo<span class=\"js-rfref\" data-id=\"he-2019-momentum\"></span></td> \n    <td>original</td> \n    <td class=\"text-right\">8192</td> \n    <td class=\"text-right\">375M</td> \n    <td class=\"text-right\">68.6</td> \n   </tr> \n   <tr style=\"background-color:var(--td-highlight) !important\"> \n    <td>iGPT-XL</td> \n    <td>64x64</td> \n    <td class=\"text-right\">3072</td> \n    <td class=\"text-right\">6801M</td> \n    <td class=\"text-right\">68.7</td> \n   </tr> \n   <tr> \n    <td>SimCLR<span class=\"js-rfref\" data-id=\"chen-2020-simple\"></span></td> \n    <td>original</td> \n    <td class=\"text-right\">2048</td> \n    <td class=\"text-right\">24M</td> \n    <td class=\"text-right\">69.3</td> \n   </tr> \n   <tr> \n    <td>CPC v2<span class=\"js-rfref\" data-id=\"henaff-2019-data\"></span></td> \n    <td>original</td> \n    <td class=\"text-right\">4096</td> \n    <td class=\"text-right\">303M</td> \n    <td class=\"text-right\">71.5</td> \n   </tr> \n   <tr style=\"background-color:var(--td-highlight) !important\"> \n    <td>iGPT-XL</td> \n    <td>64x64</td> \n    <td class=\"text-right\">3072 x 5</td> \n    <td class=\"text-right\">6801M</td> \n    <td class=\"text-right\">72.0</td> \n   </tr> \n   <tr> \n    <td>SimCLR</td> \n    <td>original</td> \n    <td class=\"text-right\">8192</td> \n    <td class=\"text-right\">375M</td> \n    <td class=\"text-right\"><strong>76.5</strong></td> \n   </tr> \n  </tbody> \n </table> \n <div class=\"caption mb-1.5\">\n   A comparison of linear probe accuracies between our models and state-of-the-art self-supervised models. We achieve competitive performance while training at much lower input resolutions, though our method requires more parameters and compute. \n </div> \n <p>Because masked language models like BERT have outperformed generative models on most language tasks, we also evaluate the performance of BERT on our image models. Instead of training our model to predict the next pixel given all preceding pixels, we mask out 15% of the pixels and train our model to predict them from the unmasked ones. We find that though linear probe performance on BERT models is significantly worse, they excel during fine-tuning:</p> \n <div class=\"my-1.5\"> \n  <h5 class=\"mb-0.1\">CIFAR-10</h5> \n  <div id=\"chart-cifar-legend\" class=\"mb-0.5\"></div> \n  <div id=\"chart-cifar-10\"></div> \n  <h5 class=\"mt-0.5 mb-0.1\">ImageNet</h5> \n  <div id=\"chart-imagenet-legend\" class=\"mb-0.5\"></div> \n  <div id=\"chart-imagenet\" class=\"mb-1\"></div> \n  <div class=\"caption\">\n    Comparison of generative pre-training with BERT pre-training using iGPT-L at an input resolution of 32 \n   <sup>2</sup> × 3. Bold colors show the performance boost from ensembling BERT masks. We see that generative models produce much better features than BERT models after pre-training, but BERT models catch up after fine-tuning. \n  </div> \n </div> \n <p>While unsupervised learning promises excellent features without the need for human-labeled data, significant recent progress has been made under the more forgiving framework of semi-supervised learning, which allows for limited amounts of human-labeled data. Successful semi-supervised methods often rely on clever techniques such as consistency regularization, data augmentation, or pseudo-labeling, and purely generative-based approaches<span class=\"js-rfref\" data-id=\"welling-2014-semi\"></span><span class=\"js-rfref\" data-id=\"salimans-2016-improved\"></span> have not been competitive for years. We evaluate iGPT-L<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/image-gpt/#fn7\" id=\"fnref7\">[7]</a></sup> on a competitive benchmark for this sub-field and find that a simple linear probe on features from non-augmented images outperforms Mean Teacher<span class=\"js-rfref\" data-id=\"tarvainen-2017-mean\"></span> and MixMatch,<span class=\"js-rfref\" data-id=\"berthelot-2019-mixmatch\"></span> though it underperforms FixMatch.<span class=\"js-rfref\" data-id=\"sohn-2020-fixmatch\"></span></p> \n <!-- table: models, labels --> \n <div class=\"d-block d-md-table w-100 mt-n1\"> \n  <table class=\"table-compact\"> \n   <thead> \n    <tr> \n     <th style=\"width:25%;min-width:6rem\">Model</th> \n     <th style=\"width:25%;min-width:6rem\" class=\"text-right\">40 labels</th> \n     <th style=\"width:25%;min-width:6rem\" class=\"text-right\">250 labels</th> \n     <th style=\"width:25%;min-width:6rem\" class=\"text-right\">4000 labels</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td>Improved GAN<span class=\"js-rfref\" data-id=\"salimans-2016-improved\"></span></td> \n     <td class=\"text-right\">—</td> \n     <td class=\"text-right\">—</td> \n     <td class=\"text-right\">81.4 ± 2.3</td> \n    </tr> \n    <tr> \n     <td>Mean Teacher<span class=\"js-rfref\" data-id=\"tarvainen-2017-mean\"></span></td> \n     <td class=\"text-right\">—</td> \n     <td class=\"text-right\">67.7 ± 2.3</td> \n     <td class=\"text-right\">90.8 ± 0.2</td> \n    </tr> \n    <tr> \n     <td>MixMatch<span class=\"js-rfref\" data-id=\"berthelot-2019-mixmatch\"></span></td> \n     <td class=\"text-right\">52.5 ± 11.5</td> \n     <td class=\"text-right\">89.0 ± 0.9</td> \n     <td class=\"text-right\">93.6 ± 0.1</td> \n    </tr> \n    <tr style=\"background-color:var(--td-highlight) !important\"> \n     <td>iGPT-L</td> \n     <td class=\"text-right\">73.2 ± <span style=\"visibility:hidden\">0</span>1.5</td> \n     <td class=\"text-right\">87.6 ± 0.6</td> \n     <td class=\"text-right\">94.3 ± 0.1</td> \n    </tr> \n    <tr> \n     <td>UDA<span class=\"js-rfref\" data-id=\"xie-2019-unsupervised\"></span></td> \n     <td class=\"text-right\">71.0 ± <span style=\"visibility:hidden\">0</span>5.9</td> \n     <td class=\"text-right\">91.2 ± 1.1</td> \n     <td class=\"text-right\">95.1 ± 0.2</td> \n    </tr> \n    <tr> \n     <td>FixMatch<span class=\"js-rfref\" data-id=\"sohn-2020-fixmatch\"></span> RA</td> \n     <td class=\"text-right\">86.2 ± <span style=\"visibility:hidden\">0</span>3.4</td> \n     <td class=\"text-right\">94.9 ± 0.7</td> \n     <td class=\"text-right\"><strong>95.7 ± 0.1</strong></td> \n    </tr> \n    <tr> \n     <td>FixMatch CTA</td> \n     <td class=\"text-right\"><strong>88.6 ± <span style=\"visibility:hidden\">0</span>3.4</strong></td> \n     <td class=\"text-right\"><strong>94.9 ± 0.3</strong></td> \n     <td class=\"text-right\">95.7 ± 0.2</td> \n    </tr> \n   </tbody> \n  </table> \n </div> \n <!-- end table: models, labels --> \n <div class=\"caption mb-1.5\">\n   A comparison of performance on low-data CIFAR-10. By leveraging many unlabeled ImageNet images, iGPT-L is able to outperform methods such as Mean Teacher and MixMatch but still underperforms the state of the art methods. Our approach to semi-supervised learning is very simple since we only fit a logistic regression classifier on iGPT-L's features without any data augmentation or fine-tuning—a significant difference from specially designed semi-supervised approaches. \n </div> \n <h2 id=\"limitations\">Limitations</h2> \n <p>While we have shown that iGPT is capable of learning powerful image features, there are still significant limitations to our approach. Because we use the generic sequence transformer used for GPT-2 in language, our method requires large amounts of compute: iGPT-L was trained for roughly 2500 V100-days while a similarly performing MoCo<span class=\"js-rfref\" data-id=\"he-2019-momentum\"></span> model can be trained in roughly 70 V100-days.</p> \n <p>Relatedly, we model low resolution inputs using a transformer, while most self-supervised results use convolutional-based encoders which can easily consume inputs at high resolution. A new architecture, such as a domain-agnostic multiscale transformer, might be needed to scale further. Given these limitations, our work primarily serves as a proof-of-concept demonstration of the ability of large transformer-based language models to learn excellent unsupervised representations in novel domains, without the need for hardcoded domain knowledge. However, the significant resource cost to train these models and the greater accuracy of convolutional neural-network based methods precludes these representations from practical real-world applications in the vision domain.</p> \n <p>Finally, generative models can exhibit biases that are a consequence of the data they've been trained on. Many of these biases are useful, like assuming that a combination of brown and green pixels represents a branch covered in leaves, then using this bias to continue the image. But some of these biases will be harmful, when considered through a lens of fairness and representation. For instance, if the model develops a visual notion of a scientist that skews male, then it might consistently complete images of scientists with male-presenting people, rather than a mix of genders. We expect that developers will need to pay increasing attention to the data that they feed into their systems and to better understand how it relates to biases in trained models.</p> \n <h2 id=\"conclusion\">Conclusion</h2> \n <p>We have shown that by trading off 2-D knowledge for scale<span class=\"js-rfref\" data-id=\"sutton-2019-bitter\"></span> and by choosing predictive features from the middle of the network, a sequence transformer can be competitive with top convolutional nets for unsupervised image classification. Notably, we achieved our results by directly applying the GPT-2 language model to image generation. Our results suggest that due to its simplicity and generality, a sequence transformer given sufficient compute might ultimately be an effective way to learn excellent features in many domains.</p> \n <p>If you’re excited to work with us on this area of research, <a href=\"https://openai.com/jobs/\">we’re hiring</a>!</p> \n <footer class=\"post-footer js-post-footer\"> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\" id=\"acknowledgments\"> \n     <div class=\"col\">\n       Acknowledgments \n     </div> \n     <div class=\"col\"> \n      <p>Foremost, we would like to acknowledge our paper co-authors Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, and David Luan.</p> \n      <p>Thanks to the following for their feedback on this work and contributions to this release: Vedant Misra, Noah Golmant, Johannes Otterbach, Pranav Shyam, Aditya Ramesh, Yura Burda, Harri Edwards, Chris Hallacy, Jeff Clune, Jack Clark, Irene Solaiman, Ryan Lowe, Greg Brockman, Kelly Sims, David Farhi, Will Guss, Quoc V. Le, and Ashish Vaswani.</p> \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\" id=\"acknowledgments\"> \n     <div class=\"col\">\n       Editor \n     </div> \n     <div class=\"col\">\n       Ashley Pilipiszyn \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\" id=\"acknowledgments\"> \n     <div class=\"col\">\n       Design \n     </div> \n     <div class=\"col\">\n       Justin Jay Wang \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\" id=\"acknowledgments\"> \n     <div class=\"col\">\n       Cover Artwork \n     </div> \n     <div class=\"col\">\n       Ben Barry \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\" id=\"references\"> \n     <div class=\"col\">\n       References \n     </div> \n     <div class=\"col\"> \n      <ol> \n       <li class=\"js-ref\" data-id=\"lecun-2017-predictive\"> LeCun, Y. (2017). \"<a href=\"https://channel9.msdn.com/events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Predictive-Learning\">Predictive Learning</a>.\" </li> \n       <li class=\"js-ref\" data-id=\"vaswani-2017-attention\"> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., &amp; Polosukhin, I. \"<a href=\"http://papers.nips.cc/paper/7181-attention-is-all-you-need\">Attention is All you Need</a>.\" In NeurIPS 2017. </li> \n       <li class=\"js-ref\" data-id=\"devlin-2018-bert\"> Devlin, J., Chang, M., Lee, K., &amp; Toutanova, K. (2018). \"<a href=\"https://arxiv.org/abs/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"radford-2019-language\"> Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). \"<a href=\"https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf\">Language Models are Unsupervised Multitask Learners</a>.\" Technical Report, OpenAI. </li> \n       <li class=\"js-ref\" data-id=\"liu-2019-roberta\"> Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). \"<a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"raffel-2019-exploring\"> Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &amp; Liu, P. (2019). \"<a href=\"https://arxiv.org/abs/1910.10683\">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"dai-2015-semi\"> Dai, A., Le, Q. V. (2015). \"<a href=\"http://papers.nips.cc/paper/5949-semi-supervised-sequence\">Semi-supervised sequence learning</a>.\" In NeurIPS 2015. </li> \n       <li class=\"js-ref\" data-id=\"peters-2018-deep\"> Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). \"<a href=\"https://arxiv.org/abs/1802.05365\">Deep Contextualized Word Representations</a>.\" In NAACL 2018. </li> \n       <li class=\"js-ref\" data-id=\"howard-2018-universal\"> Howard, J., Ruder, S. (2018). \"<a href=\"https://arxiv.org/abs/1801.06146\">Universal Language Model Fine-tuning for Text Classification</a>.\" In ACL 2018. </li> \n       <li class=\"js-ref\" data-id=\"radford-2018-improving\"> Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). \"<a href=\"https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\">Improving language understanding by generative pre-training</a>.\" Technical Report, OpenAI. </li> \n       <li class=\"js-ref\" data-id=\"ke-2018-sparse\"> Ke N., Goyal, A., Bilaniuk,O., Binas, J., Mozer, M., Pal, C., Bengio, Y (2018). \"<a href=\"http://papers.nips.cc/paper/7991-sparse-attentive-backtracking-temporal-credit-assignment-through-reminding\">Sparse attentive backtracking: Temporal credit assignment through reminding</a>.\" In NeurIPS 2018. </li> \n       <li class=\"js-ref\" data-id=\"chen-2020-simple\"> Chen, T., Kornblith, S., Norouzi, M., Hinton, G. (2020). \"<a href=\"https://arxiv.org/abs/2002.05709\">A Simple Framework for Contrastive Learning of Visual Representations</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"bachman-2019-learning\"> Bachman, P., Hjelm, R., &amp; Buchwalter, W. (2019). \"<a href=\"http://papers.nips.cc/paper/9686-learning-representations-by-maximizing-mutual-information-across-views\">Learning representations by maximizing mutual information across views</a>.\" In NeurIPS 2019. </li> \n       <li class=\"js-ref\" data-id=\"kolesnikov-2019-big\"> Kolesnikov, A. &amp; Beyer, L. &amp; Zhai, X., Puigcerver, J., Yung, J., Gelly, S., Houlsby, N. (2019). \"<a href=\"https://arxiv.org/abs/1912.11370\">Big Transfer (BiT): General Visual Representation Learning</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"huang-2019-gpipe\"> Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., &amp; Chen, Z. (2019) \"<a href=\"http://papers.nips.cc/paper/8305-gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism\">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a>.\" In NeurIPS 2019. </li> \n       <li class=\"js-ref\" data-id=\"sandler-2019-nondiscriminative\"> Sandler, M., Baccash, J., Zhmoginov, A., &amp; Howard, A. (2019). \"<a href=\"https://arxiv.org/abs/1909.03205\">Non-discriminative data or weak model? On the relative importance of data and model resolution</a>.\" In ICCV 2019. </li> \n       <li class=\"js-ref\" data-id=\"lasserre-2006-principled\"> Lasserre, J., Bishop, C., &amp; Minka, T. P. (2006). \"<a href=\"https://ieeexplore.ieee.org/document/1640745\">Principled Hybrids of Generative and Discriminative Models</a>.\" In CVPR 2006. </li> \n       <li class=\"js-ref\" data-id=\"erhan-2010-why\"> Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., Bengio, S. (2010). \"<a href=\"http://www.jmlr.org/papers/v11/erhan10a.html\">Why does unsupervised pre-training help deep learning?</a>.\" In JMLR 2010. </li> \n       <li class=\"js-ref\" data-id=\"elman-1990-finding\"> Elman, J. (1990). \"<a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1\">Finding Structure in Time</a>.\" In Cognitive Science 1990. </li> \n       <li class=\"js-ref\" data-id=\"mikolov-2010-recurrent\"> Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., Khudanpur, S. (2010). \"<a href=\"https://www.isca-speech.org/archive/interspeech_2010/i10_1045.html\">Recurrent neural network based language model</a>.\" In INTERSPEECH-2010. </li> \n       <li class=\"js-ref\" data-id=\"larochelle-2011-neural\"> Larochelle, H., Murray, I. (2011). \"<a href=\"http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf\">The neural autoregressive distribution estimator</a>.\" In AISTATS 2011. </li> \n       <li class=\"js-ref\" data-id=\"graves-2013-generating\"> Graves, A. (2013). \"<a href=\"https://arxiv.org/abs/1308.0850\">Generating sequences with recurrent neural networks</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"tian-2019-contrastive\"> Tian, Y., Krishnan, D., &amp; Isola, P. (2019). \"<a href=\"https://arxiv.org/abs/1906.05849\">Contrastive multiview coding</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"he-2019-momentum\"> He, K., Fan, H., Wu, Y., Xie, S., &amp; Girshick, R. (2019). \"<a href=\"https://arxiv.org/abs/1911.05722\">Momentum Contrast for Unsupervised Visual Representation Learning</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"henaff-2019-data\"> Henaff, O., Srinivas, A., De Fauw, J., Razavi, A., Doersch, C., Eslami, S., Oord, A. (2019). \"<a href=\"https://arxiv.org/abs/1905.09272\">Data-Efficient Image Recognition with Contrastive Predictive Coding </a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"oord-2016-pixel\"> Oord, A., Kalchbrenner, N., Kavukcuoglu, K. (2016). \"<a href=\"https://arxiv.org/abs/1601.06759\">Pixel recurrent neural networks</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"parmar-2018-image\"> Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., &amp; Tran, D. (2018). \"<a href=\"https://arxiv.org/abs/1802.05751\">Image transformer</a>.\" In ICML 2018. </li> \n       <li class=\"js-ref\" data-id=\"menick-2018-generating\"> Menick, J., Kalchbrenner, N. (2018). \"<a href=\"https://arxiv.org/abs/1812.01608\">Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"mumford-1992-computational\"> Mumford, D. (1992). \"<a href=\"https://link.springer.com/article/10.1007/BF00198477\">On the computational architecture of the neocortex</a>.\" In Biol. Cybern. </li> \n       <li class=\"js-ref\" data-id=\"rao-1999-predictive\"> Rao, R., Ballard, D. (1999). \"<a href=\"https://www.nature.com/articles/nn0199_79\">Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</a>.\" In Nature Neuroscience. </li> \n       <li class=\"js-ref\" data-id=\"smolensky-1986-information\"> Smolensky, P. (1986). \"<a href=\"https://apps.dtic.mil/docs/citations/ADA620727\">Information processing in dynamical systems: Foundations of harmony theory</a>.\" </li> \n       <li class=\"js-ref\" data-id=\"hinton-2002-training\"> Hinton, G. (2002). \"<a href=\"https://www.mitpressjournals.org/doi/abs/10.1162/089976602760128018\">Training Products of Experts by Minimizing Contrastive Divergence</a>.\" In MIT Press. </li> \n       <li class=\"js-ref\" data-id=\"hinton-2006-fast\"> Hinton, G., Osindero, S., &amp; Teh, Y. (2006). \"<a href=\"https://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527\">A fast learning algorithm for deep belief nets</a>.\" In Neural Computation. </li> \n       <li class=\"js-ref\" data-id=\"vincent-2008-extracting\"> Vincent, P., Larochelle, H., Bengio, Y., &amp; Manzagol, P. (2008). \"<a href=\"https://dl.acm.org/doi/abs/10.1145/1390156.1390294\">Extracting and composing robust features with denoising autoencoders</a>.\" In ICML 2008. </li> \n       <li class=\"js-ref\" data-id=\"coates-2011-analysis\"> Coates, A., Lee, H., &amp; Ng, A. Y. (2011). \"<a href=\"http://proceedings.mlr.press/v15/coates11a/coates11a.pdf\">An analysis of single-layer networks in unsupervised feature learning</a>.\" In AISTATS 2011. </li> \n       <li class=\"js-ref\" data-id=\"le-2012-building\"> Le, Q. V., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G., Dean, J. &amp; Ng, A. Y. (2012). \"<a href=\"https://icml.cc/2012/papers/73.pdf\">Building high-level features using large scale unsupervised learning</a>.\" In ICML 2012. </li> \n       <li class=\"js-ref\" data-id=\"donahue-2019-large\"> Donahue, J., Simonyan, K. (2019). \"<a href=\"http://papers.nips.cc/paper/9240-large-scale-adversarial-representation-learning\">Large scale adversarial representation learning</a>.\" In NeurIPS 2019. </li> \n       <li class=\"js-ref\" data-id=\"ciresan-2010-deep\"> Ciresan, D., Meier, U., Gambardella, L. &amp; Schmidhuber, J. (2010). \"<a href=\"https://arxiv.org/pdf/1003.0358.pdf\">Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition</a>.\" In CoRR 2010. </li> \n       <li class=\"js-ref\" data-id=\"shaw-2018-self\"> Shaw, P., Uszkoreit, J., &amp; Vaswani A. (2018). \"<a href=\"https://arxiv.org/abs/1803.02155\">Self-attention with relative position representations</a>.\" In NAACL 2018. </li> \n       <li class=\"js-ref\" data-id=\"child-2019-generating\"> Child, R., Gray, S., Radford, A., &amp; Sutskever, I. (2019). \"<a href=\"https://arxiv.org/abs/1904.10509\">Generating long sequences with sparse transformers</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"becker-1991-self\"> Becker, S., Hinton, G. (1991). \"<a href=\"https://www.nature.com/articles/355161a0\">Self-organizing neural network that discovers surfaces in random-dot stereograms</a>.\" In Nature. </li> \n       <li class=\"js-ref\" data-id=\"bromley-1994-signature\"> Bromley, J., Guyon, I., LeCun, Y., Sackinger, E., &amp; Shah, R. (1994). \"<a href=\"http://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf\">Signature verification using a\" siamese\" time delay neural network</a>.\" In NeurIPS 1994. </li> \n       <li class=\"js-ref\" data-id=\"mikolov-2013-distributed\"> Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). \"<a href=\"http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality\">Distributed Representations of Words and Phrases and their Compositionality </a>.\" In NeurIPS 2013. </li> \n       <li class=\"js-ref\" data-id=\"oord-2018-representation\"> Oord, A., Li, Y., Vinyals, O. (2018). \"<a href=\"https://arxiv.org/abs/1807.03748\">Representation Learning with Contrastive Predictive Coding </a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"hjelm-2018-learning\"> Hjelm, R., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., &amp; Bengio, Y. (2018). \"<a href=\"https://arxiv.org/abs/1808.06670\">Learning deep representations by mutual information estimation and maximization</a>.\" In ICLR 2019. </li> \n       <li class=\"js-ref\" data-id=\"alley-2019-unified\"> Alley, E., Khimulya, G., Biswas, S., AlQuraishi, M., Church, G. (2019). \"<a href=\"https://www.biorxiv.org/content/10.1101/589333v1\">Unified rational protein engineering with sequence-only deep representation learning</a>.\" In Nature Methods. </li> \n       <li class=\"js-ref\" data-id=\"rives-2019-biological\"> Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C., Ma, J., Fergus, R. (2019). \"<a href=\"https://www.biorxiv.org/content/10.1101/622803v1\">Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences</a>.\" bioRxiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"torralba-2008-80\"> Torralba, A., Fergus, R., Freeman, W. (2008). \"<a href=\"https://ieeexplore.ieee.org/abstract/document/4531741\">80 million tiny images: A large data set for nonparametric object and scene recognition</a>.\" In IEEE transactions on pattern analysis and machine intelligence. </li> \n       <li class=\"js-ref\" data-id=\"wiki-8bit\"> \"<a href=\"https://en.wikipedia.org/wiki/List_of_8-bit_computer_hardware_graphics\">List of 8-Bit Computer Hardware Graphics</a>.\" Wikipedia, 8 May 2020 </li> \n       <li class=\"js-ref\" data-id=\"kornblith-2019-better\"> Kornblith, S., Shlens, J., &amp; Le, Q. V. (2019). \"<a href=\"http://openaccess.thecvf.com/content_CVPR_2019/html/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.html\">Do Better ImageNet Models Transfer Better?</a>.\" In CVPR 2019. </li> \n       <li class=\"js-ref\" data-id=\"cubuk-2019-autoaugment\"> Cubuk, E., Zoph, B., Mane, D., Vasudevan, V., &amp; Le, Q. V. (2019). \"<a href=\"http://openaccess.thecvf.com/content_CVPR_2019/html/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html\">AutoAugment: Learning Augmentation Strategies From Data</a>.\" In CVPR 2019. </li> \n       <li class=\"js-ref\" data-id=\"tan-2019-efficientnet\"> Tan, M., Le, Q. V. (2019). \"<a href=\"https://arxiv.org/abs/1905.11946\">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a>.\" In ICML 2019. </li> \n       <li class=\"js-ref\" data-id=\"gidaris-2018-unsupervised\"> Gidaris, S., Singh, P., &amp; Komodakis, N. (2018). \"<a href=\"https://arxiv.org/abs/1803.07728\">Unsupervised Representation Learning by Predicting Image Rotations</a>.\" In ICLR 2018. </li> \n       <li class=\"js-ref\" data-id=\"welling-2014-semi\"> Kingma, D., Rezende, D. J., Mohamed, S., &amp; Welling, M. (2014). \"<a href=\"https://arxiv.org/pdf/1406.5298.pdf\">Semi-Supervised Learning with Deep Generative Models</a>.\" In NeurIPS 2014. </li> \n       <li class=\"js-ref\" data-id=\"salimans-2016-improved\"> Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X. (2016). \"<a href=\"http://papers.nips.cc/paper/6124-improved-techniques-for-training-gans\">Improved techniques for training gans</a>.\" In NeurIPS 2016. </li> \n       <li class=\"js-ref\" data-id=\"tarvainen-2017-mean\"> Tarvainen, A., Valpola, H. (2017). \"<a href=\"https://papers.nips.cc/paper/6719-mean-teachers-are-better-role-models-weight-averaged-consistency-targets-improve-semi-supervised-deep-learning-results\">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</a>.\" In NeurIPS 2017. </li> \n       <li class=\"js-ref\" data-id=\"berthelot-2019-mixmatch\"> Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raffel, C. (2019). \"<a href=\"http://papers.nips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning\">MixMatch: A Holistic Approach to Semi-Supervised Learning</a>.\" In NeurIPS 2019. </li> \n       <li class=\"js-ref\" data-id=\"xie-2019-unsupervised\"> Xie, Q., Dai, Z., Hovy, E., Luong, M., &amp; Le, Q. V. (2019). \"<a href=\"https://arxiv.org/abs/1904.12848\">Unsupervised Data Augmentation for Consistency Training</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"sohn-2020-fixmatch\"> Sohn, K., Berthelot, D., Li, C., Zhang, Z., Carlini, N., Cubuk, E., Kurakin, A., Zhang, H., Raffel, C. (2020). \"<a href=\"https://arxiv.org/abs/2001.07685\">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</a>.\" arXiv preprint. </li> \n       <li class=\"js-ref\" data-id=\"sutton-2019-bitter\"> Sutton, R. (2019). \"<a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">The Bitter Lesson</a>.\" </li> \n      </ol> \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- special footer item for footnotes --> \n  <div data-order=\"-1\"> \n   <hr> \n    <div class=\"row\" id=\"footnotes\"> \n     <div class=\"col\">\n       Footnotes \n     </div> \n     <div class=\"col\"> \n      <hr class=\"footnotes-sep\"> \n       <section class=\"footnotes\"> \n        <ol class=\"footnotes-list\"> \n         <li id=\"fn1\" class=\"footnote-item\"><p>Measured through logistic regression on learned features (linear probe). <a href=\"https://openai.com/blog/image-gpt/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn2\" class=\"footnote-item\"><p>A transformer is trained to maximize the likelihood, and thus is mode covering, which automatically ensures the diversity of its samples. <a href=\"https://openai.com/blog/image-gpt/#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn3\" class=\"footnote-item\"><p>The original analysis by synthesis idea is more an argument for generative models with latent variables, but because generative models without latent variables were so much better at modeling the data distribution, we thought the analysis-by-synthesis conjecture should hold for them as well. <a href=\"https://openai.com/blog/image-gpt/#fnref3\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn4\" class=\"footnote-item\"><p>We only show linear probe accuracy on ImageNet for iGPT-XL since other experiments did not finish before we needed to transition to different supercomputing facilities. <a href=\"https://openai.com/blog/image-gpt/#fnref4\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn5\" class=\"footnote-item\"><p>To extract features for a linear probe, we take the post layernorm attention block inputs at some layer and average pool over the sequence dimension. <a href=\"https://openai.com/blog/image-gpt/#fnref5\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn6\" class=\"footnote-item\"><p>To fine-tune, we take the post layernorm transformer output and average pool over the sequence dimension as input for the classification head. <a href=\"https://openai.com/blog/image-gpt/#fnref6\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn7\" class=\"footnote-item\"><p>A generative model which learns features in a purely unsupervised fashion. <a href=\"https://openai.com/blog/image-gpt/#fnref7\" class=\"footnote-backref\">↩︎</a></p> </li> \n        </ol> \n       </section> \n       <!--kg-card-end: markdown--> \n      </hr> \n     </div> \n    </div> \n   </hr> \n  </div> \n </footer> \n</hr>","descriptionType":"html","publishedDate":"Wed, 17 Jun 2020 18:29:51 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/06/completions-3x5-clean-1.png","linkMd5":"9fa06ceea597a649f120b106fe508bd1","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn94@2020_4/2020/08/24/22-09-05-340_aa8ef6f38bbafcef.webp","destWidth":1145,"destHeight":687,"sourceBytes":130502,"destBytes":86534,"author":"OpenAI","articleImgCdnMap":{"https://openai.com/content/images/2020/06/completions-3x5-clean-1.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn94@2020_4/2020/08/24/22-09-05-340_aa8ef6f38bbafcef.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn38@2020_6/2020/08/24/22-09-07-584_6a289928470ebcb1.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-0.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn91@2020_6/2020/08/24/22-09-09-374_bdd8f69742e12b17.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-1.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn50@2020_3/2020/08/24/22-09-08-779_a2850ee094470170.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-2.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn18@2020_1/2020/08/24/22-09-07-639_dbe9e0c982239eca.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-3.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn2@2020_5/2020/08/24/22-09-07-657_a14f41e9cacddb1a.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn72@2020_2/2020/08/24/22-09-08-358_f862c22d1a477ebb.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-0.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn33@2020_6/2020/08/24/22-09-07-561_007b96ad1e73d606.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-1.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn53@2020_6/2020/08/24/22-09-07-605_b235bc48bc5a8bb3.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-2.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn83@2020_2/2020/08/24/22-09-08-461_2351d8a587b07cdc.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-3.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn63@2020_4/2020/08/24/22-09-08-368_b1b7f27faec2c37b.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn35@2020_4/2020/08/24/22-09-08-645_98d58afff264004e.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-0.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn56@2020_2/2020/08/24/22-09-07-583_abd232dcba60c07a.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-1.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn45@2020_1/2020/08/24/22-09-07-643_1b5a9220c077b479.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-2.png":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn21@2020_6/2020/08/24/22-09-07-641_634d4a6cd1727e41.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-3.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn72@2020_2/2020/08/24/22-09-08-715_e5f125cede0590b6.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn67@2020_6/2020/08/24/22-09-08-399_081789c3fd7816f6.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-0.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn55@2020_1/2020/08/24/22-09-08-674_6687019aee196929.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-1.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn5@2020_6/2020/08/24/22-09-07-587_54a7a78336ae041f.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-2.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn79@2020_2/2020/08/24/22-09-08-830_b79b38026c18d4c3.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-3.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn40@2020_4/2020/08/24/22-09-08-691_6babd830c1c26575.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn98@2020_5/2020/08/24/22-09-07-582_3a76b65c80243a25.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-0.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn28@2020_4/2020/08/24/22-09-08-682_f8526341eb06dd10.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-1.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn63@2020_2/2020/08/24/22-09-08-710_bc25dee7276c12ce.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-2.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn95@2020_3/2020/08/24/22-09-09-253_a4db036ebe440238.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-3.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn34@2020_2/2020/08/24/22-09-07-609_a785ace9dd21842c.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn6@2020_6/2020/08/24/22-09-07-547_e60167c3e45deed2.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-0.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn46@2020_4/2020/08/24/22-09-07-495_cf41837b8967f506.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-1.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn65@2020_3/2020/08/24/22-09-07-689_c5a28d4efb6779ee.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-2.png":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn8@2020_2/2020/08/24/22-09-08-527_0e97fab3357bdab3.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-3.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn16@2020_6/2020/08/24/22-09-08-604_fdca3bfe5e2e8a98.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn20@2020_1/2020/08/24/22-09-08-605_1d82f80c2d2a7d8c.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-0.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn81@2020_6/2020/08/24/22-09-07-631_2c945152fff06e92.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-1.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn79@2020_4/2020/08/24/22-09-08-534_b4a8fb607d3b92c7.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-2.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn75@2020_3/2020/08/24/22-09-08-460_7564307594d4a4aa.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-3.png":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn17@2020_5/2020/08/24/22-09-07-593_d817fec25280f63b.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-orig.png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn69@2020_2/2020/08/24/22-09-07-577_6b7fd48ecba7a45b.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-0.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn37@2020_5/2020/08/24/22-09-07-619_017fb5ec931b5960.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-1.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn12@2020_4/2020/08/24/22-09-08-527_a23a904b5f5b6698.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-2.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn92@2020_2/2020/08/24/22-09-08-456_3b0b1255c3d28b53.webp","https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-3.png":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn48@2020_1/2020/08/24/22-09-08-597_0f386417818c90c1.webp","https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg"},"publishedOrCreatedDate":1598306916760},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"OpenAI Microscope","link":"https://openai.com/blog/microscope/","description":"<!--kg-card-begin: markdown-->\n<div class=\"js-custom-media\"> \n <video class=\"w-100 mb-0\" autoplay=\"\" loop=\"\" muted=\"\" playsinline=\"\" src=\"https://cdn.openai.com/microscope/microscope-header-web-medium-bitrate.mp4\" poster=\"https://cdn.openai.com/microscope/microscope-video-poster.jpg\"> \n </video> \n</div> \n<div class=\"js-excerpt\"> \n <img src=\"https://openai.com/content/images/2020/04/microscope-social_4-8a.jpg\" alt=\"OpenAI Microscope\"><p>We’re introducing <a href=\"https://microscope.openai.com\">OpenAI Microscope</a>, a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.</p> </img>\n</div> \n<section class=\"btns\"> \n <a href=\"https://microscope.openai.com/\" class=\"btn btn-padded right icon-external\">Browse Microscope</a> \n</section> \n<p>The abilities of modern neural networks are the result of the interactions of thousands of neurons (sometimes tens of thousands or more!). In order to understand their behavior, we’d like to be able to quickly and easily investigate these neurons interactions in detail, and share those observations. This is especially true in collaborative environments. For instance, one researcher might speculate:</p> \n<blockquote> \n <p>InceptionV1 <a href=\"https://microscope.openai.com/models/inceptionv1/mixed4c_0/447\">4c:447</a> is a car detector which is built from a wheel detector (<a href=\"https://microscope.openai.com/models/inceptionv1/mixed4b_0/373\">4b:373</a>) and a window detector (<a href=\"https://microscope.openai.com/models/inceptionv1/mixed4b_0/237\">4b:237</a>).</p> \n</blockquote> \n<p>When someone makes a claim like this, it’s useful if others can quickly explore those neurons, evaluating the claim and discovering new things. This is the goal of the OpenAI Microscope.</p> \n<div class=\"wide mb-1.5\"> \n <div class=\"row\"> \n  <div class=\"col-12 col-md-6\"> \n   <div class=\"inset-border\">\n    <img src=\"https://openai.com/content/images/2020/04/models.jpg\" alt=\"OpenAI Microscope\" />\n   </div> \n  </div> \n  <div class=\"col-12 col-md-6\"> \n   <div class=\"inset-border\">\n    <img src=\"https://openai.com/content/images/2020/04/feature-vis.jpg\" alt=\"OpenAI Microscope\" />\n   </div> \n  </div> \n  <div class=\"pl-container ml-row\"></div> \n </div> \n</div> \n<p>Microscope systematically visualizes every neuron in several commonly studied vision models, and makes all of those neurons linkable. We hope this will support the interpretability community in several ways:</p> \n<ol> \n <li>Although these models and visualizations are already open source (we help maintain the <a href=\"https://github.com/tensorflow/lucid/\">lucid library</a>, which is used to generate all the visualizations in Microscope) visualizing neurons is tedious. Microscope changes the feedback loop of exploring neurons from minutes to seconds. This quick feedback loop has been essential for us in discovering unexpected features like high-low frequency detectors in the ongoing <a href=\"https://distill.pub/2020/circuits/zoom-in/\">circuits project</a>.</li> \n <li>Making models and neurons linkable allows immediate scrutiny and further exploration of research making claims about those neurons. It also removes potential confusion about which model and neuron is being discussed (which of the five versions of InceptionV1 are we talking about again?). This is really helpful for collaboration, especially when researchers are at different institutions.</li> \n <li>One of the wonderful things about interpretability as an area of ML is how accessible it is. Compared to many other areas, it requires comparatively little access to compute. But systematically visualizing neural networks can still take hundreds of GPU hours. We hope that, by sharing our visualizations, we can help keep interpretability highly accessible.</li> \n</ol> \n<p>Just as biologists often focus on the study of a few “model organisms,” Microscope focuses on exploring a small number of models in detail. Our initial release includes nine frequently studied vision models, along with several visualization techniques we’ve found particularly useful in studying them. We plan to expand to other models and techniques in the coming months.</p> \n<p>We’re excited to see how the community will use Microscope, and we encourage you to reuse these assets. In particular, we think it has a lot of potential in supporting the <a href=\"https://distill.pub/2020/circuits/zoom-in/\">Circuits collaboration</a>—a project to reverse engineer neural networks by analyzing individual neurons and their connections—or similar work.</p> \n<section class=\"btns mt-1.5\"> \n <a href=\"https://microscope.openai.com/\" class=\"btn btn-padded right icon-external\">Browse Microscope</a> \n</section> \n<!--kg-card-end: markdown-->","descriptionType":"html","publishedDate":"Tue, 14 Apr 2020 15:45:20 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/04/microscope-social_4-8a.jpg","linkMd5":"d19c175ab490b478d46e6df4c1bb3dc7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn98@2020_4/2020/08/24/22-09-05-296_ebfe0e5070af7744.webp","destWidth":1600,"destHeight":1200,"sourceBytes":323052,"destBytes":235586,"author":"Ludwig Schubert","articleImgCdnMap":{"https://openai.com/content/images/2020/04/microscope-social_4-8a.jpg":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn98@2020_4/2020/08/24/22-09-05-296_ebfe0e5070af7744.webp","https://openai.com/content/images/2020/04/models.jpg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn59@2020_2/2020/08/24/22-09-08-803_d173d6961976b8ed.webp","https://openai.com/content/images/2020/04/feature-vis.jpg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn26@2020_5/2020/08/24/22-09-07-640_a61ad39b2df09aec.webp"},"publishedOrCreatedDate":1598306916761},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"GPT-2: 1.5B Release","link":"https://openai.com/blog/gpt-2-1-5b-release/","description":"<!--kg-card-begin: markdown--> \n<div class=\"medium-copy color-fg-80 mt-n0.5\"> \n <img src=\"https://openai.com/content/images/2019/11/gpt-update_11-4b.jpg\" alt=\"GPT-2: 1.5B Release\"><p>As the final model release of <a href=\"https://openai.com/blog/better-language-models/\">GPT-2</a>’s <a href=\"https://openai.com/blog/gpt-2-6-month-follow-up/\">staged release</a>, we’re releasing the largest version (1.5B parameters) of GPT-2 along with <a href=\"https://github.com/openai/gpt-2-output-dataset\">code and model weights</a> to facilitate detection of outputs of GPT-2 models. While there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication.</p> </img> \n</div> \n<section class=\"btns\"> \n <a href=\"https://arxiv.org/abs/1908.09203\" class=\"btn btn-padded icon-paper\">Report</a> \n <a href=\"https://github.com/openai/gpt-2\" class=\"btn btn-padded icon-code\">GPT-2 Model</a> \n <a href=\"https://github.com/openai/gpt-2-output-dataset/tree/master/detector\" class=\"btn btn-padded icon-code\">Detector Model</a> \n <a href=\"https://github.com/openai/gpt-2/blob/master/model_card.md\" class=\"btn btn-padded icon-slides\">Model Card</a> \n</section> \n<h2 id=\"ourfindings\">Our findings</h2> \n<p><strong>1. Humans find GPT-2 outputs convincing.</strong> Our partners at Cornell University surveyed people to assign GPT-2 text a credibility score across model sizes. People gave the 1.5B model a “credibility score” of 6.91 out of 10. This is marginally greater than outputs from the 774M model (6.72) and significantly above the medium 355M model (6.07). These results make us more inclined to release the 1.5B model, as the incremental increase in human-perceived credibility relative to 774M seems low.</p> \n<p><strong>2. GPT-2 can be fine-tuned for misuse.</strong> Our partners at the Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism (CTEC) found that extremist groups can use GPT-2 for misuse, specifically by fine-tuning GPT-2 models on four ideological positions: white supremacy, Marxism, jihadist Islamism, and anarchism. CTEC demonstrated that it’s possible to create models that can generate synthetic propaganda for these ideologies. They also show that, despite having low detection accuracy on synthetic outputs, ML-based detection methods can give experts reasonable suspicion that an actor is generating synthetic text.</p> \n<p><strong>3. Detection is challenging.</strong> We expect that content-based detection of synthetic text is a long-term challenge. To test whether machine learning approaches may help today, we conducted in-house detection research and developed a <a href=\"https://github.com/openai/gpt-2-output-dataset\">detection model</a> that has detection rates of ~95% for detecting 1.5B GPT-2-generated text.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/gpt-2-1-5b-release/#fn1\" id=\"fnref1\">[1]</a></sup> We believe this is not high enough accuracy for standalone detection and needs to be paired with metadata-based approaches, human judgment, and public education to be more effective. We are releasing this model to aid the study of research into the detection of synthetic text, although this does let adversaries with access better evade detection.</p> \n<p>While we found detection accuracy depends heavily on the sampling methods used in training and testing, we also found detection to be more reliable when training across a range of sampling techniques. As seen in the figure below, we observed that larger models’ outputs are more difficult to classify, but training on larger models’ outputs makes detection results more accurate and robust. We expect this trend to continue and that detection will be more challenging with increased model size.</p> \n<h5 id=\"transferredmodelaccuracynucleussamples\">Transferred model accuracy (nucleus samples)</h5> \n<!-- copied from observable HTML output --> \n<div id=\"chart\" class=\"mb-1.5\" style=\"overflow-x:auto\"> \n <style> #matrix { border-collapse: collapse; } #matrix tr:not(:last-child) { border-bottom: 1px solid rgba(var(--fg), 0.0875); } #matrix th, #matrix td { padding-left: 0.25rem; padding-right: 0.25rem; min-width: 108px; } #matrix th:first-child, #matrix td:first-child { padding-left: 0; } #matrix th:last-child, #matrix td:last-child { padding-right: 0; } #matrix td { padding-top: 0.25rem; padding-bottom: 0.25rem; vertical-align: middle; } </style> \n <table class=\"table-unstyled d-block d-md-table small-copy color-fg-80\" id=\"matrix\"> \n  <thead> \n   <tr> \n    <th style=\"vertical-align:bottom;width:20%\" class=\"color-fg-50\">Trained on <span class=\"icon position-relative\" style=\"top:0.12em\">down</span></th> \n    <th style=\"vertical-align:bottom;width:20%\"><span class=\"color-fg-50\">Tested on <span class=\"icon position-relative\" style=\"top:0.12em\">right</span></span><br>Small (124M)</br></th> \n    <th style=\"vertical-align:bottom;width:20%\">Medium (355M)</th> \n    <th style=\"vertical-align:bottom;width:20%\">Large (774M)</th> \n    <th style=\"vertical-align:bottom;width:20%\">XL (1.5B)</th> \n   </tr> \n  </thead> \n  <thead></thead> \n  <tbody> \n   <tr> \n    <td>Small (124M)</td> \n    <td> \n     <div style=\"background-color: rgb(16, 35, 104)\" class=\"text-center py-0.125 rounded color-white\">\n       99.3% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(34, 66, 152)\" class=\"text-center py-0.125 rounded color-white\">\n       96.6% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(49, 164, 193)\" class=\"text-center py-0.125 rounded color-white\">\n       90.9% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(255, 255, 217)\" class=\"text-center py-0.125 rounded color-black\">\n       79.3% \n     </div></td> \n   </tr> \n   <tr> \n    <td>Medium (355M)</td> \n    <td> \n     <div style=\"background-color: rgb(19, 38, 111)\" class=\"text-center py-0.125 rounded color-white\">\n       99.0% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(24, 43, 121)\" class=\"text-center py-0.125 rounded color-white\">\n       98.5% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(34, 62, 149)\" class=\"text-center py-0.125 rounded color-white\">\n       96.9% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(39, 150, 191)\" class=\"text-center py-0.125 rounded color-white\">\n       91.8% \n     </div></td> \n   </tr> \n   <tr> \n    <td>Large (774M)</td> \n    <td> \n     <div style=\"background-color: rgb(25, 44, 124)\" class=\"text-center py-0.125 rounded color-white\">\n       98.4% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(29, 49, 133)\" class=\"text-center py-0.125 rounded color-white\">\n       97.9% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(29, 49, 133)\" class=\"text-center py-0.125 rounded color-white\">\n       97.9% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(35, 80, 161)\" class=\"text-center py-0.125 rounded color-white\">\n       95.7% \n     </div></td> \n   </tr> \n   <tr> \n    <td>XL (1.5B)</td> \n    <td> \n     <div style=\"background-color: rgb(34, 62, 149)\" class=\"text-center py-0.125 rounded color-white\">\n       96.9% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(34, 65, 151)\" class=\"text-center py-0.125 rounded color-white\">\n       96.7% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(34, 66, 152)\" class=\"text-center py-0.125 rounded color-white\">\n       96.6% \n     </div></td> \n    <td> \n     <div style=\"background-color: rgb(35, 75, 158)\" class=\"text-center py-0.125 rounded color-white\">\n       96.0% \n     </div></td> \n   </tr> \n  </tbody> \n </table> \n</div> \n<p><strong>4. We’ve seen no strong evidence of misuse so far.</strong> While we’ve seen some discussion around GPT-2’s potential to augment high-volume/low-yield operations like spam and phishing, we haven’t seen evidence of writing code, documentation, or instances of misuse. We think synthetic text generators have a higher chance of being misused if their outputs become more reliable and coherent. We acknowledge that we cannot be aware of all threats, and that motivated actors can replicate language models without model release.</p> \n<p><strong>5. We need standards for studying bias.</strong> Language models have biases. Working out how to study these biases, discuss them, and address them, is a challenge for the AI research community. We’ve approached the challenge of bias in two ways:</p> \n<ul> \n <li>Publishing a <a href=\"https://github.com/openai/gpt-2/blob/master/model_card.md\">model card</a><sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/gpt-2-1-5b-release/#fn2\" id=\"fnref2\">[2]</a></sup> alongside our models on GitHub to give people a sense of the issues inherent to language models such as GPT-2.</li> \n <li>Performing a qualitative, in-house evaluation of some of the biases in GPT-2: We probed GPT-2 for some gender, race, and religious biases, using those findings to inform our model card. These probes are not comprehensive and raise the need for collaboration on bias analysis frameworks.</li> \n</ul> \n<h2 id=\"nextsteps\">Next steps</h2> \n<p>Our experience with GPT-2 over the past 9 months has given us valuable insight into the challenges and opportunities for creating responsible publication norms in AI. We’re continuing our work on this issue via participation in the Partnership on AI’s “Responsible Publication Norms for Machine Learning” project and discussions with our colleagues in the research community.</p> \n<p><em>If you’d like to develop large-scale AI systems and think about their implications, <a href=\"https://openai.com/jobs/\">we’re hiring</a>.</em></p> \n<footer class=\"post-footer js-post-footer\"> \n <!-- footer item --> \n <div> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Acknowledgments \n    </div> \n    <div class=\"col\">\n      Thanks to the following for feedback on this post: Greg Brockman, Jeffrey Wu, Alec Radford, Jong Wook Kim, Gretchen Krueger, Alex Newhouse, Jason Blazakis, Sarah Kreps, Miles McCain, Cody Wild, Mona Wang, Jeremy Gillula, Larissa Schiavo, Aviv Ovadya, Rebecca Crootof \n    </div> \n   </div> \n  </hr> \n </div> \n <!-- special footer item for footnotes --> \n <div data-order=\"-1\"> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Footnotes \n    </div> \n    <div class=\"col\"> \n     <hr class=\"footnotes-sep\"> \n      <section class=\"footnotes\"> \n       <ol class=\"footnotes-list\"> \n        <li id=\"fn1\" class=\"footnote-item\"><p>Specifically, we based a sequence classifier on RoBERTa<sub>BASE</sub> (125 million parameters) and RoBERTa<sub>LARGE</sub> (355 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model. <a href=\"https://openai.com/blog/gpt-2-1-5b-release/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn2\" class=\"footnote-item\"><p>Which we’ve based on “<a href=\"https://arxiv.org/abs/1810.03993\">Model Cards for Model Reporting</a>” by Mitchell et al. <a href=\"https://openai.com/blog/gpt-2-1-5b-release/#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> \n       </ol> \n      </section> \n      <!--kg-card-end: markdown--> \n     </hr> \n    </div> \n   </div> \n  </hr> \n </div> \n</footer>","descriptionType":"html","publishedDate":"Tue, 05 Nov 2019 17:05:24 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2019/11/gpt-update_11-4b.jpg","linkMd5":"ddd4109bea62c13a4ef5ef3630d8102e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn66@2020_5/2020/08/24/22-09-05-353_976bc3ee70c66275.webp","destWidth":2000,"destHeight":833,"sourceBytes":414538,"destBytes":325310,"author":"Irene Solaiman","articleImgCdnMap":{"https://openai.com/content/images/2019/11/gpt-update_11-4b.jpg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn66@2020_5/2020/08/24/22-09-05-353_976bc3ee70c66275.webp"},"publishedOrCreatedDate":1598306916762},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Jukebox","link":"https://openai.com/blog/jukebox/","description":"<!--kg-card-begin: markdown--> \n<div class=\"full mt-0 mb-1.5 mb-md-3\" style=\"background-color:#fafafb\"> \n <div class=\"container\"> \n  <div class=\"row\"> \n   <div class=\"content\"> \n    <!-- Header samples --> \n    <div class=\"mt-2.5 mb-2\"> \n     <h2 id=\"curated\">Curated samples</h2> \n     <div class=\"medium-copy mb-1\">\n       Provided with genre, artist, and lyrics as input, Jukebox outputs a new music sample produced from scratch. Below, we show some of our favorite samples. \n     </div> \n     <div id=\"samples-css\"></div> \n     <div id=\"samples\"></div> \n     <div class=\"small-copy color-fg-50 mt-1.5 mb-0.5\">\n       To hear all uncurated samples, check out our sample&nbsp;explorer. \n     </div> \n     <a href=\"https://jukebox.openai.com/\" class=\"btn btn-padded icon-external right mb-0\">Explore All Samples</a> \n    </div> \n    <!-- when pasting in embeds, make sure `show_user` is `false` --> \n   </div> \n  </div> \n </div> \n</div> \n<!-- end .full --> \n<aside class=\"aside color-fg-50 small-copy\"> \n <div class=\"mt-0.25 mb-0.125\">\n   Contents \n </div> \n <ol class=\"list-indented list-compact\"> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/jukebox/#motivationandpriorwork\">Motivation and prior work</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/jukebox/#approach\">Approach</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/jukebox/#limitations\">Limitations</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/jukebox/#futuredirections\">Future directions</a></li> \n  <li><a class=\"no-underline\" href=\"https://openai.com/blog/jukebox/#timeline\">Timeline</a></li> \n </ol> \n</aside> \n<hr class=\"d-md-none\"> \n <h2 id=\"motivationandpriorwork\">Motivation and prior work</h2> \n <img src=\"https://openai.com/content/images/2020/04/2x-no-mark-1.jpg\" alt=\"Jukebox\"><p>Automatic music generation dates back to more than half a century.<span class=\"js-rfref\" data-id=\"hiller\"></span><span class=\"js-rfref\" data-id=\"moorer\"></span><span class=\"js-rfref\" data-id=\"beyls\"></span><span class=\"js-rfref\" data-id=\"conklin\"></span> A prominent approach is to generate music symbolically in the form of a piano roll, which specifies the timing, pitch, velocity, and instrument of each note to be played. This has led to impressive results like producing Bach chorals,<span class=\"js-rfref\" data-id=\"hadjeres\"></span><span class=\"js-rfref\" data-id=\"huang-2019\"></span> polyphonic music with multiple instruments,<span class=\"js-rfref\" data-id=\"dong\"></span><span class=\"js-rfref\" data-id=\"yang\"></span><span class=\"js-rfref\" data-id=\"roberts\"></span> as well as minute long musical pieces.<span class=\"js-rfref\" data-id=\"huang-2018\"></span><span class=\"js-rfref\" data-id=\"payne\"></span><span class=\"js-rfref\" data-id=\"wu\"></span></p> <p>But symbolic generators have limitations—they cannot capture human voices or many of the more subtle timbres, dynamics, and expressivity that are essential to music. A different approach<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/jukebox/#fn1\" id=\"fnref1\">[1]</a></sup> is to model music directly as raw audio.<span class=\"js-rfref\" data-id=\"oord-2016\"></span><span class=\"js-rfref\" data-id=\"mehri\"></span><span class=\"js-rfref\" data-id=\"yamamoto\"></span><span class=\"js-rfref\" data-id=\"vasquez\"></span> Generating music at the audio level is challenging since the sequences are very long.<span class=\"js-rfref\" data-id=\"dieleman\"></span> A typical 4-minute song at CD quality (44 kHz, 16-bit) has over 10 million timesteps. For comparison, GPT-2 had 1,000 timesteps and <a href=\"https://openai.com/projects/five/\">OpenAI Five</a> took tens of thousands of timesteps per game. Thus, to learn the high level semantics of music, a model would have to deal with extremely long-range dependencies.</p> <p>One way of addressing the long input problem is to use an autoencoder that compresses raw audio to a lower-dimensional space by discarding some of the perceptually irrelevant bits of information. We can then train a model to generate audio in this compressed space, and upsample back to the raw audio space.<span class=\"js-rfref\" data-id=\"van-den-oord\"></span><span class=\"js-rfref\" data-id=\"dieleman\"></span></p> <p>We chose to work on music because we want to continue to push the boundaries of generative models. Our previous work on <a href=\"https://openai.com/blog/musenet\">MuseNet</a> explored synthesizing music based on large amounts of MIDI data. Now in raw audio, our models must learn to tackle high diversity as well as very long range structure, and the raw audio domain is particularly unforgiving of errors in short, medium, or long term timing.</p> \n  <figure id=\"overview\" class=\"mt-2 mb-2.5\"> \n   <!-- 1 --> \n   <div class=\"mb-1/3\"> \n    <div class=\"position-relative\" style=\"padding-bottom:5.555556%\"> \n     <div id=\"overview-1\" class=\"position-absolute trbl-0\"> \n      <img src=\"https://cdn.openai.com/jukebox/assets/waveforms/1-original.png\" alt=\"Jukebox\" /> \n     </div> \n    </div> \n   </div> \n   <div class=\"small-copy color-fg-50\"> \n    <strong class=\"color-fg\">Raw audio</strong> 44.1k samples per second, where each sample is a float that represents the amplitude of sound at that moment in&nbsp;time \n   </div> \n   <div class=\"row narrow-gutters align-items-center mb-0.25\"> \n    <div class=\"col-auto col-sm-6\"> \n     <img src=\"https://cdn.openai.com/jukebox/assets/overview-arrow.svg\" class=\"ml-auto mb-0\" style=\"width:5px\" alt=\"Jukebox\"> </img> \n    </div> \n    <div class=\"col col-sm-6\"> \n     <div class=\"small-copy balance-text\">\n       Encode using CNNs (convolutional neural networks) \n     </div> \n    </div> \n   </div> \n   <!-- 2 --> \n   <div class=\"mb-1/3\"> \n    <div class=\"bg-light-warm-gray position-relative\" style=\"padding-bottom:5.555556%\"> \n     <div id=\"overview-2\" class=\"position-absolute trbl-0\"> \n      <img src=\"https://cdn.openai.com/jukebox/assets/overview-2.svg\" alt=\"Jukebox\" /> \n     </div> \n    </div> \n   </div> \n   <div class=\"small-copy color-fg-50\"> \n    <strong class=\"color-fg\">Compressed audio</strong> 344 samples per second, where each sample is 1 of 2048 possible vocab&nbsp;tokens \n   </div> \n   <div class=\"row narrow-gutters align-items-center mb-0.25\"> \n    <div class=\"col-auto col-sm-6\"> \n     <img src=\"https://cdn.openai.com/jukebox/assets/overview-arrow.svg\" class=\"ml-auto mb-0\" style=\"width:5px\" alt=\"Jukebox\"> </img> \n    </div> \n    <div class=\"col col-sm-6\"> \n     <div class=\"small-copy balance-text\">\n       Generate novel patterns from trained transformer conditioned on lyrics \n     </div> \n    </div> \n   </div> \n   <!-- 3 --> \n   <div class=\"mb-1/3\"> \n    <div class=\"position-relative\" style=\"padding-bottom:5.555556%\"> \n     <div id=\"overview-3\" class=\"position-absolute trbl-0\"> \n      <img src=\"https://cdn.openai.com/jukebox/assets/overview-3.svg\" alt=\"Jukebox\" /> \n     </div> \n    </div> \n   </div> \n   <div class=\"small-copy color-fg-50\"> \n    <strong class=\"color-fg\">Novel compressed audio</strong> 344 samples per second \n   </div> \n   <div class=\"row narrow-gutters align-items-center mb-0.25\"> \n    <div class=\"col-auto col-sm-6\"> \n     <img src=\"https://cdn.openai.com/jukebox/assets/overview-arrow.svg\" class=\"ml-auto mb-0\" style=\"width:5px\" alt=\"Jukebox\"> </img> \n    </div> \n    <div class=\"col col-sm-6\"> \n     <div class=\"small-copy balance-text\">\n       Upsample using transformers and decode using CNNs \n     </div> \n    </div> \n   </div> \n   <!-- 4 --> \n   <div class=\"mb-1/3\"> \n    <div class=\"position-relative\" style=\"padding-bottom:5.555556%\"> \n     <div id=\"overview-4\" class=\"position-absolute trbl-0\"> \n      <img src=\"https://cdn.openai.com/jukebox/assets/waveforms/1-novel.png\" alt=\"Jukebox\" /> \n     </div> \n    </div> \n   </div> \n   <div class=\"small-copy color-fg-50\"> \n    <strong class=\"color-fg\">Novel raw audio</strong> 44.1k samples per second \n   </div> \n  </figure> \n  <!-- end #overview --> <h2 id=\"approach\">Approach</h2> <h3 id=\"compressingmusictodiscretecodes\">Compressing music to discrete codes</h3> <p>Jukebox's autoencoder model compresses audio to a discrete space, using a quantization-based approach called VQ-VAE.<span class=\"js-rfref\" data-id=\"van-den-oord\"></span> Hierarchical VQ-VAEs<span class=\"js-rfref\" data-id=\"dieleman\"></span> can generate short instrumental pieces from a few sets of instruments, however they suffer from hierarchy collapse due to use of successive encoders coupled with autoregressive decoders. A simplified variant called VQ-VAE-2<span class=\"js-rfref\" data-id=\"razavi\"></span> avoids these issues by using feedforward encoders and decoders only, and they show impressive results at generating high-fidelity images.</p> <p>We draw inspiration from VQ-VAE-2 and apply their approach to music. We modify their architecture as follows:</p> \n  <ul> \n   <li>To alleviate codebook collapse common to VQ-VAE models, we use random restarts where we randomly reset a codebook vector to one of the encoded hidden states whenever its usage falls below a threshold.</li> \n   <li>To maximize the use of the upper levels, we use separate decoders and independently reconstruct the input from the codes of each level.</li> \n   <li>To allow the model to reconstruct higher frequencies easily, we add a spectral loss<span class=\"js-rfref\" data-id=\"oord-2017\"></span><span class=\"js-rfref\" data-id=\"arik\"></span> that penalizes the norm of the difference of input and reconstructed spectrograms.</li> \n  </ul> <p>We use three levels in our VQ-VAE, shown below, which compress the 44kHz raw audio by 8x, 32x, and 128x, respectively, with a codebook size of 2048 for each level. This downsampling loses much of the audio detail, and sounds noticeably noisy as we go further down the levels. However, it retains essential information about the pitch, timbre, and volume of the audio.</p> \n  <!-- start vqvae --> \n  <div id=\"vqvae\" class=\"full my-2 py-2 bg-fg-2\"> \n   <div class=\"container\"> \n    <div class=\"row\"> \n     <div class=\"content\"> \n      <!-- switch --> \n      <div class=\"clearfix mb-2/3\"> \n       <div class=\"switch\"> \n        <input type=\"radio\" class=\"switch-input\" name=\"switch-vqvae\" value=\"compress\" id=\"compress-switch\" onclick=\"toggle('vqvae', 'is-swapped', false)\" checked=\"\"> <label for=\"compress-switch\" class=\"switch-label switch-label-off small-caps\">1. Compress</label> <input type=\"radio\" class=\"switch-input\" name=\"switch-vqvae\" value=\"generate\" id=\"generate-switch\" onclick=\"toggle('vqvae', 'is-swapped', true)\"> <label for=\"generate-switch\" class=\"switch-label switch-label-on small-caps\">2. Generate</label> <span class=\"switch-selection\"></span> </input></input> \n       </div> \n      </div> \n      <!-- text --> \n      <div class=\"medium-small-copy position-relative mb-1 mb-md-0.25\"> \n       <div id=\"vqvae-text-1\">\n         Each VQ-VAE level independently encodes the input. The bottom level encoding produces the highest quality reconstruction, while the top level encoding retains only the essential musical information. \n       </div> \n       <div id=\"vqvae-text-2\" class=\"position-absolute trbl-0\">\n         To generate novel songs, a cascade of transformers generates codes from top to bottom level, after which the bottom-level decoder can convert them to raw audio. \n       </div> \n      </div> \n      <!-- image --> \n      <div class=\"wide my-0 overflow-hidden\"> \n       <div class=\"position-relative w-100\"> \n        <div id=\"vqvae-image-1\" class=\"mx-auto\" style=\"max-width:1050px\"> \n         <div class=\"position-relative\"> \n          <img src=\"https://cdn.openai.com/jukebox/assets/vqvae-1.svg\" class=\"mb-0\" alt=\"Jukebox\"> \n           <div class=\"position-absolute trbl-0\"> \n            <svg width=\"1050\" height=\"616\" viewbox=\"0 0 1050 616\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"max-width:100%;height:auto\"> \n             <g id=\"vqvae-1-play\"> \n              <rect id=\"audio-0\" y=\"515\" width=\"240\" height=\"82\" fill=\"transparent\" /> \n              <rect id=\"audio-1\" x=\"270\" y=\"515\" width=\"240\" height=\"82\" fill=\"transparent\" /> \n              <rect id=\"audio-2\" x=\"540\" y=\"515\" width=\"240\" height=\"82\" fill=\"transparent\" /> \n              <rect id=\"audio-3\" x=\"810\" y=\"515\" width=\"240\" height=\"82\" fill=\"transparent\" /> \n             </g> \n            </svg> \n           </div> </img> \n         </div> \n        </div> \n        <div id=\"vqvae-image-2\" class=\"mx-auto position-absolute trbl-0\" style=\"max-width:1050px\"> \n         <div class=\"position-relative\"> \n          <img src=\"https://cdn.openai.com/jukebox/assets/vqvae-2.svg\" class=\"mb-0\" alt=\"Jukebox\"> \n           <div class=\"position-absolute trbl-0\"> \n            <svg width=\"1050\" height=\"616\" viewbox=\"0 0 1050 616\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"> \n             <g id=\"vqvae-2-play\"> \n              <rect id=\"audio-novel\" x=\"270\" y=\"515\" width=\"240\" height=\"82\" fill=\"transparent\" /> \n             </g> \n            </svg> \n           </div> </img> \n         </div> \n        </div> \n       </div> \n      </div> \n     </div> \n    </div> \n    <!-- end .row .content --> \n   </div> \n   <!-- end .container --> \n  </div> \n  <!-- end #vqvae --> <h3 id=\"generatingcodesusingtransformers\">Generating codes using transformers</h3> <p>Next, we train the prior models whose goal is to learn the distribution of music codes encoded by VQ-VAE and to generate music in this compressed discrete space. Like the VQ-VAE, we have three levels of priors: a top-level prior that generates the most compressed codes, and two upsampling priors that generate less compressed codes conditioned on above.</p> <p>The top-level prior models the long-range structure of music, and samples decoded from this level have lower audio quality but capture high-level semantics like singing and melodies. The middle and bottom upsampling priors add local musical structures like timbre, significantly improving the audio quality.</p> <p>We train these as autoregressive models using a simplified variant of Sparse Transformers.<span class=\"js-rfref\" data-id=\"child\"></span><span class=\"js-rfref\" data-id=\"vaswani\"></span> Each of these models has 72 layers of factorized self-attention on a context of 8192 codes, which corresponds to approximately 24 seconds, 6 seconds, and 1.5 seconds of raw audio at the top, middle and bottom levels, respectively.</p> <p>Once all of the priors are trained, we can generate codes from the top level, upsample them using the upsamplers, and decode them back to the raw audio space using the VQ-VAE decoder to sample novel songs.</p> <h3 id=\"dataset\">Dataset</h3> <p>To train this model, we crawled the web to curate a new dataset of 1.2 million songs (600,000 of which are in English), paired with the corresponding lyrics and metadata from <a href=\"https://lyrics.fandom.com/wiki/LyricWiki\">LyricWiki</a>. The metadata includes artist, album genre, and year of the songs, along with common moods or playlist keywords associated with each song. We train on 32-bit, 44.1 kHz raw audio, and perform data augmentation by randomly downmixing the right and left channels to produce mono audio.</p> <h3 id=\"artistandgenreconditioning\">Artist and genre conditioning</h3> <p>The top-level transformer is trained on the task of predicting compressed audio tokens. We can provide additional information, such as the artist and genre for each song. This has two advantages: first, it reduces the entropy of the audio prediction, so the model is able to achieve better quality in any particular style; second, at generation time, we are able to steer the model to generate in a style of our choosing.</p> <p>This t-SNE<span class=\"js-rfref\" data-id=\"maaten\"></span> below shows how the model learns, in an unsupervised way, to cluster similar artists and genres close together, and also makes some surprising associations like Jennifer Lopez being so close to Dolly Parton!</p> \n  <!-- t-SNE --> \n  <div id=\"tsne-styles\"></div> \n  <div class=\"full pt-0.5 my-2 mb-md-2.5\" style=\"background-color:#fafafb\"> \n   <div class=\"container\"> \n    <div class=\"mx-auto\" style=\"max-width:1200px\"> \n     <div id=\"tsne\" class=\"aspect-1/1\"></div> \n    </div> \n   </div> \n  </div> <h3 id=\"lyricsconditioning\">Lyrics conditioning</h3> <p>In addition to conditioning on artist and genre, we can provide more context at training time by conditioning the model on the lyrics for a song. A significant challenge is the lack of a well-aligned dataset: we only have lyrics at a song level without alignment to the music, and thus for a given chunk of audio we don’t know precisely which portion of the lyrics (if any) appear. We also may have song versions that don’t match the lyric versions, as might occur if a given song is performed by several different artists in slightly different ways. Additionally, singers frequently repeat phrases, or otherwise vary the lyrics, in ways that are not always captured in the written lyrics.</p> <p>To match audio portions to their corresponding lyrics, we begin with a simple heuristic that aligns the characters of the lyrics to linearly span the duration of each song, and pass a fixed-size window of characters centered around the current segment during training. While this simple strategy of linear alignment worked surprisingly well, we found that it fails for certain genres with fast lyrics, such as hip hop. To address this, we use Spleeter<span class=\"js-rfref\" data-id=\"hennequin\"></span> to extract vocals from each song and run NUS AutoLyricsAlign<span class=\"js-rfref\" data-id=\"gupta\"></span> on the extracted vocals to obtain precise word-level alignments of the lyrics. We chose a large enough window so that the actual lyrics have a high probability of being inside the window.</p> <p>To attend to the lyrics, we add an encoder to produce a representation for the lyrics, and add attention layers that use queries from the music decoder to attend to keys and values from the lyrics encoder. After training, the model learns a more precise alignment.</p> \n  <!-- start lyrics visual --> \n  <div class=\"mt-1.5 mb-2\"> \n   <p><img src=\"https://cdn.openai.com/jukebox/assets/lyrics-attention.svg\" alt=\"Jukebox\" /></p> \n   <div class=\"caption\"> \n    <strong class=\"color-fg\">Lyric–music alignment learned by encoder–decoder attention layer</strong> \n    <br>Attention progresses from one lyric token to the next as the music progresses, with a few moments of uncertainty.</br> \n   </div> \n  </div> \n  <!-- end lyrics visual --> <h2 id=\"limitations\">Limitations</h2> <p>While Jukebox represents a step forward in musical quality, coherence, length of audio sample, and ability to condition on artist, genre, and lyrics, there is a significant gap between these generations and human-created music.</p> <p>For example, while the generated songs show local musical coherence, follow traditional chord patterns, and can even feature impressive solos, we do not hear familiar larger musical structures such as choruses that repeat. Our downsampling and upsampling process introduces discernable noise. Improving the VQ-VAE so its codes capture more musical information would help reduce this. Our models are also slow to sample from, because of the autoregressive nature of sampling. It takes approximately 9 hours to fully render one minute of audio through our models, and thus they cannot yet be used in interactive applications. Using techniques<span class=\"js-rfref\" data-id=\"oord-2017\"></span><span class=\"js-rfref\" data-id=\"kingma\"></span> that distill the model into a parallel sampler can significantly speed up the sampling speed. Finally, we currently train on English lyrics and mostly Western music, but in the future we hope to include songs from other languages and parts of the world.</p> <h2 id=\"futuredirections\">Future directions</h2> <p>Our audio team is continuing to work on generating audio samples conditioned on different kinds of priming information. In particular, we've seen early success conditioning on MIDI files and stem files. Here's an example of a <a href=\"https://soundcloud.com/openai_audio/generated-raw\">raw audio sample</a> conditioned on <a href=\"https://soundcloud.com/openai_audio/midi-input-given-to-model-as-midi-tokens-rendered-here-by-timidity\">MIDI tokens</a>. We hope this will improve the musicality of samples (in the way conditioning on lyrics improved the singing), and this would also be a way of giving musicians more control over the generations. We expect human and model collaborations to be an increasingly exciting creative space. If you’re excited to work on these problems with us, <a href=\"https://openai.com/jobs/\">we’re hiring</a>.</p> <p>As generative modeling across various domains continues to advance, we are also conducting research into issues like <a href=\"https://arxiv.org/abs/1908.09203\">bias</a> and <a href=\"https://cdn.openai.com/policy-submissions/OpenAI+Comments+on+Intellectual+Property+Protection+for+Artificial+Intelligence+Innovation.pdf\">intellectual property rights</a>, and are engaging with people who work in the domains where we develop tools. To better understand future implications for the music community, we shared Jukebox with an initial set of 10 musicians from various genres to discuss their feedback on this work. While Jukebox is an interesting research result, these musicians did not find it immediately applicable to their creative process given some of its current <a href=\"https://openai.com/blog/jukebox/#limitations\">limitations</a>. We are connecting with the wider creative community as we think generative work across text, images, and audio will continue to improve. If you're interested in being a creative collaborator to help us build <a href=\"https://openai.com/blog/musenet/\">useful tools</a> or new works of art in these domains, please <a href=\"https://forms.gle/8npHSMnE5hfSxkkU9\">let us know</a>!</p> \n  <section class=\"btns\"> \n   <a href=\"https://forms.gle/8npHSMnE5hfSxkkU9\" class=\"btn btn-padded icon-external right\">Creative Collaborator Sign-Up</a> \n  </section> <p><em>To connect with the corresponding authors, please email <a href=\"mailto:jukebox@openai.com\">jukebox@openai.com</a>.</em></p> \n  <!-- start timeline --> \n  <div class=\"full bg-shadow\"> \n   <div class=\"container\"> \n    <div class=\"row\"> \n     <div class=\"content\"> \n      <ul class=\"timeline\" id=\"timeline\"> \n       <h4>Timeline</h4> \n       <li data-date=\"August 2019\"> Our first raw audio model, which learns to recreate instruments like Piano and Violin. We try a dataset of rock and pop songs, and surprisingly it works. \n        <!-- Early August: Piano --> \n        <hr class=\"my-5/12 mt-1\"> \n         <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762987670%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n         <!-- Late August: Hints of Pop --> \n         <hr class=\"my-5/12\"> \n          <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762983845%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n          <hr class=\"my-5/12\"> \n          </hr> \n         </hr> \n        </hr></li> \n       <br> <li data-date=\"October 2019\"> We collect a larger and more diverse dataset of songs, with labels for genres and artists. Model picks up artist and genre styles more consistently with diversity, and at convergence can also produce full-length songs with long-range coherence. \n         <!-- Late October: In the style of Bob Marley --> \n         <hr class=\"my-5/12 mt-1\"> \n          <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762992857%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n          <!-- Late October: In the style of Rock and Roll --> \n          <hr class=\"my-5/12\"> \n           <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762992881%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n           <!-- Late October: In the style of Louis Armstrong --> \n           <hr class=\"my-5/12\"> \n            <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762993565%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n            <hr class=\"my-5/12\"> \n             <!-- Late October: In the style of Blues --> \n             <!-- <hr class=\"my-5/12\">\n<iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762992890%3F&color=%23ff5500&inverse=false&auto_play=false&show_user=false\"></iframe> --> \n            </hr> \n           </hr> \n          </hr> \n         </hr></li> <br> <li data-date=\"January 2020\"> We scale our VQ-VAE from 22 to 44kHz to achieve higher quality audio. We also scale top-level prior from 1B to 5B to capture the increased information. We see better musical quality, clear singing, and long-range coherence. We also make novel completions of real songs. \n          <!-- Early January: In the style of Hip Hop/Rap --> \n          <hr class=\"my-5/12 mt-1\"> \n           <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762974362%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n           <!-- Early January: In the style of David Bowie --> \n           <hr class=\"my-5/12\"> \n            <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762970192%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n            <!-- Late January: In the style of Frank Sinatra --> \n            <hr class=\"my-5/12\"> \n             <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762972598%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n             <!-- Early January: Early January: Generation following 12 seconds of priming --> \n             <hr class=\"my-5/12\"> \n              <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/762975382%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n              <hr class=\"my-5/12\"> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr></li> <br> <li data-date=\"February 2020\"> We start training models conditioned on lyrics to incorporate further conditioning information. We only have unaligned lyrics, so model has to learn alignment and pronunciation, as well as singing. \n           <!-- February: In the style of Kylie Minogue (\"Count Every Minute\") --> \n           <hr class=\"my-5/12 mt-1\"> \n            <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/758998687%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n            <!-- February: In the style of Frank Sinatra (\"See Your Face Again\") --> \n            <hr class=\"my-5/12\"> \n             <iframe width=\"100%\" height=\"20\" scrolling=\"no\" frameborder=\"no\" allow=\"autoplay\" class=\"js-lazy\" data-src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/758998978%3F&amp;color=%23ff5500&amp;inverse=false&amp;auto_play=false&amp;show_user=false\"></iframe> \n             <hr class=\"my-5/12\"> \n             </hr> \n            </hr> \n           </hr></li> </br></br></br> \n      </ul> \n     </div> \n     <!-- end .content --> \n    </div> \n    <!-- end .row --> \n   </div> \n   <!-- end .container --> \n  </div> \n  <!-- end .full --> \n  <!-- end timeline --> \n  <footer class=\"post-footer js-post-footer\"> \n   <!-- footer item --> \n   <div> \n    <hr> \n     <div class=\"row\"> \n      <div class=\"col\">\n        Acknowledgments \n      </div> \n      <div class=\"col\">\n        Thank you to the following for their feedback on this work and contributions to this release: Jack Clark, Gretchen Krueger, Miles Brundage, Jeff Clune, Jakub Pachocki, Ryan Lowe, Shan Carter, David Luan, Vedant Misra, Daniela Amodei, Greg Brockman, Kelly Sims, Karson Elmgren, Bianca Martin, Rewon Child, Will Guss, Rob Laidlow, Rachel White, Delwin Campbell, Tasso Smith, Matthew Suttor, Konrad Kaczmarek, Scott Petersen, Dakota Stipp, Jena Ezzeddine \n      </div> \n     </div> \n    </hr> \n   </div> \n   <!-- footer item --> \n   <div> \n    <hr> \n     <div class=\"row\"> \n      <div class=\"col\">\n        Editor \n      </div> \n      <div class=\"col\">\n        Ashley Pilipiszyn \n      </div> \n     </div> \n    </hr> \n   </div> \n   <!-- footer item --> \n   <div> \n    <hr> \n     <div class=\"row\"> \n      <div class=\"col\">\n        Design &amp; Development \n      </div> \n      <div class=\"col\">\n        Justin Jay Wang &amp; Brooke Chan \n      </div> \n     </div> \n    </hr> \n   </div> \n   <!-- footer item --> \n   <div> \n    <hr> \n     <div class=\"row\"> \n      <div class=\"col\">\n        Cover Art \n      </div> \n      <div class=\"col\">\n        Ben Barry \n      </div> \n     </div> \n    </hr> \n   </div> \n   <!-- footer item --> \n   <div> \n    <hr> \n     <div class=\"row\" id=\"references\"> \n      <div class=\"col\">\n        References \n      </div> \n      <div class=\"col\"> \n       <ol> \n        <li class=\"js-ref\" data-id=\"hiller\"> Hiller Jr, L. A., and L. M. Isaacson. \"<a href=\"http://www.aes.org/e-lib/browse.cfm?elib=231\">Musical Composition with a High-Speed Digital Computer</a>.\" Journal of the Audio Engineering Society 6.3 (1958): 154-160. </li> \n        <li class=\"js-ref\" data-id=\"moorer\"> Moorer, James Anderson. \"<a href=\"https://dl.acm.org/doi/10.1145/361254.361265\">Music and computer composition</a>.\" Communications of the ACM 15.2 (1972): 104-113. </li> \n        <li class=\"js-ref\" data-id=\"beyls\"> Beyls, Peter. \"<a href=\"https://quod.lib.umich.edu/i/icmc/bbp2372.1989.009/--musical-universe-of-cellular-automata\">The musical universe of cellular automata</a>.\" Proceedings of international computer music conference. 1989. </li> \n        <li class=\"js-ref\" data-id=\"conklin\"> Conklin, Darrell. \"<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.2086&amp;rep=rep1&amp;type=pdf\">Music generation from statistical models</a>.\" Proceedings of the AISB 2003 Symposium on Artificial Intelligence and Creativity in the Arts and Sciences. 2003. </li> \n        <li class=\"js-ref\" data-id=\"hadjeres\"> Hadjeres, Gaëtan, François Pachet, and Frank Nielsen. \"<a href=\"https://dl.acm.org/doi/10.5555/3305381.3305522\">Deepbach: a steerable model for bach chorales generation</a>.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. </li> \n        <li class=\"js-ref\" data-id=\"huang-2019\"> Huang, Cheng-Zhi Anna, et al. \"<a href=\"https://arxiv.org/abs/1903.07227\">Counterpoint by convolution</a>.\" arXiv preprint arXiv:1903.07227 (2019). </li> \n        <li class=\"js-ref\" data-id=\"dong\"> Dong, Hao-Wen, et al. \"<a href=\"https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17286\">Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment</a>.\" Thirty-Second AAAI Conference on Artificial Intelligence. 2018. </li> \n        <li class=\"js-ref\" data-id=\"yang\"> Yang, Li-Chia, Szu-Yu Chou, and Yi-Hsuan Yang. \"<a href=\"https://arxiv.org/abs/1703.10847\">MidiNet: A convolutional generative adversarial network for symbolic-domain music generation</a>.\" arXiv preprint arXiv:1703.10847 (2017). </li> \n        <li class=\"js-ref\" data-id=\"roberts\"> Roberts, Adam, et al. \"<a href=\"https://arxiv.org/abs/1803.05428\">A hierarchical latent vector model for learning long-term structure in music</a>.\" arXiv preprint arXiv:1803.05428 (2018). </li> \n        <li class=\"js-ref\" data-id=\"huang-2018\"> Huang, Cheng-Zhi Anna, et al. \"<a href=\"https://arxiv.org/abs/1809.04281\">Music transformer</a>.\" arXiv preprint arXiv:1809.04281 (2018). </li> \n        <li class=\"js-ref\" data-id=\"payne\"> Payne, Christine. \"<a href=\"https://openai.com/blog/musenet\">MuseNet, 2019.</a>\" URL <samp>openai.com/blog/musenet</samp> (2019). </li> \n        <li class=\"js-ref\" data-id=\"wu\"> Wu, Jian, et al. \"<a href=\"https://ieeexplore.ieee.org/abstract/document/8918424\">A hierarchical recurrent neural network for symbolic melody generation</a>.\" IEEE Transactions on Cybernetics (2019). </li> \n        <li class=\"js-ref\" data-id=\"oord-2016\"> Oord, Aaron van den, et al. \"<a href=\"https://arxiv.org/abs/1609.03499\">Wavenet: A generative model for raw audio</a>.\" arXiv preprint arXiv:1609.03499 (2016). </li> \n        <li class=\"js-ref\" data-id=\"mehri\"> Mehri, Soroush, et al. \"<a href=\"https://arxiv.org/abs/1612.07837\">SampleRNN: An unconditional end-to-end neural audio generation model</a>.\" arXiv preprint arXiv:1612.07837 (2016). </li> \n        <li class=\"js-ref\" data-id=\"yamamoto\"> Yamamoto, Ryuichi, Eunwoo Song, and Jae-Min Kim. \"<a href=\"https://ieeexplore.ieee.org/abstract/document/9053795/\">Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</a>.\" ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020. </li> \n        <li class=\"js-ref\" data-id=\"vasquez\"> Vasquez, Sean, and Mike Lewis. \"<a href=\"https://arxiv.org/abs/1906.01083\">Melnet: A generative model for audio in the frequency domain</a>.\" arXiv preprint arXiv:1906.01083 (2019). </li> \n        <li class=\"js-ref\" data-id=\"dieleman\"> Dieleman, Sander, Aaron van den Oord, and Karen Simonyan. \"<a href=\"http://papers.nips.cc/paper/8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale\">The challenge of realistic music generation: modelling raw audio at scale</a>.\" Advances in Neural Information Processing Systems. 2018. </li> \n        <li class=\"js-ref\" data-id=\"kim\"> Kim, Jong Wook, et al. \"<a href=\"https://neural-music-synthesis.github.io/\">Neural music synthesis for flexible timbre control</a>.\" ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019. </li> \n        <li class=\"js-ref\" data-id=\"hawthorne\"> Hawthorne, Curtis, et al. \"<a href=\"https://arxiv.org/abs/1810.12247\">Enabling factorized piano music modeling and generation with the MAESTRO dataset</a>.\" arXiv preprint arXiv:1810.12247 (2018). </li> \n        <li class=\"js-ref\" data-id=\"engel-2017\"> Engel, Jesse, et al. \"<a href=\"https://magenta.tensorflow.org/nsynth\">Neural audio synthesis of musical notes with wavenet autoencoders</a>.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. </li> \n        <li class=\"js-ref\" data-id=\"engel-2019\"> Engel, Jesse, et al. \"<a href=\"https://magenta.tensorflow.org/gansynth\">Gansynth: Adversarial neural audio synthesis</a>.\" arXiv preprint arXiv:1902.08710 (2019). </li> \n        <li class=\"js-ref\" data-id=\"brunner\"> Brunner, Gino, et al. \"<a href=\"https://arxiv.org/abs/1809.07600\">MIDI-VAE: Modeling dynamics and instrumentation of music with applications to style transfer</a>.\" arXiv preprint arXiv:1809.07600 (2018). </li> \n        <li class=\"js-ref\" data-id=\"donahue\"> Donahue, Chris, et al. \"<a href=\"https://arxiv.org/abs/1907.04868\">LakhNES: Improving multi-instrumental music generation with cross-domain pre-training</a>.\" arXiv preprint arXiv:1907.04868 (2019). </li> \n        <li class=\"js-ref\" data-id=\"mor\"> Mor, Noam, et al. \"<a href=\"https://arxiv.org/abs/1805.07848\">A universal music translation network</a>.\" arXiv preprint arXiv:1805.07848 (2018). </li> \n        <li class=\"js-ref\" data-id=\"van-den-oord\"> van den Oord, Aaron, and Oriol Vinyals. \"<a href=\"http://papers.nips.cc/paper/7210-neural-discrete-representation-learning\">Neural discrete representation learning</a>.\" Advances in Neural Information Processing Systems. 2017. </li> \n        <li class=\"js-ref\" data-id=\"razavi\"> Razavi, Ali, Aaron van den Oord, and Oriol Vinyals. \"<a href=\"http://papers.nips.cc/paper/9625-generating-diverse-high-fidelity-images-with-vq-vae-2\">Generating diverse high-fidelity images with VQ-VAE-2</a>.\" Advances in Neural Information Processing Systems. 2019. </li> \n        <li class=\"js-ref\" data-id=\"oord-2017\"> Oord, Aaron van den, et al. \"<a href=\"https://arxiv.org/abs/1711.10433\">Parallel wavenet: Fast high-fidelity speech synthesis</a>.\" arXiv preprint arXiv:1711.10433 (2017). </li> \n        <li class=\"js-ref\" data-id=\"arik\"> Arık, Sercan Ö., Heewoo Jun, and Gregory Diamos. \"<a href=\"https://arxiv.org/abs/1808.06719\">Fast spectrogram inversion using multi-head convolutional neural networks</a>.\" IEEE Signal Processing Letters 26.1 (2018): 94-98. </li> \n        <li class=\"js-ref\" data-id=\"child\"> Child, Rewon, et al. \"<a href=\"https://arxiv.org/abs/1904.10509\">Generating long sequences with sparse transformers</a>.\" arXiv preprint arXiv:1904.10509 (2019). </li> \n        <li class=\"js-ref\" data-id=\"vaswani\"> Vaswani, Ashish, et al. \"<a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>.\" Advances in neural information processing systems. 2017. </li> \n        <li class=\"js-ref\" data-id=\"maaten\"> Maaten, Laurens van der, and Geoffrey Hinton. \"<a href=\"https://lvdmaaten.github.io/tsne/\">Visualizing data using t-SNE</a>.\" Journal of machine learning research 9.Nov (2008): 2579-2605. </li> \n        <li class=\"js-ref\" data-id=\"hennequin\"> Hennequin, Romain, et al. \"<a href=\"https://github.com/deezer/spleeter\">Spleeter: A fast and state-of-the art music source separation tool with pre-trained models</a>.\" Proc. International Society for Music Information Retrieval Conference. 2019. </li> \n        <li class=\"js-ref\" data-id=\"gupta\"> Gupta, Chitralekha, Emre Yılmaz, and Haizhou Li. \"<a href=\"https://autolyrixalign.hltnus.org/\">Lyrics-to-Audio Alignment with Music-aware Acoustic Models</a>.\" </li> \n        <li class=\"js-ref\" data-id=\"kingma\"> Kingma, Durk P., et al. \"<a href=\"http://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow\">Improved variational inference with inverse autoregressive flow</a>.\" Advances in neural information processing systems. 2016. </li> \n       </ol> \n      </div> \n     </div> \n    </hr> \n   </div> \n   <!-- special footer item for footnotes --> \n   <div data-order=\"-1\"> \n    <hr> \n     <div class=\"row\" id=\"footnotes\"> \n      <div class=\"col\">\n        Footnotes \n      </div> \n      <div class=\"col\"> \n       <hr class=\"footnotes-sep\"> \n        <section class=\"footnotes\"> \n         <ol class=\"footnotes-list\"> \n          <li id=\"fn1\" class=\"footnote-item\"><p>One can also use a hybrid approach—first generate the symbolic music, then render it to raw audio using a wavenet conditioned on piano rolls,<span class=\"js-rfref\" data-id=\"kim\"></span><span class=\"js-rfref\" data-id=\"hawthorne\"></span> an autoencoder,<span class=\"js-rfref\" data-id=\"engel-2017\"></span> or a GAN<span class=\"js-rfref\" data-id=\"engel-2019\"></span>—or do music style transfer, to transfer styles between classical and jazz music,<span class=\"js-rfref\" data-id=\"brunner\"></span> generate chiptune music,<span class=\"js-rfref\" data-id=\"donahue\"></span> or disentangle musical style and content.<span class=\"js-rfref\" data-id=\"mor\"></span> For a deeper dive into raw audio modelling, we recommend this excellent <a href=\"https://benanne.github.io/2020/03/24/audio-generation.html\">overview</a>. <a href=\"https://openai.com/blog/jukebox/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n         </ol> \n        </section> \n        <!--kg-card-end: markdown--> \n       </hr> \n      </div> \n     </div> \n    </hr> \n   </div> \n  </footer></img> \n</hr>","descriptionType":"html","publishedDate":"Thu, 30 Apr 2020 15:53:13 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/04/2x-no-mark-1.jpg","linkMd5":"84bcb665b077b620d0a3fe04d556418e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn77@2020_5/2020/08/24/22-09-05-383_9a99f33aae6cc85e.webp","destWidth":1276,"destHeight":1696,"sourceBytes":329167,"destBytes":330270,"author":"OpenAI","articleImgCdnMap":{"https://openai.com/content/images/2020/04/2x-no-mark-1.jpg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn77@2020_5/2020/08/24/22-09-05-383_9a99f33aae6cc85e.webp","https://cdn.openai.com/jukebox/assets/waveforms/1-original.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn14@2020_5/2020/08/24/22-09-07-749_d1269f97b720bfeb.webp","https://cdn.openai.com/jukebox/assets/overview-arrow.svg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn14@2020_5/2020/08/24/22-09-07-586_3ff9695ed46776cd.svg","https://cdn.openai.com/jukebox/assets/overview-2.svg":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn88@2020_5/2020/08/24/22-09-08-532_9db855dcb9b222a7.svg","https://cdn.openai.com/jukebox/assets/overview-3.svg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn29@2020_3/2020/08/24/22-09-07-576_6588c3b9703f23f3.svg","https://cdn.openai.com/jukebox/assets/waveforms/1-novel.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn78@2020_2/2020/08/24/22-09-07-604_be7576aa9ba7268f.webp","https://cdn.openai.com/jukebox/assets/vqvae-1.svg":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn31@2020_1/2020/08/24/22-09-08-566_95c0e1de807a64ff.svg","https://cdn.openai.com/jukebox/assets/vqvae-2.svg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn84@2020_6/2020/08/24/22-09-09-244_e6ad1265d40b9d66.svg","https://cdn.openai.com/jukebox/assets/lyrics-attention.svg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn61@2020_2/2020/08/24/22-09-07-558_a67d194b385529c1.svg"},"publishedOrCreatedDate":1598306916761},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Safety Gym","link":"https://openai.com/blog/safety-gym/","description":"<!--kg-card-begin: markdown--> \n<div class=\"js-excerpt\"> \n <img src=\"https://openai.com/content/images/2019/11/safety-gym-cover.png\" alt=\"Safety Gym\"><p>We're releasing Safety Gym, a suite of environments and tools for measuring progress towards reinforcement learning agents that respect safety constraints while training. We also provide a standardized method of comparing algorithms and how well they avoid costly mistakes while learning. If deep reinforcement learning is applied to the real world, whether in robotics or internet-based tasks, it will be important to have algorithms that are safe even while learning—like a self-driving car that can learn to avoid accidents without actually having to experience them.</p> </img> \n</div> \n<section class=\"btns\"> \n <a href=\"https://cdn.openai.com/safexp-short.pdf\" class=\"btn btn-padded icon-paper\">Paper</a> \n <a href=\"https://github.com/openai/safety-gym\" class=\"btn btn-padded icon-code\">Safety Gym</a> \n <a href=\"https://github.com/openai/safety-starter-agents\" class=\"btn btn-padded icon-code\">Safety Starter Agents</a> \n</section> \n<h3 id=\"explorationisrisky\">Exploration is risky</h3> \n<p>Reinforcement learning agents need to explore their environments in order to learn optimal behaviors. Essentially, they operate on the principle of trial and error: they try things out, see what works or doesn’t work, and then increase the likelihood of good behaviors and decrease the likelihood of bad behaviors. However, <a href=\"https://bair.berkeley.edu/blog/2017/07/06/cpo/\">exploration</a> is <a href=\"https://ai.facebook.com/blog/lyapunov-based-safe-reinforcement-learning/\">fundamentally</a> <a href=\"http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf\">risky</a>: agents might try dangerous behaviors that lead to unacceptable errors. This is the <a href=\"https://deepmind.com/research/publications/safe-exploration-continuous-action-spaces\">“safe exploration” problem</a> in a nutshell.</p> \n<p>Consider an example of an autonomous robot arm in a factory using reinforcement learning (RL) to learn how to assemble widgets. At the start of RL training, the robot might try flailing randomly, since it doesn’t know what to do yet. This poses a safety risk to humans who might be working nearby, since they could get hit.</p> \n<p>For restricted examples like the robot arm, we can imagine simple ways to ensure that humans aren’t harmed by just keeping them out of harm’s way: shutting down the robot whenever a human gets too close, or putting a barrier around the robot. But for general RL systems that operate under a wider range of conditions, simple physical interventions won’t always be possible, and we will need to consider other approaches to safe exploration.</p> \n<h3 id=\"constrainedreinforcementlearning\">Constrained reinforcement learning</h3> \n<p>The first step towards making progress on a problem like safe exploration is to quantify it: figure out what can be measured, and how going up or down on those metrics gets us closer to the desired outcome. Another way to say it is that we need to pick a formalism for the safe exploration problem. A formalism allows us to design algorithms that achieve our goals.</p> \n<p>While there are several options, there is not yet a universal consensus in the field of safe exploration research about the right formalism. We spent some time thinking about it, and the formalism we think makes the most sense to adopt is constrained reinforcement learning.</p> \n<p><a href=\"https://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf\">Constrained RL</a> is like normal RL, but in addition to a reward function that the agent wants to maximize, environments have cost functions that the agent needs to constrain. For example, consider an agent controlling a self-driving car. We would want to reward this agent for getting from point A to point B as fast as possible. But naturally, we would also want to constrain the driving behavior to match traffic safety standards.</p> \n<p>We think constrained RL may turn out to be more useful than normal RL for ensuring that agents satisfy safety requirements. A big problem with normal RL is that everything about the agent’s eventual behavior is described by the reward function, but reward design is fundamentally hard. A key part of the challenge comes from picking trade-offs between competing objectives, such as task performance and satisfying safety requirements. In constrained RL, we don’t have to pick trade-offs—instead, we pick outcomes, and let algorithms figure out the trade-offs that get us the outcomes we want.</p> \n<p>We can use the self-driving car case to sketch what this means in practice. Suppose the car earns some amount of money for every trip it completes, and has to pay a fine for every collision.</p> \n<p>In normal RL, you would pick the collision fine at the beginning of training and keep it fixed forever. The problem here is that if the pay-per-trip is high enough, the agent may not care whether it gets in lots of collisions (as long as it can still complete its trips). In fact, it may even be advantageous to drive recklessly and risk those collisions in order to get the pay. We have seen this before when training <a href=\"https://openai.com/blog/faulty-reward-functions/\">unconstrained RL agents</a>.</p> \n<p>By contrast, in constrained RL you would pick the acceptable collision rate at the beginning of training, and adjust the collision fine until the agent is meeting that requirement. If the car is getting in too many fender-benders, you raise the fine until that behavior is no longer incentivized.</p> \n<h3 id=\"safetygym\">Safety Gym</h3> \n<p>To study constrained RL for safe exploration, we developed a new set of environments and tools called Safety Gym. By comparison to existing environments for constrained RL, Safety Gym environments are richer and feature a wider range of difficulty and complexity.</p> \n<p>In all Safety Gym environments, a robot has to navigate through a cluttered environment to achieve a task. There are three pre-made robots (Point, Car, and Doggo), three main tasks (Goal, Button, and Push), and two levels of difficulty for each task. We give an overview of the robot-task combinations below, but make sure to check out <a href=\"https://cdn.openai.com/safexp-short.pdf\">the paper</a> for details.</p> \n<p>In these videos, we show how an agent without constraints tries to solve these environments. Every time the robot does something unsafe—which here, means running into clutter—a red warning light flashes around the agent, and the agent incurs a cost (separate from the task reward). Because these agents are unconstrained, they often wind up behaving unsafely while trying to maximize reward.</p> \n<div class=\"medium-small-copy mt-1.5 mb-1\"> \n <strong>Point</strong> is a simple robot constrained to the 2D plane, with one actuator for turning and another for moving forward or backward. Point has a front-facing small square which helps with the Push task. \n</div> \n<div class=\"wide my-0\"> \n <div class=\"row\"> \n  <div class=\"col-12 col-xl-10 offset-xl-1\"> \n   <div class=\"row narrow-gutters\"> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"point-goal\" src=\"https://player.vimeo.com/video/374272573?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Goal</strong>: Move to a series of goal positions. \n     </div> \n    </div> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"point-button\" src=\"https://player.vimeo.com/video/374272592?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Button</strong>: Press a series of goal buttons. \n     </div> \n    </div> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"point-push\" src=\"https://player.vimeo.com/video/374272608?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Push</strong>: Move a box to a series of goal positions. \n     </div> \n    </div> \n   </div> \n  </div> \n </div> \n <!-- end .row --> \n</div> \n<!-- end .wide --> \n<div class=\"medium-small-copy mt-1.5 mb-1\"> \n <strong>Car</strong> has two independently-driven parallel wheels and a free-rolling rear wheel. For this robot, turning and moving forward or backward require coordinating both of the actuators. \n</div> \n<div class=\"wide my-0\"> \n <div class=\"row\"> \n  <div class=\"col-12 col-xl-10 offset-xl-1\"> \n   <div class=\"row narrow-gutters\"> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"car-goal\" src=\"https://player.vimeo.com/video/374272630?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Goal</strong>: Move to a series of goal positions. \n     </div> \n    </div> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"car-button\" src=\"https://player.vimeo.com/video/374272643?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Button</strong>: Press a series of goal buttons. \n     </div> \n    </div> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"car-push\" src=\"https://player.vimeo.com/video/374272664?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Push</strong>: Move a box to a series of goal positions. \n     </div> \n    </div> \n   </div> \n  </div> \n </div> \n <!-- end .row --> \n</div> \n<!-- end .wide --> \n<div class=\"medium-small-copy mt-1.5 mb-1\"> \n <strong>Doggo</strong> is a quadruped with bilateral symmetry. Each of its four legs has two controls at the hip, for azimuth and elevation relative to the torso, and one in the knee, controlling angle. A uniform random policy keeps the robot from falling over and generates travel. \n</div> \n<div class=\"wide mt-0 mb-1\"> \n <div class=\"row\"> \n  <div class=\"col-12 col-xl-10 offset-xl-1\"> \n   <div class=\"row narrow-gutters\"> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"doggo-goal\" src=\"https://player.vimeo.com/video/374272674?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Goal</strong>: Move to a series of goal positions. \n     </div> \n    </div> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"doggo-button\" src=\"https://player.vimeo.com/video/374272701?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Button</strong>: Press a series of goal buttons. \n     </div> \n    </div> \n    <div class=\"col-12 col-sm\"> \n     <iframe data-monitor=\"\" data-id=\"doggo-push\" src=\"https://player.vimeo.com/video/374272713?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"640\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n     <div class=\"caption mt-0\"> \n      <strong>Push</strong>: Move a box to a series of goal positions. \n     </div> \n    </div> \n   </div> \n  </div> \n </div> \n <!-- end .row --> \n</div> \n<!-- end .wide --> \n<h3 id=\"benchmark\">Benchmark</h3> \n<p>To help make Safety Gym useful out-of-the-box, we evaluated some standard RL and constrained RL algorithms on the Safety Gym benchmark suite: <a href=\"https://openai.com/blog/openai-baselines-ppo/\">PPO</a>, <a href=\"https://arxiv.org/abs/1502.05477\">TRPO</a>, <a href=\"http://www.mit.edu/~dimitrib/Constrained-Opt.pdf\">Lagrangian penalized versions</a> of PPO and TRPO, and <a href=\"https://arxiv.org/abs/1705.10528\">Constrained Policy Optimization</a> (CPO).</p> \n<p>Our preliminary results demonstrate the wide range of difficulty of Safety Gym environments: the simplest environments are easy to solve and allow fast iteration, while the hardest environments may be too challenging for current techniques. We also found that Lagrangian methods were surprisingly better than CPO, overturning a previous result in the field.</p> \n<p>Below, we show learning curves for average episodic return and average episodic sum of costs. In our <a href=\"https://cdn.openai.com/safexp-short.pdf\">paper</a>, we describe how to use these and a third metric (the average cost over training) to compare algorithms and measure progress.</p> \n<div class=\"mt-1.5 mb-1.5\"> \n <div id=\"chartStyles\"></div> \n <h4 id=\"results\" class=\"font-small\">Return and cost trade off against each other meaningfully</h4> \n <div class=\"d-flex flex-wrap mb-1\"> \n  <div id=\"chartSelectRobot\" class=\"mr-0.5\"></div> \n  <div id=\"chartSelectTask\" class=\"mr-0.5\"></div> \n  <div id=\"chartSelectLevel\"></div> \n </div> \n <div id=\"chartLegend\" class=\"mb-1\"></div> \n <div class=\"wide my-0\"> \n  <div class=\"row\"> \n   <div class=\"col-12 col-xl-10 offset-xl-1\"> \n    <div class=\"row\"> \n     <div class=\"col-12 col-md\"> \n      <div id=\"chartRet\" class=\"mb-0.5\"></div> \n     </div> \n     <div class=\"col-12 col-md\"> \n      <div id=\"chartCost\" class=\"mb-0.5\"></div> \n     </div> \n    </div> \n   </div> \n  </div> \n </div> \n</div> \n<p>To facilitate reproducibility and future work, we’re also releasing the algorithms code we used to run these experiments as the <a href=\"https://github.com/openai/safety-starter-agents\">Safety Starter Agents repo</a>.</p> \n<h3 id=\"openproblems\">Open problems</h3> \n<p>There is still a lot of work to do on refining algorithms for constrained RL, and combining them with other problem settings and safety techniques. There are three things we are most interested in at the moment:</p> \n<ol> \n <li>Improving performance on the current Safety Gym environments.</li> \n <li>Using Safety Gym tools to investigate safe transfer learning and distributional shift problems.</li> \n <li>Combining constrained RL with implicit specifications (like <a href=\"https://openai.com/blog/fine-tuning-gpt-2/\">human preferences</a>) for rewards and costs.</li> \n</ol> \n<p>Our expectation is that, in the same way we today measure the accuracy or performance of systems at a given task, we’ll eventually measure the “safety” of systems as well. Such measures could feasibly be integrated into assessment schemes that developers use to test their systems, and could potentially be used by the government to <a href=\"https://www.nist.gov/system/files/documents/2019/06/10/nist-ai-rfi-openai-001.pdf\">create standards for safety</a>.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/safety-gym/#fn1\" id=\"fnref1\">[1]</a></sup> We also hope that systems like Safety Gym can make it easier for AI developers to collaborate on safety across the AI sector via work on open, shared systems.</p> \n<p><em>If you’re excited to work on safe exploration problems with us, <a href=\"https://openai.com/jobs/\">we’re hiring</a>!</em></p> \n<footer class=\"post-footer js-post-footer\"> \n <!-- footer item --> \n <div> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Acknowledgments \n    </div> \n    <div class=\"col\"> \n     <p>We gratefully acknowledge the many people who contributed to this release. Thanks Christy Dennison, Ethan Knight, and Adam Stooke for discussions, research, and testing of Safety Gym along the way, and to Mira Murati for supporting the project team. Thanks Karl Cobbe, Matthias Plappert, and Jacob Hilton for feedback on the paper. Thanks Ashley Pilipiszyn, Ben Barry, Justin Jay Wang, Richard Perez, Jen Derosa, and Greg Brockman for work on editing, designing, illustrating, and shipping this post. Thanks Amanda Askell, Jack Clark, and Miles Brundage for discussions and blog post contributions on measurements for AI safety and policy implications. Thanks Chris Hesse for liaising on open source release guidelines.</p> \n    </div> \n   </div> \n  </hr> \n </div> \n <!-- special footer item for footnotes --> \n <div data-order=\"-1\"> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Footnotes \n    </div> \n    <div class=\"col\"> \n     <hr class=\"footnotes-sep\"> \n      <section class=\"footnotes\"> \n       <ol class=\"footnotes-list\"> \n        <li id=\"fn1\" class=\"footnote-item\"><p>OpenAI’s comments in response to a request for information from the US agency NIST regarding Artificial Intelligence Standards. <a href=\"https://openai.com/blog/safety-gym/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n       </ol> \n      </section> \n      <!--kg-card-end: markdown--> \n     </hr> \n    </div> \n   </div> \n  </hr> \n </div> \n</footer>","descriptionType":"html","publishedDate":"Thu, 21 Nov 2019 16:55:00 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2019/11/safety-gym-cover.png","linkMd5":"9702d253f96205db69ffac519550137e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn69@2020_2/2020/08/24/22-09-05-664_3e9e50d02b910f8e.webp","destWidth":1920,"destHeight":1120,"sourceBytes":184826,"destBytes":66860,"author":"Joshua Achiam","articleImgCdnMap":{"https://openai.com/content/images/2019/11/safety-gym-cover.png":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn69@2020_2/2020/08/24/22-09-05-664_3e9e50d02b910f8e.webp"},"publishedOrCreatedDate":1598306916765},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"OpenAI Scholars Spring 2020: Final Projects","link":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/","description":"<!--kg-card-begin: markdown--> \n<div class=\"js-excerpt d-none\"> \n <img src=\"https://openai.com/content/images/2020/07/openai-scholars-gradient-horizontal-stacked.png\" alt=\"OpenAI Scholars Spring 2020: Final Projects\"><p>Our third class of <a href=\"https://openai.com/blog/openai-scholars-spring-2020/\">OpenAI Scholars</a> presented their final projects at virtual Demo Day, showcasing their research results from over the past five months. These projects investigated problems such as analyzing how GPT-2 represents grammar, measuring the interpretability of models trained on Coinrun, and predicting epileptic seizures using brain recordings. More information about the next class of Scholars and how to apply will be announced this fall.</p> </img> \n</div> \n<div class=\"js-custom-media d-none\"> \n <div class=\"scholars-header-image\"></div> \n</div> \n<style> </style> \n<p>The <a href=\"https://openai.com/blog/openai-scholars/\">OpenAI Scholars program</a> provides stipends and mentorship to individuals from underrepresented groups to study deep learning and open-source a project.</p> \n<p>Our Scholars have demonstrated core technical skills across various expert domains and self-motivation—critical competencies for a self-directed program like this one. They each entered the field of machine learning as relative newcomers, and we hope their progress shows how accessible machine learning is.</p> \n<p><em>Learn more about our <a href=\"https://openai.com/blog/openai-scholars-spring-2020/\">Scholars program</a>.</em></p> \n<figure class=\"my-1.5\"> \n <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JZOHW-eYBtQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n <div class=\"caption mt-0\">\n   Demo Day introductions by Sam Altman and Greg Brockman \n </div> \n</figure> \n<div class=\"full py-2.5 my-2.5 bg-fg-2\"> \n <div class=\"container p-relative\"> \n  <div class=\"row\"> \n   <div class=\"d-none d-xl-block col-3\"> \n    <nav class=\"sticky js-sticky pt-1 mt-n1 pb-1\" style=\"left: 0;\"> \n     <ul class=\"list-unstyled mb-0\"> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#alethea\">Alethea Power</a></li> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#andre\">Andre Carerra</a></li> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#cathy\">Cathy Yeh</a></li> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#jorge\">Jorge Orbay</a></li> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#kamal\">Kamal Ndousse</a></li> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#kata\">Kata Slama</a></li> \n      <li><a class=\"scholars-nav-item js-scrollspy d-block small-copy no-underline\" href=\"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#pamela\">Pamela Mishkin</a></li> \n     </ul> \n    </nav> \n   </div> \n   <div class=\"content\" style=\"margin-left: 0\"> \n    <div id=\"alethea\"> \n     <figure> \n      <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6gilgehNTNw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n     </figure> \n     <h3 class=\"my-0\">Alethea Power</h3> \n     <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n       Looking for Grammar in All The Right Places \n     </div> \n     <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n       Mentor: Christine Payne \n      <br> Previous Roles: B.S. in Applied Mathematics, MSc in Philosophy of Mind from Ediburgh, Software and Site Reliability Engineer at Facebook </br> \n     </aside> \n     <p>I’m fascinated by neural network interpretability. Understanding how networks of various architectures represent information can help us build simpler and more efficient networks, as well as predict how the networks we’ve built will behave, and perhaps even give us some insight into how human beings think. Along these lines, I analyzed how GPT-2 represents English grammar, and found smaller sub-networks that seem to correspond to various grammatical structures. I will present my methodology and results.</p> \n     <blockquote> \n      <p>Next, I want to work on understanding how neural networks represent information, and use that understanding to better predict how deep learning systems behave. I believe this work will make such systems safer and more beneficial to humanity, as well as making them simpler, faster, and more computationally efficient.</p> \n     </blockquote> \n     <section class=\"btns\"> \n      <a href=\"https://aletheap.github.io/\" class=\"btn btn-padded icon-external right\">Blog</a> \n      <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n     </section> \n    </div> \n    <hr> \n     <div id=\"andre\"> \n      <figure> \n       <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/liMJS5DrnlQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n      </figure> \n      <h3 class=\"my-0\">Andre Carerra</h3> \n      <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n        Semantic Parsing English to GraphQL \n      </div> \n      <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n        Mentor: Melanie Subbiah \n       <br> Previous Roles: CTO at Droplii, Founder at Lambdo </br> \n      </aside> \n      <p>My scholars program project is semantic parsing English-to-GraphQL. Given an English prompt such as “How many employees do we have?”, find a corresponding GraphQL query to return the information. The project involved creating a dataset, training models, and creating an interaction tool to see results.</p> \n      <blockquote> \n       <p>I wanted to have a say in how AI is shaped—the Scholars program has been a great opportunity to learn and participate.</p> \n      </blockquote> \n      <section class=\"btns\"> \n       <a href=\"https://blog.lambdo.com/\" class=\"btn btn-padded icon-external right\">Blog</a> \n       <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n      </section> \n     </div> \n     <hr> \n      <div id=\"cathy\"> \n       <figure> \n        <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jjmTmYMsET0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n       </figure> \n       <h3 class=\"my-0\">Cathy Yeh</h3> \n       <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n         Long Term Credit Assignment with Temporal Reward Transport \n       </div> \n       <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n         Mentor: Jerry Tworek \n        <br> Previous Roles: Data Scientist at Square and Driver </br> \n       </aside> \n       <p>Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we use “temporal reward transport” (TRT) to augment the immediate rewards of significant state-action pairs with rewards from the distant future, using an attention mechanism to identify candidates for TRT. A series of gridworld experiments show clear improvements in learning when TRT is used in conjunction with a standard advantage actor critic algorithm.</p> \n       <blockquote> \n        <p>I appreciate that this program gave me the freedom to learn deeply and flex my creativity.</p> \n       </blockquote> \n       <section class=\"btns\"> \n        <a href=\"https://www.efavdb.com/\" class=\"btn btn-padded icon-external right\">Blog</a> \n        <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n       </section> \n      </div> \n      <hr> \n       <div id=\"jorge\"> \n        <figure> \n         <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/aEe_dTUfK4c\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n        </figure> \n        <h3 class=\"my-0\">Jorge Orbay</h3> \n        <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n          Quantifying Interpretability of Models Trained on Coinrun \n        </div> \n        <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n          Mentor: Karl Cobbe \n         <br> Previous Roles: CS Engineering at Columbia, Research at the Creative Machines Lab, Software Engineer at Autonomic </br> \n        </aside> \n        <p>This project’s purpose is to create a scalar that measures the interpretability of an A2C model trained on Procgen’s Coinrun. The scalar is generated using a combination of attribution on the model and masks of Coinrun’s assets. The scalar is used to test the validity of the diversity hypothesis.</p> \n        <blockquote> \n         <p>This program, and specifically my mentor, has fostered a self-confidence in me to dive into a field I don’t understand and breakdown problems until I can solve them. I’m hoping to take the self-confidence I’ve learned from this program to continue breaking-down problems in and with AI.</p> \n        </blockquote> \n        <section class=\"btns\"> \n         <a href=\"https://jorbay.github.io/\" class=\"btn btn-padded icon-external right\">Blog</a> \n         <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n        </section> \n       </div> \n       <hr> \n        <div id=\"kamal\"> \n         <figure> \n          <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Qy9J5519s68\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n         </figure> \n         <h3 class=\"my-0\">Kamal Ndousse</h3> \n         <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n           Social Learning in Independent Multi-Agent Reinforcement Learning \n         </div> \n         <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n           Mentor: Natasha Jaques \n          <br> Previous Roles: Math and Physics at MIT, Algorithms Research Scientist at Fitbit, Independent Algorithms/ML consultant, ML Engineer at Coinbase </br> \n         </aside> \n         <p>My project has explored the social transfer of expertise among completely independent RL agents trained in shared environments. The motivating question is whether novice agents can learn to mimic expert behavior to solve hard-exploration tasks that they couldn't master in isolation. I’ll discuss my observations as well as the environments I developed to experiment with social skill transfer.</p> \n         <blockquote> \n          <p>I joined the Scholars program in order to learn from the brilliant folks at OpenAI and to immerse myself in AI research. I’m grateful to have had the opportunity to explore state of the art research with the support of such talented researchers (special thanks to my mentor Natasha Jaques!)</p> \n         </blockquote> \n         <section class=\"btns\"> \n          <a href=\"https://kam.al/blog/marl1/\" class=\"btn btn-padded icon-external right\">Blog</a> \n          <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n         </section> \n        </div> \n        <hr> \n         <div id=\"kata\"> \n          <figure> \n           <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AT2XkqJAZns\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n          </figure> \n          <figure> \n           <!-- <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jjmTmYMsET0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> --> \n          </figure> \n          <h3 class=\"my-0\">Kata Slama</h3> \n          <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n            Towards Epileptic Seizure Prediction with Deep Network \n          </div> \n          <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n            Mentor: Johannes Otterbach \n           <br> Previous Roles: PhD in Neuroscience at UC Berkeley, Behavioral Research at Harvard and Brown </br> \n          </aside> \n          <p>I have been working on a project to predict epileptic seizures using brain recordings. I framed it as an image classification problem based on the spectrogram representation of the brain data. My most successful model so far has been a ResNet18. In my post-Scholars life, I plan to continue working on this project, and make my way to interpretability of spectrogram classification networks.</p> \n          <blockquote> \n           <p>I wanted to learn how to apply deep learning for solving scientific and real-world problems. The OpenAI Scholars program was this magical opportunity to get started by learning from the very best minds in the field.</p> \n          </blockquote> \n          <section class=\"btns\"> \n           <a href=\"https://katarinaslama.github.io/\" class=\"btn btn-padded icon-external right\">Blog</a> \n           <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n          </section> \n         </div> \n         <hr> \n          <div id=\"pamela\"> \n           <figure> \n            <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7wqmXo0Jqa4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe> \n           </figure> \n           <figure> \n            <!-- <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jjmTmYMsET0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> --> \n           </figure> \n           <h3 class=\"my-0\">Pamela Mishkin</h3> \n           <div class=\"large-copy color-fg-80 mb-0.75 balance-text\">\n             Universal Adversarial Perturbations and Language Models \n           </div> \n           <aside class=\"aside-right small-copy color-fg-50 mb-1\">\n             Mentor: Alec Radford \n            <br> Previous Roles: Math and CS at Williams College, Research Analyst at the Federal Reserve Bank of NY, Herchel Smith Scholar at Cambridge, Product Manager at The Whistle, Researcher at Lumi Labs </br> \n           </aside> \n           <p>Adversarial perturbations are well-understood for images but less so for language. My presentation will review the literature on how universal adversarial examples can inform understanding of generative models, replicating results generating universal adversarial triggers for GPT-2 and for attacking NLI models.</p> \n           <blockquote> \n            <p>This program strengthened my technical basis in machine learning and helped me understand how AI researchers understand policy implications of their work.</p> \n           </blockquote> \n           <section class=\"btns mb-0\"> \n            <a href=\"https://manlikemishap.github.io/year-archive/\" class=\"btn btn-padded icon-external right\">Blog</a> \n            <!-- <a href=\"\" class=\"btn btn-padded icon-code right\">GitHub Repo</a> --> \n           </section> \n          </div> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </div> \n   <!-- end div.content --> \n  </div> \n  <!-- end div.row --> \n </div> \n <!-- end div.container --> \n</div> \n<!-- end div.full --> \n<p>Diversity is core to AI having a positive effect on the world—it’s necessary to ensure the advanced AI systems in the future are built to <a href=\"https://openai.com/charter\">benefit everyone</a>.</p> \n<p>If you’re excited to begin your own journey into ML, check out some of our <a href=\"https://openai.com/resources/\">educational materials</a>. More information about the next class of scholars and how to apply will be announced this fall. Stay tuned!</p> \n<p><em>Huge thanks to Microsoft for providing Azure compute credits to scholars, to our mentors for their time and commitment, and to all the supporters that made this program possible.</em></p> \n<!--kg-card-end: markdown-->","descriptionType":"html","publishedDate":"Thu, 09 Jul 2020 14:56:33 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/07/openai-scholars-gradient-horizontal-stacked.png","linkMd5":"b72be7dda75994ea5d073ce47d8f5294","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn53@2020_4/2020/08/24/22-09-05-274_d9cd4cb80ae851ff.webp","destWidth":1600,"destHeight":900,"sourceBytes":171312,"destBytes":38552,"author":"OpenAI","articleImgCdnMap":{"https://openai.com/content/images/2020/07/openai-scholars-gradient-horizontal-stacked.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn53@2020_4/2020/08/24/22-09-05-274_d9cd4cb80ae851ff.webp"},"publishedOrCreatedDate":1598306916755},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"OpenAI API","link":"https://openai.com/blog/openai-api/","description":"<!--kg-card-begin: markdown-->\n<img src=\"https://openai.com/content/images/2020/08/api-poster.jpg\" alt=\"OpenAI API\"><p>We’re releasing an <a href=\"https://beta.openai.com/\">API</a> for accessing new AI models developed by OpenAI. Unlike most AI systems which are designed for one use-case, the API today provides a general-purpose “text in, text out” interface, allowing users to try it on virtually any English language task. You can now <a href=\"https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u\">request access</a> in order to integrate the API into your product, develop an entirely new application, or help us explore the strengths and limits of this technology.</p> \n <div class=\"my-1.5\"> \n  <video autoplay=\"\" muted=\"\" loop=\"\" width=\"100%\" controls=\"\"> \n   <source src=\"https://cdn.openai.com/API/curlFINAL.mp4\" type=\"video/mp4\"> Your browser does not support video </source>\n  </video> \n </div> \n <section class=\"btns\">\n  <a href=\"https://beta.openai.com/\" class=\"btn btn-padded icon-external right\">See how our users are applying the API</a>\n  <a href=\"https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u\" class=\"btn btn-padded icon-papers right\">Join waitlist</a>\n </section> <p>Given any text prompt, the API will return a text completion, attempting to match the pattern you gave it. You can \"program\" it by showing it just a few examples of what you'd like it to do; its success generally varies depending on how complex the task is. The API also allows you to hone performance on specific tasks by training on a dataset (small or large) of examples you provide, or by learning from human feedback provided by users or labelers.</p> <p>We've designed the API to be both simple for anyone to use but also flexible enough to make machine learning teams more productive. In fact, many of our teams are now using the API so that they can focus on machine learning research rather than distributed systems problems. Today the API runs models with weights from the <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3</a> family with many speed and throughput improvements. Machine learning is moving very fast, and we're constantly upgrading our technology so that our users stay up to date.</p> \n <div class=\"wide\"> \n  <div class=\"row mb-n1\"> \n   <div class=\"col-12 col-md-6\"> \n    <div class=\"inset-border\"> \n     <video autoplay=\"\" muted=\"\" loop=\"\" width=\"100%\" controls=\"\"> \n      <source src=\"https://cdn.openai.com/API/excel_tabulate_v3_biz.mp4\" type=\"video/mp4\"> Your browser does not support video </source>\n     </video> \n    </div> \n   </div> \n   <div class=\"col-12 col-md-6\"> \n    <div class=\"inset-border\"> \n     <video autoplay=\"\" muted=\"\" loop=\"\" width=\"100%\" controls=\"\"> \n      <source src=\"https://cdn.openai.com/API/FINAL_bread.mp4\" type=\"video/mp4\"> Your browser does not support video </source>\n     </video> \n    </div> \n   </div> \n  </div> \n </div> <p>The field's pace of progress means that there are frequently surprising new applications of AI, both positive and negative. We will terminate API access for obviously harmful use-cases, such as harassment, spam, radicalization, or astroturfing. But we also know we can't anticipate all of the possible consequences of this technology, so we are launching today in a private beta rather than general availability, building tools to help users better control the content our API returns, and researching safety-relevant aspects of language technology (such as analyzing, mitigating, and intervening on harmful bias). We'll share what we learn so that our users and the broader community can build more human-positive AI systems.</p> <p>In addition to being a revenue source to help us <a href=\"https://openai.com/blog/openai-lp/\">cover costs</a> in pursuit of <a href=\"https://openai.com/charter/\">our mission</a>, the API has pushed us to sharpen our focus on general-purpose AI technology — advancing the technology, making it usable, and considering its impacts in the real world. We hope that the API will greatly lower the <a href=\"https://medium.com/@aidungeon/how-we-scaled-ai-dungeon-2-to-support-over-1-000-000-users-d207d5623de9\">barrier</a> to producing beneficial AI-powered products, resulting in tools and services that are hard to imagine today.</p> <p>Interested in exploring the API? Join companies like <a href=\"https://www.algolia.com/\">Algolia</a>, <a href=\"https://quizlet.com/\">Quizlet</a>, and <a href=\"https://www.reddit.com/\">Reddit</a>, and researchers at institutions like the <a href=\"https://www.middlebury.edu/institute/academics/centers-initiatives/ctec\">Middlebury Institute</a> in our <a href=\"https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8s0v5E5gdyQhOuZCXNuMR8i1UQjFWVTVUVEpGNkg3U1FNRDVVRFg3U0w4Vi4u\">private beta</a>.</p> \n <div class=\"my-1.5\"> \n  <div class=\"inset-border\"> \n   <video autoplay=\"\" muted=\"\" loop=\"\" width=\"100%\" controls=\"\"> \n    <source src=\"https://cdn.openai.com/API/English_Bash_Python.mp4\" type=\"video/mp4\"> Your browser does not support video </source>\n   </video> \n  </div> \n </div> \n <section class=\"btns\">\n  <a href=\"https://openai.com/jobs/#applied-ai\" class=\"btn btn-padded icon-next right\">Join our Applied AI team</a>\n </section> \n <hr> \n  <h2 id=\"faq\">FAQ</h2> \n  <h4 id=\"whydidopenaidecidetoreleaseacommercialproduct\">Why did OpenAI decide to release a commercial product?</h4> \n  <p>Ultimately, what we <a href=\"https://openai.com/charter/\">care about most</a> is ensuring artificial general intelligence benefits everyone. We see developing commercial products as one of the ways to make sure we have enough funding to succeed.</p> \n  <p>We also believe that safely deploying powerful AI systems in the world will be hard to get right. In releasing the API, we are working closely with our partners to see what challenges arise when AI systems are used in the real world. This will help guide our efforts to understand how deploying future AI systems will go, and what we need to do to make sure they are safe and beneficial for everyone.</p> \n  <h4 id=\"whydidopenaichoosetoreleaseanapiinsteadofopensourcingthemodels\">Why did OpenAI choose to release an API instead of open-sourcing the models?</h4> \n  <p>There are three main reasons we did this. First, commercializing the technology helps us pay for our ongoing AI research, safety, and policy efforts.</p> \n  <p>Second, many of the models underlying the API are very large, taking a lot of expertise to develop and deploy and making them very expensive to run. This makes it hard for anyone except larger companies to benefit from the underlying technology. We’re hopeful that the API will make powerful AI systems more accessible to smaller businesses and organizations.</p> \n  <p>Third, the API model allows us to more easily respond to misuse of the technology. Since it is hard to predict the downstream use cases of our models, it feels inherently safer to release them via an API and broaden access over time, rather than release an open source model where access cannot be adjusted if it turns out to have harmful applications.</p> \n  <h4 id=\"whatspecificallywillopenaidoaboutmisuseoftheapigivenwhatyouvepreviouslysaidaboutgpt2\">What specifically will OpenAI do about misuse of the API, given what you’ve previously said about GPT-2?</h4> \n  <p>We will terminate API access for use-cases that cause physical or mental harm to people, including but not limited to harassment, intentional deception, radicalization, astroturfing, or spam; as we gain more experience operating the API in practice we expect to expand and refine these categories.</p> \n  <p>We are also conducting research into the potential misuses of models served by the API, including with third-party researchers via our <a href=\"https://forms.office.com/Pages/ResponsePage.aspx?id=VsqMpNrmTkioFJyEllK8sx3ELsv0PEhHphhNz30FttVUNkYwTlNPMVI1V0lXNjExMlExUlc4SE5YSS4u\">academic access</a> program.</p> \n  <p>So far, we have been conducting the private beta with users who we’ve individually vetted, which further reduces the odds of misuse while we get a better understanding of the implications and limitations of the API.</p> \n  <h4 id=\"howwillopenaimitigateharmfulbiasandothernegativeeffectsofmodelsservedbytheapi\">How will OpenAI mitigate harmful bias and other negative effects of models served by the API?</h4> \n  <p>Mitigating negative effects such as harmful bias is a hard, industry-wide issue that is extremely important. Ultimately, our API models do exhibit biases (as shown in the <a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 paper</a>) that will appear on occasion in generated text. Our API models could also cause harm in ways that we haven’t thought of yet. Here are the steps we’re taking to address these issues:</p> \n  <ul> \n   <li>We're developing usage guidelines with users to help them learn from each other and mitigate these problems in practice.</li> \n   <li>We are working closely with users to deeply understand their use cases and develop tools to label and intervene on manifestations of harmful bias.</li> \n   <li>We’re conducting our own research into harmful bias and broader issues in fairness and representation, which will help inform our work with our users.</li> \n  </ul> \n  <p>Our goal over time is to develop a thorough understanding of the API’s potential harms, and continually improve our tools and processes to help minimize them.</p> \n  <p>We'll seek to share what we learn, so that others can benefit. We also hope to see other AI developers do the same, as bias is a <a href=\"https://openai.com/blog/cooperation-on-safety/\">collective action problem</a> in AI development.</p> \n  <!--kg-card-end: markdown-->\n </hr></img>","descriptionType":"html","publishedDate":"Thu, 11 Jun 2020 15:00:00 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/08/api-poster.jpg","linkMd5":"7cbfa891ea212d1291308cddf7c70c55","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn57@2020_2/2020/08/24/22-09-05-270_8fe7c868834add25.webp","destWidth":1800,"destHeight":1200,"sourceBytes":165873,"destBytes":142018,"author":"Greg Brockman","articleImgCdnMap":{"https://openai.com/content/images/2020/08/api-poster.jpg":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn57@2020_2/2020/08/24/22-09-05-270_8fe7c868834add25.webp"},"publishedOrCreatedDate":1598306916755},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"OpenAI → PyTorch","link":"https://openai.com/blog/openai-pytorch/","description":"<!--kg-card-begin: markdown-->\n<img src=\"https://openai.com/content/images/2020/02/openai-pytorch-vertical.png\" alt=\"OpenAI → PyTorch\"><p>We are standardizing OpenAI’s deep learning framework on <a href=\"https://pytorch.org/\">PyTorch</a>. In the past, we implemented projects in many frameworks depending on their relative strengths. We’ve now chosen to standardize to make it easier for our team to create and share optimized implementations of our models.</p> <p><img src=\"https://openai.com/content/images/2020/02/openai-pytorch.png\" alt=\"OpenAI → PyTorch\" /></p> <p>As part of this move, we’ve just released a <a href=\"https://github.com/openai/spinningup\">PyTorch-enabled version</a> of <a href=\"https://openai.com/blog/spinning-up-in-deep-rl/\">Spinning Up in Deep RL</a>, an open-source educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning. We are also in the process of writing PyTorch bindings for our highly-optimized <a href=\"https://openai.com/blog/block-sparse-gpu-kernels/\">blocksparse kernels</a>, and will open-source those bindings in upcoming months.</p> <p>The main reason we've chosen PyTorch is to increase our research productivity at scale on GPUs. It is very easy to try and execute new research ideas in PyTorch; for example, switching to PyTorch decreased our iteration time on research ideas in generative modeling from weeks to days. We’re also excited to be joining a rapidly-growing developer community, including organizations like Facebook and Microsoft, in pushing scale and performance on GPUs.</p> <p>Going forward we'll primarily use PyTorch as our deep learning framework but sometimes use other ones when there's a specific technical reason to do so. Many of our teams have already made the switch, and we look forward to contributing to the PyTorch community in upcoming months.</p> \n <!--kg-card-end: markdown--></img>","descriptionType":"html","publishedDate":"Thu, 30 Jan 2020 16:57:01 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/02/openai-pytorch-vertical.png","linkMd5":"1260981f8f5edfc1c6516e2901c9d5ae","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn74@2020_1/2020/08/24/22-09-05-311_dc251f2eafc26d10.webp","destWidth":1280,"destHeight":720,"sourceBytes":36479,"destBytes":20938,"author":"OpenAI","articleImgCdnMap":{"https://openai.com/content/images/2020/02/openai-pytorch-vertical.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn74@2020_1/2020/08/24/22-09-05-311_dc251f2eafc26d10.webp","https://openai.com/content/images/2020/02/openai-pytorch.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn44@2020_3/2020/08/24/22-09-08-668_bee0d358bbc22930.webp"},"publishedOrCreatedDate":1598306916761},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"AI and Efficiency","link":"https://openai.com/blog/ai-and-efficiency/","description":"<!--kg-card-begin: markdown--> \n<div class=\"medium-copy color-fg-80 mt-n0.5\"> \n <img src=\"https://openai.com/content/images/2020/05/ai-and-efficiency-social.png\" alt=\"AI and Efficiency\"><p>We’re releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet<span class=\"js-rfref\" data-id=\"imagenet\"></span> classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a neural network to the level of AlexNet<span class=\"js-rfref\" data-id=\"alexnet\"></span> (by contrast, Moore’s Law<span class=\"js-rfref\" data-id=\"moore\"></span> would yield an 11x cost improvement over this period). Our results suggest that for AI tasks with high levels of recent investment, algorithmic progress has yielded more gains than classical hardware efficiency.</p> </img> \n</div> \n<section class=\"btns\"> \n <a href=\"https://arxiv.org/abs/2005.04305\" class=\"btn btn-padded icon-paper\">Read Paper</a> \n <!--<a href=\"#sotas\" class=\"btn btn-padded icon-down right\">Algorithmic Efficiency SOTAs</a>--> \n</section> \n<p>Algorithmic improvement is a key factor driving the advance of AI. It’s important to search for measures that shed light on overall algorithmic progress, even though it’s harder than measuring such trends in compute.<span class=\"js-rfref\" data-id=\"aiandcompute\"></span></p> \n<h5 class=\"pt-1 mt-n0.5\">44x less compute required to get to AlexNet performance 7 years later</h5> \n<div id=\"chart-css\"></div> \n<div id=\"chart-switch\" class=\"my-0.5\"></div> \n<div class=\"wide my-0\"> \n <div id=\"chart\" class=\"mx-xl-auto mb-1\" style=\"max-width:960px\"></div> \n</div> \n<!-- end .wide --> \n<div class=\"caption mt-0\">\n  Total amount of compute in teraflops/s-days used to train to AlexNet level performance. Lowest compute points at any given time shown in blue, all points measured shown in gray. \n <span class=\"js-rfref\" data-id=\"alexnet\"></span> \n <span class=\"js-rfref\" data-id=\"googlenet\"></span> \n <span class=\"js-rfref\" data-id=\"vgg\"></span> \n <span class=\"js-rfref\" data-id=\"resnet\"></span> \n <span class=\"js-rfref\" data-id=\"squeezenet\"></span> \n <span class=\"js-rfref\" data-id=\"wideresnet\"></span> \n <span class=\"js-rfref\" data-id=\"resnext\"></span> \n <span class=\"js-rfref\" data-id=\"densenet\"></span> \n <span class=\"js-rfref\" data-id=\"mobilenet\"></span> \n <span class=\"js-rfref\" data-id=\"shufflenet\"></span> \n <span class=\"js-rfref\" data-id=\"mobilenet_v2\"></span> \n <span class=\"js-rfref\" data-id=\"shufflenet_v2\"></span> \n <span class=\"js-rfref\" data-id=\"efficientnet\"></span> \n</div> \n<p><a class=\"btn btn-padded icon-download\" href=\"https://cdn.openai.com/ai-and-efficiency/charts.zip\">Download charts</a></p> \n<h2 id=\"measuringefficiency\">Measuring efficiency</h2> \n<p>Algorithmic efficiency can be defined as reducing the compute needed to train a specific capability. Efﬁciency is the primary way we measure algorithmic progress on classic computer science problems like sorting. Efficiency gains on traditional problems like sorting are more straightforward to measure than in ML because they have a clearer measure of task difficulty.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn1\" id=\"fnref1\">[1]</a></sup> However, we can apply the efficiency lens to machine learning by holding performance constant. Efficiency trends can be compared across domains like DNA sequencing<span class=\"js-rfref\" data-id=\"dna\"></span> (10-month doubling), solar energy<span class=\"js-rfref\" data-id=\"solar\"></span> (6-year doubling), and transistor density<span class=\"js-rfref\" data-id=\"moore\"></span> (2-year doubling).</p> \n<p>For our analysis, we primarily leveraged open-source re-implementations<span class=\"js-rfref\" data-id=\"pytorch\"></span><span class=\"js-rfref\" data-id=\"shufflenet_github\"></span><span class=\"js-rfref\" data-id=\"mobilenet_github\"></span> to measure progress on AlexNet level performance over a long horizon. We saw a similar rate of training efficiency improvement for ResNet-50 level performance on ImageNet (17-month doubling time).<span class=\"js-rfref\" data-id=\"resnet\"></span><span class=\"js-rfref\" data-id=\"efficientnet\"></span> We saw faster rates of improvement over shorter timescales in Translation, Go, and Dota 2:</p> \n<ol> \n <li>Within translation, the Transformer<span class=\"js-rfref\" data-id=\"transformer\"></span> surpassed seq2seq<span class=\"js-rfref\" data-id=\"seq2seq\"></span> performance on English to French translation on WMT’14 with 61x less training compute 3 years later.</li> \n <li>We estimate AlphaZero<span class=\"js-rfref\" data-id=\"alphazero\"></span> took 8x less compute to get to AlphaGoZero<span class=\"js-rfref\" data-id=\"alphagozero\"></span> level performance 1 year later.</li> \n <li>OpenAI Five Rerun required 5x less training compute to surpass OpenAI Five<span class=\"js-rfref\" data-id=\"openaifive\"></span> (which beat the world champions, <a href=\"https://liquipedia.net/dota2/OG\">OG</a>) 3 months later.</li> \n</ol> \n<p>It can be helpful to think of compute in 2012 not being equal to compute in 2019 in a similar way that dollars need to be inflation-adjusted over time. A fixed amount of compute could accomplish more in 2019 than in 2012. One way to think about this is that some types of AI research progress in two stages, similar to the “tick tock” model of development seen in semiconductors; new capabilities (the “tick”) typically require a significant amount of compute expenditure to obtain, then refined versions of those capabilities (the “tock”) become much more efficient to deploy due to process improvements.</p> \n<p>Increases in algorithmic efficiency allow researchers to do more experiments of interest in a given amount of time and money. In addition to being a measure of overall progress, algorithmic efficiency gains speed up future AI research in a way that's somewhat analogous to having more compute.</p> \n<h2 id=\"othermeasuresofaiprogress\">Other measures of AI progress</h2> \n<p>In addition to efficiency, many other measures shed light on overall algorithmic progress in AI. Training cost in dollars<span class=\"js-rfref\" data-id=\"dawnbench\"></span> is related, but less narrowly focused on algorithmic progress because it’s also affected by improvement in the underlying hardware, hardware utilization, and cloud infrastructure. Sample efficiency is key when we’re in a low data regime, which is the case for many tasks of interest. The ability to train models faster<span class=\"js-rfref\" data-id=\"aiindex\"></span> also speeds up research and can be thought of as a measure of the parallelizability<span class=\"js-rfref\" data-id=\"scienceofai\"></span> of learning capabilities of interest. We also find increases in inference efficiency in terms of GPU time<span class=\"js-rfref\" data-id=\"wavenet\"></span>, parameters<span class=\"js-rfref\" data-id=\"efficientnet\"></span>, and flops meaningful, but mostly as a result of their economic implications<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn2\" id=\"fnref2\">[2]</a></sup> rather than their effect on future research progress. Shufflenet<span class=\"js-rfref\" data-id=\"shufflenet\"></span> achieved AlexNet-level performance with an 18x inference efficiency increase in 5 years (15-month doubling time), which suggests that training efficiency and inference efficiency might improve at similar rates. The creation of datasets/​environments/​benchmarks is a powerful method of making specific AI capabilities of interest more measurable.</p> \n<h2 id=\"primarylimitations\">Primary limitations</h2> \n<ol> \n <li>We have only a small number of algorithmic efficiency data points on a few tasks. It's unclear the degree to which the efficiency trends we've observed generalize to other AI tasks. Systematic measurement could make it clear whether an algorithmic equivalent to Moore's Law<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn3\" id=\"fnref3\">[3]</a></sup> in the domain of AI exists, and if it exists, clarify its nature. We consider this a highly interesting open question. We suspect we're more likely to observe similar rates of efficiency progress on similar tasks. By similar tasks, we mean tasks within these sub-domains of AI, on which the field agrees we've seen substantial progress, and that have comparable levels of investment (compute and/or researcher time).</li> \n <li>Even though we believe AlexNet represented a lot of progress, this analysis doesn’t attempt to quantify that progress. More generally, the first time a capability is created, algorithmic breakthroughs may have reduced the resources required from totally infeasible<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn4\" id=\"fnref4\">[4]</a></sup> to merely high. We think new capabilities generally represent a larger share of overall conceptual progress than observed efficiency increases of the type shown here.</li> \n <li>This analysis focuses on the final training run cost for an optimized model rather than total development costs. Some algorithmic improvements make it easier to train a model by making the space of hyperparameters that will train stably and get good final performance much larger. On the other hand, architecture searches increase the gap between the final training run cost and total training costs.</li> \n <li>We don't speculate<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn5\" id=\"fnref5\">[5]</a></sup> on the degree to which we expect efficiency trends will extrapolate in time, we merely present our results and discuss the implications if the trends persist.</li> \n</ol> \n<h2 id=\"measurementandaipolicy\">Measurement and AI policy</h2> \n<p>We believe<span class=\"js-rfref\" data-id=\"testimony\"></span> that policymaking related to AI will be improved by a greater focus on the measurement and assessment of AI systems, both in terms of technical attributes and societal impact. We think such measurement initiatives can shed light on important questions in policy; our AI and Compute<span class=\"js-rfref\" data-id=\"aiandcompute\"></span> analysis suggests policymakers should increase funding for compute resources for academia, so that academic research can replicate, reproduce, and extend industry research. This efficiency analysis suggests that policymakers could develop accurate intuitions about the cost of deploying AI capabilities—and how these costs are going to alter over time—by more closely assessing the rate of improvements in efficiency for AI systems.</p> \n<h2 id=\"trackingefficiencygoingforward\">Tracking efficiency going forward</h2> \n<p>If large scale compute continues to be important to achieving state of the art (SOTA) overall performance in domains like language and games then it’s important to put effort into measuring notable progress achieved with smaller amounts of compute (contributions often made by academic institutions). Models that achieve training efficiency state of the arts on meaningful capabilities are promising candidates for scaling up and potentially achieving overall top performance. Additionally, figuring out the algorithmic efficiency improvements are straightforward<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn6\" id=\"fnref6\">[6]</a></sup> since they are just a particularly meaningful slice of the learning curves that all experiments generate.</p> \n<p>We also think that measuring long run trends in efficiency SOTAs will help paint a quantitative picture of overall algorithmic progress. We observe that hardware and algorithmic efficiency gains are multiplicative and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.</p> \n<p>Our results suggest that for AI tasks with high levels of investment (researcher time and/or compute) algorithmic efficiency might outpace gains from hardware efficiency (Moore's Law). Moore's Law was coined in 1965 when integrated circuits had a mere 64 transistors (6 doublings) and naively extrapolating it out predicted personal computers and smartphones (an iPhone 11 has 8.5 billion transistors). If we observe decades of exponential improvement in the algorithmic efficiency of AI, what might it lead to? We're not sure. That these results make us ask this question is a modest update for us towards a future with powerful AI services and technology.</p> \n<p>For all these reasons, we’re going to start tracking efficiency SOTAs publicly. We’ll start with vision and translation efficiency benchmarks (ImageNet<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/ai-and-efficiency/#fn7\" id=\"fnref7\">[7]</a></sup> and WMT14), and we’ll consider adding more benchmarks over time. We believe there are efficiency SOTAs on these benchmarks we’re unaware of and encourage the research community to <a href=\"https://github.com/openai/ai-and-efficiency\">submit them here</a> (we’ll give credit to original authors and collaborators).</p> \n<p>Industry leaders, policymakers, economists, and potential researchers are all trying to better understand AI progress and decide how much attention they should invest and where to direct it. Measurement efforts can help ground such decisions. If you’re interested in this type of work, <a href=\"https://openai.com/jobs/\">consider applying</a> to work at OpenAI’s Foresight or Policy team!</p> \n<div class=\"full bg-fg-2 pt-2.5 pb-0.5\"> \n <div class=\"container\"> \n  <div class=\"row\"> \n   <div class=\"content\"> \n    <h2 id=\"sotas\" class=\"mt-n1 pt-1\">Algorithmic efficiency SOTAs</h2> \n    <p><a href=\"https://github.com/openai/ai-and-efficiency\" class=\"btn btn-padded icon-external right\">Submit on GitHub</a></p> \n    <div class=\"wide mt-2 mb-0\"> \n     <div id=\"leaderboard\" class=\"mx-xl-auto\" style=\"max-width:960px\"></div> \n    </div> \n   </div> \n  </div> \n </div> \n</div> \n<!-- end .full --> \n<footer class=\"post-footer js-post-footer\"> \n <!-- footer item --> \n <div> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Acknowledgments \n    </div> \n    <div class=\"col\"> \n     <p>We’d like to thank the following people helpful conversations and/or feedback on this post: Dario Amodei, Jack Clark, Alec Radford, Paul Christiano, Sam McCandlish, Ilya Sutskever, Jacob Steinhardt, Jared Kaplan, Amanda Askell, John Schulman, Ryan Lowe, Jacob Hilton, Asya Bergal, Katja Grace, Ryan Carey, Nicholas Joseph, Geoffrey Irving, Jeff Clune, and Ashley Pilipiszyn.</p> \n     <p>Thanks to Justin Jay Wang for design.</p> \n     <p>Thanks to Niki Parmar for providing the relevant points from the original <a href=\"https://arxiv.org/abs/1706.03762\">transformer</a> learning curves.</p> \n     <p>Also thanks to Mingxing Tan for providing the relevant points from <a href=\"https://arxiv.org/abs/1905.11946\">EfficientNet</a> learning curves and running an experiment with reduced warmup.</p> \n    </div> \n   </div> \n  </hr> \n </div> \n <!-- footer item --> \n <div> \n  <hr> \n   <div class=\"row\" id=\"references\"> \n    <div class=\"col\">\n      References \n    </div> \n    <div class=\"col\"> \n     <ol> \n      <li class=\"js-ref\" data-id=\"imagenet\"> Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). \"<a href=\"http://www.image-net.org/papers/imagenet_cvpr09.pdf\">ImageNet: A Large-Scale Hierarchical Image Database</a>.\" In CVPR09. </li> \n      <li class=\"js-ref\" data-id=\"alexnet\"> Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). \"<a href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\">Imagenet classification with deep convolutional neural networks</a>.\" In F. Pereira, C. J. C. Burges, L. Bottou, &amp; K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 25 (pp. 1097–1105). Curran Associates, Inc. </li> \n      <li class=\"js-ref\" data-id=\"moore\"> Moore, G. E. (1965). \"<a href=\"https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf\">Cramming more components onto integrated circuits</a>.\" Electronics 38(8). </li> \n      <li class=\"js-ref\" data-id=\"aiandcompute\"> Amodei, D. &amp; Hernandez, D. (2018). \"<a href=\"https://openai.com/blog/ai-and-compute/\">AI and Compute</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"googlenet\"> Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., &amp; Rabinovich, A. (2014). \"<a href=\"https://arxiv.org/abs/1409.4842\">Going deeper with convolutions</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"vgg\"> Simonyan, K. &amp; Zisserman, A. (2014). \"<a href=\"https://arxiv.org/abs/1409.1556\">Very deep convolutional networks for large-scale image recognition</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"resnet\"> He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). \"<a href=\"https://arxiv.org/abs/1512.03385\">Deep residual learning for image recognition </a>.\" </li> \n      <li class=\"js-ref\" data-id=\"squeezenet\"> Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., &amp; Keutzer, K. (2016). \"<a href=\"https://arxiv.org/abs/1602.07360\">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &amp;lt0.5mb model size</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"wideresnet\"> Zagoruyko, S. &amp; Komodakis, N. (2016). \"<a href=\"https://arxiv.org/abs/1605.07146\">Wide residual networks</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"resnext\"> Xie, S., Girshick, R., Dollár, P., Tu, Z., &amp; He, K. (2016). \"<a href=\"https://arxiv.org/abs/1611.05431\">Aggregated residual transforma- tions for deep neural networks</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"densenet\"> Huang,G.,Liu,Z.,vanderMaaten,L.,&amp;Weinberger,K.Q.(2016). \"<a href=\"https://arxiv.org/abs/1608.06993\">Densely connected convolutional networks</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"mobilenet\"> Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An- dreetto, M., &amp; Adam, H. (2017). \"<a href=\"https://arxiv.org/pdf/1704.04861.pdf\">Mobilenets: Efficient convolutional neural networks for mobile vision applications</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"shufflenet\"> Zhang, X., Zhou, X., Lin, M., &amp; Sun, J. (2017). \"<a href=\"https://arxiv.org/abs/1707.01083\">Shufflenet: An extremely efficient convolutional neural network for mobile devices</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"mobilenet_v2\"> Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., &amp; Chen, L.-C. (2018). \"<a href=\"https://arxiv.org/abs/1801.04381\">Mobilenetv2: Inverted residuals and linear bottlenecks</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"shufflenet_v2\"> Ma, N., Zhang, X., Zheng, H.-T., &amp; Sun, J. (2018). \"<a href=\"https://arxiv.org/abs/1807.11164\">Practical guidelines for efficient cnn architecture design</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"efficientnet\"> Tan, M. &amp; Le, Q. V. (2019). \"<a href=\"https://arxiv.org/abs/1905.11946\">Efficientnet: Rethinking model scaling for convolutional neural networks</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"dna\"> Sawyer, Eric (2011). \"<a href=\"https://www.nature.com/scitable/blog/bio2.0/high_throughput_sequencing_and_cost/\">High Throughput Sequencing and Cost Trends</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"solar\"> Roberts, David (2019). \"<a href=\"https://www.vox.com/energy-and-environment/2019/8/9/20767886/renewable-energy-storage-cost-electricity\">Getting to 100% renewables requires cheap energy storage. But how cheap?</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"pytorch\"> Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., &amp; Lerer, A. (2017). \"<a href=\"https://openreview.net/pdf?id=BJJsrmfCZ\">Automatic differentiation in PyTorch. In NIPS Autodiff Workshop</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"shufflenet_github\"> Huang, J. (2017). \"<a href=\"https://github.com/jaxony/shufflenet\">Shufflenet in pytorch</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"mobilenet_github\"> Xiao, H. (2017). \"<a href=\"https://github.com/marvis/pytorch-mobilenet.\">Pytorch mobilenet implementation of \"mobilenets: Efficient convolutional neural networks for mobile vision applications\"</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"transformer\"> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). \"<a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need. CoRR, abs/1706.03762</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"seq2seq\"> Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). \"<a href=\"https://arxiv.org/abs/1409.3215\">Sequence to sequence learning with neural networks. CoRR, abs/1409.3215</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"alphazero\"> Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., &amp; Hassabis, D. (2018). \"<a href=\"https://arxiv.org/abs/1712.01815\">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419), 1140–1144</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"alphagozero\"> Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., &amp; Hassabis, D. (2017). \"<a href=\"https://www.nature.com/articles/nature24270\">Mastering the game of go without human knowledge. Nature, 550, 354–</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"openaifive\"> OpenAI et. al, :, Berner, C., Brockman, G., Chan, B., Cheung, V., De ̨biak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., &amp; Zhang, S. (2019). \"<a href=\"https://cdn.openai.com/dota-2.pdf\">Dota 2 with Large Scale Deep Reinforcement Learning</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"dawnbench\"> Cody A. Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia (2017). \"<a href=\"https://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf\">DAWNBench: An End-to-End Deep Learning Benchmark and Competition. NIPS ML SYSTEMS WORKSHOP, 2017</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"pytorch\"> Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmai- son, A., Antiga, L., &amp; Lerer, A. (2017). \"<a href=\"https://openreview.net/pdf?id=BJJsrmfCZ\">Automatic differentiation in PyTorch</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"aiindex\"> Raymond Perrault, Yoav Shoham, E. B. J. C. J. E. B. G. T. L. J. M. S. M. &amp; Niebles, J. C. (2019). \"<a href=\"https://hai.stanford.edu/sites/default/files/ai_index_2019_report.pdf\">The AI Index 2019 Annual Report”. Technical report, AI Index Steering Committee, Human-Centered AI Institute, Stanford University, Stanford, CA</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"scienceofai\"> McCandlish, S., Kaplan, J., Amodei, D., &amp; Team, O. D. (2018). \"<a href=\"https://arxiv.org/pdf/1812.06162.pdf\">An empirical model of large-batch training\"</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"wavenet\"> van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., &amp; Hassabis, D. (2017). \"<a href=\"https://arxiv.org/abs/1711.10433\">Parallel wavenet: Fast high-fidelity speech synthesis.</a>.\" </li> \n      <li class=\"js-ref\" data-id=\"testimony\"> Jack Clark (2019). \"<a href=\"https://science.house.gov/imo/media/doc/Clark%20Testimony.pdf\">Written Testimony of Jack Clark, Policy Director at OpenAI. Hearing on “Artificial Intelligence: Societal and Ethical Implications” before the House Committee on Science, Space, &amp; Technology</a>.\" </li> \n     </ol> \n    </div> \n   </div> \n  </hr> \n </div> \n <!-- special footer item for footnotes --> \n <div data-order=\"-1\"> \n  <hr> \n   <div class=\"row\"> \n    <div class=\"col\">\n      Footnotes \n    </div> \n    <div class=\"col\"> \n     <hr class=\"footnotes-sep\"> \n      <section class=\"footnotes\"> \n       <ol class=\"footnotes-list\"> \n        <li id=\"fn1\" class=\"footnote-item\"><p>In the sorting example, the “difficulty” of the problem is the length of the list. The cost for quicksort, a commonly used algorithm is denoted in Big O notation: $O(n\\log{}n)$. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn2\" class=\"footnote-item\"><p>Inference costs dominate total costs for successful deployed systems. Inference costs scale with usage of the system, whereas training costs only need to be paid once. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn3\" class=\"footnote-item\"><p>Throughout this post we refer to Moore's Law as the consistent, long-observed 2-year doubling time of dollars/flop. One could also interpret Moore's Law as the trend in dollars/flop, that has recently slowed down. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref3\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn4\" class=\"footnote-item\"><p>For instance algorithmic progress could change the complexity class on some task from exponential to polynomial cost. Such efficiency gains on capabilities of interest are intractable to directly observe, though they may be observable through asymptotic analysis or extrapolating empirically derived scaling laws. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref4\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn5\" class=\"footnote-item\"><p>Making credible forecasts on such topics is a substantial enterprise, we'd rather avoid here than give insufficient treatment. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref5\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn6\" class=\"footnote-item\"><p>In fact, this work was primarily done by training PyTorch examples models, with tweaks to improve early learning. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref6\" class=\"footnote-backref\">↩︎</a></p> </li> \n        <li id=\"fn7\" class=\"footnote-item\"><p>ImageNet is the only training data source allowed for the vision benchmark. No human captioning, other images, or other data is allowed. Automated augmentation is ok. <a href=\"https://openai.com/blog/ai-and-efficiency/#fnref7\" class=\"footnote-backref\">↩︎</a></p> </li> \n       </ol> \n      </section> \n      <!--kg-card-end: markdown--> \n     </hr> \n    </div> \n   </div> \n  </hr> \n </div> \n</footer>","descriptionType":"html","publishedDate":"Tue, 05 May 2020 15:58:44 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2020/05/ai-and-efficiency-social.png","linkMd5":"c3d26b8fa15978939bc952a0e1d23591","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn62@2020_6/2020/08/24/22-09-05-365_7247c2b124142321.webp","destWidth":2400,"destHeight":1256,"sourceBytes":167874,"destBytes":45272,"author":"Danny Hernandez","articleImgCdnMap":{"https://openai.com/content/images/2020/05/ai-and-efficiency-social.png":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn62@2020_6/2020/08/24/22-09-05-365_7247c2b124142321.webp"},"publishedOrCreatedDate":1598306916757},{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","title":"Solving Rubik’s Cube with a Robot Hand","link":"https://openai.com/blog/solving-rubiks-cube/","description":"<!--kg-card-begin: markdown--> \n<div class=\"post-excerpt medium-copy mb-1 color-fg-80\"> \n <img src=\"https://openai.com/content/images/2019/10/twitter-og.jpg\" alt=\"Solving Rubik’s Cube with a Robot Hand\"><p>We've trained a pair of neural networks to solve the Rubik’s Cube with a human-like robot hand. The neural networks are trained entirely in simulation, using the same reinforcement learning code as <a href=\"https://openai.com/blog/openai-five\">OpenAI Five</a> paired with a new technique called Automatic Domain Randomization (ADR). The system can handle situations it never saw during training, such as being prodded by a <a href=\"https://openai.com/blog/solving-rubiks-cube/#perturbations\">stuffed giraffe</a>. This shows that reinforcement learning isn't just a tool for virtual tasks, but can solve physical-world problems requiring unprecedented dexterity.</p> </img> \n</div> \n<section class=\"btns\"> \n <a href=\"https://arxiv.org/abs/1910.07113\" class=\"btn btn-padded icon-paper\">Read paper</a> \n <a href=\"https://www.youtube.com/playlist?list=PLOXw6I10VTv9HODt7TFEL72K3Q6C4itG6\" class=\"btn btn-padded icon-play\">Watch all videos</a> \n</section> \n<p>Human hands let us solve a wide variety of tasks. For the past 60 years of robotics, hard tasks which humans accomplish with their fixed pair of hands have required designing a custom robot for <a href=\"https://www.guinnessworldrecords.com/world-records/fastest-robot-to-solve-a-rubiks-cube\">each task</a>. As an alternative, people have spent <a href=\"https://www.youtube.com/playlist?list=PLOXw6I10VTv8XX8Qnil18ilSaZvyQ4m2S\">many decades trying to use general-purpose robotic hardware</a>, but with limited success due to their high degrees of freedom. In particular, the hardware we use here is not new—the robot hand we use has been around for the last 15 years—but the software approach is.</p> \n<p>Since May 2017, we've been trying to train a human-like robotic hand to solve the <a href=\"https://en.wikipedia.org/wiki/Rubik%27s_Cube\">Rubik’s Cube</a>. We set this goal because we believe that successfully training such a robotic hand to do complex manipulation tasks lays the foundation for general-purpose robots. We solved the Rubik’s Cube in simulation in July 2017. But as of July 2018, we could only <a href=\"https://openai.com/blog/learning-dexterity/\">manipulate a block</a> on the robot. Now, we've reached our initial goal.</p> \n<!-- uncut video embed --> \n<figure class=\"mt-1 mb-1.5\"> \n <iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/kVmp0uGtShk?rel=0&amp;color=white\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe> \n <figcaption class=\"mt-0\">\n   A full solve of the Rubik’s Cube. This video plays at real-time and was not edited in any way. \n </figcaption> \n</figure> \n<p>Solving a Rubik’s Cube one-handed is a challenging task even for humans, and it takes children several years to gain the dexterity required to master it. Our robot <a href=\"https://openai.com/blog/solving-rubiks-cube/#challenges\">still hasn't perfected its technique</a> though, as it solves the Rubik’s Cube 60% of the time (and only 20% of the time for a <a href=\"http://cube20.org/qtm/\">maximally difficult</a> scramble).</p> \n<h2 id=\"ourapproach\">Our approach</h2> \n<p>We train neural networks to solve the Rubik’s Cube in <a href=\"http://mujoco.org/\">simulation</a> using <a href=\"https://openai.com/blog/how-to-train-your-openai-five/\">reinforcement learning</a> and <a href=\"https://en.wikipedia.org/wiki/Optimal_solutions_for_Rubik%27s_Cube#Kociemba's_algorithm\">Kociemba’s algorithm</a> for picking the solution steps.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/solving-rubiks-cube/#fn1\" id=\"fnref1\">[1]</a></sup> <a href=\"https://arxiv.org/abs/1703.06907\">Domain</a> <a href=\"https://arxiv.org/abs/1710.06537\">randomization</a> enables networks trained solely in simulation to transfer to a real robot.</p> \n<img src=\"https://cdn.openai.com/solving-rubiks-cube/images/dr.jpg\" alt=\"Solving Rubik’s Cube with a Robot Hand\"> \n <div class=\"caption\">\n   Domain randomization exposes the neural network to many different variants of the same problem, in this case solving a Rubik’s Cube. \n </div> <p>The biggest challenge we faced was to create environments in simulation diverse enough to capture the physics of the real world. Factors like friction, elasticity and dynamics are incredibly difficult to measure and model for objects as complex as Rubik’s Cubes or robotic hands and we found that domain randomization alone is not enough.</p> <p>To overcome this, we developed a new method called <em>Automatic Domain Randomization</em> (ADR), which endlessly generates progressively more difficult environments in simulation.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/solving-rubiks-cube/#fn2\" id=\"fnref2\">[2]</a></sup> This frees us from having an accurate model of the real world, and enables the transfer of neural networks learned in simulation to be applied to the real world.</p> <p>ADR starts with a single, nonrandomized environment, wherein a neural network learns to solve Rubik’s Cube. As the neural network gets better at the task and reaches a performance threshold, the amount of domain randomization is increased automatically. This makes the task harder, since the neural network must now learn to generalize to more randomized environments. The network keeps learning until it again exceeds the performance threshold, when more randomization kicks in, and the process is repeated.</p> \n <!-- ADR visualization --> \n <div class=\"mt-1 mb-0.75\"> \n  <h5 id=\"adrappliedtothesizeoftherubikscube\">ADR applied to the size of the Rubik’s Cube</h5> \n  <div id=\"chart-adr\"></div> \n </div> <p>One of the parameters we randomize is the size of the Rubik’s Cube (above). ADR begins with a fixed size of the Rubik’s Cube and gradually increases the randomization range as training progresses. We apply the same technique to all other parameters, such as the mass of the cube, the friction of the robot fingers, and the visual surface materials of the hand. The neural network thus has to learn to solve the Rubik’s Cube under all of those increasingly more difficult conditions.</p> \n <!-- ADR entropy scatterplot --> \n <div class=\"mt-1 mb-0.75\"> \n  <h5 id=\"automaticvsmanualdomainrandomization\">Automatic vs. manual domain randomization</h5> \n  <div id=\"chart-adr-entropy\"></div> \n </div> <p>Domain randomization required us to manually specify randomization ranges, which is difficult since too much randomization makes learning difficult but too little randomization hinders transfer to the real robot. ADR solves this by automatically expanding randomization ranges over time with no human intervention. ADR removes the need for domain knowledge and makes it simpler to apply our methods to new tasks. In contrast to manual domain randomization, ADR also keeps the task always challenging with training never converging.</p> <p>We compared ADR to manual domain randomization on the block flipping task, where we already had a <a href=\"https://openai.com/blog/learning-dexterity\">strong baseline</a>. In the beginning ADR performs worse in terms of number of successes on the real robot. But as ADR increases the entropy, which is a measure of the complexity of the environment, the transfer performance eventually doubles over the baseline—without human tuning.</p> <h2 id=\"analysis\">Analysis</h2> <h3 id=\"testingforrobustness\">Testing for robustness</h3> <p>Using ADR, we are able to train neural networks in simulation that can solve the Rubik’s Cube on the real robot hand. This is because ADR exposes the network to an endless variety of randomized simulations. It is this exposure to complexity during training that prepares the network to transfer from simulation to the real world since it has to learn to quickly identify and adjust to whatever physical world it is confronted with.</p> \n <!-- perturbations GIFs --> \n <div class=\"wide mt-n1 pt-1.5 mb-1\" id=\"perturbations\"> \n  <div class=\"row\"> \n   <div class=\"col-12 col-xl-10 offset-xl-1\"> \n    <div class=\"row narrow-gutters\"> \n     <figure class=\"col-6 col-md-4\"> \n      <a href=\"https://youtu.be/kVmp0uGtShk\" class=\"no-style d-block position-relative\"> \n       <div class=\"position-absolute w-100 h-100\" style=\"top:0;left:0;z-index:1;cursor:pointer\"></div> <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"perturbation-1\" data-src=\"https://player.vimeo.com/video/365132002?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\" style=\"cursor:pointer\"></iframe> </a> \n      <figcaption class=\"mt-0\">\n        Unperturbed (for reference) \n      </figcaption> \n     </figure> \n     <figure class=\"col-6 col-md-4\"> \n      <a href=\"https://youtu.be/QyJGXc9WeNo\" class=\"no-style d-block position-relative\"> \n       <div class=\"position-absolute w-100 h-100\" style=\"top:0;left:0;z-index:1;cursor:pointer\"></div> <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"perturbation-2\" data-src=\"https://player.vimeo.com/video/365130786?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\" style=\"cursor:pointer\"></iframe> </a> \n      <figcaption class=\"mt-0\">\n        Rubber glove \n      </figcaption> \n     </figure> \n     <figure class=\"col-6 col-md-4\"> \n      <a href=\"https://youtu.be/QyJGXc9WeNo?t=24\" class=\"no-style d-block position-relative\"> \n       <div class=\"position-absolute w-100 h-100\" style=\"top:0;left:0;z-index:1;cursor:pointer\"></div> <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"perturbation-3\" data-src=\"https://player.vimeo.com/video/365130801?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\" style=\"cursor:pointer\"></iframe> </a> \n      <figcaption class=\"mt-0\">\n        Tied fingers \n      </figcaption> \n     </figure> \n     <figure class=\"col-6 col-md-4\"> \n      <a href=\"https://youtu.be/QyJGXc9WeNo?t=48\" class=\"no-style d-block position-relative\"> \n       <div class=\"position-absolute w-100 h-100\" style=\"top:0;left:0;z-index:1;cursor:pointer\"></div> <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"perturbation-4\" data-src=\"https://player.vimeo.com/video/365130697?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\" style=\"cursor:pointer\"></iframe> \n       <figcaption class=\"mt-0\">\n         Blanket occlusion and perturbation \n       </figcaption> </a> \n     </figure> \n     <figure class=\"col-6 col-md-4\"> \n      <a href=\"https://youtu.be/QyJGXc9WeNo?t=72\" class=\"no-style d-block position-relative\"> \n       <div class=\"position-absolute w-100 h-100\" style=\"top:0;left:0;z-index:1;cursor:pointer\"></div> <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"perturbation-5\" data-src=\"https://player.vimeo.com/video/365130768?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\" style=\"cursor:pointer\"></iframe> </a> \n      <figcaption class=\"mt-0\">\n        Plush giraffe perturbation \n      </figcaption> \n     </figure> \n     <figure class=\"col-6 col-md-4\"> \n      <a href=\"https://youtu.be/QyJGXc9WeNo?t=81\" class=\"no-style d-block position-relative\"> \n       <div class=\"position-absolute w-100 h-100\" style=\"top:0;left:0;z-index:1;cursor:pointer\"></div> <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"perturbation-6\" data-src=\"https://player.vimeo.com/video/365130730?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\" style=\"cursor:pointer\"></iframe> </a> \n      <figcaption class=\"mt-0\">\n        Pen perturbation \n      </figcaption> \n     </figure> \n    </div> \n   </div> \n  </div> \n </div> \n <!-- end .wide --> \n <div class=\"caption\">\n   Perturbations that we apply to the real robot hand while it solves the Rubik’s Cube. All videos play at&nbsp;real-time. \n </div> <p>To test the limits of our method, we experiment with a variety of perturbations while the hand is solving the Rubik’s Cube. Not only does this test for the robustness of our control network but also tests our vision network, which we here use to estimate the cube’s position and orientation.</p> <p>We find that our system trained with ADR is surprisingly robust to perturbations even though we never trained with them: The robot can successfully perform most flips and face rotations under all tested perturbations, though not at peak performance.</p> <h3 id=\"emergentmetalearning\">Emergent meta-learning</h3> <p>We believe that <a href=\"https://en.wikipedia.org/wiki/Meta_learning_(computer_science)\">meta-learning</a>, or learning to learn, is an important prerequisite for building general-purpose systems, since it enables them to quickly adapt to changing conditions in their environments. The hypothesis behind ADR is that a memory-augmented networks combined with a sufficiently randomized environment leads to <em>emergent meta-learning</em>, where the network implements a learning algorithm that allows itself to rapidly adapt its behavior to the environment it is deployed in.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/solving-rubiks-cube/#fn3\" id=\"fnref3\">[3]</a></sup></p> <p>To test this systematically, we measure the time to success per cube flip (rotating the cube such that a different color faces up) for our neural network under different perturbations, such as resetting the network’s memory, resetting the dynamics, or breaking a joint. We perform these experiments in simulation, which allows us to average performance over 10,000 trials in a controlled setting.</p> \n <!-- perturbation charts --> \n <div class=\"mt-1 mb-0.75\"> \n  <div class=\"mb-1\"> \n   <button onclick=\"toggle(['memory'], ['dynamics','joint'])\" class=\"js-toggler active button-unstyled small-copy py-0.25 mr-0.5\">Reset memory</button> \n   <button onclick=\"toggle(['dynamics'], ['memory', 'joint'])\" class=\"js-toggler button-unstyled small-copy py-0.25 mr-0.5\">Reset dynamics</button> \n   <button onclick=\"toggle(['joint'], ['memory', 'dynamics'])\" class=\"js-toggler button-unstyled small-copy py-0.25\">Broken joint</button> \n   <hr class=\"my-0\"> \n   </hr> \n  </div> \n  <div class=\"position-relative mt-0.5\"> \n   <figure id=\"memory\"> \n    <h5 class=\"mt-0 mb-0.5\">Time to success when the network’s memory is erased</h5> \n    <div id=\"chart-perturbation-memory\"></div> \n   </figure> \n   <figure id=\"dynamics\" class=\"position-absolute w-100\" style=\"top:0;left:0;opacity:0\"> \n    <h5 class=\"mt-0 mb-0.5\">Time to success when friction, mass, or gravity change</h5> \n    <div id=\"chart-perturbation-dynamics\"></div> \n   </figure> \n   <figure id=\"joint\" class=\"position-absolute w-100\" style=\"top:0;left:0;opacity:0\"> \n    <h5 class=\"mt-0 mb-0.5\">Time to success when the robot is impaired by breaking a random joint</h5> \n    <div id=\"chart-perturbation-joint\"></div> \n   </figure> \n  </div> \n </div> <p>In the beginning, as the neural network successfully achieves more flips, each successive time to success decreases because the network learns to adapt. When perturbations are applied (vertical gray lines in the above chart), we see a spike in time to success. This is because the strategy the network is employing doesn't work in the changed environment. The network then relearns about the new environment and we again see time to success decrease to the previous baseline.</p> <p>We also measure failure probability and performed the same experiments for face rotations (rotating the top face 90 degrees clockwise or counterclockwise) and find the same pattern of adaptation.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/solving-rubiks-cube/#fn4\" id=\"fnref4\">[4]</a></sup></p> <h3 id=\"understandingourneuralnetworks\">Understanding our neural networks</h3> <p>Visualizing our networks enables us to understand what they are storing in memory. This becomes increasingly important as the networks grow in complexity.</p> \n <!-- hidden state visualization --> \n <figure class=\"mt-1 mb-1.25\"> \n  <iframe class=\"js-custom-lazy\" data-monitor=\"\" data-id=\"hidden-state\" data-src=\"https://player.vimeo.com/video/365164292?autopause=0&amp;autoplay=0&amp;background=1&amp;loop=1&amp;muted=1&amp;playsinline=1&amp;transparent=1\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen=\"\"></iframe> \n  <div class=\"position-relative\"> \n   <img id=\"hidden-state-progress-image\" src=\"https://cdn.openai.com/solving-rubiks-cube/images/hidden-state-color-20191014b.png\" class=\"mb-0\" alt=\"Solving Rubik’s Cube with a Robot Hand\"> <label for=\"hidden-state-progress\" id=\"hidden-state-progress-label\" class=\"xxsmall-copy mb-0\" style=\"line-height:1\"> <span id=\"hidden-state-progress-time\" class=\"font-tnum rounded bg-cool-gray-2 color-white py-0.1 position-relative\">0:00</span> </label> <input type=\"range\" name=\"hidden-state-progress\" id=\"hidden-state-progress\" value=\"0\" min=\"0\" max=\"1000\"> </input></img> \n  </div> \n </figure> <p>The memory of our neural network is visualized above. We use a <a href=\"https://distill.pub/2018/building-blocks/\">building block from the interpretability toolbox</a>, namely non-negative matrix factorization, to condense this high-dimensional vector into 6 groups and assign each a unique color. We then display the color of the currently dominant group for every timestep.</p> <p>We find that each memory group has a semantically meaningful behavior associated with it. For example, we can tell by looking at only the dominant group of the network’s memory if it is about to spin the cube or rotate the top clockwise <em>before it happens</em>.</p> <h2 id=\"challenges\">Challenges</h2> <p>Solving the Rubik’s Cube with a robot hand is still not easy. Our method currently solves the Rubik’s Cube 20% of the time when applying a <a href=\"http://cube20.org/qtm/\">maximally difficult scramble</a> that requires 26 face rotations. For simpler scrambles that require 15 rotations to undo, the success rate is 60%. When the Rubik’s Cube is dropped or a timeout is reached, we consider the attempt failed. However, our network is capable of solving the Rubik’s Cube from any initial condition. So if the cube is dropped, it is possible to put it back into the hand and continue solving.</p> <p>We generally find that our neural network is much more likely to fail during the first few face rotations and flips. This is the case because the neural network needs to balance solving the Rubik’s Cube with adapting to the physical world during those early rotations and flips.</p> \n <div class=\"full\" style=\"background-color:rgba(0,6,121,0.03)\"> \n  <div class=\"container\"> \n   <div class=\"row\"> \n    <div class=\"content pt-1 pb-1\"> \n     <h3 id=\"behindthescenesrubikscubeprototypes\">Behind the scenes: Rubik’s Cube prototypes</h3> \n     <p>In order to benchmark our progress and make the problem tractable, we built and designed custom versions of cubes as stepping stones towards ultimately solving a regular Rubik’s Cube.<sup class=\"footnote-ref\"><a href=\"https://openai.com/blog/solving-rubiks-cube/#fn5\" id=\"fnref5\">[5]</a></sup></p> \n     <div class=\"wide mt-1.5 mb-0.5\"> \n      <div class=\"row\"> \n       <div class=\"col-12 col-xl-10 offset-xl-1\"> \n        <!-- prototypes picture --> \n        <p><img src=\"https://cdn.openai.com/solving-rubiks-cube/images/openai-robotics-rubiks-prototypes.jpg\" alt=\"Solving Rubik’s Cube with a Robot Hand\" /></p> \n        <div class=\"caption\"> \n         <p>Rubik’s Cube prototypes, from left to right: Locked cube, Face cube, Full cube, <a href=\"https://www.xiaomitoday.com/xiaomi-giiker-m3-intelligent-rubik-cube-review/\">Giiker</a> cube, regular Rubik’s Cube.</p> \n        </div> \n       </div> \n      </div> \n     </div> \n     <div class=\"d-block d-md-table w-100 mt-n1\"> \n      <table> \n       <thead> \n        <tr> \n         <th>Prototype</th> \n         <th>Position + Orientation</th> \n         <th>Internal Degrees of Freedom (Sensor)</th> \n        </tr> \n       </thead> \n       <tbody> \n        <tr> \n         <td>Locked cube</td> \n         <td>Vision</td> \n         <td>0 (No sensor)</td> \n        </tr> \n        <tr> \n         <td>Face cube</td> \n         <td><a href=\"http://phasespace.com/\">PhaseSpace</a></td> \n         <td>2 (PhaseSpace)</td> \n        </tr> \n        <tr> \n         <td>Full cube</td> \n         <td>PhaseSpace</td> \n         <td>6 (PhaseSpace)</td> \n        </tr> \n        <tr> \n         <td>Giiker cube</td> \n         <td>Vision</td> \n         <td>6 (<a href=\"https://www.xiaomitoday.com/xiaomi-giiker-m3-intelligent-rubik-cube-review/\">Built-in sensors</a>)</td> \n        </tr> \n        <tr> \n         <td>Regular&nbsp;Rubik’s&nbsp;Cube</td> \n         <td>Vision</td> \n         <td>6 (Vision)</td> \n        </tr> \n       </tbody> \n      </table> \n     </div> \n    </div> \n   </div> \n  </div> \n </div> \n <!-- end .content .row .container .full --> <h2 id=\"nextsteps\">Next steps</h2> <p>We believe that human-level dexterity is on the path towards building general-purpose robots and we are excited to push forward in this direction.</p> <p><em>If you want to help make increasingly general AI systems, whether robotic or virtual, <a href=\"https://openai.com/jobs/#robotics\">we're hiring</a>!</em></p> \n <footer class=\"post-footer js-post-footer\"> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\"> \n     <div class=\"col\">\n       Acknowledgments \n     </div> \n     <div class=\"col\">\n       Thanks to the following for feedback on drafts of this post and paper: Josh Achiam, Greg Brockman, Nick Cammarata, Jack Clark, Jeff Clune, Ruben D’Sa, Harri Edwards, David Farhi, Ken Goldberg, Leslie P. Kaelbling, Hyeonwoo Noh, Lerrel Pinto, John Schulman, Ilya Sutskever &amp; Tao Xu. \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\"> \n     <div class=\"col\">\n       Video \n     </div> \n     <div class=\"col\">\n       Peter Jordan (Director), Yvette Solis (Producer), Brooke Chan (Producer) \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\"> \n     <div class=\"col\">\n       Editor \n     </div> \n     <div class=\"col\">\n       Ashley Pilipiszyn \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\"> \n     <div class=\"col\">\n       Design \n     </div> \n     <div class=\"col\">\n       Justin Jay Wang &amp; Ben Barry \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- footer item --> \n  <div> \n   <hr> \n    <div class=\"row\"> \n     <div class=\"col\">\n       Photography \n     </div> \n     <div class=\"col\">\n       Eric Haines \n     </div> \n    </div> \n   </hr> \n  </div> \n  <!-- special footer item for footnotes --> \n  <div data-order=\"-1\"> \n   <hr> \n    <div class=\"row\"> \n     <div class=\"col\">\n       Footnotes \n     </div> \n     <div class=\"col\"> \n      <hr class=\"footnotes-sep\"> \n       <section class=\"footnotes\"> \n        <ol class=\"footnotes-list\"> \n         <li id=\"fn1\" class=\"footnote-item\"><p>We focus on the problems that are currently difficult for machines to master: perception and dexterous manipulation. We therefore train our neural networks to achieve the required face rotations and cube flips as generated by Kociemba’s algorithm. <a href=\"https://openai.com/blog/solving-rubiks-cube/#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn2\" class=\"footnote-item\"><p>Our work is strongly related to <a href=\"https://arxiv.org/abs/1901.01753\">POET</a>, which automatically generates 2D environments. However, our work learns a joint policy over all environments, which transfers to any newly generated environment. <a href=\"https://openai.com/blog/solving-rubiks-cube/#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn3\" class=\"footnote-item\"><p>More concretely, we hypothesize that a neural network with finite capacity trained on environments with unbounded complexity forces the network to learn a special-purpose learning algorithm since it cannot memorize solutions for each individual environment and there exists no single robust policy that works under all randomizations. <a href=\"https://openai.com/blog/solving-rubiks-cube/#fnref3\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn4\" class=\"footnote-item\"><p>Please refer to our <a href=\"https://arxiv.org/abs/1910.07113\">paper</a> for full results. <a href=\"https://openai.com/blog/solving-rubiks-cube/#fnref4\" class=\"footnote-backref\">↩︎</a></p> </li> \n         <li id=\"fn5\" class=\"footnote-item\"><p>The only modification we made was cutting out a small piece of each center cublet’s colorful sticker. This was necessary to break <a href=\"https://en.wikipedia.org/wiki/Rotational_symmetry\">rotational symmetry</a>. <a href=\"https://openai.com/blog/solving-rubiks-cube/#fnref5\" class=\"footnote-backref\">↩︎</a></p> </li> \n        </ol> \n       </section> \n       <!--kg-card-end: markdown--> \n      </hr> \n     </div> \n    </div> \n   </hr> \n  </div> \n </footer></img>","descriptionType":"html","publishedDate":"Tue, 15 Oct 2019 15:51:18 +0000","feedId":4773,"bgimg":"https://openai.com/content/images/2019/10/twitter-og.jpg","linkMd5":"b09762d9ff3f5b1e20cfb8a36810dd5f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn90@2020_4/2020/08/24/22-09-05-657_51964eb0b5945302.webp","destWidth":1920,"destHeight":1080,"sourceBytes":208788,"destBytes":141000,"author":"OpenAI","articleImgCdnMap":{"https://openai.com/content/images/2019/10/twitter-og.jpg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn90@2020_4/2020/08/24/22-09-05-657_51964eb0b5945302.webp","https://cdn.openai.com/solving-rubiks-cube/images/dr.jpg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_5/2020/08/24/22-09-08-852_f7356c7861b8171f.webp","https://cdn.openai.com/solving-rubiks-cube/images/hidden-state-color-20191014b.png":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn2@2020_2/2020/08/24/22-09-07-639_5a26847ad503acd7.webp","https://cdn.openai.com/solving-rubiks-cube/images/openai-robotics-rubiks-prototypes.jpg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn24@2020_3/2020/08/24/22-09-07-655_f092c34125137ea6.webp"},"publishedOrCreatedDate":1598306916765}],"record":{"createdTime":"2020-08-25 06:08:36","updatedTime":"2020-08-25 06:08:36","feedId":4773,"fetchDate":"Mon, 24 Aug 2020 22:08:36 +0000","fetchMs":176,"handleMs":898,"totalMs":42366,"newArticles":0,"totalArticles":15,"status":1,"type":0,"ip":"54.209.91.147","hostName":"us-024.herokuapp.com","requestId":"019c61f1cb154f829811c8ea4c56d984_4773","contentType":"text/xml; charset=utf-8","totalBytes":11076734,"bgimgsTotal":14,"bgimgsGithubTotal":14,"articlesImgsTotal":88,"articlesImgsGithubTotal":88,"successGithubMap":{"myreaderx8":3,"myreaderx14":3,"myreaderx15":3,"myreaderx7":3,"myreaderx6":4,"myreaderx16":3,"myreaderx4":4,"myreaderx10":3,"myreaderx32":3,"myreaderx3":2,"myreaderx33":3,"myreaderx11":3,"myreaderx12":3,"myreaderx2":4,"myreaderx13":3,"myreaderx1":3,"myreaderx30":3,"myreaderx31":3,"myreaderx18":3,"myreaderx19":3,"myreaderx":2,"myreaderx25":3,"myreaderx27":3,"myreaderx21":4,"myreaderx22":2,"myreaderx23":2,"myreaderx24":3,"myreaderx5oss":3,"myreaderx29":3},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:29:53","updatedTime":"2020-08-25 04:29:53","id":4773,"name":"OpenAI","url":"https://blog.openai.com/rss/","subscriber":null,"website":null,"icon":"https://openai.com/favicon.png","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn20@2020_4/2020/08/24/22-08-35-966_19355e913963dddf.png","description":"OpenAI is an AI research and deployment company with the mission to ensure that artificial general intelligence benefits all of humanity.","weekly":null,"link":"https://openai.com"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":2406856,"tmpBodyImgCdnBytes":8669878,"tmpBgImgCdnBytes":0,"extra4":{"start":1598306915676,"total":0,"statList":[{"spend":191,"msg":"获取xml内容"},{"spend":898,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":1,"msg":"修正封面图上传失败重新上传"},{"spend":10606,"msg":"正文链接上传到cdn"}]},"extra5":88,"extra6":88,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":4773,"totalSpendMs":603,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:07","host":"us-034*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":4773,"totalSpendMs":268,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:08","host":"europe-58*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"}],"extra10_invalidATagHrefValue":{"https://openai.com/blog/safety-gym/_#fnref1":"https://openai.com/blog/safety-gym/#fnref1","https://openai.com/blog/image-gpt/_#introduction":"https://openai.com/blog/image-gpt/#introduction","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#jorge":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#jorge","https://openai.com/blog/ai-and-efficiency/_#fnref6":"https://openai.com/blog/ai-and-efficiency/#fnref6","https://openai.com/blog/ai-and-efficiency/_#fnref7":"https://openai.com/blog/ai-and-efficiency/#fnref7","https://openai.com/blog/jukebox/_#motivationandpriorwork":"https://openai.com/blog/jukebox/#motivationandpriorwork","https://openai.com/blog/ai-and-efficiency/_#fnref4":"https://openai.com/blog/ai-and-efficiency/#fnref4","https://openai.com/blog/procgen-minerl-competitions/_#fn1":"https://openai.com/blog/procgen-minerl-competitions/#fn1","https://openai.com/blog/ai-and-efficiency/_#fnref5":"https://openai.com/blog/ai-and-efficiency/#fnref5","https://openai.com/blog/ai-and-efficiency/_#fnref2":"https://openai.com/blog/ai-and-efficiency/#fnref2","https://openai.com/blog/ai-and-efficiency/_#fnref3":"https://openai.com/blog/ai-and-efficiency/#fnref3","https://openai.com/blog/ai-and-efficiency/_#fnref1":"https://openai.com/blog/ai-and-efficiency/#fnref1","https://openai.com/blog/image-gpt/_#approach":"https://openai.com/blog/image-gpt/#approach","https://openai.com/blog/solving-rubiks-cube/_#challenges":"https://openai.com/blog/solving-rubiks-cube/#challenges","https://openai.com/blog/solving-rubiks-cube/_#fn1":"https://openai.com/blog/solving-rubiks-cube/#fn1","https://openai.com/blog/jukebox/_#futuredirections":"https://openai.com/blog/jukebox/#futuredirections","https://openai.com/blog/solving-rubiks-cube/_#fn5":"https://openai.com/blog/solving-rubiks-cube/#fn5","https://openai.com/blog/improving-verifiability/_mailto:gretchen@openai.com":"mailto:gretchen@openai.com","https://openai.com/blog/solving-rubiks-cube/_#fn4":"https://openai.com/blog/solving-rubiks-cube/#fn4","https://openai.com/blog/solving-rubiks-cube/_#fn3":"https://openai.com/blog/solving-rubiks-cube/#fn3","https://openai.com/blog/solving-rubiks-cube/_#fn2":"https://openai.com/blog/solving-rubiks-cube/#fn2","https://openai.com/blog/image-gpt/_#experimentalresults":"https://openai.com/blog/image-gpt/#experimentalresults","https://openai.com/blog/image-gpt/_#fnref6":"https://openai.com/blog/image-gpt/#fnref6","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#alethea":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#alethea","https://openai.com/blog/image-gpt/_#fnref5":"https://openai.com/blog/image-gpt/#fnref5","https://openai.com/blog/image-gpt/_#fnref4":"https://openai.com/blog/image-gpt/#fnref4","https://openai.com/blog/image-gpt/_#fnref3":"https://openai.com/blog/image-gpt/#fnref3","https://openai.com/blog/image-gpt/_#fnref2":"https://openai.com/blog/image-gpt/#fnref2","https://openai.com/blog/image-gpt/_#fnref1":"https://openai.com/blog/image-gpt/#fnref1","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#cathy":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#cathy","https://openai.com/blog/improving-verifiability/_mailto:miles@openai.com":"mailto:miles@openai.com","https://openai.com/blog/gpt-2-1-5b-release/_#fn1":"https://openai.com/blog/gpt-2-1-5b-release/#fn1","https://openai.com/blog/gpt-2-1-5b-release/_#fn2":"https://openai.com/blog/gpt-2-1-5b-release/#fn2","https://openai.com/blog/solving-rubiks-cube/_#fnref5":"https://openai.com/blog/solving-rubiks-cube/#fnref5","https://openai.com/blog/image-gpt/_#conclusion":"https://openai.com/blog/image-gpt/#conclusion","https://openai.com/blog/improving-verifiability/_mailto:hb492@cam.ac.uk":"mailto:hb492@cam.ac.uk","https://openai.com/blog/jukebox/_#fn1":"https://openai.com/blog/jukebox/#fn1","https://openai.com/blog/image-gpt/_#fnref7":"https://openai.com/blog/image-gpt/#fnref7","https://openai.com/blog/solving-rubiks-cube/_#fnref4":"https://openai.com/blog/solving-rubiks-cube/#fnref4","https://openai.com/blog/solving-rubiks-cube/_#fnref3":"https://openai.com/blog/solving-rubiks-cube/#fnref3","https://openai.com/blog/solving-rubiks-cube/_#fnref2":"https://openai.com/blog/solving-rubiks-cube/#fnref2","https://openai.com/blog/solving-rubiks-cube/_#fnref1":"https://openai.com/blog/solving-rubiks-cube/#fnref1","https://openai.com/blog/safety-gym/_#fn1":"https://openai.com/blog/safety-gym/#fn1","https://openai.com/blog/image-gpt/_#limitations":"https://openai.com/blog/image-gpt/#limitations","https://openai.com/blog/image-gpt/_#fromlanguagegpttoimagegpt":"https://openai.com/blog/image-gpt/#fromlanguagegpttoimagegpt","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#kata":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#kata","https://openai.com/blog/improving-verifiability/_mailto:sa478@cam.ac.uk":"mailto:sa478@cam.ac.uk","https://openai.com/blog/image-gpt/_#completions":"https://openai.com/blog/image-gpt/#completions","https://openai.com/blog/improving-verifiability/_mailto:jasminewang76@gmail.com":"mailto:jasminewang76@gmail.com","https://openai.com/blog/jukebox/_#approach":"https://openai.com/blog/jukebox/#approach","https://openai.com/blog/jukebox/_mailto:jukebox@openai.com":"mailto:jukebox@openai.com","https://openai.com/blog/image-gpt/_#fn2":"https://openai.com/blog/image-gpt/#fn2","https://openai.com/blog/jukebox/_#limitations":"https://openai.com/blog/jukebox/#limitations","https://openai.com/blog/image-gpt/_#samples":"https://openai.com/blog/image-gpt/#samples","https://openai.com/blog/image-gpt/_#fn3":"https://openai.com/blog/image-gpt/#fn3","https://openai.com/blog/ai-and-efficiency/_#fn7":"https://openai.com/blog/ai-and-efficiency/#fn7","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#pamela":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#pamela","https://openai.com/blog/ai-and-efficiency/_#fn6":"https://openai.com/blog/ai-and-efficiency/#fn6","https://openai.com/blog/image-gpt/_#fn1":"https://openai.com/blog/image-gpt/#fn1","https://openai.com/blog/ai-and-efficiency/_#fn5":"https://openai.com/blog/ai-and-efficiency/#fn5","https://openai.com/blog/ai-and-efficiency/_#fn4":"https://openai.com/blog/ai-and-efficiency/#fn4","https://openai.com/blog/ai-and-efficiency/_#fn3":"https://openai.com/blog/ai-and-efficiency/#fn3","https://openai.com/blog/ai-and-efficiency/_#fn2":"https://openai.com/blog/ai-and-efficiency/#fn2","https://openai.com/blog/procgen-minerl-competitions/_#fnref1":"https://openai.com/blog/procgen-minerl-competitions/#fnref1","https://openai.com/blog/ai-and-efficiency/_#fn1":"https://openai.com/blog/ai-and-efficiency/#fn1","https://openai.com/blog/solving-rubiks-cube/_#perturbations":"https://openai.com/blog/solving-rubiks-cube/#perturbations","https://openai.com/blog/improving-verifiability/_#authors":"https://openai.com/blog/improving-verifiability/#authors","https://openai.com/blog/image-gpt/_#towardsgeneralunsupervisedlearning":"https://openai.com/blog/image-gpt/#towardsgeneralunsupervisedlearning","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#andre":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#andre","https://openai.com/blog/image-gpt/_#igpt-xl":"https://openai.com/blog/image-gpt/#igpt-xl","https://openai.com/blog/image-gpt/_#fn6":"https://openai.com/blog/image-gpt/#fn6","https://openai.com/blog/image-gpt/_#fn7":"https://openai.com/blog/image-gpt/#fn7","https://openai.com/blog/image-gpt/_#fn4":"https://openai.com/blog/image-gpt/#fn4","https://openai.com/blog/image-gpt/_#fn5":"https://openai.com/blog/image-gpt/#fn5","https://openai.com/blog/openai-scholars-spring-2020-final-projects/_#kamal":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/#kamal","https://openai.com/blog/gpt-2-1-5b-release/_#fnref2":"https://openai.com/blog/gpt-2-1-5b-release/#fnref2","https://openai.com/blog/jukebox/_#fnref1":"https://openai.com/blog/jukebox/#fnref1","https://openai.com/blog/gpt-2-1-5b-release/_#fnref1":"https://openai.com/blog/gpt-2-1-5b-release/#fnref1","https://openai.com/blog/image-gpt/_#bit-l":"https://openai.com/blog/image-gpt/#bit-l","https://openai.com/blog/jukebox/_#timeline":"https://openai.com/blog/jukebox/#timeline"},"extra111_proxyServerAndStatMap":{"http://europe-56.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-037.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe68.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-002.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe-60.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-52.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-23.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-017.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe64.herokuapp.com/":{"failCount":0,"successCount":4,"resultList":[200,200,200,200]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-013.herokuapp.com/":{"failCount":0,"successCount":4,"resultList":[200,200,200,200]},"http://us-018.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-24.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-55.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe65.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-034.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[404,200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://europe-58.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-022.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-030.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-026.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-001.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-031.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-010.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-014.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/07/openai-scholars-gradient-horizontal-stacked.png","sourceStatusCode":200,"destWidth":1600,"destHeight":900,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn53@2020_4/2020/08/24/22-09-05-274_d9cd4cb80ae851ff.webp","sourceBytes":171312,"destBytes":38552,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1018,"convertSpendMs":103,"createdTime":"2020-08-25 06:09:05","host":"us-52*","referer":"https://openai.com/blog/openai-scholars-spring-2020-final-projects/","linkMd5ListStr":"b72be7dda75994ea5d073ce47d8f5294,b72be7dda75994ea5d073ce47d8f5294","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"167.3 KB","destSize":"37.6 KB","compressRate":"22.5%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/05/ai-and-efficiency-social.png","sourceStatusCode":200,"destWidth":2400,"destHeight":1256,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn62@2020_6/2020/08/24/22-09-05-365_7247c2b124142321.webp","sourceBytes":167874,"destBytes":45272,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1184,"convertSpendMs":189,"createdTime":"2020-08-25 06:09:05","host":"us-017*","referer":"https://openai.com/blog/ai-and-efficiency/","linkMd5ListStr":"c3d26b8fa15978939bc952a0e1d23591,c3d26b8fa15978939bc952a0e1d23591","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"163.9 KB","destSize":"44.2 KB","compressRate":"27%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/02/openai-pytorch-vertical.png","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn74@2020_1/2020/08/24/22-09-05-311_dc251f2eafc26d10.webp","sourceBytes":36479,"destBytes":20938,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1151,"convertSpendMs":28,"createdTime":"2020-08-25 06:09:05","host":"europe-56*","referer":"https://openai.com/blog/openai-pytorch/","linkMd5ListStr":"1260981f8f5edfc1c6516e2901c9d5ae,1260981f8f5edfc1c6516e2901c9d5ae","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"35.6 KB","destSize":"20.4 KB","compressRate":"57.4%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/OPEN_AI_DOTA_FINALS_3_DSC1231.JPG","sourceStatusCode":200,"destWidth":2000,"destHeight":1333,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn46@2020_6/2020/08/24/22-09-05-306_8a441a626ecf2d01.webp","sourceBytes":366714,"destBytes":204738,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1281,"convertSpendMs":106,"createdTime":"2020-08-25 06:09:05","host":"us-001*","referer":"https://openai.com/projects/five/","linkMd5ListStr":"006992e573a7d2cc169071f39b466593,006992e573a7d2cc169071f39b466593","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"358.1 KB","destSize":"199.9 KB","compressRate":"55.8%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/08/api-poster.jpg","sourceStatusCode":200,"destWidth":1800,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn57@2020_2/2020/08/24/22-09-05-270_8fe7c868834add25.webp","sourceBytes":165873,"destBytes":142018,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1282,"convertSpendMs":93,"createdTime":"2020-08-25 06:09:05","host":"us-030*","referer":"https://openai.com/blog/openai-api/","linkMd5ListStr":"7cbfa891ea212d1291308cddf7c70c55,7cbfa891ea212d1291308cddf7c70c55","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"162 KB","destSize":"138.7 KB","compressRate":"85.6%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/04/microscope-social_4-8a.jpg","sourceStatusCode":200,"destWidth":1600,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn98@2020_4/2020/08/24/22-09-05-296_ebfe0e5070af7744.webp","sourceBytes":323052,"destBytes":235586,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1369,"convertSpendMs":92,"createdTime":"2020-08-25 06:09:05","host":"us-038*","referer":"https://openai.com/blog/microscope/","linkMd5ListStr":"d19c175ab490b478d46e6df4c1bb3dc7,d19c175ab490b478d46e6df4c1bb3dc7","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"315.5 KB","destSize":"230.1 KB","compressRate":"72.9%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/11/gpt-update_11-4b.jpg","sourceStatusCode":200,"destWidth":2000,"destHeight":833,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn66@2020_5/2020/08/24/22-09-05-353_976bc3ee70c66275.webp","sourceBytes":414538,"destBytes":325310,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1419,"convertSpendMs":187,"createdTime":"2020-08-25 06:09:05","host":"us-005*","referer":"https://openai.com/blog/gpt-2-1-5b-release/","linkMd5ListStr":"ddd4109bea62c13a4ef5ef3630d8102e,ddd4109bea62c13a4ef5ef3630d8102e","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"404.8 KB","destSize":"317.7 KB","compressRate":"78.5%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/04/2x-no-mark-1.jpg","sourceStatusCode":200,"destWidth":1276,"destHeight":1696,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn77@2020_5/2020/08/24/22-09-05-383_9a99f33aae6cc85e.webp","sourceBytes":329167,"destBytes":330270,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1444,"convertSpendMs":127,"createdTime":"2020-08-25 06:09:05","host":"us-034*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e,84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"321.5 KB","destSize":"322.5 KB","compressRate":"100.3%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/06/completions-3x5-clean-1.png","sourceStatusCode":200,"destWidth":1145,"destHeight":687,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn94@2020_4/2020/08/24/22-09-05-340_aa8ef6f38bbafcef.webp","sourceBytes":130502,"destBytes":86534,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1538,"convertSpendMs":36,"createdTime":"2020-08-25 06:09:05","host":"europe-60*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"127.4 KB","destSize":"84.5 KB","compressRate":"66.3%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate1.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn82@2020_1/2020/08/24/22-09-05-802_94a67421a27b5482.webp","sourceBytes":231127,"destBytes":225140,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1791,"convertSpendMs":576,"createdTime":"2020-08-25 06:09:05","host":"us-021*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336,352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"225.7 KB","destSize":"219.9 KB","compressRate":"97.4%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/11/safety-gym-cover.png","sourceStatusCode":200,"destWidth":1920,"destHeight":1120,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn69@2020_2/2020/08/24/22-09-05-664_3e9e50d02b910f8e.webp","sourceBytes":184826,"destBytes":66860,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1681,"convertSpendMs":53,"createdTime":"2020-08-25 06:09:05","host":"europe68*","referer":"https://openai.com/blog/safety-gym/","linkMd5ListStr":"9702d253f96205db69ffac519550137e,9702d253f96205db69ffac519550137e","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"180.5 KB","destSize":"65.3 KB","compressRate":"36.2%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/og-image.jpg","sourceStatusCode":200,"destWidth":1804,"destHeight":1082,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn86@2020_4/2020/08/24/22-09-05-644_0c7dba03dbb35a3c.webp","sourceBytes":434743,"destBytes":307400,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1848,"convertSpendMs":410,"createdTime":"2020-08-25 06:09:05","host":"us-009*","referer":"https://openai.com/blog/procgen-benchmark/","linkMd5ListStr":"7b1f49144e8f08b38626c454b9087deb,7b1f49144e8f08b38626c454b9087deb","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"424.6 KB","destSize":"300.2 KB","compressRate":"70.7%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/Frame-1--3-.png","sourceStatusCode":200,"destWidth":2270,"destHeight":1130,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn49@2020_1/2020/08/24/22-09-05-479_2ba7b23a226055b6.webp","sourceBytes":107872,"destBytes":237238,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1985,"convertSpendMs":195,"createdTime":"2020-08-25 06:09:05","host":"europe64*","referer":"https://openai.com/blog/deep-double-descent/","linkMd5ListStr":"309259fcaa8289b1c6ef46dffd6a76f9,309259fcaa8289b1c6ef46dffd6a76f9","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"105.3 KB","destSize":"231.7 KB","compressRate":"219.9%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/10/twitter-og.jpg","sourceStatusCode":200,"destWidth":1920,"destHeight":1080,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn90@2020_4/2020/08/24/22-09-05-657_51964eb0b5945302.webp","sourceBytes":208788,"destBytes":141000,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":2174,"convertSpendMs":61,"createdTime":"2020-08-25 06:09:05","host":"europe-22*","referer":"https://openai.com/blog/solving-rubiks-cube/","linkMd5ListStr":"b09762d9ff3f5b1e20cfb8a36810dd5f,b09762d9ff3f5b1e20cfb8a36810dd5f","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"203.9 KB","destSize":"137.7 KB","compressRate":"67.5%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn46@2020_4/2020/08/24/22-09-07-495_cf41837b8967f506.webp","sourceBytes":2653,"destBytes":962,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":802,"convertSpendMs":12,"createdTime":"2020-08-25 06:09:07","host":"us-021*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"962 B","compressRate":"36.3%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn56@2020_2/2020/08/24/22-09-07-583_abd232dcba60c07a.webp","sourceBytes":2447,"destBytes":1444,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":840,"convertSpendMs":7,"createdTime":"2020-08-25 06:09:07","host":"us-005*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 KB","destSize":"1.4 KB","compressRate":"59%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn6@2020_6/2020/08/24/22-09-07-547_e60167c3e45deed2.webp","sourceBytes":3010,"destBytes":1118,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":849,"convertSpendMs":2,"createdTime":"2020-08-25 06:09:07","host":"us-034*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 KB","destSize":"1.1 KB","compressRate":"37.1%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn38@2020_6/2020/08/24/22-09-07-584_6a289928470ebcb1.webp","sourceBytes":2639,"destBytes":1480,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":844,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:07","host":"us-001*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"1.4 KB","compressRate":"56.1%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn69@2020_2/2020/08/24/22-09-07-577_6b7fd48ecba7a45b.webp","sourceBytes":2424,"destBytes":1750,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":868,"convertSpendMs":5,"createdTime":"2020-08-25 06:09:07","host":"us-013*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 KB","destSize":"1.7 KB","compressRate":"72.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/overview-3.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn29@2020_3/2020/08/24/22-09-07-576_6588c3b9703f23f3.svg","sourceBytes":1120,"destBytes":1120,"feedId":4773,"totalSpendMs":869,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:07","host":"us-005*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.1 KB","destSize":"1.1 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn18@2020_1/2020/08/24/22-09-07-639_dbe9e0c982239eca.webp","sourceBytes":2679,"destBytes":1508,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":906,"convertSpendMs":44,"createdTime":"2020-08-25 06:09:07","host":"us-52*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"1.5 KB","compressRate":"56.3%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/fig_data_hurts.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn86@2020_4/2020/08/24/22-09-07-514_5054b0b04ac75889.svg","sourceBytes":45113,"destBytes":45113,"feedId":4773,"totalSpendMs":908,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:07","host":"us-030*","referer":"https://openai.com/blog/deep-double-descent/","linkMd5ListStr":"309259fcaa8289b1c6ef46dffd6a76f9","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"44.1 KB","destSize":"44.1 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn81@2020_6/2020/08/24/22-09-07-631_2c945152fff06e92.webp","sourceBytes":3995,"destBytes":1490,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":910,"convertSpendMs":32,"createdTime":"2020-08-25 06:09:07","host":"us-52*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.9 KB","destSize":"1.5 KB","compressRate":"37.3%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/lyrics-attention.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn61@2020_2/2020/08/24/22-09-07-558_a67d194b385529c1.svg","sourceBytes":55039,"destBytes":55039,"feedId":4773,"totalSpendMs":1001,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:07","host":"us-038*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"53.7 KB","destSize":"53.7 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn34@2020_2/2020/08/24/22-09-07-609_a785ace9dd21842c.webp","sourceBytes":2980,"destBytes":1102,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":990,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:07","host":"us-013*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 KB","destSize":"1.1 KB","compressRate":"37%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn98@2020_5/2020/08/24/22-09-07-582_3a76b65c80243a25.webp","sourceBytes":3198,"destBytes":1302,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":873,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:07","host":"europe68*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.1 KB","destSize":"1.3 KB","compressRate":"40.7%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn45@2020_1/2020/08/24/22-09-07-643_1b5a9220c077b479.webp","sourceBytes":2357,"destBytes":1534,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1006,"convertSpendMs":47,"createdTime":"2020-08-25 06:09:07","host":"us-52*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.3 KB","destSize":"1.5 KB","compressRate":"65.1%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn2@2020_5/2020/08/24/22-09-07-657_a14f41e9cacddb1a.webp","sourceBytes":2457,"destBytes":1394,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1019,"convertSpendMs":35,"createdTime":"2020-08-25 06:09:07","host":"us-026*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 KB","destSize":"1.4 KB","compressRate":"56.7%"},{"code":1,"isDone":false,"source":"https://openai.com/blog/image-gpt/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNiYAAAAAkAAxkR2eQAAAAASUVORK5CYII=","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":4773,"totalSpendMs":268,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:08","host":"europe-58*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/waveforms/1-original.png","sourceStatusCode":200,"destWidth":1440,"destHeight":80,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn14@2020_5/2020/08/24/22-09-07-749_d1269f97b720bfeb.webp","sourceBytes":2707,"destBytes":6540,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1017,"convertSpendMs":80,"createdTime":"2020-08-25 06:09:07","host":"us-009*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"6.4 KB","compressRate":"241.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/overview-arrow.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn14@2020_5/2020/08/24/22-09-07-586_3ff9695ed46776cd.svg","sourceBytes":512,"destBytes":512,"feedId":4773,"totalSpendMs":983,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:07","host":"europe64*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e,84bcb665b077b620d0a3fe04d556418e,84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"512 B","destSize":"512 B","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn37@2020_5/2020/08/24/22-09-07-619_017fb5ec931b5960.webp","sourceBytes":2604,"destBytes":1740,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":917,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:07","host":"europe-56*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.7 KB","compressRate":"66.8%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn33@2020_6/2020/08/24/22-09-07-561_007b96ad1e73d606.webp","sourceBytes":2706,"destBytes":1252,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":980,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:07","host":"europe68*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"1.2 KB","compressRate":"46.3%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn53@2020_6/2020/08/24/22-09-07-605_b235bc48bc5a8bb3.webp","sourceBytes":2840,"destBytes":1406,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":991,"convertSpendMs":2,"createdTime":"2020-08-25 06:09:07","host":"europe-22*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 KB","destSize":"1.4 KB","compressRate":"49.5%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/modeldd.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn22@2020_2/2020/08/24/22-09-07-566_6a9ed98f6074ec0d.svg","sourceBytes":48860,"destBytes":48860,"feedId":4773,"totalSpendMs":1047,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:07","host":"us-016*","referer":"https://openai.com/blog/deep-double-descent/","linkMd5ListStr":"309259fcaa8289b1c6ef46dffd6a76f9","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"47.7 KB","destSize":"47.7 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn5@2020_6/2020/08/24/22-09-07-587_54a7a78336ae041f.webp","sourceBytes":1611,"destBytes":1156,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1072,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:07","host":"us-013*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.6 KB","destSize":"1.1 KB","compressRate":"71.8%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/waveforms/1-novel.png","sourceStatusCode":200,"destWidth":1440,"destHeight":80,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn78@2020_2/2020/08/24/22-09-07-604_be7576aa9ba7268f.webp","sourceBytes":2821,"destBytes":6756,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":932,"convertSpendMs":8,"createdTime":"2020-08-25 06:09:07","host":"europe64*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 KB","destSize":"6.6 KB","compressRate":"239.5%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn65@2020_3/2020/08/24/22-09-07-689_c5a28d4efb6779ee.webp","sourceBytes":2904,"destBytes":976,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1076,"convertSpendMs":59,"createdTime":"2020-08-25 06:09:07","host":"us-026*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 KB","destSize":"976 B","compressRate":"33.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/solving-rubiks-cube/images/openai-robotics-rubiks-prototypes.jpg","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn24@2020_3/2020/08/24/22-09-07-655_f092c34125137ea6.webp","sourceBytes":74730,"destBytes":50110,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1116,"convertSpendMs":42,"createdTime":"2020-08-25 06:09:07","host":"us-017*","referer":"https://openai.com/blog/solving-rubiks-cube/","linkMd5ListStr":"b09762d9ff3f5b1e20cfb8a36810dd5f","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"73 KB","destSize":"48.9 KB","compressRate":"67.1%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn17@2020_5/2020/08/24/22-09-07-593_d817fec25280f63b.webp","sourceBytes":4460,"destBytes":1614,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":997,"convertSpendMs":2,"createdTime":"2020-08-25 06:09:07","host":"europe-22*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.4 KB","destSize":"1.6 KB","compressRate":"36.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn21@2020_6/2020/08/24/22-09-07-641_634d4a6cd1727e41.webp","sourceBytes":2629,"destBytes":1752,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1062,"convertSpendMs":2,"createdTime":"2020-08-25 06:09:07","host":"europe-60*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"1.7 KB","compressRate":"66.6%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/04/feature-vis.jpg","sourceStatusCode":200,"destWidth":1886,"destHeight":1178,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn26@2020_5/2020/08/24/22-09-07-640_a61ad39b2df09aec.webp","sourceBytes":226054,"destBytes":164026,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1196,"convertSpendMs":86,"createdTime":"2020-08-25 06:09:07","host":"us-038*","referer":"https://openai.com/blog/microscope/","linkMd5ListStr":"d19c175ab490b478d46e6df4c1bb3dc7","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"220.8 KB","destSize":"160.2 KB","compressRate":"72.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/solving-rubiks-cube/images/hidden-state-color-20191014b.png","sourceStatusCode":200,"destWidth":1500,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn2@2020_2/2020/08/24/22-09-07-639_5a26847ad503acd7.webp","sourceBytes":3210,"destBytes":1368,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1122,"convertSpendMs":2,"createdTime":"2020-08-25 06:09:07","host":"europe-56*","referer":"https://openai.com/blog/solving-rubiks-cube/","linkMd5ListStr":"b09762d9ff3f5b1e20cfb8a36810dd5f","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.1 KB","destSize":"1.3 KB","compressRate":"42.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat3.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn49@2020_2/2020/08/24/22-09-07-829_aeec7daa62f1bfc7.webp","sourceBytes":52567,"destBytes":57606,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1292,"convertSpendMs":229,"createdTime":"2020-08-25 06:09:07","host":"us-030*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"51.3 KB","destSize":"56.3 KB","compressRate":"109.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn72@2020_2/2020/08/24/22-09-08-358_f862c22d1a477ebb.webp","sourceBytes":2491,"destBytes":1180,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":775,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"us-013*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 KB","destSize":"1.2 KB","compressRate":"47.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn67@2020_6/2020/08/24/22-09-08-399_081789c3fd7816f6.webp","sourceBytes":1595,"destBytes":1154,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":843,"convertSpendMs":2,"createdTime":"2020-08-25 06:09:08","host":"us-025*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.6 KB","destSize":"1.1 KB","compressRate":"72.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn83@2020_2/2020/08/24/22-09-08-461_2351d8a587b07cdc.webp","sourceBytes":2995,"destBytes":1332,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":834,"convertSpendMs":4,"createdTime":"2020-08-25 06:09:08","host":"us-55*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 KB","destSize":"1.3 KB","compressRate":"44.5%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn75@2020_3/2020/08/24/22-09-08-460_7564307594d4a4aa.webp","sourceBytes":4289,"destBytes":1510,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":867,"convertSpendMs":5,"createdTime":"2020-08-25 06:09:08","host":"us-002*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.2 KB","destSize":"1.5 KB","compressRate":"35.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-29-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn63@2020_4/2020/08/24/22-09-08-368_b1b7f27faec2c37b.webp","sourceBytes":2593,"destBytes":1166,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":927,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"us-037*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.1 KB","compressRate":"45%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn92@2020_2/2020/08/24/22-09-08-456_3b0b1255c3d28b53.webp","sourceBytes":2614,"destBytes":1766,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":831,"convertSpendMs":13,"createdTime":"2020-08-25 06:09:08","host":"us-021*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"1.7 KB","compressRate":"67.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/overview-2.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn88@2020_5/2020/08/24/22-09-08-532_9db855dcb9b222a7.svg","sourceBytes":1120,"destBytes":1120,"feedId":4773,"totalSpendMs":857,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:08","host":"us-033*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.1 KB","destSize":"1.1 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn12@2020_4/2020/08/24/22-09-08-527_a23a904b5f5b6698.webp","sourceBytes":2642,"destBytes":1780,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":792,"convertSpendMs":4,"createdTime":"2020-08-25 06:09:08","host":"us-030*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"1.7 KB","compressRate":"67.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn8@2020_2/2020/08/24/22-09-08-527_0e97fab3357bdab3.webp","sourceBytes":2882,"destBytes":970,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":847,"convertSpendMs":9,"createdTime":"2020-08-25 06:09:08","host":"us-52*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 KB","destSize":"970 B","compressRate":"33.7%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/epoch_test.png","sourceStatusCode":200,"destWidth":2400,"destHeight":1460,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn49@2020_6/2020/08/24/22-09-08-112_5bd77856d3258ead.webp","sourceBytes":93401,"destBytes":347012,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1875,"convertSpendMs":578,"createdTime":"2020-08-25 06:09:07","host":"us-009*","referer":"https://openai.com/blog/deep-double-descent/","linkMd5ListStr":"309259fcaa8289b1c6ef46dffd6a76f9","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"91.2 KB","destSize":"338.9 KB","compressRate":"371.5%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-32-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn16@2020_6/2020/08/24/22-09-08-604_fdca3bfe5e2e8a98.webp","sourceBytes":2647,"destBytes":970,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":845,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"us-018*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.6 KB","destSize":"970 B","compressRate":"36.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival4.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn54@2020_6/2020/08/24/22-09-08-350_edf0bfa580384ae0.webp","sourceBytes":168174,"destBytes":196518,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1899,"convertSpendMs":448,"createdTime":"2020-08-25 06:09:07","host":"us-004*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"164.2 KB","destSize":"191.9 KB","compressRate":"116.9%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn20@2020_1/2020/08/24/22-09-08-605_1d82f80c2d2a7d8c.webp","sourceBytes":3945,"destBytes":1432,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":868,"convertSpendMs":4,"createdTime":"2020-08-25 06:09:08","host":"us-008*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.9 KB","destSize":"1.4 KB","compressRate":"36.3%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2019/12/epoch_train.png","sourceStatusCode":200,"destWidth":2400,"destHeight":1460,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn30@2020_6/2020/08/24/22-09-08-228_12c68bf716e27a4b.webp","sourceBytes":80589,"destBytes":330614,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1942,"convertSpendMs":657,"createdTime":"2020-08-25 06:09:07","host":"us-026*","referer":"https://openai.com/blog/deep-double-descent/","linkMd5ListStr":"309259fcaa8289b1c6ef46dffd6a76f9","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"78.7 KB","destSize":"322.9 KB","compressRate":"410.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn55@2020_1/2020/08/24/22-09-08-674_6687019aee196929.webp","sourceBytes":1914,"destBytes":1378,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":867,"convertSpendMs":4,"createdTime":"2020-08-25 06:09:08","host":"us-54*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.9 KB","destSize":"1.3 KB","compressRate":"72%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate4.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn74@2020_3/2020/08/24/22-09-08-437_4e79865946d7be21.webp","sourceBytes":186610,"destBytes":194610,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1980,"convertSpendMs":546,"createdTime":"2020-08-25 06:09:07","host":"us-001*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"182.2 KB","destSize":"190 KB","compressRate":"104.3%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-16-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn48@2020_1/2020/08/24/22-09-08-597_0f386417818c90c1.webp","sourceBytes":2533,"destBytes":1784,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":863,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"europe64*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.7 KB","compressRate":"70.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn63@2020_2/2020/08/24/22-09-08-710_bc25dee7276c12ce.webp","sourceBytes":3286,"destBytes":1290,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":906,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"us-022*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.2 KB","destSize":"1.3 KB","compressRate":"39.3%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn72@2020_2/2020/08/24/22-09-08-715_e5f125cede0590b6.webp","sourceBytes":2580,"destBytes":1682,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":829,"convertSpendMs":4,"createdTime":"2020-08-25 06:09:08","host":"us-002*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.6 KB","compressRate":"65.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/vqvae-1.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn31@2020_1/2020/08/24/22-09-08-566_95c0e1de807a64ff.svg","sourceBytes":77130,"destBytes":77130,"feedId":4773,"totalSpendMs":999,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:08","host":"us-52*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"75.3 KB","destSize":"75.3 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/02/openai-pytorch.png","sourceStatusCode":200,"destWidth":2400,"destHeight":800,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn44@2020_3/2020/08/24/22-09-08-668_bee0d358bbc22930.webp","sourceBytes":68971,"destBytes":34854,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1006,"convertSpendMs":69,"createdTime":"2020-08-25 06:09:08","host":"us-010*","referer":"https://openai.com/blog/openai-pytorch/","linkMd5ListStr":"1260981f8f5edfc1c6516e2901c9d5ae","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"67.4 KB","destSize":"34 KB","compressRate":"50.5%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-33-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn79@2020_4/2020/08/24/22-09-08-534_b4a8fb607d3b92c7.webp","sourceBytes":4187,"destBytes":1412,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1096,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"europe66*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.1 KB","destSize":"1.4 KB","compressRate":"33.7%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-3.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn40@2020_4/2020/08/24/22-09-08-691_6babd830c1c26575.webp","sourceBytes":1693,"destBytes":1188,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1035,"convertSpendMs":6,"createdTime":"2020-08-25 06:09:08","host":"us-020*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 KB","destSize":"1.2 KB","compressRate":"70.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn28@2020_4/2020/08/24/22-09-08-682_f8526341eb06dd10.webp","sourceBytes":3732,"destBytes":1158,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":946,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"europe63*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.6 KB","destSize":"1.1 KB","compressRate":"31%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-17-orig.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn35@2020_4/2020/08/24/22-09-08-645_98d58afff264004e.webp","sourceBytes":2609,"destBytes":1732,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1109,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"us-031*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1,9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.7 KB","compressRate":"66.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival2.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn9@2020_4/2020/08/24/22-09-08-532_201162c17c179b27.webp","sourceBytes":247452,"destBytes":212852,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":2121,"convertSpendMs":626,"createdTime":"2020-08-25 06:09:07","host":"us-001*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"241.7 KB","destSize":"207.9 KB","compressRate":"86%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-1.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn50@2020_3/2020/08/24/22-09-08-779_a2850ee094470170.webp","sourceBytes":2891,"destBytes":1576,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":996,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"europe65*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 KB","destSize":"1.5 KB","compressRate":"54.5%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-10-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn79@2020_2/2020/08/24/22-09-08-830_b79b38026c18d4c3.webp","sourceBytes":2917,"destBytes":1510,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":888,"convertSpendMs":3,"createdTime":"2020-08-25 06:09:08","host":"europe-56*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 KB","destSize":"1.5 KB","compressRate":"51.8%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/solving-rubiks-cube/images/dr.jpg","sourceStatusCode":200,"destWidth":1500,"destHeight":960,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_5/2020/08/24/22-09-08-852_f7356c7861b8171f.webp","sourceBytes":113195,"destBytes":108046,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1147,"convertSpendMs":72,"createdTime":"2020-08-25 06:09:08","host":"us-012*","referer":"https://openai.com/blog/solving-rubiks-cube/","linkMd5ListStr":"b09762d9ff3f5b1e20cfb8a36810dd5f","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"110.5 KB","destSize":"105.5 KB","compressRate":"95.5%"},{"code":1,"isDone":false,"source":"https://openai.com/content/images/2020/04/models.jpg","sourceStatusCode":200,"destWidth":2000,"destHeight":1250,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn59@2020_2/2020/08/24/22-09-08-803_d173d6961976b8ed.webp","sourceBytes":352869,"destBytes":273210,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1313,"convertSpendMs":116,"createdTime":"2020-08-25 06:09:08","host":"us-033*","referer":"https://openai.com/blog/microscope/","linkMd5ListStr":"d19c175ab490b478d46e6df4c1bb3dc7","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"344.6 KB","destSize":"266.8 KB","compressRate":"77.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-2-0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn91@2020_6/2020/08/24/22-09-09-374_bdd8f69742e12b17.webp","sourceBytes":2523,"destBytes":1412,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":897,"convertSpendMs":9,"createdTime":"2020-08-25 06:09:09","host":"us-014*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.4 KB","compressRate":"56%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/image-gpt/completions/igpt-xl-miscellaneous-12-2.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn95@2020_3/2020/08/24/22-09-09-253_a4db036ebe440238.webp","sourceBytes":4088,"destBytes":1376,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":940,"convertSpendMs":5,"createdTime":"2020-08-25 06:09:09","host":"us-004*","referer":"https://openai.com/blog/image-gpt/","linkMd5ListStr":"9fa06ceea597a649f120b106fe508bd1","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4 KB","destSize":"1.3 KB","compressRate":"33.7%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/jukebox/assets/vqvae-2.svg","sourceStatusCode":200,"destWidth":0,"destHeight":0,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn84@2020_6/2020/08/24/22-09-09-244_e6ad1265d40b9d66.svg","sourceBytes":74630,"destBytes":74630,"feedId":4773,"totalSpendMs":1023,"convertSpendMs":0,"createdTime":"2020-08-25 06:09:09","host":"us-036*","referer":"https://openai.com/blog/jukebox/","linkMd5ListStr":"84bcb665b077b620d0a3fe04d556418e","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"72.9 KB","destSize":"72.9 KB","compressRate":"100%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival3.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn95@2020_5/2020/08/24/22-09-09-227_f2c9ec47ec9058d9.webp","sourceBytes":216599,"destBytes":246208,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1883,"convertSpendMs":351,"createdTime":"2020-08-25 06:09:08","host":"us-010*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"211.5 KB","destSize":"240.4 KB","compressRate":"113.7%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed1.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn89@2020_4/2020/08/24/22-09-09-457_5e1d1164288f53b5.webp","sourceBytes":628816,"destBytes":545294,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":3193,"convertSpendMs":1788,"createdTime":"2020-08-25 06:09:07","host":"us-017*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"614.1 KB","destSize":"532.5 KB","compressRate":"86.7%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate3.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn41@2020_2/2020/08/24/22-09-08-985_2bff4faf6ebb1e88.webp","sourceBytes":347930,"destBytes":215762,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":3152,"convertSpendMs":291,"createdTime":"2020-08-25 06:09:07","host":"europe64*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"339.8 KB","destSize":"210.7 KB","compressRate":"62%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/survival1.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn87@2020_1/2020/08/24/22-09-09-761_d652cd14204ee7d8.webp","sourceBytes":225585,"destBytes":185922,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":1665,"convertSpendMs":253,"createdTime":"2020-08-25 06:09:09","host":"us-025*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"220.3 KB","destSize":"181.6 KB","compressRate":"82.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat4.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn58@2020_3/2020/08/24/22-09-09-364_2d31b4f27551eee7.webp","sourceBytes":146216,"destBytes":149124,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":3316,"convertSpendMs":709,"createdTime":"2020-08-25 06:09:07","host":"europe-60*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"142.8 KB","destSize":"145.6 KB","compressRate":"102%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/navigate2.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn4@2020_6/2020/08/24/22-09-09-803_b3d448de75365716.webp","sourceBytes":209071,"destBytes":151466,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":2649,"convertSpendMs":260,"createdTime":"2020-08-25 06:09:08","host":"europe63*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"204.2 KB","destSize":"147.9 KB","compressRate":"72.4%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed4.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn23@2020_4/2020/08/24/22-09-10-388_6a6b719ca01b44fc.webp","sourceBytes":450407,"destBytes":381140,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":3521,"convertSpendMs":569,"createdTime":"2020-08-25 06:09:08","host":"europe-23*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"439.9 KB","destSize":"372.2 KB","compressRate":"84.6%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat1.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn100@2020_3/2020/08/24/22-09-10-915_d1afe67763540e0a.webp","sourceBytes":490432,"destBytes":565004,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":3963,"convertSpendMs":1168,"createdTime":"2020-08-25 06:09:08","host":"europe-24*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"478.9 KB","destSize":"551.8 KB","compressRate":"115.2%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/omeat2.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn94@2020_4/2020/08/24/22-09-12-875_ad6ecf1ca5184893.webp","sourceBytes":339100,"destBytes":375998,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":6533,"convertSpendMs":5270,"createdTime":"2020-08-25 06:09:07","host":"us-005*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"331.2 KB","destSize":"367.2 KB","compressRate":"110.9%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed3.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn9@2020_6/2020/08/24/22-09-13-410_3fbb1aae443af78e.webp","sourceBytes":736889,"destBytes":874734,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":7382,"convertSpendMs":5829,"createdTime":"2020-08-25 06:09:07","host":"us-021*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"719.6 KB","destSize":"854.2 KB","compressRate":"118.7%"},{"code":1,"isDone":false,"source":"https://cdn.openai.com/procgen-minerl-competitions/minerl/obed2.mp4.gif","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn75@2020_2/2020/08/24/22-09-15-448_04f7cc5fe7549bac.webp","sourceBytes":2271123,"destBytes":2635842,"targetWebpQuality":75,"feedId":4773,"totalSpendMs":9152,"convertSpendMs":4992,"createdTime":"2020-08-25 06:09:08","host":"europe67*","referer":"https://openai.com/blog/procgen-minerl-competitions/","linkMd5ListStr":"352ecd9e9376ddbc4cf6744344f96336","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.2 MB","destSize":"2.5 MB","compressRate":"116.1%"}],"successGithubMap":{"myreaderx8":3,"myreaderx14":3,"myreaderx15":3,"myreaderx7":3,"myreaderx6":4,"myreaderx16":3,"myreaderx4":4,"myreaderx10":3,"myreaderx32":3,"myreaderx3":2,"myreaderx33":3,"myreaderx11":3,"myreaderx12":3,"myreaderx2":4,"myreaderx13":3,"myreaderx1":3,"myreaderx30":3,"myreaderx31":3,"myreaderx18":3,"myreaderx19":3,"myreaderx":2,"myreaderx25":3,"myreaderx27":3,"myreaderx21":4,"myreaderx22":2,"myreaderx23":2,"myreaderx24":3,"myreaderx5oss":3,"myreaderx29":3},"failGithubMap":{}}