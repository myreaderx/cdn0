{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-10-13 22:28:40","updatedTime":"2020-10-13 22:28:40","title":"顔画像生成のためのデータセットを作る","link":"https://memo.sugyan.com/entry/2020/01/20/001258","description":"<h3>動機</h3>\n\n<p>TensorFlowの登場をきっかけに <a href=\"https://memo.sugyan.com/search?q=%E3%82%A2%E3%82%A4%E3%83%89%E3%83%AB+%E9%A1%94%E8%AD%98%E5%88%A5\">機械学習によるアイドル顔識別</a> という取り組みをしていて、3年以上かけてコツコツとアイドルの自撮りを収集してラベルをつけてデー<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8\">タセット</a>を作ってきたけど、 <a href=\"https://blog.sugyan.com/entry/2017/07/19/011648\">アイドルヲタクはもう辞めてしまって</a> 現場にも全然行かなくなり、卒業・脱退の情報を追いながらラベルを更新していく作業を続ける情熱はすっかり薄れてしまった。\nもうアイドル顔識別プロジェクトは終了にしよう、と思った。</p>\n\n<p>しかし折角今まで集めたデータを捨ててしまうのは勿体無い。せめて最後に何か活用できないものか。\nと考えて、「画像生成」に再び取り組んでみることにした。</p>\n\n<p>過去に試したことはあったけど、それほど上手くはいっていない。</p>\n\n<ul>\n<li><a href=\"https://memo.sugyan.com/entry/20160516/1463359395\">TensorFlowによるDCGANでアイドルの顔画像生成</a></li>\n<li><a href=\"https://memo.sugyan.com/entry/2016/10/12/084751\">TensorFlowによるDCGANでアイドルの顔画像生成 その後の実験など</a></li>\n</ul>\n\n\n<p>この記事を書いたのが2016年。\nこの後の数年だけでもGANの技術はすさまじく進歩していて、今や <code>1024x1024</code> のような高解像度の 写真と見分けがつかないくらいの綺麗な顔画像を生成できるようになったらしい。是非とも試してみたいところだ。</p>\n\n<h3>目標</h3>\n\n<p>PGGAN (<a href=\"https://github.com/tkarras/progressive_growing_of_gans\">Progressive Growing of GANs</a>) や <a href=\"https://github.com/NVlabs/stylegan\">StyleGAN</a> で使われた <code>CelebA-HQ</code> datasetは、<code>1024x1024</code> サイズの高解像度の画像を <code>30,000</code> 枚用意して作られているようだ。</p>\n\n<p>今回はそこまでいかなくとも、せめて <code>512x512</code> の画像を <code>10,000</code> 枚くらいは集めたい。</p>\n\n<h4>設計の失敗</h4>\n\n<p>しかし自分がアイドル顔識別のために収集してラベル付けしたデー<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8\">タセット</a>は、投稿された自撮り画像から顔領域を検出し <code>96x96</code> にリサイズして切り抜いたもの<strong>だけ</strong>しか保存していなかった。\nあまりストレージに余裕が無くケチった運用をしていたため、元の高解像度の画像を<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%AF%A5%E9%A5%A6%A5%C9\">クラウド</a>上に残しておくなどをまったくしていなかった。\nつまり <code>96x96</code> よりも高解像度の顔画像は手に入らない…。</p>\n\n<h3>集め直し</h3>\n\n<h4>DBから候補となる画像URLを抽出</h4>\n\n<p>とはいえ、手元には「元画像のURL」「元画像にひもづいた、抽出された顔画像」「顔画像に対するラベル」のデータは残っている。</p>\n\n<ul>\n<li>各アイドルの<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/Twitter\">Twitter</a>から取得した画像情報 <code>1,654,503</code> 件</li>\n<li>自作の検出器で検出して抽出した顔画像 <code>2,158,681</code> 件\n\n<ul>\n<li>そのうち、人力の手作業でラベル付けしたもの <code>204,791</code> 件</li>\n</ul>\n</li>\n</ul>\n\n\n<p>などが、自分が3年以上かけて続けた<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%A2%A5%CE%A5%C6%A1%BC%A5%B7%A5%E7%A5%F3\">アノテーション</a>作業の成果だ。</p>\n\n<p>高解像度のアイドル顔画像デー<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8\">タセット</a>を構築するためには、resize &amp; crop する前の元画像を取得しなおして、今度は解像度を保ったままで顔領域を抽出しなおせば良い。</p>\n\n<p>目当ての「アイドルの自撮り顔画像」だけを選別するには、</p>\n\n<ul>\n<li>写真の中に1枚だけ顔が検出されている\n\n<ul>\n<li>→ 集合写真などではない単独の自撮りで 高解像度で写っている可能性が高い</li>\n</ul>\n</li>\n<li>その顔画像が正しくアイドルとしてラベル付けされている\n\n<ul>\n<li>→ 顔検出されていても誤検出が一定割合で起きているし、認識対象外のラベル付けをしていたりするので、それらを除外する</li>\n</ul>\n</li>\n</ul>\n\n\n<p>という条件のものを抽出すればできるはず。</p>\n\n<pre class=\"code lang-sql\" data-lang=\"sql\" data-unlink><span class=\"synStatement\">SELECT</span>\n    faces.id,\n    photos.id, photos.source_url, photos.photo_url, photos.posted_at,\n    labels.id, labels.name\n<span class=\"synSpecial\">FROM</span> faces\n    <span class=\"synSpecial\">INNER</span> <span class=\"synSpecial\">JOIN</span> photos <span class=\"synSpecial\">ON</span> photos.id = faces.photo_id\n    <span class=\"synSpecial\">INNER</span> <span class=\"synSpecial\">JOIN</span> labels <span class=\"synSpecial\">ON</span> labels.id = faces.label_id\n<span class=\"synSpecial\">WHERE</span>\n    photos.id <span class=\"synStatement\">in</span> (\n        <span class=\"synStatement\">SELECT</span>\n            photos.id\n        <span class=\"synSpecial\">FROM</span> faces\n            <span class=\"synSpecial\">INNER</span> <span class=\"synSpecial\">JOIN</span> photos <span class=\"synSpecial\">ON</span> photos.id = faces.photo_id\n        <span class=\"synSpecial\">WHERE</span> faces.label_id <span class=\"synSpecial\">IS</span> <span class=\"synStatement\">NOT</span> <span class=\"synSpecial\">NULL</span>\n        <span class=\"synSpecial\">GROUP</span> <span class=\"synSpecial\">BY</span> photos.id\n        <span class=\"synSpecial\">HAVING</span> <span class=\"synIdentifier\">COUNT</span>(faces.id) = <span class=\"synConstant\">1</span>\n    )\n<span class=\"synSpecial\">ORDER</span> <span class=\"synSpecial\">BY</span> faces.updated_at <span class=\"synSpecial\">DESC</span>\n</pre>\n\n\n<p>こうして、「おそらくアイドルが単独で写っていたであろう元画像」<code>196,455</code> 枚のURLを取得できた。</p>\n\n<p>しかし 画像URLが取得できていても、それを投稿したアイドルさんが卒業・解散などの後に<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/Twitter\">Twitter</a>アカウントが削除されたり非公開になっていたりすると、もうその画像は参照できなくなってしまう。</p>\n\n<p>実際に取得を試みてダウンロードできたのは このうち <code>132,513</code> 件だった。</p>\n\n<p>ちょうど休眠アカウント削除というのが最近ニュースになった。卒業後に残っているアイドルのアカウントたちはどうなってしまうのだろうか…。今のうちに画像だけでも取得しておくことが出来て良かったのかもしれない。</p>\n\n<h3>Dlibによる単一顔検出</h3>\n\n<p>さて、高解像度(といっても <code>900x1200</code> 程度だけど)の アイドルさんたちの画像を入手することが出来た。</p>\n\n<p>以前はここから <a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/OpenCV\">OpenCV</a> の <a href=\"https://docs.opencv.org/master/db/d28/tutorial_cascade_classifier.html\">Haar Feature-based Cascade Classifiers</a> を使って顔検出し、その領域を resize &amp; crop してデータとして使っていた。\nまた、アイドルの自撮りの特徴として「斜めに傾いて写っているもの」が多く検出しづらい問題があり、それを考慮して回転補正をかけて検出するという仕組みを自作していた。</p>\n\n<ul>\n<li><a href=\"https://memo.sugyan.com/entry/20151203/1449137219\">Heroku + OpenCVで簡易顔検出API</a></li>\n</ul>\n\n\n<p>今回も同様の検出をすることになるが、より高精度に また目・口の位置も検出したいというのもあり、ここでは <a href=\"http://dlib.net/\">dlib</a> を使ってみることにした。\ndlib は <a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/OpenCV\">OpenCV</a>同様に顔領域を検出できるほか、その顔領域内のlandmarkとして顔の輪郭や目・鼻・口などの位置まで簡単に検出することができる。</p>\n\n<p>やはり斜めに傾いた顔などにはあまり強くないようなので、以前のものと同様に回転補正をかけて検出を試みるといったことは必要そうだった。\nただ今回はそもそも「対象の画像には顔が一つだけ写っている」という仮定で その単一の顔の部分だけ検出できれば良いので 少し処理は簡単になる。</p>\n\n<p>例えば、宇宙一輝くぴょんぴょこアイドル <a href=\"https://twitter.com/usami_yukino\">宇佐美幸乃ちゃん</a> の場合。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><a href=\"http://f.hatena.ne.jp/sugyan/20200118212146\" class=\"hatena-fotolife\" itemprop=\"url\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212146.jpg\" alt=\"f:id:sugyan:20200118212146j:image:w200\" title=\"f:id:sugyan:20200118212146j:image:w200\" class=\"hatena-fotolife\" style=\"width:200px\" itemprop=\"image\"></a></span></p>\n\n<p>まずは画像を回転することによってはみ出して消えてしまう部分がないように 元画像対角線の長さを持つ正方形領域を作って、その中央に元画像を配置する。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synStatement\">def</span> <span class=\"synIdentifier\">detect</span>(self, img):\n    <span class=\"synComment\"># Create a large image that does not protrude by rotation</span>\n    h, w, c = img.shape\n    hypot = math.ceil(math.hypot(h, w))\n    hoffset = <span class=\"synIdentifier\">round</span>((hypot-h)/<span class=\"synConstant\">2</span>)\n    woffset = <span class=\"synIdentifier\">round</span>((hypot-w)/<span class=\"synConstant\">2</span>)\n    padded = np.zeros((hypot, hypot, c), np.uint8)\n    padded[hoffset:hoffset+h, woffset:woffset+w, :] = img\n</pre>\n\n\n<p>この画像をそれぞれ少しずつ回転させたものを生成し、それぞれに対して顔検出を試みる。\nこのとき、 <code>fhog_object_detector.run(image, upsample_num_times, adjust_threshold)</code> の<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/API\">API</a>で検出をかけることで、その検出結果の confidence score も取得できるので それらを含めて全パターンの結果を集める。</p>\n\n<ul>\n<li><a href=\"http://dlib.net/python/index.html#dlib.fhog_object_detector.run\">http://dlib.net/python/index.html#dlib.fhog_object_detector.run</a></li>\n</ul>\n\n\n<p>手元で試した限りでは <code>-48</code>° 〜 <code>+48</code>° で <code>12</code>°ずつの回転幅で試すのが、多くの回転角を少ない検出試行で網羅できて良さそうだった。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>    self.detector = dlib.get_frontal_face_detector()\n    self.predictor = dlib.shape_predictor(datafile)\n\n    ...\n\n    <span class=\"synComment\"># Attempt detection by rotating at multiple angles</span>\n    results = []\n    <span class=\"synStatement\">for</span> angle <span class=\"synStatement\">in</span> [-<span class=\"synConstant\">48</span>, -<span class=\"synConstant\">36</span>, -<span class=\"synConstant\">24</span>, -<span class=\"synConstant\">12</span>, <span class=\"synConstant\">0</span>, <span class=\"synConstant\">12</span>, <span class=\"synConstant\">24</span>, <span class=\"synConstant\">36</span>, <span class=\"synConstant\">48</span>]:\n        rotated = self._rotate(padded, angle)\n        dets, scores, indices = self.detector.run(rotated, <span class=\"synConstant\">0</span>, <span class=\"synConstant\">0.0</span>)\n        <span class=\"synStatement\">if</span> <span class=\"synIdentifier\">len</span>(dets) == <span class=\"synConstant\">1</span>:\n            results.append([dets[<span class=\"synConstant\">0</span>], scores[<span class=\"synConstant\">0</span>], angle, rotated])\n    <span class=\"synStatement\">if</span> <span class=\"synIdentifier\">len</span>(results) == <span class=\"synConstant\">0</span>:\n        self.logger.info(<span class=\"synConstant\">'there are no detected faces'</span>)\n        <span class=\"synStatement\">return</span>\n\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">_rotate</span>(self, img, angle):\n    h, w, _ = img.shape\n    mat = cv2.getRotationMatrix2D((w/<span class=\"synConstant\">2</span>, h/<span class=\"synConstant\">2</span>), angle, <span class=\"synConstant\">1.0</span>)\n    <span class=\"synStatement\">return</span> cv2.warpAffine(img, mat, (w, h), cv2.INTER_LANCZOS4)\n</pre>\n\n\n<p>9つのパターンの中で<strong>もっとも高い</strong>scoreで顔が検出されたものが、おそらく最も正解に近い傾き角度である、として それを採用する。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212320.png\" alt=\"f:id:sugyan:20200118212320p:plain\" title=\"f:id:sugyan:20200118212320p:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>この場合はまったく回転しない <code>0</code>° でも顔は検出されている(score: <code>0.3265</code>)が、少し傾けた <code>-12</code>°のものの方が <code>0.5834</code> と高いscoreになっているので、そちらを仮の回転角として採用する。</p>\n\n<p>その回転後の画像に対して landmark を検出し、<strong>左右の目の中央位置</strong>を算出する。\n正しく回転して真っ直ぐになっていたら目の高さは同じになるはず、それがズレているのなら そのぶんだけまだ少し傾きがある、という考えで、その左右の目の位置座標から <code>atan2</code> を使ってその微妙な角度を計算する。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>    ...\n\n    <span class=\"synComment\"># Choose the best angle by scores, and then adjust the angle using the eyes coordinates</span>\n    results.sort(key=<span class=\"synStatement\">lambda</span> x: x[<span class=\"synConstant\">1</span>], reverse=<span class=\"synIdentifier\">True</span>)\n    det, _, angle, rotated = results[<span class=\"synConstant\">0</span>]\n    shape = self.predictor(rotated, det)\n    eyel, eyer = self._eye_center(shape)\n    d = eyer - eyel\n    angle += math.degrees(math.atan2(d[<span class=\"synConstant\">1</span>], d[<span class=\"synConstant\">0</span>]))\n    self.logger.info(f<span class=\"synConstant\">'angle: {angle:.5f}'</span>)\n\n\n    <span class=\"synStatement\">def</span> <span class=\"synIdentifier\">_eye_center</span>(self, shape):\n        eyel, eyer = np.array([<span class=\"synConstant\">0</span>, <span class=\"synConstant\">0</span>]), np.array([<span class=\"synConstant\">0</span>, <span class=\"synConstant\">0</span>])\n        <span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(<span class=\"synConstant\">36</span>, <span class=\"synConstant\">42</span>):\n            eyel[<span class=\"synConstant\">0</span>] += shape.part(i).x\n            eyel[<span class=\"synConstant\">1</span>] += shape.part(i).y\n        <span class=\"synStatement\">for</span> i <span class=\"synStatement\">in</span> <span class=\"synIdentifier\">range</span>(<span class=\"synConstant\">42</span>, <span class=\"synConstant\">48</span>):\n            eyer[<span class=\"synConstant\">0</span>] += shape.part(i).x\n            eyer[<span class=\"synConstant\">1</span>] += shape.part(i).y\n        <span class=\"synStatement\">return</span> eyel / <span class=\"synConstant\">6</span>, eyer / <span class=\"synConstant\">6</span>\n</pre>\n\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212350.png\" alt=\"f:id:sugyan:20200118212350p:plain\" title=\"f:id:sugyan:20200118212350p:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>元々の回転角度と 計算した角度を足して、最終的な回転角とする。\nこの画像の場合は <code>-12 + 0.156403</code> = <code>-11.843597</code>° の回転でほぼ真っ直ぐの状態になる、と計算された。</p>\n\n<p>その回転角での画像をもう一度生成し、正しく顔とlandmarkが検出されることを確認する。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>    ...\n\n    <span class=\"synComment\"># Detect face and shapes from adjusted angle</span>\n    adjusted = self._rotate(padded, angle)\n    dets = self.detector(adjusted)\n    <span class=\"synStatement\">if</span> <span class=\"synIdentifier\">len</span>(dets) != <span class=\"synConstant\">1</span>:\n        self.logger.info(<span class=\"synConstant\">'faces are not detected in the rotated image'</span>)\n        <span class=\"synStatement\">return</span>\n    shape = self.predictor(adjusted, dets[<span class=\"synConstant\">0</span>])\n</pre>\n\n\n<p>次に、見切れている部分の補完を行う。\nデー<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8\">タセット</a>には顔の周辺部分まで含めて切り取って使うことになるので、その周辺部分で画像が切れていたりすると非常に不自然な領域が存在してしまうことになる。</p>\n\n<p>PGGANの手法では 元の画像から鏡面反射した画像を繋げて広げて(mirror padding)、そこから切り取ることで不自然さを和らげているようだ。\n同様にやってみる。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>    ...\n\n    <span class=\"synComment\"># Create a large mirrored image to rotate and crop</span>\n    margin = math.ceil(hypot * (math.sqrt(<span class=\"synConstant\">2</span>) - <span class=\"synConstant\">1.0</span>) / <span class=\"synConstant\">2</span>)\n    mirrored = np.pad(\n        img,\n        ((hoffset + margin, hypot - h - hoffset + margin),\n         (woffset + margin, hypot - w - woffset + margin),\n         (<span class=\"synConstant\">0</span>, <span class=\"synConstant\">0</span>)), mode=<span class=\"synConstant\">'symmetric'</span>)\n    rotated = self._rotate(mirrored, angle)[margin:margin+hypot, margin:margin+hypot, :]\n</pre>\n\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212412.png\" alt=\"f:id:sugyan:20200118212412p:plain\" title=\"f:id:sugyan:20200118212412p:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>たしかに背景の壁などはそのまま続いているかのように見えて不自然な領域は減りそうだ。</p>\n\n<p>ここから、両目の位置と口の端の位置・その各点間の距離を使って 切り取るべき顔領域の中心座標と大きさを算出している。\n論文内の手法では</p>\n\n<ul>\n<li><code>x</code>: 両目の幅 <code>= e1 - e0</code></li>\n<li><code>y</code>: 両目の中心 から 口の中心 の距離 <code>= (e0 + e1) / 2 - (m0 + m1) / 2</code></li>\n<li><code>c</code>: 切り取る中心座標 <code>= (e0 + e1) / 2 - 0.1 * y</code></li>\n<li><code>s</code>: 切り取るサイズ <code>= max(4.0 * x, 3.6 * y)</code></li>\n</ul>\n\n\n<p>といった計算でやっているようだ。そのまま使って適用してみる。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink>    <span class=\"synComment\"># Calculate the center position and cropping size</span>\n    <span class=\"synComment\"># https://arxiv.org/pdf/1710.10196v3.pdf</span>\n    e0, e1 = self._eye_center(shape)\n    m0 = np.array([shape.part(<span class=\"synConstant\">48</span>).x, shape.part(<span class=\"synConstant\">48</span>).y])\n    m1 = np.array([shape.part(<span class=\"synConstant\">54</span>).x, shape.part(<span class=\"synConstant\">54</span>).y])\n    x = e1 - e0\n    y = (e0 + e1) / <span class=\"synConstant\">2</span> - (m0 + m1) / <span class=\"synConstant\">2</span>\n    c = (e0 + e1) / <span class=\"synConstant\">2</span> + y * <span class=\"synConstant\">0.1</span>\n    s = <span class=\"synIdentifier\">max</span>(np.linalg.norm(x) * <span class=\"synConstant\">4.0</span>, np.linalg.norm(y) * <span class=\"synConstant\">3.6</span>)\n    xoffset = <span class=\"synIdentifier\">int</span>(np.rint(c[<span class=\"synConstant\">0</span>] - s/<span class=\"synConstant\">2</span>))\n    yoffset = <span class=\"synIdentifier\">int</span>(np.rint(c[<span class=\"synConstant\">1</span>] - s/<span class=\"synConstant\">2</span>))\n    <span class=\"synStatement\">if</span> xoffset &lt; <span class=\"synConstant\">0</span> <span class=\"synStatement\">or</span> yoffset &lt; <span class=\"synConstant\">0</span> <span class=\"synStatement\">or</span> xoffset + s &gt;= hypot <span class=\"synStatement\">or</span> yoffset + s &gt;= hypot:\n        self.logger.info(<span class=\"synConstant\">'cropping area has exceeded the image area'</span>)\n        <span class=\"synStatement\">return</span>\n    size = <span class=\"synIdentifier\">int</span>(np.rint(s))\n    cropped = rotated[yoffset:yoffset+size, xoffset:xoffset+size, :]\n</pre>\n\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212441.png\" alt=\"f:id:sugyan:20200118212441p:plain\" title=\"f:id:sugyan:20200118212441p:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>いい感じにそれっぽく、正規化された顔画像として切り抜くことが出来そうだ。</p>\n\n<p>こうして 検出器が出来たので、<code>132,513</code> 件のURLから実際にこの方法による検出を試みた。\nそこそこ重い処理ではあるものの、手元の<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/MacBook\">MacBook</a>でも数日かけてゆっくり実行し続けた結果 <code>72,334</code> 件ほどの顔画像を収集することができた。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119232129.gif\" alt=\"f:id:sugyan:20200119232129g:plain\" title=\"f:id:sugyan:20200119232129g:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<h3>見切れ領域の多い画像に起こる問題点</h3>\n\n<p>こうして見ると良い画像データが揃っているように見えるが、実際には全然そんなに上手くはいかない。</p>\n\n<p>多くの自撮り画像は かなり寄り気味に撮られていて、顔や頭の輪郭まで全部は写っていない場合が多い。\nそうするとどうなるか。鏡面反射で補完しても見切れた顔や頭が反射されて映るだけで 結局不自然な画像になってしまう。</p>\n\n<p>例えば前述の例でも、もしもっと寄り気味に撮られていて頭などが見切れていたら…</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118214632.png\" alt=\"f:id:sugyan:20200118214632p:plain\" title=\"f:id:sugyan:20200118214632p:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>という感じになって、顔やlandmarkは確かに検出されるかもしれないけど、頭や他の部分が変な形に繋がってしまっておかしなものになってしまう。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119232246.gif\" alt=\"f:id:sugyan:20200119232246g:plain\" title=\"f:id:sugyan:20200119232246g:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>ちょっとくらいなら問題ないかもしれないけど、流石に目が複数見えてたりするのはヤバそう…</p>\n\n<h3>抽出した顔画像を使った生成テスト</h3>\n\n<p>とりあえずは変な形になってしまったデータが存在してしまっていても仕方ない、と割り切って、検出して得ることが出来た顔画像を <code>10,000</code> 件ほど使って生成モデルでの学習を試みてみた。</p>\n\n<p><code>512 x 512</code> にリサイズしたものをデー<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8\">タセット</a>として使い、 <a href=\"https://github.com/NVlabs/stylegan\">StyleGAN</a> を使って何epochか学習してみた。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118215620.gif\" alt=\"f:id:sugyan:20200118215620g:plain\" title=\"f:id:sugyan:20200118215620g:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>確かにアイドルの顔っぽい画像が生成されるが、やはり右上や左上などに鏡面反射した顔が繋がっているような奇妙な形のものが生成されやすいようだ。。</p>\n\n<p>まぁ、そういう画像を含んだものを学習データとして与えてしまっているのでそうなるのは当然の結果ではある。</p>\n\n<h3>画像選別と管理のためのWebアプリケーション</h3>\n\n<p>となると今度はデータのクリーニングが必要になってくる。</p>\n\n<p>目視で1枚1枚 画像を確認し、「学習データに使える、顔全体がきれいに入っている画像」と「学習データに使いたくない、不自然な画像」を選別することにした。</p>\n\n<p>ローカル環境でデータを管理したくない、自分好みのUIで作業・確認したい、などの理由もあり、例によって管理用のWebアプリケーションを自作した。</p>\n\n<p><a href=\"https://cloud.google.com/appengine/\">Google App Engine</a> 上で動作するよう、画像を <a href=\"https://cloud.google.com/storage/\">Cloud Storage</a> にアップロード、それにひもづく情報を <a href=\"https://cloud.google.com/firestore/\">Cloud Firestore</a> に保存 (以前は <a href=\"https://cloud.google.com/datastore/\">Cloud Datastore</a> だったけど 次の時代はFirestoreらしい、ということで今回初めて触ってみた)。Frontendを <a href=\"https://github.com/facebook/create-react-app\">Create React App</a> で作って、SPAから App Engine Go Standard Environment で作った <a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/API\">API</a> を叩く形のアプリケーション。自分しか閲覧・操作できないよう <a href=\"https://firebase.google.com/docs/auth\">Firebase Authentication</a> で認証するようにしている。</p>\n\n<p>各画像に対して <code>Status</code> というフィールドを用意しておき、 <code>Ready</code>, <code>NG</code>, <code>Pending</code>, <code>OK</code> の4つの値をセットできるようにした。初期値はすべて <code>Ready</code>。 <code>Ready</code> のものをひたすら見ていって、きれいで使えそうなものを <code>OK</code>、ダメそうなものを <code>NG</code> に変更する。判断に迷ったものは <code>Pending</code> に。<code>1</code>, <code>2</code>, <code>3</code> のボタン操作1つで次々スピーディーに更新していけるようにUIを工夫した。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119203714.gif\" alt=\"f:id:sugyan:20200119203714g:plain\" title=\"f:id:sugyan:20200119203714g:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p>誤操作も有り得るので <code>NG</code> だからといって削除したりはせず、 <code>NG</code> として残しておく。これが後で役に立った。</p>\n\n<p>こうして選別作業していって、<code>OK</code> になったものだけを抽出して学習データに使えば、きっときれいな画像が生成できるようになるはず…</p>\n\n<h4>選別作業効率化へ</h4>\n\n<p>作業は1枚1秒程度でサクサク進むが、実際にやってみると <code>NG</code> の画像が非常に多いことが分かった。</p>\n\n<p>やはり多くのアイドルさんは顔までは写していても頭全体まで写るような自撮りをしていることは少なく、それによってmirror paddingされたものはだいたい頭の形がおかしい画像になってしまう。</p>\n\n<p><code>8,500</code> 枚ほど選別作業してみてようやく <code>OK</code> のものが <code>1,250</code> 枚ほど。 約7枚に1枚しか現れず、8割以上は <code>NG</code> もしくは <code>Pending</code> にする感じになった。思った以上に <code>NG</code> の山の中から少数の <code>OK</code> を探すのはストレスフルだし効率が悪い。</p>\n\n<p><code>NG</code> の 頭の形がおかしくなってるような画像なんて誰でも区別できるし機械にでもやらせればええやん…</p>\n\n<p>と思ったので、一次選別するための<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%B5%A1%B3%A3%B3%D8%BD%AC\">機械学習</a>モデルを自作することにした。</p>\n\n<p>幸い、 <code>NG</code> にした画像も削除せずに明確に「<code>NG</code> である」とラベル付けした状態で残している。\n画像を入力して、「<code>OK</code> か <code>NG</code> か」だけを予測する分類モデルを用意した。\n画像に写っている人物が誰か、は関係なく、生成用のデータとして <code>OK</code> なものか <code>NG</code> なものか、だけを判別させるモデルとして学習させることになる。</p>\n\n<p>そこまで厳密に精度を求めるものでもないし、適当に <a href=\"https://tfhub.dev/\">TensorFlow Hub</a> からImageNetで学習済みの <code>InceptionV3</code> を利用して 2 classes の classification のためのmodelとした。</p>\n\n<pre class=\"code lang-python\" data-lang=\"python\" data-unlink><span class=\"synPreProc\">import</span> tensorflow <span class=\"synStatement\">as</span> tf\n<span class=\"synPreProc\">import</span> tensorflow_hub <span class=\"synStatement\">as</span> hub\n\nIMAGE_SIZE = (<span class=\"synConstant\">299</span>, <span class=\"synConstant\">299</span>)\n\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">cnn</span>()\n    <span class=\"synStatement\">return</span> hub.KerasLayer(<span class=\"synConstant\">&quot;https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4&quot;</span>,\n                          trainable=trainable, arguments=<span class=\"synIdentifier\">dict</span>(batch_norm_momentum=<span class=\"synConstant\">0.997</span>))\n\n\n<span class=\"synStatement\">def</span> <span class=\"synIdentifier\">train</span>():\n    labels = [<span class=\"synConstant\">'ok'</span>, <span class=\"synConstant\">'ng'</span>]\n\n    model = tf.keras.Sequential([\n        cnn(),\n        tf.keras.layers.Dropout(rate=<span class=\"synConstant\">0.1</span>),\n        tf.keras.layers.Dense(\n            <span class=\"synIdentifier\">len</span>(labels),\n            activation=<span class=\"synConstant\">'softmax'</span>,\n            kernel_regularizer=tf.keras.regularizers.l2(<span class=\"synConstant\">1e-4</span>)),\n    ])\n    model.build([<span class=\"synIdentifier\">None</span>, *IMAGE_SIZE, <span class=\"synConstant\">3</span>])\n    model.summary()\n    model.compile(\n        optimizer=tf.keras.optimizers.RMSprop(),\n        loss=tf.keras.losses.CategoricalCrossentropy(),\n        metrics=[tf.keras.metrics.CategoricalAccuracy()])\n</pre>\n\n\n<p>デー<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8\">タセット</a>としては、既にラベル付け済みの <code>NG</code> のものを <code>4,800</code> 件、 <code>OK</code> のものを <code>1,200</code> 件 抽出して使用した。\nそれぞれを <code>4000:800</code>, <code>1000:200</code> で <code>train</code> と <code>validation &amp; test</code> に分割。</p>\n\n<p>最初は全結合層を学習させるだけの転移学習だけでどうにかなるかな、と思ってちょっと試してみたけどダメそうだったので、結局ネットワーク全体を学習させるfine-tuningで。\n<a href=\"https://colab.research.google.com/\">Google Colaboratory</a> の<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/GPU\">GPU</a> Runtimeで数十分ほど学習。</p>\n\n<p><code>NG</code> データの方が多いので学習初期は <code>NG</code> に全振りして accuracy <code>0.8</code> とかになるけど、だんだん改善していって 40 epochほど進めると <code>0.947</code> まで上がった。</p>\n\n<p>最終的な結果としては、学習に使わなかった <code>validation &amp; test</code> セットに対する推論で 以下のようなConfusion Matrixになった。</p>\n\n<p><span itemscope itemtype=\"http://schema.org/Photograph\"><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118235824.png\" alt=\"f:id:sugyan:20200118235824p:plain\" title=\"f:id:sugyan:20200118235824p:plain\" class=\"hatena-fotolife\" itemprop=\"image\"></span></p>\n\n<p><code>OK</code> label に対しては Precision <code>0.9016</code>, Recall <code>0.8250</code> といったところで、まぁそれなりに学習してくれている、という感覚ではある。</p>\n\n<p>実際に、未使用の <code>Ready</code> の画像 <code>1,000</code> 枚に対してこの分類モデルに判別させてみたところ、<code>197</code> 枚が <code>OK</code> として分類された。\nそれらを目視で確認してみたところ、 そのうち <code>128</code> 枚が <code>OK</code> になった。\n期待したよりは低かったが、これまでは 数枚に1枚しか現れない <code>OK</code> を探し続ける作業だったのが 今後はこの 一次選別されたものから作業すれば 半数以上は <code>OK</code> を選べるので、作業の<a class=\"keyword\" href=\"http://d.hatena.ne.jp/keyword/%BF%B4%CD%FD%C5%AA\">心理的</a>ストレスは格段に軽減されて効率的になる。</p>\n\n<p>また、今後さらにデータが増えたら この分類モデルも再度学習させることで、さらに高精度に一次選別を進めることが出来るようになることが期待できる。</p>\n\n<h3>現状と今後</h3>\n\n<p>こうして、現時点で <code>1,900</code> 枚くらいまでは <code>OK</code> な画像を集めることができた。\nもう少し増えたらそれらを使って生成を再度試してみたいところ。</p>\n\n<p>が、全体の枚数と割合で概算すると 今あるすべての収集画像に対して選別してもまだ <code>OK</code> が <code>10,000</code> 枚に届かないかもしれない…。\n自撮りのキレイなオススメアイドルさんをご存知の方がいらっしゃったら是非とも教えていただきたいところ。。</p>\n\n<h3>Repository</h3>\n\n<ul>\n<li><a href=\"https://github.com/sugyan/image-dataset\">https://github.com/sugyan/image-dataset</a></li>\n</ul>\n\n","descriptionType":"html","publishedDate":"Sun, 19 Jan 2020 15:12:58 +0000","feedId":36544,"bgimg":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118215620.gif","linkMd5":"cf906d3fd1e9e05085049d2025d0e967","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn36@2020_3/2020/10/13/14-28-44-183_1434f92bd4bc8b38.webp","destWidth":768,"destHeight":768,"sourceBytes":9807389,"destBytes":846982,"author":"sugyan","enclosureType":"image/gif","enclosureUrl":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118215620.gif","articleImgCdnMap":{"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212146.jpg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_1/2020/10/13/14-28-46-889_fe321fb8fc67da60.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212320.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_2/2020/10/13/14-28-47-251_844c8730fab444e2.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212350.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn47@2020_6/2020/10/13/14-28-49-093_73aa6a91b2f24ae1.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212412.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn60@2020_5/2020/10/13/14-28-47-289_481318385db81919.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212441.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn55@2020_3/2020/10/13/14-28-48-939_56551d43d12d831b.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119232129.gif":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn64@2020_1/2020/10/13/14-28-47-966_8c5a345aaea343f4.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118214632.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn76@2020_3/2020/10/13/14-28-48-203_98e1c5c92dd1d627.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119232246.gif":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn51@2020_1/2020/10/13/14-28-50-366_dc0f678ab25ab271.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118215620.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn36@2020_3/2020/10/13/14-28-44-183_1434f92bd4bc8b38.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119203714.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn84@2020_2/2020/10/13/14-28-55-353_a1726c3487b6de3d.webp","https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118235824.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn71@2020_3/2020/10/13/14-28-47-001_d76837768020cc13.webp"},"publishedOrCreatedDate":1602599320572}],"record":{"createdTime":"2020-10-13 22:28:40","updatedTime":"2020-10-13 22:28:40","feedId":36544,"fetchDate":"Tue, 13 Oct 2020 14:28:40 +0000","fetchMs":3624,"handleMs":6503,"totalMs":27577,"newArticles":0,"totalArticles":30,"status":1,"type":0,"ip":"d74ee4cd41d209f5fd2464bcfd300583","hostName":"europe61*","requestId":"52d006885d7c496e92cd8a1a16190c65_36544","contentType":"application/atom+xml; charset=utf-8","totalBytes":10353414,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":11,"articlesImgsGithubTotal":11,"successGithubMap":{"myreaderx8":1,"myreaderx15":1,"myreaderx7":1,"myreaderx32":1,"myreaderx4":1,"myreaderx11":1,"myreaderx3":1,"myreaderx2":1,"myreaderx30":1,"myreaderx31":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 03:08:05","updatedTime":"2020-09-07 03:08:05","id":36544,"name":"すぎゃーんメモ","url":"http://d.hatena.ne.jp/sugyan/rss","subscriber":88,"website":null,"icon":"https://memo.sugyan.com/favicon.ico","icon_jsdelivr":null,"description":"","weekly":null,"link":"https://memo.sugyan.com"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":846982,"tmpBodyImgCdnBytes":9506432,"tmpBgImgCdnBytes":0,"extra4":{"start":1602599310042,"total":0,"statList":[{"spend":4026,"msg":"获取xml内容"},{"spend":6503,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":11009,"msg":"正文链接上传到cdn"}]},"extra5":11,"extra6":11,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118215620.gif","sourceStatusCode":200,"destWidth":768,"destHeight":768,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn36@2020_3/2020/10/13/14-28-44-183_1434f92bd4bc8b38.webp","sourceBytes":9807389,"destBytes":846982,"targetWebpQuality":37,"feedId":36544,"totalSpendMs":5939,"convertSpendMs":1548,"createdTime":"2020-10-13 22:28:40","host":"europe-58*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967,cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.4 MB","destSize":"827.1 KB","compressRate":"8.6%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118235824.png","sourceStatusCode":200,"destWidth":640,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn71@2020_3/2020/10/13/14-28-47-001_d76837768020cc13.webp","sourceBytes":17296,"destBytes":8008,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":1011,"convertSpendMs":22,"createdTime":"2020-10-13 22:28:46","host":"us-004*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.9 KB","destSize":"7.8 KB","compressRate":"46.3%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212146.jpg","sourceStatusCode":200,"destWidth":900,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_1/2020/10/13/14-28-46-889_fe321fb8fc67da60.webp","sourceBytes":216978,"destBytes":115968,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":1282,"convertSpendMs":45,"createdTime":"2020-10-13 22:28:46","host":"us-54*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"211.9 KB","destSize":"113.2 KB","compressRate":"53.4%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212320.png","sourceStatusCode":200,"destWidth":1080,"destHeight":1080,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn68@2020_2/2020/10/13/14-28-47-251_844c8730fab444e2.webp","sourceBytes":1036398,"destBytes":118880,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":1575,"convertSpendMs":88,"createdTime":"2020-10-13 22:28:46","host":"us-016*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1,012.1 KB","destSize":"116.1 KB","compressRate":"11.5%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212412.png","sourceStatusCode":200,"destWidth":1200,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn60@2020_5/2020/10/13/14-28-47-289_481318385db81919.webp","sourceBytes":1805270,"destBytes":128216,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":1674,"convertSpendMs":108,"createdTime":"2020-10-13 22:28:46","host":"us-040*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 MB","destSize":"125.2 KB","compressRate":"7.1%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119232129.gif","sourceStatusCode":200,"destWidth":256,"destHeight":256,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn64@2020_1/2020/10/13/14-28-47-966_8c5a345aaea343f4.webp","sourceBytes":5316788,"destBytes":1260050,"targetWebpQuality":67,"feedId":36544,"totalSpendMs":2493,"convertSpendMs":1026,"createdTime":"2020-10-13 22:28:46","host":"us-028*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.1 MB","destSize":"1.2 MB","compressRate":"23.7%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118214632.png","sourceStatusCode":200,"destWidth":1200,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn76@2020_3/2020/10/13/14-28-48-203_98e1c5c92dd1d627.webp","sourceBytes":2082873,"destBytes":160342,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":3218,"convertSpendMs":88,"createdTime":"2020-10-13 22:28:46","host":"europe67*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2 MB","destSize":"156.6 KB","compressRate":"7.7%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212350.png","sourceStatusCode":200,"destWidth":1200,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn47@2020_6/2020/10/13/14-28-49-093_73aa6a91b2f24ae1.webp","sourceBytes":976391,"destBytes":80248,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":1403,"convertSpendMs":105,"createdTime":"2020-10-13 22:28:48","host":"us-012*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"953.5 KB","destSize":"78.4 KB","compressRate":"8.2%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200118/20200118212441.png","sourceStatusCode":200,"destWidth":1200,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn55@2020_3/2020/10/13/14-28-48-939_56551d43d12d831b.webp","sourceBytes":1817408,"destBytes":125164,"targetWebpQuality":75,"feedId":36544,"totalSpendMs":3928,"convertSpendMs":114,"createdTime":"2020-10-13 22:28:46","host":"europe63*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 MB","destSize":"122.2 KB","compressRate":"6.9%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119232246.gif","sourceStatusCode":200,"destWidth":256,"destHeight":256,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn51@2020_1/2020/10/13/14-28-50-366_dc0f678ab25ab271.webp","sourceBytes":5437125,"destBytes":1235904,"targetWebpQuality":67,"feedId":36544,"totalSpendMs":6079,"convertSpendMs":1027,"createdTime":"2020-10-13 22:28:46","host":"europe-25*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.2 MB","destSize":"1.2 MB","compressRate":"22.7%"},{"code":1,"isDone":false,"source":"https://cdn-ak.f.st-hatena.com/images/fotolife/s/sugyan/20200119/20200119203714.gif","sourceStatusCode":200,"destWidth":800,"destHeight":497,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn84@2020_2/2020/10/13/14-28-55-353_a1726c3487b6de3d.webp","sourceBytes":5743065,"destBytes":6273652,"targetWebpQuality":67,"feedId":36544,"totalSpendMs":10844,"convertSpendMs":8419,"createdTime":"2020-10-13 22:28:46","host":"us-032*","referer":"https://memo.sugyan.com/entry/2020/01/20/001258","linkMd5ListStr":"cf906d3fd1e9e05085049d2025d0e967","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"5.5 MB","destSize":"6 MB","compressRate":"109.2%"}],"successGithubMap":{"myreaderx8":1,"myreaderx15":1,"myreaderx7":1,"myreaderx32":1,"myreaderx4":1,"myreaderx11":1,"myreaderx3":1,"myreaderx2":1,"myreaderx30":1,"myreaderx31":1,"myreaderx":1},"failGithubMap":{}}