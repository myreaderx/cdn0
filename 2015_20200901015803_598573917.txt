{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-09-01 09:57:54","updatedTime":"2020-09-01 09:57:54","title":"Problem of the between-state correlations in the Fivethirtyeight election forecast","link":"https://statmodeling.stat.columbia.edu/?p=44436","description":"<p>Elliott writes:</p>\n<blockquote><p>I think we&#8217;re onto something with the low between-state correlations [see item 1 of our <a href=\"https://statmodeling.stat.columbia.edu/2020/08/31/more-on-that-fivethirtyeight-prediction-that-biden-might-only-get-42-of-the-vote-in-florida/\">earlier post</a>]. Someone sent me this collage of maps from Nate&#8217;s model that show:</p>\n<p>&#8211; Biden winning every state except NJ<br />\n&#8211; Biden winning LA and MS but not MI and WI<br />\n&#8211; Biden losing OR but winning WI, PA</p>\n<p>And someone says that in the 538 simulations where Trump wins CA, he only has a 60% chance of winning the elec overall.</p>\n<p>Seems like the arrows are pointing to a very weird covariance structure.</p></blockquote>\n<p><img src=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Egw7NUkXcAEHyqr.png\" alt=\"\" width=\"550\"  class=\"alignnone size-full wp-image-44437\" srcset=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Egw7NUkXcAEHyqr.png 819w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Egw7NUkXcAEHyqr-300x235.png 300w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Egw7NUkXcAEHyqr-768x601.png 768w\" sizes=\"(max-width: 819px) 100vw, 819px\" /></p>\n<p><img src=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxGFxeX0AA0S7P.png\" alt=\"\" width=\"285\" height=\"246\" class=\"alignnone size-full wp-image-44438\" /></p>\n<p><img src=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxghkGUMAAuDNW.jpg\" alt=\"\" width=\"330\" height=\"344\" class=\"alignnone size-full wp-image-44439\" srcset=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxghkGUMAAuDNW.jpg 330w, https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxghkGUMAAuDNW-288x300.jpg 288w\" sizes=\"(max-width: 330px) 100vw, 330px\" /></p>\n<p><img src=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxgjxbVoAA5z95.jpg\" alt=\"\" width=\"254\" height=\"183\" class=\"alignnone size-full wp-image-44440\" /></p>\n<p>I agree that these maps look really implausible for 2020.  How&#8217;s Biden gonna win Idaho, Wyoming, Alabama, etc. . . . but not New Jersey?</p>\n<p>But this does all seem consistent with correlations of uncertainties between states that are too low.</p>\n<p>Perhaps this is a byproduct of Fivethirtyeight relying too strongly on state polls and not fully making use of the information from national polls and from the relative positions of the states in previous elections.</p>\n<p>If you think of the goal as forecasting the election outcome (by way of vote intentions; see item 4 in the above-linked post), then state polls are just one of many sources of information.  But if you start by aggregating state polls, and then try to hack your way into a national election forecast, then you can run into all sorts of problems.  The issue here is that the between-state correlation is  mostly not coming from the polling process at all; it&#8217;s coming from uncertainty in public opinion changes among states.  So you need some underlying statistical model of opinion swings in the 50 states, or else you need to hack in a correlation just right.  I don&#8217;t think we did this perfectly either!  But I can see how the Fivethirtyeight team could&#8217;ve not even realized the difficulty of this problem, if they were too focused on creating simulations based on state polls without thinking about the larger forecasting problem.</p>\n<p>There&#8217;s a Bayesian point here, which is that correlation in the prior induces correlation in the posterior, even if there&#8217;s no correlation in the likelihood.</p>\n<p>And, as we discussed earlier, if your between-state correlations are too low, and at the same time you&#8217;re aiming for a realistic uncertainty in the national level, then you&#8217;re gonna end up with too much uncertainty for each individual state.</p>\n<p>At some level, the Fivethirtyeight team must realize this&#8212;earlier this year, Nate Silver wrote that correlated errors are &#8220;where often *most* of the work is in modeling if you want your models to remotely resemble real-world conditions&#8221;&#8212;but recognizing the general principle is not the same thing as doing something reasonable in a live application.</p>\n<p><strong>These things happen</strong></p>\n<p>Again, assuming the above maps actually reflect the Fivethirtyeight forecast and they&#8217;re not just some sort of computer glitch, this does <em>not</em> mean that what they&#8217;re doing at that website is useless, nor does it mean that we&#8217;re &#8220;right&#8221; and they&#8217;re &#8220;wrong&#8221; in whatever other disagreements we might have (although I&#8217;m standing fast on the Carmelo Anthony <a href=\"https://statmodeling.stat.columbia.edu/2020/02/03/mrp-carmelo-anthony-update-trash-talkings-fine-but-you-gotta-give-details-or-links-or-something/\">thing</a>).  Everybody makes mistakes!  We made mistakes in our forecast too (see item 3 in our <a href=\"https://statmodeling.stat.columbia.edu/2020/08/31/more-on-that-fivethirtyeight-prediction-that-biden-might-only-get-42-of-the-vote-in-florida/\">earlier post</a>)!  Multivariate forecasting is harder than it looks.  In our case, it helped that we had a team of 3 people staring at our model, but of course that didn&#8217;t stop us from making our mistakes the first time.</p>\n<p>At the very least, maybe this will remind us all that knowing that a forecast is based on 40,000 simulations or 40,000,000 simulations or 40,000,000,000 simulations doesn&#8217;t really tell us anything until we know how the simulations are produced.</p>\n<img src=\"http://feeds.feedburner.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~4/3dU36ecjnTc\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Tue, 01 Sep 2020 01:05:11 +0000","feedId":2015,"bgimg":"","linkMd5":"456c3680b521d39f1f5e59e38e9a290f","bgimgJsdelivr":"","metaImg":"","author":"Andrew","articleImgCdnMap":{"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Egw7NUkXcAEHyqr.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn53@2020_4/2020/09/01/01-57-57-132_fd917f111a3f9d75.webp","https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxGFxeX0AA0S7P.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn51@2020_5/2020/09/01/01-57-56-323_e6db14196f32129f.webp","https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxghkGUMAAuDNW.jpg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn57@2020_4/2020/09/01/01-57-56-994_e17983eec94d9faa.webp","https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxgjxbVoAA5z95.jpg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn40@2020_3/2020/09/01/01-57-56-358_c5e8e83fc4ca3637.webp","http://feeds.feedburner.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~4/3dU36ecjnTc":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn38@2020_1/2020/09/01/01-57-59-014_2fe305b58a22f4ec.webp"},"publishedOrCreatedDate":1598925474255},{"createdTime":"2020-09-01 09:57:54","updatedTime":"2020-09-01 09:57:54","title":"More on that Fivethirtyeight prediction that Biden might only get 42% of the vote in Florida","link":"https://statmodeling.stat.columbia.edu/?p=44409","description":"<p><img src=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Screen-Shot-2020-08-26-at-8.55.28-AM.png\" alt=\"\" width=\"450\" /></p>\n<p>I&#8217;ve been chewing more on <a href=\"https://statmodeling.stat.columbia.edu/2020/08/27/florida-comparing-economist-and-fivethirtyeight-forecasts/\">the above Florida forecast</a> from Fivethirtyeight.</p>\n<p>Their 95% interval for the election-day vote margin in Florida is something like [+16% Trump, +20% Biden], which corresponds to an approximate 95% interval of [42%, 60%] for Biden&#8217;s share of the two-party vote.</p>\n<p>This is buggin me because it&#8217;s really hard for me to picture Biden only getting 42% of the vote in Florida.</p>\n<p>By comparison, our Economist forecast gives a 95% interval of [47%, 58%] for Biden&#8217;s Florida vote share.</p>\n<p>Is there really a serious chance that Biden gets only 42% of the vote in Florida?</p>\n<p>Let&#8217;s look at this in a few ways:</p>\n<p>1.  Where did the Fivethirtyeight interval come from?</p>\n<p>2.  From 95% intervals to 50% intervals.</p>\n<p>3.  Using weird predictions to discover problems with your model.</p>\n<p>4.  Vote intentions vs. the ultimate official vote count.</p>\n<p><strong>1. Where did the Fivethirtyeight interval come from?</strong></p>\n<p>How did they get such a wide interval for Florida?</p>\n<p>I think two things happened.</p>\n<p>First, they made the national forecast wider.  Biden has a clear lead in the polls and a lead in the fundamentals (poor economy and unpopular incumbent).  Put that together and you give Biden a big lead in the forecast; for example, we <a href=\"https://projects.economist.com/us-2020-forecast/president\">give</a> him a 90% chance of winning the electoral college.  For understandable reasons, the Fivethirtyeight team didn&#8217;t think Biden&#8217;s chances of winning were so high.  I disagree on this&#8212;I&#8217;ll stand by our forecast&#8212;but I can see where they&#8217;re coming from.  After all, this is kind of a replay of 2016 when Trump <em>did</em> win the electoral college, also he has the advantages of incumbency, for all that&#8217;s worth.  You can lower Biden&#8217;s win probability by lowering his expected vote&#8212;you can&#8217;t do much with the polls, but you can choose a fundamentals model that forecasts less than 54% for the challenger&#8212;and you can widen the interval.  Part of what Fivethirtyeight did is widen their intervals, and when you widen the interval for the national vote, this will also widen your interval for individual states.</p>\n<p>Second, I suspect they screwed up a bit in their model of correlation between states.  I can&#8217;t be sure of this&#8212;I couldn&#8217;t find a full description of their forecasting method anywhere&#8212;but I&#8217;m guessing that the correlation of uncertainties between states is too low.  Why do I say this?  Because the lower the correlation between states, the more uncertainty you need for each individual state forecast to get a desired national uncertainty.</p>\n<p>Also, setting up between-state uncertainties is tricky. I know this because Elliott, Merlin, and I struggled when setting up <a href=\"https://projects.economist.com/us-2020-forecast/president/how-this-works\">our own model</a>, which indeed is a bit of a kluge when it comes to that bit.</p>\n<p>Alternatively, you could argue that [42%, 60%] is just fine as a 95% interval for Biden&#8217;s Florida vote share&#8212;I&#8217;ll get back to that in a bit.  But if you feel, as we do that this 42% is too low to be plausible, then the above two model features&#8212;an expanded national uncertainty and too-low between-state correlations&#8212;are one way that Fivethirtyeight could&#8217;ve ended up there.</p>\n<p><strong>2.  From 95% intervals to 50% intervals.</strong></p>\n<p>95% intervals are hard to calibrate.  If all is good with your modeling, your 95% intervals will be wrong only 1 time in 20.  To put it another way, you&#8217;d expect only 50 such mispredicted state-level events in 80 years of national elections.  So you might say that the interval for Florida <em>should</em> be super-wide.  This doesn&#8217;t answer the question of <em>how</em> wide:  should the lower bound of that interval be 47% (as we have it), or 42% (as per 538), or maybe 37%???&#8212;but it does tell us that it&#8217;s hard to think about such intervals.</p>\n<p>It&#8217;s easier to think about 50% intervals, and, fortunately, we can read these off the above graphic too.  The 50% prediction interval for Florida is roughly (+4% Trump, +8% Biden), i.e. (0.48, 0.54) for Biden’s two-party vote share.</p>\n<p>Given that Biden&#8217;s currently at 52% in the polls in <a href=\"https://projects.economist.com/us-2020-forecast/president/florida\">Florida</a> (and at 55% in <a href=\"https://projects.economist.com/us-2020-forecast/president\">national</a> polls, so it&#8217;s not like the Florida polls are some kind of fluke), I don&#8217;t really buy the (0.48, 0.54) interval.</p>\n<p>To put it another way, I think there&#8217;s less than a 1-in-4 probability that Biden less than 48% of the two-party vote in Florida.  This is not to say I think Biden is certain to win, just that I think the Fivethirtyeight interval is too wide.  I already thought this about the 95% interval, and I think this about the 50% interval too.</p>\n<p>That&#8217;s just my take (and the take of our statistical model).  The Fivethirtyeight is under no obligation to spit out numbers that are consistent with my view of the race.  I&#8217;m just explaining where I&#8217;m coming from.</p>\n<p>In their defense, back in 2016, some of the polls were biased.  Indeed, back in September of that year, the New York Times gave data from a Florida poll to Sam Corbett-Davies, David Rothschild, and me.  We <a href=\"https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html\">estimated</a> Trump with a 1% lead in the state&#8212;even while the Times and three other pollsters (one Republican, one Democratic, and one nonpartisan) all pointed toward Clinton, giving her a lead of between 1 and 4 points.</p>\n<p>In that case, we adjusted the raw poll data for party registration, the other pollsters didn&#8217;t, and that explains why they were off.  If the current Florida polls are off in the same way, then that would explain the Fivethirtyeight forecast.  But (a) I have no reason to think the current polls are off in this way, and one reason I have this assurance is that our model does allow for bias in polls that don&#8217;t adjust for partisanship of respondents, and (b) I don&#8217;t think Fivethirtyeight attempts this bias correction; it&#8217;s my impression that they take the state poll toplines as is.  Again, I do think they widen their intervals, but I think that leads to unrealistic possibilities in their forecast distribution, which is how I led off this post.</p>\n<p><strong>3.  Using weird predictions to discover problems with your model.</strong></p>\n<p>Weird predictions can be a good way of finding problems with your model.  We discussed this in our post the other day: <a href=\"https://statmodeling.stat.columbia.edu/2020/08/27/florida-comparing-economist-and-fivethirtyeight-forecasts/\">go here</a> and scroll down to &#8220;Making predictions, seeing where they look implausible, and using this to improve our modeling.&#8221;  As I wrote, it&#8217;s happened to me many times that I&#8217;ve fit a model that seemed reasonable, but then some of its predictions didn&#8217;t quite make sense, and I used this disconnect to motivate a careful look at the model, followed by a retooling.</p>\n<p>Indeed, this <a href=\"https://statmodeling.stat.columbia.edu/2020/07/31/thinking-about-election-forecast-uncertainty/\">happened to us</a> just a month ago!  It started when Nate Silver and others questioned the narrow forecast intervals of our election forecasting model&#8212;at the time, we were giving Biden a 99% chance of winning more than half the national vote.  Actually, we&#8217;d been wrestling with this ourselves, but the outside criticism motivated us to go in and think more carefully about it.  We looked at our model and found some bugs in the code! and some other places where the model could be improved.  And we even did some work on our between-state covariance matrix.</p>\n<p>We could tell when looking into this that the changes in our model would not have huge effects&#8212;of course they wouldn&#8217;t, given that we&#8217;d carefully tested our earlier model on 2008, 2012, and 2016&#8212;so we kept up our old model while we fixed up the new one, and then after about a week we were read and we released the improved model (<a href=\"https://projects.economist.com/us-2020-forecast/president/how-this-works\">go here</a> and scroll down to &#8220;Updated August 5th, 2020&#8221;).</p>\n<p><strong>4.  Vote intentions vs. the ultimate official vote count.</strong></p>\n<p>I was talking with someone about my doubts that a forecast that allowed Biden to get only 42% of the vote in Florida, and I got the following response:</p>\n<blockquote><p>Your model may be better than Nate&#8217;s in using historical and polling data.  But historical and polling data don&#8217;t help you much when one of the parties has transformed into a cult of personality that will go the extra mile to suppress opposing votes.</p></blockquote>\n<p>I responded:</p>\n<blockquote><p>How does cult of personality get to Trump winning 58% of votes in Florida?</p></blockquote>\n<p>He responded:</p>\n<blockquote><p>Proposition: Vote-suppression act X is de-facto legal and constitutional as long as SCOTUS doesn&#8217;t enforce an injunction against act X.</p></blockquote>\n<p>This made me realize that in talking about the election, we should distinguish between two things:</p>\n<p>1.  <em>Vote intentions.</em>  The total number of votes for each candidate, if everyone who wants to vote gets to vote and if all these votes are counted.</p>\n<p>2.  <em>The official vote count.</em>  Whatever that is, after some people decide not to vote because the usual polling places are closed and the new polling places are too crowded, or because they planned to vote absentee but their ballots arrived too late (this happened to me on primary day this year!), or because they followed all the rules and voted absentee but then the post office didn&#8217;t postmark their votes, or because their ballot is ruled invalid for some reason, or whatever.</p>\n<p>Both these vote counts matter.  Vote intentions matter, and the official vote count matters.  Indeed, if they differ by enough, we could have a constitutional crisis.</p>\n<p>But here&#8217;s the point.  Poll-aggregation procedures such as Fivethirtyeight&#8217;s and ours at the Economist are entirely forecasting vote intentions.  Polls are vote intentions, and any validation of these models is based on past elections, where sure there have been some gaps between vote intentions and the official vote count (notably Florida in <a href=\"https://www.cambridge.org/core/journals/perspectives-on-politics/article/wrong-man-is-president-overvotes-in-the-2000-presidential-election-in-florida/104AC05BBC067AF162E778889446C67C\">2000</a>), but nothing like what it would take to get a candidate&#8217;s vote share from, say, 47% down to 42%.</p>\n<p>When Nate Silver says, &#8220;this year’s uncertainty is about average, which means that the historical accuracy of polls in past campaigns is a reasonably good guide to how accurate they are this year,” he&#8217;s talking about vote intentions, not about potential irregularities in the vote count.</p>\n<p>If you want to model the possible effects of vote suppression, that can make sense&#8212;<a href=\"https://www.economist.com/united-states/2020/08/22/more-mail-in-voting-doubles-the-chances-of-recounts-in-close-states\">here&#8217;s Elliott Morris&#8217;s analysis</a>, which I haven&#8217;t looked at in detail myself&#8212;but we should be clear that this is separate from, or in addition to, poll aggregation.</p>\n<p><strong>Summary</strong></p>\n<p>I think that [42%, 60%] is way too wide as a 95% interval for Biden&#8217;s share of the two-party vote in Florida, and I suspect that Fivethirtyeight ended up with this super-wide interval because they messed up with their correlation model.</p>\n<p>A naive take on this might be that the super-wide interval could be plausible because maybe some huge percentage of mail-in ballots will be invalidated, but, if so, this isn&#8217;t in the Fivethirtyeight procedure (or in our Economist model), as these forecasts are based on poll aggregation and are validated based on past elections which have not had massive voting irregularities.  If you&#8217;re concerned about problems with the vote count, this is maybe worth being concerned about, but it&#8217;s a completely separate issue from how to aggregate polls and fundamentals-based forecasts.</p>\n<p><strong>P.S.</strong>  A correspondent pointed me to <a href=\"https://www.realclearpolitics.com/elections/betting_odds/2020_president/\">this summary</a> of betting odds, which suggests that the bettors see the race as a 50/50 tossup.  I&#8217;ve talked <a href=\"https://statmodeling.stat.columbia.edu/2020/06/24/election-odds-update/\">earlier</a> about my skepticism regarding betting odds; still, 50/50 is a big difference between anything you&#8217;d expect from the polls <em>or</em> the economic and political fundamentals.  I think a lot of this 50% for Trump is coming from some assessed probability of irregularities in vote counting.  If the election is disputed, I have no idea how these betting services will decide who gets paid off.</p>\n<p>Or you could disagree with me entirely and say that Trump has  a legit chance at 58% of the two-party vote preference in Florida come election day.  Then you&#8217;d have a different model than we have.</p>\n<p><img src=\"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Screen-Shot-2020-08-29-at-1.04.24-AM.png\" alt=\"\" width=\"150\" /></p>\n<img src=\"http://feeds.feedburner.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~4/xmpfFOfOIwI\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Mon, 31 Aug 2020 13:10:32 +0000","feedId":2015,"bgimg":"","linkMd5":"895467df50fd49e17692e9e766da6cd3","bgimgJsdelivr":"","metaImg":"","author":"Andrew","articleImgCdnMap":{"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Screen-Shot-2020-08-26-at-8.55.28-AM.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn35@2020_5/2020/09/01/01-57-57-456_ce0b355a6d6bcfe6.webp","https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Screen-Shot-2020-08-29-at-1.04.24-AM.png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn45@2020_1/2020/09/01/01-57-56-396_b7f5aacb84667c57.webp","http://feeds.feedburner.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~4/xmpfFOfOIwI":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn64@2020_3/2020/09/01/01-57-56-243_6f25311237ea20d2.webp"},"publishedOrCreatedDate":1598925474256}],"record":{"createdTime":"2020-09-01 09:57:54","updatedTime":"2020-09-01 09:57:54","feedId":2015,"fetchDate":"Tue, 01 Sep 2020 01:57:54 +0000","fetchMs":1966,"handleMs":14339,"totalMs":26720,"newArticles":0,"totalArticles":50,"status":1,"type":0,"ip":"0b4009930f9af5660966c5695d71af7c","hostName":"us-020*","requestId":"6e53a8edb50041d89994278ddc4f88e8_2015","contentType":"text/xml; charset=UTF-8","totalBytes":205054,"bgimgsTotal":0,"bgimgsGithubTotal":0,"articlesImgsTotal":8,"articlesImgsGithubTotal":8,"successGithubMap":{"myreaderx32":1,"myreaderx10":1,"myreaderx11":1,"myreaderx3":1,"myreaderx33":1,"myreaderx12":1,"myreaderx1":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-24 21:31:35","updatedTime":"2020-09-01 09:22:18","id":2015,"name":"Statistical Modeling, Causal Inference, and Social Science","url":"http://feeds.feedburner.com/StatisticalModelingCausalInferenceAndSocialScience","subscriber":null,"website":null,"icon":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2013/02/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx64/cdn58@2020_3/2020/09/01/01-57-52-821_3b9b6e8030d1b18a.ico","description":"","weekly":null,"link":"https://statmodeling.stat.columbia.edu"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":205054,"tmpBgImgCdnBytes":0,"extra4":{"start":1598925454345,"total":0,"statList":[{"spend":5572,"msg":"获取xml内容"},{"spend":14339,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":6204,"msg":"正文链接上传到cdn"}]},"extra5":8,"extra6":8,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-037.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-24.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-57.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-011.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxGFxeX0AA0S7P.png","sourceStatusCode":200,"destWidth":285,"destHeight":246,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn51@2020_5/2020/09/01/01-57-56-323_e6db14196f32129f.webp","sourceBytes":16744,"destBytes":9300,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":1029,"convertSpendMs":5,"createdTime":"2020-09-01 09:57:56","host":"us-011*","referer":"https://statmodeling.stat.columbia.edu/?p=44436","linkMd5ListStr":"456c3680b521d39f1f5e59e38e9a290f","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.4 KB","destSize":"9.1 KB","compressRate":"55.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~4/xmpfFOfOIwI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn64@2020_3/2020/09/01/01-57-56-243_6f25311237ea20d2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":932,"convertSpendMs":2,"createdTime":"2020-09-01 09:57:56","host":"europe63*","referer":"https://statmodeling.stat.columbia.edu/?p=44409","linkMd5ListStr":"895467df50fd49e17692e9e766da6cd3","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxgjxbVoAA5z95.jpg","sourceStatusCode":200,"destWidth":254,"destHeight":183,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn40@2020_3/2020/09/01/01-57-56-358_c5e8e83fc4ca3637.webp","sourceBytes":9782,"destBytes":7232,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":1128,"convertSpendMs":9,"createdTime":"2020-09-01 09:57:56","host":"us-036*","referer":"https://statmodeling.stat.columbia.edu/?p=44436","linkMd5ListStr":"456c3680b521d39f1f5e59e38e9a290f","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.6 KB","destSize":"7.1 KB","compressRate":"73.9%"},{"code":1,"isDone":false,"source":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Screen-Shot-2020-08-29-at-1.04.24-AM.png","sourceStatusCode":200,"destWidth":410,"destHeight":332,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn45@2020_1/2020/09/01/01-57-56-396_b7f5aacb84667c57.webp","sourceBytes":107582,"destBytes":15930,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":1164,"convertSpendMs":9,"createdTime":"2020-09-01 09:57:56","host":"us-037*","referer":"https://statmodeling.stat.columbia.edu/?p=44409","linkMd5ListStr":"895467df50fd49e17692e9e766da6cd3","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"105.1 KB","destSize":"15.6 KB","compressRate":"14.8%"},{"code":1,"isDone":false,"source":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/EgxghkGUMAAuDNW.jpg","sourceStatusCode":200,"destWidth":330,"destHeight":344,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn57@2020_4/2020/09/01/01-57-56-994_e17983eec94d9faa.webp","sourceBytes":21390,"destBytes":14390,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":1854,"convertSpendMs":8,"createdTime":"2020-09-01 09:57:56","host":"europe-25*","referer":"https://statmodeling.stat.columbia.edu/?p=44436","linkMd5ListStr":"456c3680b521d39f1f5e59e38e9a290f","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"20.9 KB","destSize":"14.1 KB","compressRate":"67.3%"},{"code":1,"isDone":false,"source":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Egw7NUkXcAEHyqr.png","sourceStatusCode":200,"destWidth":819,"destHeight":641,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn53@2020_4/2020/09/01/01-57-57-132_fd917f111a3f9d75.webp","sourceBytes":166773,"destBytes":60486,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":2176,"convertSpendMs":24,"createdTime":"2020-09-01 09:57:56","host":"europe-24*","referer":"https://statmodeling.stat.columbia.edu/?p=44436","linkMd5ListStr":"456c3680b521d39f1f5e59e38e9a290f","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"162.9 KB","destSize":"59.1 KB","compressRate":"36.3%"},{"code":1,"isDone":false,"source":"https://statmodeling.stat.columbia.edu/wp-content/uploads/2020/08/Screen-Shot-2020-08-26-at-8.55.28-AM.png","sourceStatusCode":200,"destWidth":1332,"destHeight":978,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn35@2020_5/2020/09/01/01-57-57-456_ce0b355a6d6bcfe6.webp","sourceBytes":332725,"destBytes":97572,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":2601,"convertSpendMs":122,"createdTime":"2020-09-01 09:57:56","host":"europe-58*","referer":"https://statmodeling.stat.columbia.edu/?p=44409","linkMd5ListStr":"895467df50fd49e17692e9e766da6cd3","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"324.9 KB","destSize":"95.3 KB","compressRate":"29.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/StatisticalModelingCausalInferenceAndSocialScience/~4/3dU36ecjnTc","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn38@2020_1/2020/09/01/01-57-59-014_2fe305b58a22f4ec.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":2015,"totalSpendMs":879,"convertSpendMs":2,"createdTime":"2020-09-01 09:57:58","host":"europe-57*","referer":"https://statmodeling.stat.columbia.edu/?p=44436","linkMd5ListStr":"456c3680b521d39f1f5e59e38e9a290f","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"}],"successGithubMap":{"myreaderx32":1,"myreaderx10":1,"myreaderx11":1,"myreaderx3":1,"myreaderx33":1,"myreaderx12":1,"myreaderx1":1,"myreaderx":1},"failGithubMap":{}}