{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-04-05 17:52:45","updatedTime":"2021-04-05 17:52:45","title":"Maximum Entropy RL (Provably) Solves Some Robust RL Problems","link":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","description":"<!-- twitter -->\n<meta name=\"twitter:title\" content=\"Maximum Entropy RL (Provably) Solves Some Robust RL Problems\" />\n\n<meta name=\"twitter:card\" content=\"summary_large_image\" />\n\n<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_maxent.gif\" />\n\n<meta name=\"keywords\" content=\"reinforcement, learning\" />\n\n<meta name=\"description\" content=\"The BAIR Blog\" />\n\n<meta name=\"author\" content=\"Ben Eysenbach\" />\n\n<p>Nearly all real-world applications of reinforcement learning involve some degree of shift between the training environment and the testing environment. However, prior work has observed that even small shifts in the environment cause most RL algorithms to perform <a href=\"https://arxiv.org/abs/1703.06907\">markedly</a> <a href=\"https://arxiv.org/abs/1610.01283\">worse</a>.\nAs we aim to scale reinforcement learning algorithms and apply them in the real world, it is increasingly important to learn policies that are robust to changes in the environment.</p>\n\n<p style=\"text-align:center; float:right; width:50%; padding-left:15px;\npadding-right:15px; margin-bottom:0px\">\n<img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/robust_rl.gif\" width=\"90%\" />\n<br />\n<i><b>Robust reinforcement learning</b> maximizes reward on an adversarially-chosen environment.</i>\n</p>\n\n<p>Broadly, prior approaches to handling distribution shift in RL aim to maximize performance in either the average case or the worst case. The first set of approaches, such as domain randomization, train a policy on a distribution of environments, and optimize the average performance of the policy on these environments. While these methods have been successfully applied to a number of areas\n(e.g., <a href=\"https://arxiv.org/abs/1804.09364\">self-driving cars</a>, <a href=\"https://arxiv.org/abs/1804.10332\">robot locomotion</a> and <a href=\"https://arxiv.org/abs/1703.06907\">manipulation</a>),\ntheir success rests critically on the <a href=\"https://arxiv.org/abs/1910.07113\">design of the distribution of environments</a>.\nMoreover, policies that do well on average are not guaranteed to get high reward on every environment. The policy that gets the highest reward on average might get very low reward on a small fraction of environments. The second set of approaches, typically referred to as <strong>robust RL</strong>, focus on the worst-case scenarios. The aim is to find a policy that gets high reward on every environment within some set. Robust RL can equivalently be viewed as a <a href=\"https://www.youtube.com/watch?v=xfyK03MEZ9Q&amp;t=17093s\">two-player game</a> between the policy and an environment adversary. The policy tries to get high reward, while the environment adversary tries to tweak the dynamics and reward function of the environment so that the policy gets lower reward. One important property of the robust approach is that, unlike domain randomization, it is invariant to the ratio of easy and hard tasks. Whereas robust RL always evaluates a policy on the most challenging tasks, domain randomization will predict that the policy is better if it is evaluated on a distribution of environments with more easy tasks.</p>\n\n<!--more-->\n\n<p>Prior work has suggested a number of algorithms for solving robust RL problems. Generally, these algorithms all follow the same recipe: take an existing RL algorithm and add some additional machinery on top to make it robust.\nFor example, <a href=\"https://www.ri.cmu.edu/pub_files/pub3/bagnell_james_2001_1/bagnell_james_2001_1.pdf\">robust value iteration</a> uses Q-learning as the base RL algorithm, and modifies the Bellman update by solving a convex optimization problem in the inner loop of each Bellman backup.\nSimilarly, <a href=\"http://proceedings.mlr.press/v70/pinto17a/pinto17a.pdf\">Pinto ‘17</a> uses TRPO as the base RL algorithm and periodically updates the environment based on the behavior of the current policy. These prior approaches are often difficult to implement and, even once implemented correctly, they requiring tuning of many additional hyperparameters. Might there be a simpler approach, an approach that does not require additional hyperparameters and additional lines of code to debug?</p>\n\n<p>To answer this question, we are going to focus on a type of RL algorithm known as maximum entropy RL, or <strong>MaxEnt RL</strong> for short (<a href=\"https://proceedings.neurips.cc/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf\">Todorov ‘06</a>, <a href=\"http://www.roboticsproceedings.org/rss08/p45.pdf\">Rawlik ‘08</a>, <a href=\"https://www.cs.uic.edu/pub/Ziebart/Publications/thesis-bziebart.pdf\">Ziebart ‘10</a>).\nMaxEnt RL is a slight variant of standard RL that aims to learn a policy that gets high reward while acting as randomly as possible; formally, MaxEnt maximizes the entropy of the policy. Some <a href=\"https://arxiv.org/abs/1812.11103\">prior</a> <a href=\"https://openreview.net/forum?id=r1xPh2VtPB\">work</a> has observed empirically that MaxEnt RL algorithms appear to be robust to some disturbances the environment.\nTo the best of our knowledge, no prior work has actually proven that MaxEnt RL is robust to environmental disturbances.</p>\n\n<p>In a <a href=\"https://arxiv.org/abs/2103.06257\">recent paper</a>, we prove that every MaxEnt RL problem corresponds to maximizing a lower bound on a robust RL problem. Thus, when you run MaxEnt RL, you are implicitly solving a robust RL problem. Our analysis provides a theoretically-justified explanation for the empirical robustness of MaxEnt RL, and proves that <em>MaxEnt RL is itself a robust RL algorithm.</em>\nIn the rest of this post, we’ll provide some intuition into why MaxEnt RL should be robust and what sort of perturbations MaxEnt RL is robust to. We’ll also show some experiments demonstrating the robustness of MaxEnt RL.</p>\n\n<h1 id=\"intuition\">Intuition</h1>\n\n<p>So, why would we expect MaxEnt RL to be robust to disturbances in the environment? Recall that MaxEnt RL trains policies to not only maximize reward, but to do so while acting as randomly as possible. In essence, the policy itself is injecting as much noise as possible into the environment, so it gets to “practice” recovering from disturbances. Thus, if the change in dynamics appears like just a disturbance in the original environment, our policy has already been trained on such data. Another way of viewing MaxEnt RL is as learning many different ways of solving the task (<a href=\"https://www.cs.uic.edu/pub/Ziebart/Publications/thesis-bziebart.pdf\">Kappen ‘05</a>). For example, let’s look at the task shown in videos below: we want the robot to push the white object to the green region. The top two videos show that standard RL always takes the shortest path to the goal, whereas MaxEnt RL takes many different paths to the goal. Now, let’s imagine that we add a new obstacle (red blocks) that wasn’t included during training. As shown in the videos in the bottom row, the policy learned by standard RL almost always collides with the obstacle, rarely reaching the goal. In contrast, the MaxEnt RL policy often chooses routes around the obstacle, continuing to reach the goal for a large fraction of trials.</p>\n\n<p style=\"text-align:center;\">\n<table>\n<tr>\n  <td style=\"border-top:none; border-bottom:none\">\n  </td>\n  <td style=\"border-top:none; border-bottom:none; text-align:center; padding:0px\">\n    Standard RL\n  </td>\n  <td style=\"border-top:none; border-bottom:none; text-align:center; padding:0px\">\n    MaxEnt RL\n  </td>\n</tr>\n<tr>\n  <td style=\"border-top:none; border-bottom:none; padding:0px;\n  vertical-align:middle;\">\n    <p style=\"text-align:center;\">Trained and evaluated without the obstacle:</p>\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_standard.gif\" width=\"100%\" />\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_maxent.gif\" width=\"100%\" />\n  </td>\n</tr>\n<tr>\n  <td style=\"border-top:none; border-bottom:none; padding:0px;\n  vertical-align:middle;\">\n    <p style=\"text-align:center;\">Trained without the obstacle, but evaluated with\n    the obstacle:</p>\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_standard.gif\" width=\"100%\" />\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_maxent.gif\" width=\"100%\" />\n  </td>\n</tr>\n</table>\n</p>\n\n<h1 id=\"theory\">Theory</h1>\n\n<p>We now formally describe the technical results from the paper. The aim here is not to provide a full proof (see the paper Appendix for that), but instead to build some intuition for what the technical results say. Our main result is that, when you apply MaxEnt RL with some reward function and some dynamics, you are actually maximizing a lower bound on the robust RL objective. To explain this result, we must first define the MaxEnt RL objective:\n$J_{MaxEnt}(\\pi; p, r)$ is the entropy-regularized cumulative return of policy $\\pi$ when evaluated using dynamics $p(s’ \\mid s, a)$ and reward function $r(s, a)$. While we will train the policy using one dynamics $p$, we will evaluate the policy on a different dynamics, $\\tilde{p}(s’ \\mid s, a)$, chosen by the adversary. We can now formally state our main result as follows:</p>\n\n<script type=\"math/tex; mode=display\">\\min_{\\tilde{p} \\in \\tilde{\\mathcal{P}}(\\pi)} J_\\text{MaxEnt}(\\pi; \\tilde{p},\nr) \\ge \\exp(J_\\text{MaxEnt}(\\pi; p, \\bar{r}) + \\log T).</script>\n\n<p>The left-hand-side is the robust RL objective. It says that the adversary gets\nto choose whichever dynamics function $\\tilde{p}(s’ \\mid s, a)$ makes our policy perform as poorly as\npossible, subject to some constraints (as specified by the set $\\tilde{\\mathcal{P}}$).  On\nthe right-hand-side we have the MaxEnt RL objective (note that $\\log T$ is a\nconstant, and the function $\\exp(\\cdots)$ is always increasing). Thus, this objective\nsays that a policy that has a high entropy-regularized reward (right hand-side)\nis guaranteed to also get high reward when evaluated on an adversarially-chosen\ndynamics.</p>\n\n<p>The most important part of this equation is the set $\\tilde{\\mathcal{P}}$ of dynamics that\nthe adversary can choose from. Our analysis describes precisely how this set is\nconstructed and shows that, if we want a policy to be robust to a larger set of\ndisturbances, all we have to do is increase the weight on the entropy term and\ndecrease the weight on the reward term. Intuitively, the adversary must choose\ndynamics that are “close” to the dynamics on which the policy was trained. For\nexample, in the special case where the dynamics are linear-Gaussian, this set\ncorresponds to all perturbations where the original expected next state and the\nperturbed expected next state have a Euclidean distance less than $\\epsilon$.</p>\n\n<h1 id=\"more-experiments\">More Experiments</h1>\n\n<p>Our analysis predicts that MaxEnt RL should be robust to many types of\ndisturbances. The first set of videos in this post showed that MaxEnt RL is robust to \nstatic obstacles. MaxEnt RL is also robust to dynamic perturbations introduced in the\nmiddle of an episode. To demonstrate this, we took the same robotic pushing task\nand knocked the puck out of place in the middle of the episode. The videos below\nshow that the policy learned by MaxEnt RL is more robust at handling these\nperturbations, as predicted by our analysis.</p>\n\n<p style=\"text-align:center;\">\n<table style=\"width:70%; margin-left:auto; margin-right:auto\">\n<tr>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <p style=\"text-align:center; margin-bottom:0px\">Standard RL</p>\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <p style=\"text-align:center; margin-bottom:0px\">MaxEnt RL</p>\n  </td>\n</tr>\n<tr>\n  <td style=\"border-top:none; border-bottom:none; padding:0px; vertical-align:middle;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_force_standard_v2_opt.gif\" width=\"100%\" />\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px; vertical-align:middle;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_force_maxent_v2_opt.gif\" width=\"100%\" />\n  </td>\n</tr>\n</table>\n<p style=\"text-align:center;\"><i>The policy learned by MaxEntRL is robust to dynamic perturbations of the puck (red frames).\n</i></p>\n</p>\n\n<p>Our theoretical results suggest that, even if we optimize the environment\nperturbations so the agent does as poorly as possible, MaxEnt RL policies will\nstill be robust. To demonstrate this capability, we trained both standard RL and\nMaxEnt RL on a peg insertion task shown below. During evaluation, we changed the\nposition of the hole to try to make each policy fail. If we only moved the hole\nposition a little bit ($\\le$ 1 cm), both policies always solved the task. However,\nif we moved the hole position up to 2cm, the policy learned by standard RL\nalmost never succeeded in inserting the peg, while the MaxEnt RL policy\nsucceeded in 95% of trials. This experiment validates our\ntheoretical findings that MaxEnt really is robust to (bounded) adversarial\ndisturbances in the environment.</p>\n\n<p style=\"text-align:center;\">\n<table>\n<colgroup>\n<col span=\"1\" style=\"width: 27%;\" />\n<col span=\"1\" style=\"width: 27%;\" />\n<col span=\"1\" style=\"width: 45%;\" />\n</colgroup>\n                                 \n<tr>\n  <td style=\"border-top:none; border-bottom:none; padding:0px; vertical-align:middle;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_standard_long.gif\" width=\"100%\" />\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px; vertical-align:middle;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_maxent_long.gif\" width=\"100%\" />\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px; vertical-align:middle;\">\n    <img src=\"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_minimax_100.png\" width=\"100%\" />\n  </td>\n</tr>\n<tr>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <p style=\"text-align:center;\">Standard RL</p>\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <p style=\"text-align:center;\">MaxEnt RL</p>\n  </td>\n  <td style=\"border-top:none; border-bottom:none; padding:0px\">\n    <p style=\"text-align:center;\">Evaluation on adversarial perturbations</p>\n  </td>\n</tr>\n</table>\n<p style=\"text-align:center;\">\n<i>MaxEnt RL is robust to adversarial perturbations of the hole (where the robot\ninserts the peg).</i></p>\n</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>In summary, <a href=\"https://arxiv.org/abs/2103.06257\">our paper</a> shows that a commonly-used type of RL algorithm, MaxEnt\nRL, is already solving a robust RL problem. We do not claim that MaxEnt RL will\noutperform purpose-designed robust RL algorithms. However, the striking\nsimplicity of MaxEnt RL compared with other robust RL algorithms suggests that\nit may be an appealing alternative to practitioners hoping to equip their RL\npolicies with an ounce of robustness.</p>\n\n<p><strong>Acknowledgements</strong>\nThanks to Gokul Swamy, Diba Ghosh, Colin Li, and Sergey Levine for feedback on drafts of this post,\nand to Chloe Hsu and Daniel Seita for help with the blog.</p>\n\n<hr />\n\n<p>This post is based on the following paper:</p>\n\n<ul>\n  <li><a href=\"https://arxiv.org/abs/2103.06257\">Maximum Entropy RL (Provably) Solves Some Robust RL Problems</a>. <br />\n<a href=\"https://ben-eysenbach.github.io/\">Benjamin Eysenbach</a> and <a href=\"https://people.eecs.berkeley.edu/~svlevine/\">Sergey Levine</a>.</li>\n</ul>\n\n","descriptionType":"text/html","publishedDate":"Tue, 09 Mar 2021 09:00:00 +0000","feedId":8140,"bgimg":"","linkMd5":"1a95881aa73d91ef5afa5d04bc7cd1eb","bgimgJsdelivr":"","metaImg":"","author":"","articleImgCdnMap":{"https://bair.berkeley.edu/static/blog/maxent-robust-rl/robust_rl.gif":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn45@2020_3/2021/04/05/09-52-46-580_46f0fad8db8fca9f.webp","https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_standard.gif":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn74@2020_6/2021/04/05/09-52-47-375_275fe6d1b1efa320.webp","https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_maxent.gif":null,"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_standard.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn50@2020_4/2021/04/05/09-52-47-616_609c26c46f493b20.webp","https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_maxent.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn58@2020_6/2021/04/05/09-52-51-766_e542d44564b72b49.webp","https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_force_standard_v2_opt.gif":null,"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_force_maxent_v2_opt.gif":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn61@2020_2/2021/04/05/09-54-05-749_433df5112cfbdabf.webp","https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_standard_long.gif":null,"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_maxent_long.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn65@2020_3/2021/04/05/09-52-48-612_97f0f75ad911d1b1.webp","https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_minimax_100.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn54@2020_6/2021/04/05/09-52-46-253_0e661f05b94fc874.webp"},"publishedOrCreatedDate":1617616365110}],"record":{"createdTime":"2021-04-05 17:52:45","updatedTime":"2021-04-05 17:52:45","feedId":8140,"fetchDate":"Mon, 05 Apr 2021 09:52:45 +0000","fetchMs":1880,"handleMs":4396,"totalMs":129579,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"d14e31baf87e33c96e7a5992ca4c5253","hostName":"europe66*","requestId":"c1a4076bd6594f2d9d04b82a285c3dde_8140","contentType":"application/xml","totalBytes":3949278,"bgimgsTotal":0,"bgimgsGithubTotal":0,"articlesImgsTotal":10,"articlesImgsGithubTotal":7,"successGithubMap":{"myreaderx25":1,"myreaderx15":1,"myreaderx27":1,"myreaderx1":1,"myreaderx5oss":1,"myreaderx29":1,"myreaderx18":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}},"feed":{"createdTime":"2020-08-25 04:34:09","updatedTime":"2020-08-25 07:54:41","id":8140,"name":"The Berkeley Artificial Intelligence Research Blog","url":"http://bair.berkeley.edu/blog/feed.xml","subscriber":null,"website":null,"icon":"http://bair.berkeley.edu/favicon.ico","icon_jsdelivr":null,"description":"The BAIR Blog","weekly":null,"link":"http://bair.berkeley.edu"},"noPictureArticleList":[{"createdTime":"2021-04-05 17:54:48","updatedTime":"2021-04-05 17:54:48","id":null,"feedId":8140,"linkMd5":"1a95881aa73d91ef5afa5d04bc7cd1eb"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":3949278,"tmpBgImgCdnBytes":0,"extra4":{"start":1617616358683,"total":0,"statList":[{"spend":2031,"msg":"获取xml内容"},{"spend":4396,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":123151,"msg":"正文链接上传到cdn"}]},"extra5":10,"extra6":9,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_standard_long.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"sourceBytes":139599,"destBytes":16766,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2531,"convertSpendMs":360,"createdTime":"2021-04-05 17:52:45","host":"europe63*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn81/contents/2021/04/05/09-52-47-493_aff29fad2b0c5cdf.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 05 Apr 2021 09:52:47 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["CBF6:B714:48D8135:4A7ED27:606ADDEF"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1617617223"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn81/contents/2021/04/05/09-52-47-493_aff29fad2b0c5cdf.webp","historyStatusCode":[],"spendMs":175},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"136.3 KB","destSize":"16.4 KB","compressRate":"12%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_standard_long.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"sourceBytes":139599,"destBytes":16766,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":910,"convertSpendMs":230,"createdTime":"2021-04-05 17:52:47","host":"us-033*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn81/contents/2021/04/05/09-52-48-598_aff29fad2b0c5cdf.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 05 Apr 2021 09:52:48 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A79C:21F6:9A540B:1DD7B0A:606ADDF0"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1617617223"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn81/contents/2021/04/05/09-52-48-598_aff29fad2b0c5cdf.webp","historyStatusCode":[],"spendMs":36},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"136.3 KB","destSize":"16.4 KB","compressRate":"12%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_maxent.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"sourceBytes":1302300,"destBytes":121048,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":4711,"convertSpendMs":3408,"createdTime":"2021-04-05 17:52:45","host":"us-001*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn77/contents/2021/04/05/09-52-49-904_5e40ba653c9d18e4.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 05 Apr 2021 09:52:49 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["9B2A:6F55:287F4C5:4542889:606ADDF1"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1617617189"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn77/contents/2021/04/05/09-52-49-904_5e40ba653c9d18e4.webp","historyStatusCode":[],"spendMs":45},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"118.2 KB","compressRate":"9.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_maxent.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"sourceBytes":1302300,"destBytes":121048,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3248,"convertSpendMs":2346,"createdTime":"2021-04-05 17:52:50","host":"us-009*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn77/contents/2021/04/05/09-52-53-297_5e40ba653c9d18e4.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 05 Apr 2021 09:52:53 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A8FE:06D4:2E5EF7E:4AC004C:606ADDF5"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1617617189"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn77/contents/2021/04/05/09-52-53-297_5e40ba653c9d18e4.webp","historyStatusCode":[],"spendMs":49},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"118.2 KB","compressRate":"9.3%"},null,null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-025.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-037.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-001.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe-59.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_minimax_100.png","sourceStatusCode":200,"destWidth":1800,"destHeight":900,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn54@2020_6/2021/04/05/09-52-46-253_0e661f05b94fc874.webp","sourceBytes":97509,"destBytes":56214,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1232,"convertSpendMs":154,"createdTime":"2021-04-05 17:52:45","host":"us-009*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"95.2 KB","destSize":"54.9 KB","compressRate":"57.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/robust_rl.gif","sourceStatusCode":200,"destWidth":822,"destHeight":490,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn45@2020_3/2021/04/05/09-52-46-580_46f0fad8db8fca9f.webp","sourceBytes":291276,"destBytes":80190,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":1660,"convertSpendMs":136,"createdTime":"2021-04-05 17:52:45","host":"us-033*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"284.4 KB","destSize":"78.3 KB","compressRate":"27.5%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_empty_standard.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn74@2020_6/2021/04/05/09-52-47-375_275fe6d1b1efa320.webp","sourceBytes":1037006,"destBytes":127370,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2501,"convertSpendMs":952,"createdTime":"2021-04-05 17:52:45","host":"us-013*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1,012.7 KB","destSize":"124.4 KB","compressRate":"12.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_standard.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn50@2020_4/2021/04/05/09-52-47-616_609c26c46f493b20.webp","sourceBytes":971332,"destBytes":109926,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":2708,"convertSpendMs":1280,"createdTime":"2021-04-05 17:52:45","host":"us-021*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"948.6 KB","destSize":"107.3 KB","compressRate":"11.3%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/peg_maxent_long.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn65@2020_3/2021/04/05/09-52-48-612_97f0f75ad911d1b1.webp","sourceBytes":892669,"destBytes":140914,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":3881,"convertSpendMs":2268,"createdTime":"2021-04-05 17:52:45","host":"us-037*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"871.7 KB","destSize":"137.6 KB","compressRate":"15.8%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_obstacle_maxent.gif","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn58@2020_6/2021/04/05/09-52-51-766_e542d44564b72b49.webp","sourceBytes":1283180,"destBytes":123902,"targetWebpQuality":75,"feedId":8140,"totalSpendMs":7280,"convertSpendMs":3155,"createdTime":"2021-04-05 17:52:45","host":"europe-22*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"121 KB","compressRate":"9.7%"},{"code":1,"isDone":false,"source":"https://bair.berkeley.edu/static/blog/maxent-robust-rl/pusher_force_maxent_v2_opt.gif","sourceStatusCode":200,"destWidth":640,"destHeight":480,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn61@2020_2/2021/04/05/09-54-05-749_433df5112cfbdabf.webp","sourceBytes":7025894,"destBytes":3310762,"targetWebpQuality":60,"feedId":8140,"totalSpendMs":20824,"convertSpendMs":18436,"createdTime":"2021-04-05 17:53:45","host":"us-013*","referer":"http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/","linkMd5ListStr":"1a95881aa73d91ef5afa5d04bc7cd1eb","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"6.7 MB","destSize":"3.2 MB","compressRate":"47.1%"}],"successGithubMap":{"myreaderx25":1,"myreaderx15":1,"myreaderx27":1,"myreaderx1":1,"myreaderx5oss":1,"myreaderx29":1,"myreaderx18":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}}