{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2022-03-14 19:18:20","updatedTime":"2022-03-14 19:18:20","title":"Learning from Weakly-Labeled Videos via Sub-Concepts","link":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","description":"<span class=\"byline-author\">Posted by Zizhao Zhang and Guanhang Wu, Software Engineers, Google Research, Cloud AI Team</span> <img src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s1600\" style=\"display: none;\" /> <p>Video recognition is a core task in computer vision with applications from <a href=\"https://cloud.google.com/video-intelligence\">video content analysis</a> to <a href=\"https://ai.googleblog.com/2022/03/co-training-transformer-with-videos-and.html\">action recognition</a>. However, training models for video recognition often requires untrimmed videos to be manually annotated, which can be prohibitively time consuming. In order to reduce the effort of collecting videos with annotations, learning visual knowledge from videos with weak labels, i.e., where the annotation is auto-generated without manual intervention, has attracted growing <a href=\"https://arxiv.org/abs/2007.14937\">research interest</a>, thanks to the large volume of easily accessible video data. Untrimmed videos, for example, are often acquired by querying with keywords for classes that the video recognition model aims to classify. A keyword, which we refer to as a weak label, is then assigned to each untrimmed video obtained.  </p><p>Although large-scale videos with weak labels are easier to collect, training with unverified weak labels poses another challenge in developing robust models. <a href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Ghadiyaram_Large-Scale_Weakly-Supervised_Pre-Training_for_Video_Action_Recognition_CVPR_2019_paper.pdf\">Recent studies</a> have demonstrated that, in addition to the label noise (e.g., incorrect action labels on untrimmed videos), there is temporal noise due to the lack of accurate temporal action localization — i.e., an untrimmed video may include other non-targeted content or may only show the target action in a small proportion of the video.  </p><p>Reducing noise effects for large-scale weakly-supervised pre-training is critical but particularly challenging in practice. Recent work indicates that <a href=\"https://arxiv.org/abs/1905.00561.pdf\">querying short videos</a> (e.g., ~1 minute in length) to obtain more accurate temporal localization of target actions or <a href=\"https://arxiv.org/abs/2003.13042.pdf\">applying a teacher model</a> to do filtering can yield improved results. However, such data pre-processing methods prevent models from fully utilizing available video data, especially longer videos with richer content.           </p><p>In \"<a href=\"https://arxiv.org/pdf/2101.03713.pdf\">Learning from Weakly-Labeled Web Videos via Exploring Sub-Concepts</a>\", we propose a solution to these issues that uses a simple learning framework to conduct effective pre-training on untrimmed videos. Instead of simply filtering the potential temporal noise, this approach converts such “noisy” data to useful supervision by creating a new set of meaningful “middle ground” pseudo-labels that expand the original weak label space, a novel concept we call <em>Sub-Pseudo Label</em> (SPL). The model is pre-trained on this more \"fine-grained\" space and then fine-tuned on a target dataset. Our experiments demonstrate that the learned representations are much better than previous approaches. Moreover, SPL has been shown to be effective in improving the action recognition model quality for <a href=\"https://cloud.google.com/video-intelligence\">Google Cloud Video AI</a>, which enables content producers to easily search through massive libraries of their video assets to quickly source content of interest.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: 1em;\"><tbody>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEj7HDTk6Uh65avz8qG0ul3AMi-s55bcKClzXgaipdikoeSj-Ay3Sd0UxVOIuAM-WiY6JuAK6mhQrO17NWZh3kHZdznIbBv9dX8ZlrciimY9FAQyrfr_6qxSDf2xJx9SrYxNIkaua4qqPeru6Bs3LIv6n7-nT9fddPqKTJxI1UuMm3SrQtjuLBbgnTbZGw=s1492\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"752\" data-original-width=\"1492\" height=\"323\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEj7HDTk6Uh65avz8qG0ul3AMi-s55bcKClzXgaipdikoeSj-Ay3Sd0UxVOIuAM-WiY6JuAK6mhQrO17NWZh3kHZdznIbBv9dX8ZlrciimY9FAQyrfr_6qxSDf2xJx9SrYxNIkaua4qqPeru6Bs3LIv6n7-nT9fddPqKTJxI1UuMm3SrQtjuLBbgnTbZGw=w640-h323\" width=\"640\" /></a></td></tr>  <tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEiwgR2dBtyNBVU0NnbNAwUhRu6n7sfC7PJ81-cSclOY6NpSlAZ-ap_BGZPhajUx_mX7iKGMQFi-fLgACZkgRPp1EIWZnDMXAioqWwVciJZXqtqawCVrmBxGndUiGG_LuG18WQ7uS1kpFAq-3JTg3aFaqYkq98Pezvu8AD8PshxUszyKsrRA1bsZ3tsi6A=s1019\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"628\" data-original-width=\"1019\" height=\"394\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEiwgR2dBtyNBVU0NnbNAwUhRu6n7sfC7PJ81-cSclOY6NpSlAZ-ap_BGZPhajUx_mX7iKGMQFi-fLgACZkgRPp1EIWZnDMXAioqWwVciJZXqtqawCVrmBxGndUiGG_LuG18WQ7uS1kpFAq-3JTg3aFaqYkq98Pezvu8AD8PshxUszyKsrRA1bsZ3tsi6A=w640-h394\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Sampled training clips may represent a different visual action (whisking eggs) from the query label of the whole untrimmed video (baking cookies). SPL converts the potential label noise to useful supervision signals by creating a new set of “middle ground” pseudo-classes (i.e., sub-concepts) via extrapolating two related action classes. Enriched supervision is provided for effective model pre-training.</td></tr></tbody></table><p><b>Sub-Pseudo Label (SPL)</b><br />SPL is a simple technique that advances the <a href=\"https://arxiv.org/pdf/1911.04252.pdf\">teacher-student training framework</a>, which is known to be effective for self-training and to improve semi-supervised learning. In the teacher-student framework, a teacher model is trained on high-quality labeled data and then assigns pseudo-labels to unlabeled data. The student model trains on both high-quality labeled data and the unlabeled data that has the teacher-predicted labels. While previous methods have proposed a number of ways to improve the pseudo-label quality, SPL takes a novel approach that combines knowledge from both weak labels (i.e., query text used to acquire data) and teacher-predicted labels, which results in better pseudo-labels overall. This method focuses on video recognition where temporal noise is challenging, but it can be extended easily to other domains, like image classification. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: 1em;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s1600\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"600\" data-original-width=\"1600\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s16000\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">The overall pre-training framework for learning from weakly labeled videos via SPLs. Each trimmed video clip is re-labeled using SPL given the teacher-predicted labels and the weak labels used to query the corresponding untrimmed video.</td></tr></tbody></table><p>The SPL method is motivated by the observation that within an untrimmed video “noisy” video clips have semantic relations with the target action (i.e., the weak label class), but may also include essential visual components of other actions, such as the teacher model–predicted class. Our approach uses the extrapolated SPLs from weak labels together with the distilled labels to capture the enriched supervision signals, encouraging learning better representations during pre-training that can be used for downstream fine-tuning tasks.  </p><p>It is straightforward to determine the SPL class for each video clip. We first perform inference on each video clip using the teacher model trained from a target dataset to get a teacher prediction class. Each clip is also labeled by the class (i.e., query text) of the untrimmed source video. A 2-dimensional <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> is used to summarize the alignments between the teacher model inferences and the original weak annotations. Based on this confusion matrix, we conduct label extrapolation between teacher model predictions and weak labels to obtain the raw SPL label space.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: 1em;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEhA_JU-q08tGAjK3A-njUaI2NWr7V8J6VlSTE8XPhV1aYE6_8YClc-gXF9Q5MuBt9O1QLT9vTAnGL-GVDibTzNZbrdEKFjcK4prC3OIWwSujItXk1Ae4Qzuy70S79jVNQoP9o-pZz63TtBO_968tYO92TpGVtWOIvr9QsaM6DtSBaWYCi49hUZWBucNrw=s1944\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"736\" data-original-width=\"1944\" height=\"242\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEhA_JU-q08tGAjK3A-njUaI2NWr7V8J6VlSTE8XPhV1aYE6_8YClc-gXF9Q5MuBt9O1QLT9vTAnGL-GVDibTzNZbrdEKFjcK4prC3OIWwSujItXk1Ae4Qzuy70S79jVNQoP9o-pZz63TtBO_968tYO92TpGVtWOIvr9QsaM6DtSBaWYCi49hUZWBucNrw=w640-h242\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Left:</b> The confusion matrix, which is the basis of the raw SPL label space. <b>Middle:</b> The resulting SPL label spaces (16 classes in this example). <b>Right: </b>SPL-B, another SPL version, that reduces the label space by collating agreed and disagreed entries of each row as independent SPL classes, which in this example results in only 8 classes.</td></tr></tbody></table><p><b>Effectiveness of SPL</b><br />We evaluate the effectiveness of SPL in comparison to different pre-training methods applied to a <a href=\"https://arxiv.org/abs/1711.07971.pdf\">3D ResNet50</a> model that is fine-tuned on <a href=\"https://arxiv.org/abs/1705.06950.pdf\">Kinetics-200</a> (K200). One pre-training approach simply initializes the model using <a href=\"https://www.image-net.org/\">ImageNet</a>. The other pre-training methods use 670k video clips sampled from an internal dataset of 147k videos, collected following standard processes similar to those described for Kinetics-200, that cover a broad range of actions. Weak label training and teacher prediction training use either the weak labels or teacher-predicted labels on the videos, respectively. Agreement filtering uses only the training data for which the weak labels and teacher-predicted labels match. We find that SPL outperforms each of these methods. Though the dataset used to illustrate the SPL approach was constructed for this work, in principle the method we describe applies to any dataset that has weak labels.  </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr>   <td><b>Pre-training Method</b>    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td><b>Top-1</b>    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td><b>Top-5</b>   </td>  </tr>  <tr>   <td>ImageNet Initialized     </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>80.6    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>94.7    </td>  </tr>  <tr>   <td>Weak Label Train    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>82.8    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>95.6    </td>  </tr>  <tr>   <td>Teacher Prediction Train    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>81.9    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>95.0    </td>  </tr>  <tr>   <td>Agreement Filtering Train    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>82.9    </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td>95.4    </td>  </tr>  <tr>   <td><b><em>SPL</em></b>   </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td><b>84.3</b>   </td><td>&nbsp;&nbsp;&nbsp;&nbsp;</td>   <td><b>95.7</b>   </td>  </tr></tbody></table><p>We also demonstrate that sampling more video clips from a given number of untrimmed videos can help improve the model performance. With a sufficient number of video clips available, SPL consistently outperforms weak label pre-training by providing enriched supervision.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEirNec1Z8F_tmBA0C8Ln8qbxxqkESQdqNqI8UEzUP1Xoe3jeSBFeJWxmW4j45uBE7S47DU1YFqI3xDGTu78Pw8arOi0aOnE5XcpZi8_Ql86RaWHQBhNhKZWTWVt0jUqmnZllT5ippLgpdjguXtrM3SQQB6uxoX1U2tQ6WeNjBVfCl-PMr0656D2r6uNWg=s1200\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"737\" data-original-width=\"1200\" height=\"394\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEirNec1Z8F_tmBA0C8Ln8qbxxqkESQdqNqI8UEzUP1Xoe3jeSBFeJWxmW4j45uBE7S47DU1YFqI3xDGTu78Pw8arOi0aOnE5XcpZi8_Ql86RaWHQBhNhKZWTWVt0jUqmnZllT5ippLgpdjguXtrM3SQQB6uxoX1U2tQ6WeNjBVfCl-PMr0656D2r6uNWg=w640-h394\" width=\"640\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">As more clips are sampled from 147K videos, the label noise is increased gradually. SPL becomes more and more effective at utilizing the weakly-labeled clips to achieve better pre-training.</td></tr></tbody></table><p>We visualize the visual concepts learned from SPL with attention visualization by applying <a href=\"https://arxiv.org/abs/1610.02391.pdf\">Grad-CAM</a> on the trained model. It is interesting to observe some meaningful “middle ground” concepts that can be learned by SPL. </p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: 1em;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://blogger.googleusercontent.com/img/a/AVvXsEjeX6cggXNnR4pgcK9dn_DhRPN6MYZF1ydToW3m_ahWCkXizvjKaHou68_OVyvFTuzs1CCo9vXgBWcF2_BJ23welW45Gyqb_kjA0ZNf8_2-AFyfXV274xc_5MDk2hg1_xCwUbtC3sHUtBpoTb5C-UjCShlP-UQfXjAJg459dv73EP9UnH2yNPLV1WTY9w=s956\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"237\" data-original-width=\"956\" src=\"https://blogger.googleusercontent.com/img/a/AVvXsEjeX6cggXNnR4pgcK9dn_DhRPN6MYZF1ydToW3m_ahWCkXizvjKaHou68_OVyvFTuzs1CCo9vXgBWcF2_BJ23welW45Gyqb_kjA0ZNf8_2-AFyfXV274xc_5MDk2hg1_xCwUbtC3sHUtBpoTb5C-UjCShlP-UQfXjAJg459dv73EP9UnH2yNPLV1WTY9w=s16000\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Examples of attention visualization for SPL classes. Some meaningful “middle ground” concepts can be learned by SPL, such as mixing up the eggs and flour (<b>left</b>) and using the abseiling equipment (<b>right</b>).</td></tr></tbody></table><p><b>Conclusion</b><br />We demonstrate that SPLs can provide enriched supervision for pre-training. SPL does not increase training complexity and can be treated as an off-the-shelf technique to integrate with teacher-student–based training frameworks. We believe this is a promising direction for discovering meaningful visual concepts by bridging weak labels and the knowledge distilled from teacher models. SPL has also demonstrated promising generalization to the image recognition domain and we expect future extensions that apply to tasks that have noise in labels. We have successfully applied SPL for Google Cloud Video AI where it has improved the accuracy of the action recognition models, helping users to better understand, search, and monetize their video content library. </p><p><b>Acknowledgements</b><br /><em>We gratefully acknowledge the contributions of other co-authors, including Kunpeng Li, Xuehan Xiong, Chen-Yu Lee, Zhichao Lu, Yun Fu, Tomas Pfister. We also thank Debidatta Dwibedi, David A Ross, Chen Sun, Jonathan C. Stroud, and Wei Hua for their valuable comments and help on this work, and Tom Small for figure creation.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=Ry3lYQtJOSQ:Fyk2y6Jf61Y:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div>","descriptionType":"html","publishedDate":"Mon, 07 Mar 2022 18:08:00 +0000","feedId":4244,"bgimg":"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s1600","linkMd5":"0d69c0b9ca1ec08686b53ba49470bf8f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn92@2020_3/2022/03/14/11-18-30-423_b3bf989ce028fccf.webp","destWidth":1600,"destHeight":600,"sourceBytes":983048,"destBytes":363068,"author":"Google AI","articleImgCdnMap":{"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s1600":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn92@2020_3/2022/03/14/11-18-30-423_b3bf989ce028fccf.webp","https://blogger.googleusercontent.com/img/a/AVvXsEj7HDTk6Uh65avz8qG0ul3AMi-s55bcKClzXgaipdikoeSj-Ay3Sd0UxVOIuAM-WiY6JuAK6mhQrO17NWZh3kHZdznIbBv9dX8ZlrciimY9FAQyrfr_6qxSDf2xJx9SrYxNIkaua4qqPeru6Bs3LIv6n7-nT9fddPqKTJxI1UuMm3SrQtjuLBbgnTbZGw=w640-h323":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn5@2020_5/2022/03/14/11-18-31-149_86432c6162f8703c.webp","https://blogger.googleusercontent.com/img/a/AVvXsEiwgR2dBtyNBVU0NnbNAwUhRu6n7sfC7PJ81-cSclOY6NpSlAZ-ap_BGZPhajUx_mX7iKGMQFi-fLgACZkgRPp1EIWZnDMXAioqWwVciJZXqtqawCVrmBxGndUiGG_LuG18WQ7uS1kpFAq-3JTg3aFaqYkq98Pezvu8AD8PshxUszyKsrRA1bsZ3tsi6A=w640-h394":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn2@2020_6/2022/03/14/11-18-31-174_c0e9da4f2a58aa37.webp","https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s16000":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn14@2020_1/2022/03/14/11-18-43-787_372b40338f206237.webp","https://blogger.googleusercontent.com/img/a/AVvXsEhA_JU-q08tGAjK3A-njUaI2NWr7V8J6VlSTE8XPhV1aYE6_8YClc-gXF9Q5MuBt9O1QLT9vTAnGL-GVDibTzNZbrdEKFjcK4prC3OIWwSujItXk1Ae4Qzuy70S79jVNQoP9o-pZz63TtBO_968tYO92TpGVtWOIvr9QsaM6DtSBaWYCi49hUZWBucNrw=w640-h242":"https://cdn.jsdelivr.net/gh/myreaderx/cdn95@2020_5/2022/03/14/11-18-31-052_64718637d73a270d.webp","https://blogger.googleusercontent.com/img/a/AVvXsEirNec1Z8F_tmBA0C8Ln8qbxxqkESQdqNqI8UEzUP1Xoe3jeSBFeJWxmW4j45uBE7S47DU1YFqI3xDGTu78Pw8arOi0aOnE5XcpZi8_Ql86RaWHQBhNhKZWTWVt0jUqmnZllT5ippLgpdjguXtrM3SQQB6uxoX1U2tQ6WeNjBVfCl-PMr0656D2r6uNWg=w640-h394":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn16@2020_4/2022/03/14/11-18-31-061_3f8639a28b434ff6.webp","https://blogger.googleusercontent.com/img/a/AVvXsEjeX6cggXNnR4pgcK9dn_DhRPN6MYZF1ydToW3m_ahWCkXizvjKaHou68_OVyvFTuzs1CCo9vXgBWcF2_BJ23welW45Gyqb_kjA0ZNf8_2-AFyfXV274xc_5MDk2hg1_xCwUbtC3sHUtBpoTb5C-UjCShlP-UQfXjAJg459dv73EP9UnH2yNPLV1WTY9w=s16000":null,"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn99@2020_4/2022/03/14/11-18-30-955_483d6fcb94af4f84.webp"},"publishedOrCreatedDate":1647256700828}],"record":{"createdTime":"2022-03-14 19:18:20","updatedTime":"2022-03-14 19:18:20","feedId":4244,"fetchDate":"Mon, 14 Mar 2022 11:18:20 +0000","fetchMs":1591,"handleMs":67,"totalMs":25013,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"bc1eecef1292254c09de6c1e66b750f3","hostName":"europe62*","requestId":"3dd6122b035c490796219d409da76cfd_4244","contentType":"text/xml; charset=UTF-8","totalBytes":814234,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":8,"articlesImgsGithubTotal":7,"successGithubMap":{"myreaderx25":1,"myreaderx8":1,"myreaderx27":1,"myreaderx4":1,"myreaderx2":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{"myreaderx31":1}},"feed":{"createdTime":"2020-08-25 04:29:40","updatedTime":"2020-09-01 10:46:06","id":4244,"name":"Google AI Blog","url":"http://googleresearch.blogspot.com/feeds/posts/default","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn78@2020_3/2020/09/01/02-46-06-599_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2022-03-14 19:18:44","updatedTime":"2022-03-14 19:18:44","id":null,"feedId":4244,"linkMd5":"0d69c0b9ca1ec08686b53ba49470bf8f"}],"tmpCommonImgCdnBytes":363068,"tmpBodyImgCdnBytes":451166,"tmpBgImgCdnBytes":0,"extra4":{"start":1647256699124,"total":0,"statList":[{"spend":1641,"msg":"获取xml内容"},{"spend":67,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":13324,"msg":"正文链接上传到cdn"}]},"extra5":8,"extra6":8,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEjeX6cggXNnR4pgcK9dn_DhRPN6MYZF1ydToW3m_ahWCkXizvjKaHou68_OVyvFTuzs1CCo9vXgBWcF2_BJ23welW45Gyqb_kjA0ZNf8_2-AFyfXV274xc_5MDk2hg1_xCwUbtC3sHUtBpoTb5C-UjCShlP-UQfXjAJg459dv73EP9UnH2yNPLV1WTY9w=s16000","sourceStatusCode":200,"destWidth":956,"destHeight":237,"sourceBytes":294356,"destBytes":41638,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":333,"convertSpendMs":45,"createdTime":"2022-03-14 19:18:30","host":"us-027*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx31/cdn9/contents/2022/03/14/11-18-31-176_2a10fbed1482dc8c.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69855631.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 11:18:31 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["ABAC:6B85:4C6D9AA:927F97A:622F2487"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647260081"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx31/cdn9/contents/2022/03/14/11-18-31-176_2a10fbed1482dc8c.webp","historyStatusCode":[],"spendMs":101},"base64UserPassword":null,"token":"da243******************************d9e47"},"githubUser":"myreaderx31","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"287.5 KB","destSize":"40.7 KB","compressRate":"14.1%"},{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEjeX6cggXNnR4pgcK9dn_DhRPN6MYZF1ydToW3m_ahWCkXizvjKaHou68_OVyvFTuzs1CCo9vXgBWcF2_BJ23welW45Gyqb_kjA0ZNf8_2-AFyfXV274xc_5MDk2hg1_xCwUbtC3sHUtBpoTb5C-UjCShlP-UQfXjAJg459dv73EP9UnH2yNPLV1WTY9w=s16000","sourceStatusCode":200,"destWidth":956,"destHeight":237,"sourceBytes":294356,"destBytes":41638,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":284,"convertSpendMs":26,"createdTime":"2022-03-14 19:18:31","host":"us-027*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx31/cdn9/contents/2022/03/14/11-18-31-596_2a10fbed1482dc8c.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69855631.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 14 Mar 2022 11:18:31 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["ABAC:6B85:4C6D9C9:927F9BF:622F2487"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1647260081"],"x-ratelimit-resource":["core"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx31/cdn9/contents/2022/03/14/11-18-31-596_2a10fbed1482dc8c.webp","historyStatusCode":[],"spendMs":34},"base64UserPassword":null,"token":"da243******************************d9e47"},"githubUser":"myreaderx31","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"287.5 KB","destSize":"40.7 KB","compressRate":"14.1%"}],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-039.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-002.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe61.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-027.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s1600","sourceStatusCode":200,"destWidth":1600,"destHeight":600,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn92@2020_3/2022/03/14/11-18-30-423_b3bf989ce028fccf.webp","sourceBytes":983048,"destBytes":363068,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":9783,"convertSpendMs":8977,"createdTime":"2022-03-14 19:18:20","host":"us-024*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f,0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"960 KB","destSize":"354.6 KB","compressRate":"36.9%"},{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEhA_JU-q08tGAjK3A-njUaI2NWr7V8J6VlSTE8XPhV1aYE6_8YClc-gXF9Q5MuBt9O1QLT9vTAnGL-GVDibTzNZbrdEKFjcK4prC3OIWwSujItXk1Ae4Qzuy70S79jVNQoP9o-pZz63TtBO_968tYO92TpGVtWOIvr9QsaM6DtSBaWYCi49hUZWBucNrw=w640-h242","sourceStatusCode":200,"destWidth":640,"destHeight":242,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn95@2020_5/2022/03/14/11-18-31-052_64718637d73a270d.webp","sourceBytes":24288,"destBytes":9612,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":347,"convertSpendMs":9,"createdTime":"2022-03-14 19:18:30","host":"us-012*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23.7 KB","destSize":"9.4 KB","compressRate":"39.6%"},{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEj7HDTk6Uh65avz8qG0ul3AMi-s55bcKClzXgaipdikoeSj-Ay3Sd0UxVOIuAM-WiY6JuAK6mhQrO17NWZh3kHZdznIbBv9dX8ZlrciimY9FAQyrfr_6qxSDf2xJx9SrYxNIkaua4qqPeru6Bs3LIv6n7-nT9fddPqKTJxI1UuMm3SrQtjuLBbgnTbZGw=w640-h323","sourceStatusCode":200,"destWidth":640,"destHeight":323,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn5@2020_5/2022/03/14/11-18-31-149_86432c6162f8703c.webp","sourceBytes":208509,"destBytes":31900,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":501,"convertSpendMs":52,"createdTime":"2022-03-14 19:18:30","host":"us-039*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"203.6 KB","destSize":"31.2 KB","compressRate":"15.3%"},{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEirNec1Z8F_tmBA0C8Ln8qbxxqkESQdqNqI8UEzUP1Xoe3jeSBFeJWxmW4j45uBE7S47DU1YFqI3xDGTu78Pw8arOi0aOnE5XcpZi8_Ql86RaWHQBhNhKZWTWVt0jUqmnZllT5ippLgpdjguXtrM3SQQB6uxoX1U2tQ6WeNjBVfCl-PMr0656D2r6uNWg=w640-h394","sourceStatusCode":200,"destWidth":640,"destHeight":393,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn16@2020_4/2022/03/14/11-18-31-061_3f8639a28b434ff6.webp","sourceBytes":40208,"destBytes":16432,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":504,"convertSpendMs":15,"createdTime":"2022-03-14 19:18:30","host":"us-003*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"39.3 KB","destSize":"16 KB","compressRate":"40.9%"},{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEiwgR2dBtyNBVU0NnbNAwUhRu6n7sfC7PJ81-cSclOY6NpSlAZ-ap_BGZPhajUx_mX7iKGMQFi-fLgACZkgRPp1EIWZnDMXAioqWwVciJZXqtqawCVrmBxGndUiGG_LuG18WQ7uS1kpFAq-3JTg3aFaqYkq98Pezvu8AD8PshxUszyKsrRA1bsZ3tsi6A=w640-h394","sourceStatusCode":200,"destWidth":640,"destHeight":394,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn2@2020_6/2022/03/14/11-18-31-174_c0e9da4f2a58aa37.webp","sourceBytes":133943,"destBytes":29844,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":505,"convertSpendMs":68,"createdTime":"2022-03-14 19:18:30","host":"us-004*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"130.8 KB","destSize":"29.1 KB","compressRate":"22.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn99@2020_4/2022/03/14/11-18-30-955_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":682,"convertSpendMs":12,"createdTime":"2022-03-14 19:18:30","host":"europe61*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://blogger.googleusercontent.com/img/a/AVvXsEgnHzaLovF90hbwRbWqH98Zbcv4QwsUQPaB_4ppcoyNj5QAhzzqqZE8XaQyI9L0KfdS9fOPiaOXsXFlc0ZJr8FpTSUz1xRtDbVCKBPf0bqQdNo0WiFYNJWsax7IkYVTRixwbrwsiY7uwqzzZOcwQY_4SnhRv9D2LTvIUyGdZfq7mC4Tb5f1DxJtksme6Q=s16000","sourceStatusCode":200,"destWidth":1600,"destHeight":600,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn14@2020_1/2022/03/14/11-18-43-787_372b40338f206237.webp","sourceBytes":983048,"destBytes":363068,"targetWebpQuality":75,"feedId":4244,"totalSpendMs":13162,"convertSpendMs":12676,"createdTime":"2022-03-14 19:18:30","host":"us-002*","referer":"http://ai.googleblog.com/2022/03/learning-from-weakly-labeled-videos-via.html","linkMd5ListStr":"0d69c0b9ca1ec08686b53ba49470bf8f","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"960 KB","destSize":"354.6 KB","compressRate":"36.9%"}],"successGithubMap":{"myreaderx25":1,"myreaderx8":1,"myreaderx27":1,"myreaderx4":1,"myreaderx2":1,"myreaderx19":1,"myreaderx":1},"failGithubMap":{"myreaderx31":1}}