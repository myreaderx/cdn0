{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"For Centuries, People Dreamed of a Machine That Could Produce Language. Then OpenAI Made One","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one","description":"<p><em>This is part six of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>.</em></p> \n<p>In February of this year, <a href=\"https://openai.com/\">OpenAI</a>, one of the foremost artificial intelligence labs in the world, announced that a team of researchers had built a powerful new text generator called the Generative Pre-Trained Transformer 2, or <a href=\"https://openai.com/blog/better-language-models/\">GPT-2</a> for short. The researchers used a <a href=\"https://spectrum.ieee.org/tag/reinforcement+learning\">reinforcement learning</a> algorithm to train their system on a broad set of natural language processing (NLP) capabilities, including reading comprehension, machine translation, and the ability to generate long strings of coherent text.&nbsp;</p> \n<p>But as is often the case with NLP technology, the tool held both great promise and great peril. Researchers and policy makers at the lab were concerned that their system, if widely released, could be exploited by bad actors and misappropriated for “malicious purposes.”</p> \n<p>The people of OpenAI, which defines its mission as “discovering and enacting the path to safe artificial general intelligence,” were concerned that GPT-2 could be used to flood the Internet with fake text, thereby degrading an already fragile information ecosystem. For this reason, OpenAI decided that it would not release the full version of GPT-2 to the public or other researchers.</p> \n<!--nextpage--> \n<p>GPT-2 is an example of a technique in NLP called language modeling, whereby the computational system internalizes a statistical blueprint of a text so it’s able to mimic it. Just like the predictive text on your phone—which selects words based on words you’ve used before—GPT-2 can look at a string of text and then predict what the next word is likely to be based on the probabilities inherent in that text.</p> \n<p>GPT-2 can be seen as a descendant of the statistical language modeling that the Russian mathematician <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models\">A. A. Markov developed</a> in the early 20<sup>th</sup> century (covered in part three of this series)<em>.</em></p> \n<aside class=\"inlay pullquote rt med\">\n  GPT-2 used cutting-edge machine learning algorithms to do linguistic analysis with over 1.5 million parameters. \n</aside> \n<p>What’s different with GPT-2, though, is the scale of the textual data modeled by the system. Whereas Markov analyzed a string of 20,000 letters to create a rudimentary model that could predict the likelihood of the next letter of a text being a consonant or a vowel, GPT-2 used 8 million articles scraped from Reddit to predict what the next word might be within that entire dataset.</p> \n<p>And whereas Markov manually trained his model by counting only two parameters—vowels and consonants—GPT-2 used cutting-edge machine learning algorithms to do linguistic analysis with&nbsp;over 1.5 million&nbsp;parameters, burning through huge amounts of computational power in the process.</p> \n<p>The results were impressive. In <a href=\"https://openai.com/blog/better-language-models/\">their blog post</a>, OpenAI reported that GPT-2 could generate synthetic text in response to prompts, mimicking whatever style of text&nbsp;it was shown. If you prompt the system with a line of William Blake’s poetry, it can generate a line back in the Romantic poet’s style. If you prompt the system with a cake recipe, you get a newly invented recipe in response.</p> \n<p>Perhaps the most compelling feature of GPT-2 is that it can answer questions accurately. For example, when OpenAI researchers asked the system, “Who wrote the book <em>The Origin of Species?</em>”—it responded: “Charles Darwin.” While only able to respond accurately some of the time, the feature does seem to be a limited realization of Gottfried Leibniz’s <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas\">dream of a language-generating machine</a> that could answer any and all human questions (described in part two of this series).</p> \n<p>After observing the power of the new system in&nbsp;practice, OpenAI elected not to release the fully trained model. In the lead up to its release in February, there had been heightened awareness about “<a href=\"https://spectrum.ieee.org/tag/deepfakes\">deepfakes</a>”—synthetic images and videos, generated via machine learning techniques, in which people do and say things they haven’t really done and said. Researchers at OpenAI worried that GPT-2 could be used to essentially create deepfake text, making it harder for people to trust textual information online.</p> \n<p>Responses to this decision varied.&nbsp;On one hand, OpenAI’s caution prompted an <a href=\"https://www.skynettoday.com/briefs/gpt2\">overblown reaction in the media</a>, with articles about the “dangerous” technology feeding into the Frankenstein narrative that often surrounds developments in AI.</p> \n<p>Others took issue with OpenAI’s self-promotion, with some even suggesting that OpenAI purposefully exaggerated&nbsp;GPT-2s power in order to create hype—while&nbsp;contravening a norm in the AI research community, where labs routinely share data, code, and pre-trained models. As machine learning researcher Zachary Lipton <a href=\"https://twitter.com/zacharylipton/status/1097021572714504192?s=20\">tweeted</a>, “Perhaps what's *most remarkable* about the <a href=\"https://twitter.com/OpenAI\">@OpenAI</a> controversy is how *unremarkable* the technology is. Despite their outsize attention &amp; budget, the research itself is perfectly ordinary—right in the main branch of deep learning NLP research.”</p> \n<p>OpenAI stood by its decision to release only a limited version of GPT-2, but has since released larger models for other researchers and the public to experiment with. As yet, there has been no reported case of a widely distributed fake news article generated by the system. But there have been a number of interesting spin-off projects, including <a href=\"https://twitter.com/rossgoodwin/status/1131442974812823552\">GPT-2 poetry</a> and a <a href=\"https://talktotransformer.com/\">webpage</a> where you can prompt the system with questions yourself.</p> \n<aside class=\"inlay pullquote lt med\">\n  Mimicking humans on Reddit, the bots have long conversations about a variety of topics, including conspiracy theories and \n <em>Star Wars</em> movies.&nbsp; \n</aside> \n<p>There’s even a <a href=\"https://www.reddit.com/r/SubSimulatorGPT2/\">Reddit group</a> populated entirely with&nbsp;text produced by GPT-2-powered bots. Mimicking humans on Reddit, the bots have long conversations about a variety of topics, including conspiracy theories and <em>Star Wars</em> movies.</p> \n<p>This bot-powered conversation may signify the new condition of life online, where language is increasingly created by a combination of human and non-human agents, and where maintaining the distinction between human and non-human, despite our best efforts, is increasingly difficult.</p> \n<p>The idea of using rules, mechanisms, and algorithms to generate language has inspired people in many different cultures throughout history. But it’s in the online world that this powerful form of wordcraft may really find its natural milieu—in an environment where the identity of speakers becomes more ambiguous, and perhaps, less relevant. It remains to be seen what the consequences will be for language, communication, and our sense of human identity, which is so bound up with our ability to speak in natural language.</p> \n<p><em>This is the sixth installment of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. Last week’s post explained how an innocent Microsoft chatbot <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation\">turned instantly racist on Twitter</a>. </em></p> \n<p><em>You can also check out our prior series on the </em><a href=\"https://spectrum.ieee.org/tag/AI+history\"><em>untold history of AI</em></a><em>.</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=3hDuhYQdo-A:oooi_DFqXNQ:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=3hDuhYQdo-A:oooi_DFqXNQ:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=3hDuhYQdo-A:oooi_DFqXNQ:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=3hDuhYQdo-A:oooi_DFqXNQ:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=3hDuhYQdo-A:oooi_DFqXNQ:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=3hDuhYQdo-A:oooi_DFqXNQ:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/3hDuhYQdo-A\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 02 Dec 2019 13:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxODY5OQ.jpeg","linkMd5":"eaa12b83fdd04d32713423e0cead6909","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn25@2020_5/2020/08/25/10-38-01-465_7e2cc88753233054.webp","destWidth":1240,"destHeight":930,"sourceBytes":144919,"destBytes":71344,"author":"Oscar Schwartz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=3hDuhYQdo-A:oooi_DFqXNQ:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn15@2020_1/2020/08/25/10-38-03-748_eb25a2b5a8425bd6.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=3hDuhYQdo-A:oooi_DFqXNQ:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn64@2020_2/2020/08/25/10-38-09-708_0ae6703a0f3e7276.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/3hDuhYQdo-A":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn48@2020_5/2020/08/25/10-38-05-511_9e44c058eddf499b.webp"},"publishedOrCreatedDate":1598351859712},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"AI Faces Speed Bumps and Potholes on Its Road From the Research Lab to Everyday Use","link":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/rigid-it-departments-jobhopping-data-scientists-implementing-machine-learning-isnt-easy","description":"<p>Implementing machine learning in the real world isn’t easy. The tools are available and&nbsp;the road is well-marked—but the speed bumps are many.</p> \n<p>That was the conclusion of panelists wrapping up a day of discussions at the <a href=\"https://ieee.aisymposium.org/\">IEEE AI Symposium 2019</a>, held at Cisco’s San Jose, Calif., campus last week.</p> \n<p>The toughest problem, says <a href=\"https://blogs.cisco.com/author/benirving\">Ben Irving</a>, senior manager of <a href=\"http://www.cisco.com/_\">Cisco</a>’s strategy innovations group, is people.</p> \n<p>It’s tough to find data scientist expertise, he indicated, so companies are looking into non-traditional sources of personnel, like political science. “There are some untapped areas with a lot of untapped data science expertise,” Irving says.</p> \n<p><a href=\"https://www.lazard.com/\">Lazard</a>’s artificial intelligence manager Trevor Mottl agreed that would-be data scientists don’t need formal training or experience to break into the field. “This field is changing really rapidly,” he says. “There are new language models&nbsp;coming out every month, and new tools, so [anyone should] expect to not know everything. Experiment, try out new tools and techniques, read, study, spend time; there aren’t any true experts at this point because the foundational elements are shifting so rapidly.”</p> \n<p>“It is a wonderful time to get into a field,” he reasons, noting that it&nbsp;doesn’t take long to catch up because there aren’t 20 years of history.”</p> \n<p>Confusion about what&nbsp;different kinds of machine learning specialists do doesn’t help the personnel situation. An audience member asked panelists to explain the difference between data scientist, data analyst, and data engineer. Darrin Johnson, Nvidia global director of technical marketing for enterprise, admitted it’s hard to sort out, and any two companies could define the positions differently. “Sometimes,” he says, particularly at smaller companies, “a data scientist plays all three roles. But as companies grow, there are different groups that ingest data, clean data, and use data. At some companies, training and inference are separate. It really depends, which is a challenge when you are trying to hire someone.”</p> \n<p><strong>Mitigating the risks of a hot job market</strong></p> \n<p>The competition to hire data scientists, analysts, engineers, or whatever companies call them requires that managers&nbsp;make sure any work being done is structured and comprehensible at all times, the panelists cautioned.</p> \n<p>“We need to remember that our data scientists go home every day and sometimes they don’t come back because they go home and then go to a different company,” says Lazard’s Mottl. “That’s a fact of life. If you give people choice on [how they do development], and have a successful person who gets poached by competitor, you have to either hire a team to unwrap what that person built or jettison their work and rebuild it.”</p> \n<p>By contrast, he says, “places that have structured coding and structured commits and organized constructions of software have done very well.”</p> \n<p>But keeping all of a company’s engineers working with the same languages and on the same development paths is not easy to do in a field that moves as fast as machine learning.&nbsp;Zongjie Diao, Cisco director of product management for machine learning, quipped: “I have a data scientist friend who says the speed at which he changes girlfriends is less than speed at which he changes languages.”</p> \n<p><strong>The data scientist/IT manager clash</strong></p> \n<p>Once a company finds the data engineers and scientists&nbsp;they need and get them started on the task of applying machine learning to that company’s operations, one of the first obstacles they face just might be the company’s IT department, the panelists suggested.</p> \n<p>“IT is process oriented,” Mottl says. The IT team “knows how to keep data secure, to set up servers. But when you bring in a data science team, they want sandboxes, they want freedom, they want to explore and play.”</p> \n<p>Also, Nvidia’s Johnson pointed out, “There is a language barrier.” The AI world, he says, is very different from networking or storage, and data scientists find it hard to articulate their requirements to IT.</p> \n<p><strong>On the ground or in the cloud?</strong></p> \n<p>And then there is the decision of where exactly machine learning should happen—on site, or in the cloud? At Lazard, Mottl says, the deep learning engineers do their experimentation on premises; that’s their sandbox. “But when we deploy, we deploy in the cloud,” he says.</p> \n<p>Nvidia, Johnson says, thinks the opposite approach is better. We see the cloud as “the sandbox,” he says.&nbsp;“So you can run as many experiments as possible, fail fast, and learn faster.”</p> \n<p>For Cisco’s Irving, the “where” of machine learning depends on the confidentiality of the data.</p> \n<p>Mottl, who says r<span>olling machine learning technology into operation can hit resistance from all across the company,</span>&nbsp;had one last word of caution for those aiming to implement AI:</p> \n<blockquote>\n  Data scientists are building things that might change the ways other people in the organization work, like sales and even knowledge workers.&nbsp;[You need to] think about the internal stakeholders and prepare them, because the last thing you want to do is to create a valuable new thing that nobody likes and people take potshots against. \n</blockquote> \n<p>The AI Symposium was organized by the Silicon Valley chapters of the <a href=\"http://sites.ieee.org/scv-yp/about/\">IEEE Young Professionals</a>, the <a href=\"https://californiaconsultants.org/\">IEEE Consultants’ Network</a>, and <a href=\"http://sites.ieee.org/scv-wie/\">IEEE Women in Engineering</a> and supported by <a href=\"http://www.cisco.com/\">Cisco</a>.</p> \n<p><em>A version of this post appears in the November 2019 print magazine as “For AI Rollouts, Hazards Reported Ahead.”</em></p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WWCYX6DRQI0:9DgkI1lrnnM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WWCYX6DRQI0:9DgkI1lrnnM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WWCYX6DRQI0:9DgkI1lrnnM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WWCYX6DRQI0:9DgkI1lrnnM:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WWCYX6DRQI0:9DgkI1lrnnM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WWCYX6DRQI0:9DgkI1lrnnM:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WWCYX6DRQI0\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 01 Oct 2019 15:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4NTUwNQ.jpeg","linkMd5":"110e6ac838f5a686ca9016e11c381c4e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn6@2020_4/2020/08/25/10-38-01-261_fe79eadf32a07dbb.webp","destWidth":1240,"destHeight":930,"sourceBytes":147129,"destBytes":82716,"author":"Tekla S. Perry","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WWCYX6DRQI0:9DgkI1lrnnM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn70@2020_1/2020/08/25/10-38-16-974_105415ceaf344f1f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WWCYX6DRQI0:9DgkI1lrnnM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn30@2020_5/2020/08/25/10-38-16-574_a4488da2ad10b037.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WWCYX6DRQI0":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn22@2020_3/2020/08/25/10-38-02-347_25d4fcbac5c7ecb9.webp"},"publishedOrCreatedDate":1598351859730},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Help Rescuers Find Missing Persons With Drones and Computer Vision","link":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/help-rescuers-find-missing-persons-through-emergency-response-contest","description":"<p>There’s a definite sense that robots are destined to become a critical part of search and rescue missions and disaster relief efforts, working alongside humans to help first responders&nbsp;move faster and more efficiently. And we’ve seen all kinds of studies that include&nbsp;the claim&nbsp;“<a href=\"https://xkcd.com/2128/\">this robot could potentially help with disaster relief</a>,” to varying degrees&nbsp;of plausibility.</p> \n<p>But it takes a long time, and a lot of extra effort, for academic research to actually become anything useful—especially for first responders, where there isn’t a lot of financial incentive for further development.</p> \n<p>It turns out that if you actually <em>ask</em> first responders what they most need for disaster relief, they’re not necessarily interested in the latest and greatest robotic platform or other futuristic technology. They’re using commercial off-the-shelf drones, often consumer-grade ones, because they’re simple and cheap and great at surveying large areas. The challenge is doing something useful with all of the imagery that these drones collect. Computer vision algorithms could help with that, as long as those algorithms&nbsp;are readily accessible and nearly effortless to use.&nbsp;</p> \n<p>The <a href=\"https://www.ieee-ras.org/\">IEEE Robotics and Automation Society</a> and the <a href=\"http://crasar.org/\">Center for Robotic-Assisted Search and Rescue (CRASAR)</a> at Texas A&amp;M University have <a href=\"https://computervisionrescue.wixsite.com/contest\">launched a contest</a>&nbsp;to bridge this gap between the kinds of tools that roboticists and computer vision researchers might call “basic” and a system that’s useful to first responders in the field. It’s a simple and straightforward idea, and somewhat surprising that no one had thought of it before now. And if you can develop such a system, it’s worth some cash.</p> \n<!--nextpage--> \n<p>CRASAR does already have a <a href=\"http://cver.hrail.crasar.org/\">Computer Vision Emergency Response Toolkit</a> (created right after Hurricane Harvey), which includes a few pixel filters and some edge and corner detectors. Through this contest, you can get paid your share of a $3,000 prize pool for adding some other excessively basic tools, including:</p> \n<ul> \n <li> <p>Image enhancement through histogram equalization, which can be applied to electro-optical (visible light cameras) and thermal imagery</p> </li> \n <li> <p>Color segmentation for a range</p> </li> \n <li> <p>Grayscale segmentation for a range in a thermal image</p> </li> \n</ul> \n<p>If it seems like this contest is really not that hard, that’s because it isn’t. “The first thing to understand about this contest is that strictly speaking, it’s really not that hard,” says <a href=\"http://faculty.cs.tamu.edu/murphy/\">Robin Murphy</a>, director of CRASAR. “This contest isn’t necessarily about coming up with algorithms that are brand new, or even state-of-the-art, but rather algorithms that are functional and reliable and implemented in a way that’s immediately [usable]&nbsp;by&nbsp;inexperienced users in the field.”</p> \n<p>Murphy readily admits that some of what needs to be done is not particularly challenging at all, but that’s not the point—the point is to make these functionalities&nbsp;accessible to folks who have better things to do than solve these problems themselves, as Murphy explains.</p> \n<p>“A lot of my research is driven by problems that I’ve seen in the field that you’d think somebody would have solved, but apparently not. More than half of this is available in OpenCV, but who’s going to find it, download it, learn Python, that kind of thing? We need to get these tools into an open framework. We’re happy if you take libraries that already exist (just don’t steal code)—not everything needs to be rewritten from scratch. Just use what’s already there. Some of it may seem too simple, because it IS that simple. It already exists and you just need to move some code around.”</p> \n<p>If you want to get very slightly more complicated, there’s a second category that involves a little bit of math:</p> \n<blockquote> \n <p>Coders must provide a system that does the following for each nadir image in a set:</p> \n <ol> \n  <li>Reads the geotag embedded in the .jpg</li> \n  <li>Overlays a USNG grid for a user-specified interval (e.g., every 50, 100, or 200 meters)</li> \n  <li>Gives the GPS coordinates of each pixel if a cursor is rolled over the image</li> \n  <li>Given a set of images with the GPS or USNG coordinate and a bounding box, finds all images in the set that have a pixel intersecting that location</li> \n </ol> \n</blockquote> \n<p>The final category awards prizes to&nbsp;anyone who comes up with anything else that turns out to be useful. Or, more specifically, “entrants can submit any algorithm they believe will be of value.” Whether or not it’s actually of value will be up to a panel of judges that includes both first responders and computer vision experts. More detailed rules can be found <a href=\"https://computervisionrescue.wixsite.com/contest\">here</a>, along with sample datasets that you can use for testing.</p> \n<p>The contest deadline is 16 December, so you’ve got about a month to submit an entry. Winners will be announced at the beginning of January.</p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=AFnfw88Nq3o:6KgvyGl2jOE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=AFnfw88Nq3o:6KgvyGl2jOE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=AFnfw88Nq3o:6KgvyGl2jOE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=AFnfw88Nq3o:6KgvyGl2jOE:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=AFnfw88Nq3o:6KgvyGl2jOE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=AFnfw88Nq3o:6KgvyGl2jOE:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/AFnfw88Nq3o\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 25 Nov 2019 18:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxOTU0OQ.jpeg","linkMd5":"c70149e9a4334875ea4bbf98504cc029","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn38@2020_1/2020/08/25/10-38-00-356_a93f12e0e4bc5b59.webp","destWidth":1240,"destHeight":930,"sourceBytes":409249,"destBytes":275892,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=AFnfw88Nq3o:6KgvyGl2jOE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn2@2020_1/2020/08/25/10-38-13-551_f519a75b29c0b293.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=AFnfw88Nq3o:6KgvyGl2jOE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn39@2020_4/2020/08/25/10-38-02-356_4a35676c031c9b67.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/AFnfw88Nq3o":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn68@2020_5/2020/08/25/10-38-07-296_d91e67c139ff3049.webp"},"publishedOrCreatedDate":1598351859714},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Skydio 2 Review: This Is the Drone You Want to Fly","link":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","description":"<p>Let me begin this review by saying that the <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio 2</a> is one of the most impressive robots that I have ever seen. Over the last decade, I’ve spent enough time around robots to have a very good sense of what kinds of things are particularly challenging for them, and to set my expectations accordingly. Those expectations include things like “unstructured environments are basically impossible” and “full autonomy is impractically expensive” and “robot videos rarely reflect reality.”</p> \n<p><a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio’s newest drone</a> is an exception to all of this. It’s able to fly autonomously at speed through complex environments in challenging real-world conditions in a way that’s completely effortless and stress-free for the end user, allowing you to capture the kind of video that would be otherwise impossible, even (I’m guessing) for professional drone pilots. When you see this technology in action, it’s (almost) indistinguishable from magic.&nbsp;</p> \n<!--nextpage--> \n<h3>Skydio 2 Price</h3> \n<p>To be clear, the Skydio 2 is not without compromises, and the price of $999 (on pre-order with delivery of the next batch expected in spring of 2020) requires some justification. But the week I’ve had with this drone has left me feeling like its fundamental autonomous capability is so far beyond just about anything that I’ve ever experienced that I’m questioning why I would every fly anything else ever again.</p> \n<p>We’ve written extensively about <a href=\"https://spectrum.ieee.org/tag/skydio\">Skydio</a>, beginning in early 2016 when the company <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydio-camera-drone-autonomous-flying\">posted a video of a prototype drone dodging trees while following a dude on a bike</a>. Even three years ago, Skydio’s tech was way better than anything we’d seen outside of a research lab, and in early 2018, they introduced their first consumer product, the <a href=\"https://robots.ieee.org/robots/skydior1/?utm_source=spectrum\">Skydio R1</a>. A little over a year later, <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio has introduced the Skydio 2</a>, which&nbsp;is smaller, smarter, and much more affordable. Here’s an overview video just to get you caught up:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/imt2qZ7uw1s?rel=0\" width=\"620\"></iframe></p> \n<p>Skydio sent me a Skydio 2 review unit last week, and while <a href=\"https://spectrum.ieee.org/static/tech-expedition-east-africas-big-bet-on-drones\">I’m reasonably experienced with drones in general</a>, this is the first time I’ve tried a Skydio drone in person. I had a pretty good idea what to expect, and I was absolutely blown away. Like, I was giggling to myself while running through the woods as the drone zoomed around, deftly avoiding trees and keeping me in sight. Robots aren’t supposed to be this good.</p> \n<p>A week is really not enough time to explore everything that the Skydio can do, especially Thanksgiving week in Washington, D.C. (a no-fly zone) in early winter. But I found a nearby state park in which I could legally and safely fly the drone, and I did my best to put the Skydio 2 through its paces.</p> \n<p><strong>Note:</strong>&nbsp;Throughout this review, we’ve got a bunch of GIFs to help illustrate different features of the drone. To fit them all in, these GIFs had to be&nbsp;heavily compressed. Underneath each GIF is a timestamped link to <a href=\"https://www.youtube.com/watch?v=psjBufklGwg\" target=\"_blank\">this YouTube video</a> (also available at the bottom of the post), which you can click on to see the an extended cut of the original 4K 30 fps footage. And there’s a bunch of interesting extra video in there as well.</p> \n<h3>Skydio 2 Specs</h3> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjUyMA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 is primarily made out of magnesium, which (while light) is both heavier and more rigid and durable than plastic.&nbsp;The offset props (the back pair are above the body, and the front pair are below) are necessary to maintain the field of view of the navigation cameras. \n  </figcaption> </img> \n</figure> \n<p>The Skydio 2 both looks and feels like a well-designed and carefully thought-out drone. It’s solid, and a little on the heavy side as far as drones go—it’s primarily made out of magnesium, which (while light) is both heavier and more rigid and durable than plastic. The blue and black color scheme is far more attractive than you typically see with drones.&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjczMQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    To detect and avoid obstacles, the Skydio 2 uses an array of six 4K hemispherical cameras that&nbsp;feed data into an NVIDIA Jetson TX2 at 30 fps, with the drone processing a million points in 3D space per second to plan the safest path. \n  </figcaption> </img> \n</figure> \n<p>The Skydio 2 is built around an array of six hemispherical obstacle-avoidance cameras and the <a href=\"https://developer.nvidia.com/embedded/jetson-tx2\">NVIDIA Jetson TX2</a> computing module that they’re connected to. This defines the placement of the gimbal, the motors and props, and the battery, since all of this stuff has to be as much as possible out of the view of the cameras in order for the drone to effectively avoid obstacles in any direction.&nbsp;</p> \n<p>Without the bottom-mounted battery attached, the drone is quite flat. The offset props (the back pair are above the body, and the front pair are below) are necessary to maintain the field of view of the obstacle-avoidance cameras. These hemispherical cameras are on the end of each of the prop arms as well as above and below the body of the drone. They look awfully exposed, even though each is protected from ground contact by a little fin. You need to make sure these cameras are clean and smudge-free, and Skydio includes a cleaning cloth for this purpose. Underneath the drone there are slots for microSD cards, one for recording from the camera and a second one that the drone uses to store data. The attention to detail extends to the SD card insertion, which has a sloped channel that guides the card securely into its slot.&nbsp;</p> \n<p>Once you snap the battery in, the drone goes from looking streamlined to looking a little chubby. Relative to other drones, the battery almost seems like an afterthought, like Skydio designed the drone and then remembered, “oops we have to add a battery somewhere, let’s just kludge it onto the bottom.” But again, the reason for this is to leave room inside the body for the NVIDIA TX2, while making sure that the battery stays out of view of the obstacle avoidance cameras.</p> \n<p>The magnetic latching system for the battery is both solid and satisfying. I’m not sure why it’s necessary, strictly speaking, but I do like it, and it doesn’t seem like the battery will fly off even during the most aggressive maneuvers. Each battery includes an LED array that will display its charge level in 25 percent&nbsp;increments, as well as a button that you push to turn the drone on and off. Charging takes place via a USB-C port in the top of the drone, which I don’t like, because it means that the batteries can’t be charged on their own (like the Parrot Anafi’s battery), and that you can’t charge one battery while flying with another, like basically every other drone ever. A separate battery charger that will charge two at once is available from Skydio for an eyebrow-raising $129.</p> \n<p>I appreciate that all of Skydio’s stuff (batteries, controller, and beacon) charges via USB-C, though. The included USB-C adapter with its beefy cable will output at up to 65 watts, which’ll charge a mostly depleted battery in under an hour. The drone turns itself on while charging, which seems unnecessary.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjczMg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 is not foldable, making it not nearly as easy to transport as some other drones. But it does come with a nice case that mitigates this issue somewhat, and the drone plus two batteries end up as a passably flat package about the size of a laptop case. \n  </figcaption> </img> \n</figure> \n<p>The most obvious compromise that Skydio made with the Skydio 2 is that the drone is not foldable. <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio CEO Adam Bry told us</a> that adding folding joints to the arms of the Skydio 2 would have made calibrating all six cameras a nightmare and significantly impacted performance. This makes complete sense, of course, but it does mean that the Skydio 2 is not nearly as easy to transport as some other drones.&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjczMw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    Folded and unfolded: The Skydio 2 compared to the Parrot Anafi (upper left) and the DJI Mavic Pro (upper right). \n  </figcaption> </img> \n</figure> \n<p>The Skydio 2 does come with a very nice case that mitigates this issue somewhat, and the drone plus two batteries end up as a passably flat package about the size of a laptop case. Still, it’s just not as convenient to toss into a backpack as my Anafi, although the Mavic Mini might be even more portable.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjczOQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    While the Skydio 2’s case is relatively compact, the non-foldable drone is overall a significantly larger package than the Parrot Anafi. \n  </figcaption> </img> \n</figure> \n<p>The design of the drone leads to some other compromises as well. Since landing gear would, I assume, occlude the camera system, the drone lands directly on<span>&nbsp;the bottom of&nbsp;</span>its battery pack, which has a slightly rubberized pad about the size of a playing card. This does’t feel particularly stable unless you end up on a very flat surface, and made me concerned for the exposed cameras underneath the drone as well as the lower set of props. I’d recommend hand takeoffs and landings—more on those later.</p> \n<h3>Skydio 2 Camera System</h3> \n<p><img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjc0MA.jpeg\" /></p> \n<figure class=\"xlrg\" role=\"img\"> \n <figcaption class=\"hi-cap\">\n   Photo: Evan Ackerman/IEEE Spectrum \n </figcaption> \n <figcaption>\n   The Skydio 2’s primary camera is a&nbsp;Sony IMX577 1/2.3\" 12.3-megapixel CMOS sensor. It’s mounted to a three-axis gimbal and records 4K video at 60 fps, or 1080p video at 120 fps. \n </figcaption> \n</figure> \n<p>The Skydio 2 comes with a three-axis gimbal supporting a 12-megapixel camera, just enough to record 4K video at 60 fps, or 1080p video at 120 fps. Skydio has provided <a href=\"https://cdn.shopify.com/s/files/1/2316/4925/files/image-quality-report.pdf?9930\">plenty of evidence</a> that its imaging system is at least as good if not better than other drone cameras. Tested against my Mavic Pro and Parrot Anafi, I found no reason to doubt that. To be clear, I didn’t do exhaustive pixel-peeping comparisons between them, you’re just getting my subjective opinion that the Skydio 2 has a totally decent camera that you won’t be disappointed with. I will say that I found the HDR photo function to be not all that great under the few situations in which I tested it—after looking at a few muddy sunset shots, I turned it off and was much happier.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU2Nw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2’s 12-megapixel camera is solid, although we weren’t impressed with the HDR option. \n  </figcaption> </img> \n</figure> \n<p>The video stabilization is fantastic, to the point where watching the video footage can be underwhelming because it doesn’t reflect the motion of the drone. I almost wish there was a way to change to unstabilized (or less-stabilized) video so that the viewer could get a little more of a wild ride. Or, ideally, there’d be a way for the drone to provide you with a visualization of what it was doing using the data collected by its cameras. That’s probably wishful thinking, though. The drone itself doesn’t record audio because all you’d get would be an annoying buzz, but the app <em>does</em> record audio, so the audio from your phone gets combined with the drone video. Don’t expect great quality, but it’s better than nothing.</p> \n<h3>Skydio 2 App</h3> \n<p>The app is very simple compared to every other drone app I’ve tried, and that’s a good thing. Here’s what it looks like:</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjUyMQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Skydio \n  </figcaption> \n  <figcaption>\n    Trackable subjects get a blue “+” sign over them, and if you tap them, the “+” turns into a spinny blue circle. Once you’ve got a subject selected, you can choose from a variety of cinematic skills that the drone will execute while following you. \n  </figcaption> </img> \n</figure> \n<p>You get the controls that you need and the information that you need, and nothing else. Manual flight with the on-screen buttons works adequately, and the double-tap to fly function on the phone works surprisingly well, making it easy to direct the drone to a particular spot above the ground.</p> \n<p>The settings menus are limited but functional, allowing you to change settings for the camera and a few basic tweaks for controlling the drone. One unique setting to the Skydio 2 is the height floor—since the drone only avoids static obstacles, you can set it to maintain a height of at least 8 feet above the ground while flying autonomously to make sure that if you’re flying around other people, it won’t run into anyone who isn’t absurdly tall and therefore asking for it.</p> \n<p>Trackable subjects get a blue “+” sign over them in the app, and if you tap them, the “+” turns into a spinny blue circle. Once you’ve got a subject selected, you can choose from a variety of cinematic skills that the drone will execute while following you, and in addition, you can select “one-shot” skills that involve the drone performing a specific maneuver before returning to the previously selected cinematic skill. For example, you can tell the drone to orbit around you, and then do a “rocket” one-shot where it’ll fly straight up above you (recording the whole time, of course), before returning to its orbiting.&nbsp;</p> \n<p>After you’re done flying, you can scroll through your videos and easily clip out excerpts from them and save them to your phone for sharing. Again, it’s a fairly simple interface without a lot of options. You could call it limited, I guess, but I appreciate that it just does a few things that you care about and otherwise doesn’t clutter itself up.</p> \n<p>The real limitation of the app is that it uses Wi-Fi to connect to the Skydio 2, which restricts the range. To fly much beyond a hundred meters or so, you’ll need to use the controller or beacon instead.</p> \n<h3>Skydio 2 Controller and Beacon</h3> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjc0MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    While the Skydio 2 controller provides a better hands-on flight experience than with the phone, plus an extended range of up to 3.5 km, more experienced pilots may find manual control a bit frustrating, because the underlying autonomy will supersede your maneuvers when you start getting close to objects. \n  </figcaption> </img> \n</figure> \n<p>I was looking forward to using the controller, because with every other drone I’ve had, the precision that a physically controller provides is, I find, mandatory for a good flying experience and to get the photos and videos that you want. With Skydio 2, that’s all out the window. It’s not that the controller is useless or anything, it’s just that because the drone tracks you and avoids obstacles on its own, that level of&nbsp;control precision becomes largely unnecessary.&nbsp;</p> \n<p>The controller itself is perfectly fine. It’s a rebranded Parrot Skycontroller3, which is the same as the one that you get with a <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone\">Parrot Anafi</a>. It’s too bad that the sticks don’t unscrew to make it a little more portable, and overall it’s functional rather than fancy, but it feels good to use and includes a sizeable antenna that makes a significant difference to the range that you get (up to 3.5 kilometers).</p> \n<p>You definitely get a better hands-on flight experience with the controller than with the phone, so if you want to (say) zip the drone around some big open space for fun, it’s good for that. And it’s nice to be able to hand the controller to someone who’s never flown a drone before and let them take it for a spin without freaking out about them crashing it the whole time. For more experienced pilots, though, the controller is ultimately just a bit frustrating, because the underlying autonomy will supersede your control when you start getting close to objects, which (again) limits how useful the controller is relative to your phone.</p> \n<p>I do still prefer the controller over the phone, but I’m not sure that it’s worth the extra $150, unless you plan to fly the Skydio 2 at very long distances or primarily in manual mode. And honestly, if either of those two things are your top priority, the Skydio 2 is probably not the drone for you.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjc0Mw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 beacon uses GPS tracking to help the drone follow you, extending range up to 1.5 km. You can also fly the with the beacon alone, no phone necessary. \n  </figcaption> </img> \n</figure> \n<p>The purpose of the beacon, according to Skydio, is to give the drone a way of tracking you if it can’t see you, which can happen, albeit infrequently. My initial impression of the beacon was that it was primarily useful as a range-extending bridge between my phone and the drone. But I accidentally left my phone at home one day (oops) and had to fly the drone with only the beacon, and it was a surprisingly decent experience. The beacon allows for full manual control of a sort—you can tap different buttons to rotate, fly forward, and ascend or descend. This is sufficient for takeoff, landing, to make sure that the drone is looking at you when you engage visual tracking, and to rescue it if it gets trapped somewhere.</p> \n<p>The rest of the beacon’s control functions are centered around a few different tracking modes, and with these, it works just about as well as your phone. You have fewer options overall, but all the basic stuff is there with just a few intuitive button clicks, including tracking range and angle. If you’re willing to deal with this relatively minor compromise, it’s nice to not have your phone available for other things rather than being monopolized by the drone.</p> \n<h3>Skydio 2 In Flight</h3> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4Mg.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    Hand takeoffs are simple and reliable.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=11\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>Starting up the Skydio 2 doesn’t require any kind of unusual calibration steps or anything like that. It prefers to be kept still, but you can start it up while holding it, it’ll just take a few seconds longer to tell you that it’s ready to go. While the drone will launch from any flat surface with significant clearance around it (it’ll tell you if it needs more room), the small footprint of the battery means that I was more comfortable hand launching it. This is not a “throw” launch; you just let the drone rest on your palm, tell it to take off, and then stay still while it gets its motors going and then gently lifts off. The lift off is so gentle that you have to be careful not to pull your hand away too soon—I did that once and the drone, being not quite ready, dropped towards the ground, but managed to recover without much drama.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4Mw.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    Hand landings always look scary, but the Skydio 2 is incredibly gentle. After trying this once, it became the only way I ever landed the drone.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=404\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>Catching the drone for landing is perhaps very slightly more dangerous, but not any more difficult. You put the drone above and in front of you facing away, tell it to land in the app or with the beacon, and then put your hand underneath it to grasp it as it slowly descends. It settles delicately and promptly turns itself off. Every drone should land this way. The battery pack provides a good place to grip, although you do have to be mindful of the forward set of props, which (since they’re the pair that are beneath the body of drone) are quite close to your fingers. You’ll certainly be mindful after you catch a blade with your fingers once. Which I did. For the purposes of this review and totally not by accident. No damage, for the record.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU1MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    You won’t be disappointed with the Skydio 2’s in-flight performance, unless you’re looking for a dedicated racing drone. \n  </figcaption> </img> \n</figure> \n<p>In normal flight, the Skydio 2 performs as well as you’d expect. It’s stable and manages light to moderate wind without any problems, although I did notice some occasional lateral drifting when the drone should have been in a stationary hover. While the controller gains are adjustable, the Skydio 2 isn’t quite as aggressive in flight as my Mavic Pro on Sport Mode, but again, if you’re looking for a high-speed drone, that’s really not what the Skydio is all about.</p> \n<p>The Skydio 2 is substantially louder than my Anafi, although the Anafi is notably quiet for a drone. It’s not annoying to hear (not a high-pitched whine), but you can hear it from a ways away, and farther away than my Mavic Pro. I’m not sure whether that’s because of the absolute volume or the volume plus the pitch. In some ways, this is a feature, since you can hear the drone following you even if you’re not looking at it, you just need to be aware of the noise it makes when you’re flying it around people.</p> \n<h3>Obstacle Avoidance</h3> \n<p>The primary reason Skydio 2 is the drone that you want to fly is because of its autonomous subject tracking and obstacle avoidance. Skydio’s PR videos make this capability look almost too good, and since I hadn’t tried out one of their drones before, the first thing I did with it was exactly what you’d expect: attempt to fly it directly into the nearest tree.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4NA.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 deftly slides around trees and branches. The control inputs here were simple “forward”&nbsp;or “turn,”&nbsp;all obstacle avoidance is autonomous.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=51\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>And it just won’t do it. It slows down a bit, and then slides right around one tree after another, going over and under and around branches. I pointed the drone into a forest and just held down “forward” and away it went, without any fuss, effortlessly ducking and weaving its way around. Of course, it wasn’t effortless at all—six 4K cameras were feeding data into the NVIDIA TX2 at 30 fps, and the drone was processing a million points in 3D space per second to plan the safest path while simultaneously taking into account where I wanted it to go. I spent about 10 more minutes doing my level best to crash the drone into anything at all using a flying technique probably best described as “reckless,” but the drone was utterly unfazed. It’s incredible.</p> \n<p>What knocked my socks off was telling the drone to pass through treetops—in the clip&nbsp;below, I’m just telling the drone to fly straight down. Watch as it weaves its way through gaps between the branches:</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4NQ.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The result of parking the Skydio 2 above some trees and holding “down”&nbsp;on the controller is this impressive fully autonomous descent through the branches.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=35\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>Here’s one more example, where I sent the drone across a lake and started poking around in a tree. Sometimes the Skydio 2 isn’t sure where you want it to go, and you have to give it a little bit of a nudge in a clear direction, but that’s it.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4Ng.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    In obstacle-heavy environments, the Skydio 2 prudently slows down, but it can pick its way through almost anything that it can see.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=266\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>It’s important to keep in mind that all of the Skydio 2’s intelligence is based on vision. It uses cameras to see the world, which means that it has similar challenges as your eyes do. Specifically, Skydio warns against flying in the following conditions:</p> \n<ul> \n <li>Skydio 2 can’t see certain visually challenging obstacles. Do not fly around thin branches, telephone or power lines, ropes, netting, wires, chain link fencing or other objects less than ½ inch in diameter.</li> \n <li>Do not fly around transparent surfaces like windows or reflective surfaces like mirrors greater than 60 cm wide.</li> \n <li>When the sun is low on the horizon, it can temporarily blind Skydio 2’s cameras depending on the angle of flight. Your drone may be cautious or jerky when flying directly toward the sun.</li> \n</ul> \n<p>Basically, if you’d have trouble seeing a thing, or seeing under some specific flight conditions, then the Skydio 2 almost certainly will also. It gets even more problematic when challenging obstacles are combined with challenging flight conditions, which is what I’m pretty sure led to the only near-crash I had with the drone. Here’s a video:</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4Nw.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    Flying around very thin branches and into the sun can cause problems for the Skydio 2’s obstacle avoidance.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=121\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>I had the Skydio 2 set to follow me on my bike (more about following and tracking in a bit). It was mid afternoon, but since it’s late fall here in Washington, D.C., the sun doesn’t get much higher than 30 degrees above the horizon. Late fall also means that most of the deciduous trees have lost their leaves, and so there are a bunch of skinny branches all over the place. The drone was doing a pretty good job of following me along the road at a relatively slow speed, and then it clipped the branch that you can just barely see in the video above. It recovered in an acrobatic maneuver that has been mostly video-stabilized out, and resumed tracking me before I freaked and told it to land. You can see another example <a href=\"https://youtu.be/psjBufklGwg\">here</a>, where the drone (again) clips a branch that has the sun behind it, and <a href=\"https://youtu.be/psjBufklGwg?t=137\">this clip</a> shows me stopping my bike before the drone runs into another branch in a similar orientation. As the video shows, it’s very hard to see the branches until it’s too late.</p> \n<p>As far as I can tell, the drone is no worse for wear from any of this, apart from a small nick in one of the props. But, this is a good illustration of a problematic situation for the Skydio 2: flying into a low sun angle around small bare branches. Should I not have been flying the drone in this situation? It’s hard to say. These probably qualify as “thin branches,” although there was plenty of room along with middle of the road. There is an open question with the Skydio 2 as to exactly how much responsibility the user should have about when and where it’s safe to fly—for branches, how thin is too thin? How low can the sun be? What if the branches are only kinda thin and the sun is only kinda low, but it’s also a little windy? Better to be safe than sorry, of course, but there’s really no way for the user (or the drone) to know what it can’t handle until it can’t handle it.</p> \n<p>Edge cases like these aside, the obstacle avoidance just works. Even if you’re not deliberately trying to fly into branches, it’s keeping a lookout for you all the time, which means that flying the drone goes from somewhat stressful to just pure fun. I can’t emphasize enough how amazing it is to be able to fly without worrying about running into things, and how great it feels to be able to hand the controller to someone who’s never flown a drone before and say, with complete confidence, “go ahead, fly it around!”</p> \n<h3>Skydio 2 vs. DJI Mavic</h3> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjc0NA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    Both the Skydio 2 and many models of DJI’s Mavic use visual obstacle avoidance, but the Skydio 2 is so much more advanced that you can’t really compare the two systems. \n  </figcaption> </img> \n</figure> \n<p>It’s important to note that there’s a huge difference between the sort of obstacle avoidance that you get with a DJI Mavic, and the sort of obstacle avoidance that you get with the Skydio 2. The objective of the Mavic’s obstacle avoidance is really there to prevent you from accidentally running into things, and in that capacity, it usually works. But there are two things to keep in mind here—first, not running into things is not the same as avoiding things, because avoiding things means planning several steps ahead, not just one step.</p> \n<p>Second, there’s the fact that the Mavic’s obstacle detection only works most of the time. Fundamentally, I don’t trust my Mavic Pro, because sometimes the safety system doesn’t kick in for whatever reason and the drone ends up alarmingly close to something. And that’s actually fine, because with the Mavic, I <em>expect</em> to be piloting it. It’s for this same reason that I don’t care that my Parrot Anafi doesn’t have obstacle avoidance at all: I’m piloting it anyway, and I’m a careful pilot, so it just doesn’t matter. The Skydio 2 is totally and completely different. It’s in a class by itself, and you can’t compare what it can do to what anything else out there right now. Period.</p> \n<h3>Skydio 2 Tracking</h3> \n<p>Skydio’s big selling point on the Skydio 2 is that it’ll autonomously track you while avoiding obstacles. It does this visually, by watching where you go, predicting your future motion, and then planning its own motion to keep you in frame. The works better than you might expect, in that it’s really very good at not losing you. Obviously, the drone prioritizes not running into stuff over tracking you, which means that it may not always be where you feel like it should be. It’s probably trying to get there, but in obstacle dense environments, it can take some creative paths.</p> \n<p>Having said that, I found it to be very consistent with keeping me in the frame, and I only managed to lose it when changing direction while fully occluded by an obstacle, or while it was executing an avoidance maneuver that was more dynamic than normal. If you deliberately try to hide from the drone it’s not that hard to do so if there are enough obstacles around, but I didn’t find the tracking to be something that I had to worry about it most cases. When tracking does fail and you’re not using the beacon, the drone will come to a hover. It won’t try and find you, but it will reacquire you if you get back into its field of view.</p> \n<p>The Skydio 2 had no problem tracking me running through fairly dense trees:</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4OA.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 had no problem chasing me around through these trees, even while I was asking it to continually change its tracking angle.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=70\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>It also managed to keep up with me as I rode my bike along a tree-lined road:</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU4OQ.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 is easily fast enough to keep up with me on a bike, even while avoiding tree branches.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=198\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>It lost me when I asked it to follow very close behind me as I wove through some particularly branch-y trees, but it fails more or less gracefully by just sort of nope-ing out of situations when they start to get bad and coming to a hover somewhere safe.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU5MA.gif\"> \n  <figcaption class=\"hi-cap\">\n    GIF: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    The Skydio 2 knows better than to put itself into situations that it can’t handle, and will bail to a safe spot if things get too complicated.&nbsp; \n   <a href=\"https://youtu.be/psjBufklGwg?t=338\">Click here for a full resolution clip</a>. \n  </figcaption> </img> \n</figure> \n<p>After a few days of playing with the drone, I started to get to the point where I could set it to track me and then just forget about it while I rode my bike or whatever, as opposed to constantly turning around to make sure it was still behind me, which is what I was doing initially. It’s a level of trust that I don’t think would be possible with any other drone.&nbsp;</p> \n<h3>Should You Buy a Skydio 2?</h3> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMjU1Mg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption>\n    We think the Skydio 2 is fun and relaxing to fly, with unique autonomous intelligence that makes it worth the cost. \n  </figcaption> </img> \n</figure> \n<p>In case I haven’t said it often enough in this review, the Skydio 2 is an incredible piece of technology. As far as I know (as a robotics journalist, mind you), this represents the state of the art in commercial drone autonomy, and quite possibly the state of the art in drone autonomy, period. And it’s available for $999, which is expensive, but less money than a Mavic Pro 2. If you’re interested in a new drone, you should absolutely consider the Skydio 2.</p> \n<p>There are some things to keep in mind—battery life is a solid but not stellar 20 minutes. Extra batteries are expensive at $99 each (the base kit includes just one). The controller and the beacon are also expensive, at $150 each. And while I think the Skydio 2 is definitely the drone you want to fly, it may not be the drone you want to travel with, since it’s bulky compared to other options.</p> \n<p>But there’s no denying the fact that the experience is uniquely magical. Once you’ve flown the Skydio 2, you won’t want to fly anything else. This drone makes it possible to get pictures and videos that would be otherwise impossible, and you can do it completely on your own. You can trust the drone to do what it promises, as long as you’re mindful of some basic and common sense safety guidelines. And we’ve been told that the drone is only going to get smarter and more capable over time.</p> \n<p>If you buy a Skydio 2, it comes with the following warranty from Skydio:</p> \n<blockquote> \n <p><em>“If you’re operating your Skydio 2 within our Safe Flight guidelines, and it crashes, we’ll repair or replace it for free.”</em></p> \n</blockquote> \n<p>Skydio trusts their drone to go out into a chaotic and unstructured world and dodge just about anything that comes its way. And after a week with this drone, I can see how they’re able to offer this kind of guarantee. This is the kind of autonomy that robots have been promising for years, and the Skydio 2 makes it real.</p> \n<p>Detailed technical specifications are available <a href=\"https://www.skydio.com/\">on Skydio’s website</a>, and if you have any questions, post a comment—we’ve got this drone for a little while longer, and I’d be happy to try out (nearly) anything with it.</p> \n<h3>Skydio 2 Review Video Highlights</h3> \n<p>This video is about 7 minutes of 4K, 30 fps footage directly from the Skydio 2. The only editing I did was cutting clips together, no stabilization or color correcting or anything like that. The drone will record in 4K 60 fps, so it gets smoother than this, but I, er, forgot to change the setting.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/psjBufklGwg?rel=0\" width=\"620\"></iframe></p> \n<p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ePDcbxuJmvE:HmzLziNy2zI:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ePDcbxuJmvE:HmzLziNy2zI:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ePDcbxuJmvE:HmzLziNy2zI:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ePDcbxuJmvE:HmzLziNy2zI:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ePDcbxuJmvE:HmzLziNy2zI:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ePDcbxuJmvE:HmzLziNy2zI:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ePDcbxuJmvE\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 03 Dec 2019 16:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQyMjUwMg.jpeg","linkMd5":"ac1ed2d177f3d00db61213d0a8b02970","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn49@2020_1/2020/08/25/10-37-59-961_4a0ade2551e29f57.webp","destWidth":1240,"destHeight":712,"sourceBytes":307062,"destBytes":136976,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQyMjUyMA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn50@2020_6/2020/08/25/10-38-14-232_7103681b1f79fcfa.webp","https://spectrum.ieee.org/image/MzQyMjczMQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn16@2020_6/2020/08/25/10-38-10-125_b1a318877e9dd308.webp","https://spectrum.ieee.org/image/MzQyMjczMg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn39@2020_1/2020/08/25/10-38-10-735_a799955177608a9b.webp","https://spectrum.ieee.org/image/MzQyMjczMw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn74@2020_4/2020/08/25/10-38-13-811_f9181d23f8005c98.webp","https://spectrum.ieee.org/image/MzQyMjczOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn7@2020_1/2020/08/25/10-38-06-238_6bf21babdbb4e36e.webp","https://spectrum.ieee.org/image/MzQyMjc0MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn12@2020_2/2020/08/25/10-38-09-477_a3ad8f43ebbf4e70.webp","https://spectrum.ieee.org/image/MzQyMjU2Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn45@2020_6/2020/08/25/10-38-19-329_c6e258bbfbb67f52.webp","https://spectrum.ieee.org/image/MzQyMjUyMQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn44@2020_5/2020/08/25/10-38-03-156_98aaa38d87450c96.webp","https://spectrum.ieee.org/image/MzQyMjc0MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn13@2020_1/2020/08/25/10-38-13-832_f3560897fb077d5c.webp","https://spectrum.ieee.org/image/MzQyMjc0Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn76@2020_5/2020/08/25/10-38-05-055_2b78e840f615f83c.webp","https://spectrum.ieee.org/image/MzQyMjU4Mg.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn64@2020_5/2020/08/25/10-38-06-061_fa1a100150a5f6f9.webp","https://spectrum.ieee.org/image/MzQyMjU4Mw.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn42@2020_5/2020/08/25/10-38-16-752_5790bb2d4b3f4f66.webp","https://spectrum.ieee.org/image/MzQyMjU1MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn6@2020_5/2020/08/25/10-38-15-921_f8c86f9d3d76337e.webp","https://spectrum.ieee.org/image/MzQyMjU4NA.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn54@2020_4/2020/08/25/10-38-14-383_0a001ac576480b59.webp","https://spectrum.ieee.org/image/MzQyMjU4NQ.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn35@2020_5/2020/08/25/10-38-12-659_01ef4d6d0e69c2ad.webp","https://spectrum.ieee.org/image/MzQyMjU4Ng.gif":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn78@2020_1/2020/08/25/10-38-16-309_cb47e98820a78239.webp","https://spectrum.ieee.org/image/MzQyMjU4Nw.gif":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn13@2020_6/2020/08/25/10-38-19-916_39e14162ca05c864.webp","https://spectrum.ieee.org/image/MzQyMjc0NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn31@2020_2/2020/08/25/10-38-07-106_e043c307ca172145.webp","https://spectrum.ieee.org/image/MzQyMjU4OA.gif":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn44@2020_5/2020/08/25/10-38-10-758_2f206ae410a8096a.webp","https://spectrum.ieee.org/image/MzQyMjU4OQ.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn59@2020_6/2020/08/25/10-38-13-950_2797176b67eee70b.webp","https://spectrum.ieee.org/image/MzQyMjU5MA.gif":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn6@2020_4/2020/08/25/10-38-16-371_d69b19b3096c2e64.webp","https://spectrum.ieee.org/image/MzQyMjU1Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn12@2020_5/2020/08/25/10-38-08-314_d4fb690226a7401b.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ePDcbxuJmvE:HmzLziNy2zI:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn30@2020_4/2020/08/25/10-38-15-755_48f393d4612a70e1.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ePDcbxuJmvE:HmzLziNy2zI:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn57@2020_3/2020/08/25/10-38-15-257_5e348405e682ed1f.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ePDcbxuJmvE":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn87@2020_2/2020/08/25/10-38-07-512_bc1af74c0b1249f0.webp"},"publishedOrCreatedDate":1598351859733},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"UAV-Based LiDAR Can Measure Shallow Water Depth","link":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","description":"<p><strong>World’s first small-scale topographic and bathymetric scanning LiDAR </strong></p> \n<p><a href=\"https://www.astralite.net/edgelidar\">ASTRALiTe’s edge™</a> is the world’s first small-scale topographic and bathymetric scanning LiDAR that can detect small underwater objects, measure shallow water depth, and survey critical underwater infrastructure from a small UAV platform.</p> \n<p>The edge™ can see beneath the water surface at depths from 0-5 meters and is completely self-contained with its own Inertial Navigation System with GNSS, battery, and onboard computer. It weighs about 5 kg and is designed for deployment on UAV systems for faster, safer, and more accurate bathymetric surveys. This patented 2-in-1 topographic and bathymetric LiDAR offers a centimeter-level depth resolution. There are numerous possible applications for this LiDAR, such as coastal mapping and surveying, infrastructure inspection, or even military logistics.&nbsp;</p> \n<figure class=\"ct lrg\" role=\"img\"> \n <img alt=\"SGB\" src=\"https://spectrum.ieee.org/image/MzQwMjA2MQ.png\"> </img>\n</figure> \n<p></p> \n<p></p> \n<p><strong>Importance of geo-referencing and motion stabilization </strong></p> \n<p>“We needed a motion and navigation solution for our LiDAR. Our requirements included high accuracy along with low size, weight, and power” explains Andy Gisler, Director of Lidar Systems with <a href=\"https://www.astralite.net/\">ASTRALiTe</a>. In addition, the system needed to be able to apply Post-Processing Kinematic (PPK) corrections to the LiDAR data to provide higher accuracy results to ASTRALiTe’s customers.</p> \n<p>The LiDAR provides a comprehensive point cloud that needs to be motion-compensated and geo-referenced to be usable. Two methods can be used to reach the centimeter-level accuracy requested by surveyors. The first one is Real-Time Kinematic (RTK), which makes use of corrections obtained from a base station or a base station network in real-time thanks to a radio or a GSM link. The second one is used after the mission using a PPK software. This software will apply the same correction as RTK, but it will also re-compute all the inertial data and raw GNSS observables with a forward-backward-merge algorithm to correct all the trajectories, fill any loss of position, and greatly improve the overall accuracy.</p> \n<p></p> \n<p>ASTRALiTe chose <a href=\"https://www.sbg-systems.com/products/ellipse-2-series/#ellipse2-d_rtk_gnss_ins\">SBG Systems’ dual antenna Ellipse2-D inertial navigation system</a> which provides motion, RTK, and PPK. The weight of the INS/GNSS solution was especially important to ASTRALiTe as they were designing a system to be flown on most UAVs, where light payload capacities are required for UAV compatibility. The possibility to use two antennas was a key element to consider, as they required a robust heading even during slow-speed flights. In addition to this INS, they also use <a href=\"https://www.sbg-systems.com/products/qinertia-ins-gnss-post-processing-software/\">Qinertia, SBG Systems’ in-house post-processing software</a>.&nbsp;</p> \n<p>This PPK software gives access to offline RTK corrections from more than 7,000 base stations located in 164 countries and is designed to help UAV integrators get the best of their GNSS or INS/GNSS solution.</p> \n<figure class=\"ct med-lrg\" role=\"img\"> \n <img alt=\"SBG\" src=\"https://spectrum.ieee.org/image/MzQwMjA2Mw.png\"> </img>\n</figure> \n<p></p> \n<p></p> \n<p></p> \n<p><strong>About SBG Systems INS/GNSS</strong></p> \n<p><a href=\"https://www.sbg-systems.com/\">SBG Systems</a> is an international company which develops Inertial Measurement Unit with embedded GNSS, from miniature to high accuracy ranges. Combined with cutting-edge calibration techniques and advanced embedded algorithms, SBG Systems manufactures inertial solutions for industrial &amp; research projects such as unmanned vehicle control (land, marine, and aerial), antenna tracking, camera stabilization, and surveying applications.</p> \n<p></p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=C6zMyuDvd98:JCm27VifXB0:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=C6zMyuDvd98:JCm27VifXB0:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=C6zMyuDvd98:JCm27VifXB0:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=C6zMyuDvd98:JCm27VifXB0:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=C6zMyuDvd98:JCm27VifXB0:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=C6zMyuDvd98:JCm27VifXB0:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/C6zMyuDvd98\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 21 Oct 2019 04:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NTExMQ.png","linkMd5":"4afdb44919c1840f63dbb5b7c83f76a7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn42@2020_2/2020/08/25/10-37-58-994_d72dce30983dcfe9.webp","destWidth":607,"destHeight":451,"sourceBytes":445022,"destBytes":20244,"author":"SBG Systems","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQwMjA2MQ.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn36@2020_3/2020/08/25/10-38-08-500_6c427da6ffe5e547.webp","https://spectrum.ieee.org/image/MzQwMjA2Mw.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn64@2020_5/2020/08/25/10-38-10-447_5c6704998a2104ba.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=C6zMyuDvd98:JCm27VifXB0:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn30@2020_2/2020/08/25/10-38-17-711_af17ff1c308f2c0c.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=C6zMyuDvd98:JCm27VifXB0:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn53@2020_5/2020/08/25/10-38-16-854_70db5c8e71680bc0.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/C6zMyuDvd98":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn16@2020_5/2020/08/25/10-38-05-984_c66dfedfc5e4e694.webp"},"publishedOrCreatedDate":1598351859728},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"NASA Hiring Engineers to Develop “Next Generation Humanoid Robot”","link":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","description":"<p>It’s been nearly six years since NASA unveiled <a href=\"https://robots.ieee.org/robots/valkyrie/?utm_source=spectrum\">Valkyrie</a>, a <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/nasa-jsc-unveils-valkyrie-drc-robot\">state-of-the-art full-size humanoid robot</a>. After the <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next\">DARPA Robotics Challenge</a>, NASA has continued to work with Valkyrie at <a href=\"https://www.nasa.gov/centers/johnson/home/index.html\">Johnson Space Center</a>, and has also provided <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/new-r5-valkyrie-robots\">Valkyrie robots to several different universities</a>. Although it’s not a new platform anymore (six years is a long time in robotics), Valkyrie is still very capable, with plenty of potential for robotics research.&nbsp;</p> \n<p>With that in mind, we were caught by surprise when over the last several months, <a href=\"https://www.jacobs.com/\">Jacobs</a>, a Dallas-based&nbsp;engineering company that appears to provide a wide variety of technical services to anyone who wants them, has posted several open jobs in need of roboticists in the Houston, Texas, area who are interested in working with NASA on “the next generation of humanoid robot.”</p> \n<!--nextpage--> \n<p>Here are the relevant bullet points from the one of the job descriptions (<a href=\"https://jacobs.jobs/houston-tx/electronics-design-engineer-nasa-humanoid-robot-electronics-systems/8AB3041FD085486198CDD3227EAFFB80/job/\">which you can view at this link</a>):</p> \n<blockquote> \n <p><em>Work directly with NASA Johnson Space Center in designing the next generation of humanoid robot.</em></p> \n <p><em>Join the Valkyrie humanoid robot team in NASA’s Robotic Systems Technology Branch.</em></p> \n <p><em>Build on the success of the existing Valkyrie and Robonaut 2 humanoid robots and advance NASA’s ability to project a remote human presence and dexterous manipulation capability into challenging, dangerous, and distant environments both in space and here on earth.</em></p> \n</blockquote> \n<p>The question is, why is NASA developing its own humanoid robot (again) when it could instead save a whole bunch of time and money by using a platform that already exists, whether it’s <a href=\"https://robots.ieee.org/robots/atlas2016/?utm_source=spectrum\">Atlas</a>, <a href=\"https://robots.ieee.org/robots/digit/?utm_source=spectrum\">Digit</a>, <a href=\"https://robots.ieee.org/robots/valkyrie/?utm_source=spectrum\">Valkyrie</a> itself, or one of the small handful of other humanoids that are more or less available? The only answer that I can come up with is that no existing platforms meet NASA’s requirements, whatever those may be. And if that’s the case, what kind of requirements are we talking about? The obvious one would be the ability to work in the kinds of environments that NASA specializes in—space, the Moon, and Mars.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"NASA's Valkyrie humanoid on Mars\" src=\"https://spectrum.ieee.org/image/MzM4NDY2OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: NASA \n  </figcaption> \n  <figcaption>\n    Artist’s concept of NASA’s Valkyrie humanoid robot working on the surface of Mars. \n  </figcaption> </img> \n</figure> \n<p>NASA’s existing humanoid robots, including <a href=\"https://robots.ieee.org/robots/robonaut/?utm_source=spectrum\">Robonaut 2</a>&nbsp;and <a href=\"https://robots.ieee.org/robots/valkyrie/?utm_source=spectrum\">Valkyrie</a>, were designed to operate on Earth. <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/how-robonaut-2-will-help-astronauts-in-space\">Robonaut 2 ended up going to space anyway</a>&nbsp;(<a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/robonaut-has-been-broken-for-years-and-now-nasa-is-bringing-it-home\">it’s recently returned to Earth for repairs</a>), but its hardware was certainly never intended to function outside of the International Space Station. Working in a vacuum involves designing for a much more rigorous set of environmental challenges, and things get even worse on the Moon or on Mars, where <a href=\"https://spectrum.ieee.org/aerospace/robotic-exploration/how-nasa-will-use-robots-to-create-rocket-fuel-from-martian-soil\">highly abrasive dust gets everywhere</a>.</p> \n<p>We know that it’s possible to <a href=\"https://spectrum.ieee.org/aerospace/robotic-exploration/planetary-rovers-are-we-alone\">design robots for long term operation in these kinds of environments</a> because we’ve done it before. But if you’re not actually going to send your robot off-world, there’s very little reason to bother making sure that it can operate through (say) 300° Celsius temperature swings like you’d find on the Moon. In the past, NASA has quite sensibly focused on designing robots that can be used as platforms for the development of software and techniques that could one day be applied to off-world operations, without over-engineering those specific robots to operate in places that they would almost certainly never go. As <a href=\"https://spectrum.ieee.org/static/project-moon-base\">NASA increasingly focuses on a return to the Moon</a>, though, maybe it’s time to start thinking about a humanoid robot that could actually do useful stuff on the lunar surface.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Gateway lunar orbiting station with Orion module approaching.\" src=\"https://spectrum.ieee.org/image/MzM4NDc3Mg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: NASA \n  </figcaption> \n  <figcaption>\n    Artist’s concept of the Gateway moon-orbiting space station (seen on the right) with an Orion crew vehicle approaching. \n  </figcaption> </img> \n</figure> \n<p>The other possibility that I can think of, and perhaps the more likely one, is that this next humanoid robot will be a direct successor to <a href=\"https://robots.ieee.org/robots/robonaut/?utm_source=spectrum\">Robonaut 2</a>, intended for <a href=\"https://www.nasa.gov/topics/moon-to-mars/lunar-gateway\">NASA’s Gateway space station orbiting the Moon</a>. Some of the robotics folks at NASA that we’ve talked to recently have <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/robonaut-and-astrobee-to-will-work-together-on-iss\">emphasized how important robotics will be for Gateway</a>:</p> \n<blockquote> \n <p><em><a href=\"https://ti.arc.nasa.gov/profile/trey-smith/\">Trey Smith, NASA Ames</a>: Everybody at NASA is really excited about work on the Gateway space station that would be in near lunar space. We don’t have definite plans for what would happen on the Gateway yet, but there’s a general recognition that intra-vehicular robots are important for space stations. And so, it would not be surprising to see a mobile manipulator like Robonaut, and a free flyer like Astrobee, on the Gateway.</em></p> \n <p><em>If you have an un-crewed cargo vehicle that shows up stuffed to the rafters with cargo bags and it docks with the Gateway when there’s no crew there, it would be very useful to have intra-vehicular robots that can pull all those cargo bags out, unpack them, stow all the items, and then even allow the cargo vehicle to detach before the crew show up so that the crew don’t have to waste their time with that.</em></p> \n</blockquote> \n<blockquote> \n <p><em><a href=\"https://www.linkedin.com/in/julia-badger-b690094/\">Julia Badger, NASA JSC</a>: One of the systems on board Gateway is going to be intravehicular robots. They’re not going to necessarily look like Robonaut, but they’ll have some of the same functionality as Robonaut—being mobile, being able to carry payloads from one part of the module to another, doing some dexterous manipulation tasks, inspecting behind panels, those sorts of things.</em></p> \n</blockquote> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Valkyrie humanoid robot working inside Gateway lunar station\" src=\"https://spectrum.ieee.org/image/MzM4NDc4MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: NASA \n  </figcaption> \n  <figcaption>\n    Artist’s concept of NASA’s Valkyrie humanoid robot working inside a spacecraft. \n  </figcaption> </img> \n</figure> \n<p>Since <a href=\"https://spectrum.ieee.org/aerospace/space-flight/nasas-lunar-space-station-is-a-greatterrible-idea\">Gateway won’t be crewed by humans all of the time</a>, it’ll be important to have a permanent robotic presence to keep things running while nobody is home while saving on resources by virtue of the fact that robots aren’t always eating food, drinking water, consuming oxygen, demanding that the temperature stays just so, and producing a variety of disgusting kinds of waste. Obviously, the robot won’t be as capable as humans, but if they can manage to do even basic continuing maintenance tasks (most likely through at least partial teleoperation), that would be very useful.</p> \n<figure class=\"rt med-lrg\" role=\"img\"> \n <img alt=\"Robonaut 2 with new legs\" src=\"https://spectrum.ieee.org/image/MzM4NDc4Mw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> \n  <figcaption> \n   <p>NASA’s&nbsp;Robonaut team plans to perform a variety of mobility and motion-planning experiments&nbsp;using the robot’s new legs, which can grab handrails on the International Space Station.</p> \n  </figcaption> </img> \n</figure> \n<p>As for whether robots designed for Gateway would really fall into the “humanoid” category, it’s worth considering that Gateway is designed for humans, implying that an effective robotic system on Gateway would need to be able to interact with the station in similar ways to how a human astronaut would. So, you’d expect to see arms with end-effectors that can grip things as well as push buttons, and some kind of mobility system—the <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached\">legged version of Robonaut 2</a> seems like a likely template, but redesigned from the ground up to work in space, incorporating all the advances in robotics hardware and computing that have taken place over the last decade.</p> \n<p>We’ve been pestering NASA about this for a little bit now, and they’re not ready to comment on this project, or even to confirm it. And again, everything in this article (besides the <a href=\"https://jacobs.jobs/houston-tx/electronics-design-engineer-nasa-humanoid-robot-electronics-systems/8AB3041FD085486198CDD3227EAFFB80/job/\">job post, which you should totally check out and consider applying for</a>) <span>is just speculation on our part, and we could be wrong about absolutely all of it</span>. As soon as we hear more, we’ll definitely let you know.</p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ho-KAV7HE7I:mSz7Bg_m1CE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ho-KAV7HE7I:mSz7Bg_m1CE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ho-KAV7HE7I:mSz7Bg_m1CE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ho-KAV7HE7I:mSz7Bg_m1CE:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ho-KAV7HE7I:mSz7Bg_m1CE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ho-KAV7HE7I:mSz7Bg_m1CE:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ho-KAV7HE7I\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 30 Sep 2019 17:59:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4NDg5Ng.jpeg","linkMd5":"3bfec1910aa88a37b6873b90b2190643","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn16@2020_3/2020/08/25/10-38-00-261_6cb852da5b471c03.webp","destWidth":1085,"destHeight":691,"sourceBytes":506419,"destBytes":108916,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM4NDY2OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn32@2020_3/2020/08/25/10-38-08-536_3f66d2cdbfac2e93.webp","https://spectrum.ieee.org/image/MzM4NDc3Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn76@2020_1/2020/08/25/10-38-08-868_b9ac266f1f46d0be.webp","https://spectrum.ieee.org/image/MzM4NDc4MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn62@2020_5/2020/08/25/10-38-11-444_b3506b8c4a138d6d.webp","https://spectrum.ieee.org/image/MzM4NDc4Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn57@2020_6/2020/08/25/10-38-13-329_8660c4b5ca9cdd21.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ho-KAV7HE7I:mSz7Bg_m1CE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn100@2020_1/2020/08/25/10-38-05-042_d07ee2c074838d7f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ho-KAV7HE7I:mSz7Bg_m1CE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn90@2020_2/2020/08/25/10-38-12-683_3f89acfc310cdf07.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ho-KAV7HE7I":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn17@2020_1/2020/08/25/10-38-12-142_56c298d9a64c854c.webp"},"publishedOrCreatedDate":1598351859730},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"6 Things to Know About the Biggest Chip Ever Built","link":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","description":"<p></p> \n<p></p> \n<p>On Monday at the <a href=\"https://www.hotchips.org/\">IEEE Hot Chips symposium</a> at Stanford University, startup <a href=\"https://www.cerebras.net/\">Cerebras</a> unveiled the largest chip ever built. It is roughly a silicon wafer-size&nbsp;system meant to reduce AI training time from months to minutes. It is the first commercial attempt at a wafer-scale processor since Trilogy Systems failed at the task in the 1980s.</p> \n<!--nextpage--> \n<p></p> \n<p><strong>1 | The stats</strong></p> \n<p><span>As the largest chip ever built, Cerebras’s Wafer Scale Engine (WSE) naturally comes with a bunch of&nbsp;</span><a href=\"https://www.cerebras.net/technology/\" style=\"font-family: Georgia, serif; font-size: 18px;\">superlatives</a><span>. Here they are with a bit of context where possible:</span></p> \n<p></p> \n<p><strong>Size: </strong>46,225 square millimeters. That’s about 75 percent of a sheet of letter-size&nbsp;paper, but 56 times as large&nbsp;as the biggest GPU.</p> \n<p><strong>Transistors:</strong> 1.2 trillion. Nvidia’s GV100 Volta&nbsp;packs in 21&nbsp;billion.</p> \n<p><strong>Processor cores:</strong> 400,000. Not to pick on the GV100 too much, but it has over 5000&nbsp;CUDA cores and more than 600&nbsp;tensor cores, both of which are used in AI workloads. (Fifty-six&nbsp;GV100s would then have more than 300,000 cores.)</p> \n<p><strong>Memory:&nbsp;</strong>18 gigabytes of on-chip SRAM. Cerebras says this is&nbsp;3,000 times&nbsp;as much as the GPU. The Volta has 6 MB of SRAM in its L2 cache according to this <a href=\"https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf\">whitepaper</a>&nbsp;[page 10]. But SRAM may not be a fair comparison as each GV100 works with 32 GB of high-bandwidth DRAM.&nbsp;</p> \n<p><strong>Memory bandwidth:</strong> 9 petabytes per second. A<span>ccording to Cerebras, t</span>hat’s 10,000 times our favorite GPU. Cerebras is here comparing the <a href=\"https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf\">Volta’s 900 GB/s bandwidth</a> to high-bandwidth DRAM rather than the on-chip SRAM <a href=\"https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf\">[page 21]</a>.</p> \n<p></p> \n<p></p> \n<p><strong>2 |&nbsp;Why do you need this monster?</strong></p> \n<p>Cerebras makes a pretty good case in its <a href=\"https://www.cerebras.net/wp-content/uploads/2019/08/Cerebras-Wafer-Scale-Engine-Whitepaper.pdf\">white paper</a>&nbsp;[PDF]&nbsp;for&nbsp;why such a ridiculously large chip makes sense. Basically, the company&nbsp;argues that the demand for training deep learning systems and other AI systems is getting out of hand. The company says that training a new model—creating a system that, once trained, can recognize&nbsp;people or <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/mb\">win a game of Go</a>—is taking weeks or months and costing hundreds of thousands of dollars of compute time. That cost means there’s little room for experimentation, and that’s stifling new ideas and innovation.</p> \n<p>The startup’s answer is that the world needs more, and cheaper, training compute resources. Training needs to take minutes not months, and to do that you need more cores, more memory close to those cores, and a low-latency, high-bandwidth connection between the cores.</p> \n<p>Those are goals that are clearly in effect for everyone in the AI space. But, by its own admission, Cerebras took the idea to its logical extreme. A big chip offers more silicon area for processor cores and the memory that needs to snuggle up next to it. And a high-bandwidth, low-latency connection is only achievable if data never has to leave the short, dense interconnects on a chip. Thus one big chip.</p> \n<p></p> \n<p></p> \n<p><strong>3 | What’s in those 400,000 cores?</strong></p> \n<p>According to the company, the WSE’s cores are specialized to do AI, but still programmable enough that they’re not locked into only one flavor of it. They call them Sparse Linear Algebra (SLA) cores. These processing units are specialized to “tensor” operations key to AI work, but they also include a feature that reduces the work, particularly for deep-learning networks. According to the company, 50 to 98 percent of all the data in a deep learning training set are zeros. The nonzero data is therefore “sparse.”</p> \n<p>The SLA cores cut down on the work by simply not multiplying anything by zero. The cores have built in data-flow elements that trigger computing actions based on the data, so when it encounters a zero in the data, it doesn’t waste its time.</p> \n<p></p> \n<p></p> \n<p><strong>4 | How did they do this?</strong></p> \n<p>The fundamental idea behind Cerebras’s massive single chip has been obvious for decades, but it has also been impractical. To quote myself:</p> \n<blockquote> \n <p>Back in the 1980s, parallel computing pioneer&nbsp;<a href=\"https://spectrum.ieee.org/view-from-the-valley/tech-history/silicon-revolution/remembering-gene-amdahl\">Gene Amdahl</a>&nbsp;hatched a plan to speed mainframe computing: a silicon-wafer-sized processor. By keeping most of the data on the processor itself instead of pushing it through a circuit board to memory and other chips, computing would be faster and more energy efficient.</p> \n <p>With US $230 million from venture capitalists, the most ever at the time, Amdahl founded&nbsp;<a href=\"https://en.wikipedia.org/wiki/Trilogy_Systems\">Trilogy Systems</a>&nbsp;to make his vision a reality. This first commercial attempt at “wafer-scale integration” was such a disaster that it&nbsp;<a href=\"https://en.wikipedia.org/wiki/Trilogy_Systems\">reportedly introduced the verb “to crater” into the financial press lexicon</a>.&nbsp;</p> \n</blockquote> \n<p></p> \n<p>The most basic problem is that the bigger the chip, the worse the yield; that’s the fraction of working chips you get from each wafer. Logically, this should mean a wafer-scale chip would be unprofitable, because there would always be flaws in your product. Cerebras’s solution is to add a certain amount of redundancy. According to <em><a href=\"https://www.eetimes.com/document.asp?doc_id=1335043&amp;page_number=1\">EE Times</a></em>, the Swarm communications networks have&nbsp;redundant links to route around damaged cores, and about 1 percent of the cores are spares.</p> \n<p>Cerebras also had to work around some key manufacturing limits. For one, chip tools are designed to cast their feature-defining patterns onto relatively small rectangles and do that over and over, perfectly across the wafer. That alone would keep a lot of systems from being built on a single wafer, because of the cost and difficulty of casting different patterns in different places on the wafer.</p> \n<p>But the WSE resembles a typical wafer full of the exact same chips, just as you’d ordinarily manufacture. The big difference was a method they worked out with TSMC to make connections across the space between the chips, an area called the scribe lines. This space is typically left blank because the chips are diced up along those lines.</p> \n<p>According to <em><a href=\"https://techcrunch.com/2019/08/19/the-five-technical-challenges-cerebras-overcame-in-building-the-first-trillion-transistor-chip/\">Tech Crunch</a></em>, Cerebras also had to invent a way to provide the chips 15 kilowatts of power and cool the system as well as create new kinds of connectors that could deal with the way it expands when it heats up.</p> \n<p></p> \n<p></p> \n<p></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"manufacturing closeup photo\" src=\"https://spectrum.ieee.org/image/MzM1OTI3NQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Cerebras Systems \n  </figcaption> \n  <figcaption>\n    Cerebras had to invent new manufacturing processes to make its gigantic chip. \n  </figcaption> </img> \n</figure> \n<p><strong>5 | Is this the only way to make a wafer-scale computer?</strong></p> \n<p>Of course not. For example, a team at University of California, Los Angeles, and University of Illinois Urbana-Champaign is working on a <a href=\"https://spectrum.ieee.org/tech-talk/semiconductors/processors/whats-better-than-40-gpubased-servers-a-server-with-40-gpus\">similar system that would eventually use&nbsp;bare processor&nbsp;dies</a> that have already been built and tested and mounts them on a silicon wafer that’s already patterned with the needed dense network of interconnects. This concept, called a silicon interconnect fabric, allows these dielets to sit as close as 100 micrometers from each other, allowing for interchip communication that nears the characteristics of a single chip.</p> \n<p>“This is a huge validation of the research we’ve been doing,” says the University of Illinois’s&nbsp;<a href=\"https://ece.illinois.edu/directory/profile/rakeshk\">Rakesh Kumar</a>. “We like the fact that there is commercial interest in something like this.”</p> \n<p>Kumar believes that the silicon interconnect fabric approach has some advantages over Cerebras’s monolithic wafer-scale scheme. For one, it allows a designer to mix and match technologies, and use the best manufacturing process for each. A monolithic approach means picking the best process for the most crucial subsystem—logic, for example—and using it for memory and other components even if it’s not ideal for them.</p> \n<p>In that approach, Cerebras could be limited in the amount of memory it can put on the processor, Kumar suggests. “They have 18 gigabits of SRAM on the wafer. Maybe that’s enough for some models today, but what about models tomorrow and the day after?”</p> \n<p></p> \n<p><strong>6 | When does it come out?</strong></p> \n<p>According to <em><a href=\"https://fortune.com/2019/08/19/ai-artificial-intelligence-cerebras-wafer-scale-chip/\">Fortune</a></em>, the first systems ship to customers in September, and some have already received prototypes. According to <em><a href=\"https://www.eetimes.com/document.asp?doc_id=1335043&amp;page_number=1\">EE Times</a></em>, the company plans to reveal results from complete systems at <a href=\"https://sc19.supercomputing.org/\">the Supercomputing Conference in November</a>.</p> \n<p></p> \n<p><em>This post was corrected on 22 August 2019 and again on 2 September to give the correct number of transistors and cores in the Nvidia chip, as well as to add better context to these and other numbers. </em>IEEE Spectrum<em>&nbsp;thanks to Peter Glaskowsky for keeping us honest.</em></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=A0x1pCFuojI:zBCg41xGM-k:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=A0x1pCFuojI:zBCg41xGM-k:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=A0x1pCFuojI:zBCg41xGM-k:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=A0x1pCFuojI:zBCg41xGM-k:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=A0x1pCFuojI:zBCg41xGM-k:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=A0x1pCFuojI:zBCg41xGM-k:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/A0x1pCFuojI\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 21 Aug 2019 19:22:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1OTIwOA.jpeg","linkMd5":"f4602756dbd851432f68681e9d7b807e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn96@2020_4/2020/08/25/10-38-00-127_ba903a7565541c8d.webp","destWidth":1240,"destHeight":930,"sourceBytes":209245,"destBytes":126562,"author":"Samuel K. Moore","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1OTI3NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn35@2020_6/2020/08/25/10-38-07-464_bc986b2e09b48dff.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=A0x1pCFuojI:zBCg41xGM-k:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn78@2020_4/2020/08/25/10-38-13-525_e315c8fb22b2fc83.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=A0x1pCFuojI:zBCg41xGM-k:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn55@2020_4/2020/08/25/10-38-05-868_9b22e440f26a540d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/A0x1pCFuojI":"https://cdn.jsdelivr.net/gh/myreaderx/cdn58@2020_3/2020/08/25/10-38-12-463_1f3051f83edc70e4.webp"},"publishedOrCreatedDate":1598351859736},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Bipedal Robot Cassie Cal Learns to Juggle","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle","description":"<p>There’s no particular reason why knowing how to juggle would be a useful skill for a robot. Despite this, robots are frequently taught how to juggle things. <a href=\"https://www.youtube.com/watch?v=t-g43poXmX0\">Blind robots can juggle</a>, <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/juggling-robot-takes-on-two-balls-with-one-very-fast-hand\">humanoid robots can juggle</a>, and <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-software/quadrotors-demonstrate-mad-cooperative-juggling-skills\">even drones can juggle</a>. Why? Because juggling is hard, man! You have to think about a bunch of different things at once, and also do a bunch of different things at once, which this particular human at least finds to be overly stressful. While juggling may not stress robots out, it does require carefully coordinated sensing and computing and actuation, which means that it’s as good a task as any (and a more entertaining task than most) for testing the capabilities of your system.</p> \n<p>UC Berkeley’s Cassie Cal robot, which consists of two legs and what could be called a torso if you were feeling charitable, has just learned to juggle by bouncing a ball on what would be her head if she had one of those. The idea is that if <a href=\"https://robots.ieee.org/robots/cassie/?utm_source=spectrum\">Cassie</a> can juggle while balancing at the same time, she’ll be better able to do other things that require dynamic multitasking, too. And if that doesn’t work out, she’ll still be able to join the circus.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tLrz_R_T6kg?rel=0\" width=\"620\"></iframe></p> \n<p>Cassie’s juggling is assisted by an external motion capture system that tracks the location of the ball, but otherwise everything is autonomous. <a href=\"https://robots.ieee.org/robots/cassie/?utm_source=spectrum\">Cassie</a> is able to juggle the ball by leaning forwards and backwards, left and right, and moving up and down. She does this while maintaining her own balance, which is the whole point of this research—successfully executing two dynamic behaviors that may sometimes be at odds with one another. The end goal here is not to make a better juggling robot, but rather to explore dynamic multitasking, a skill that robots will need in order to be successful in human environments.</p> \n<p>This work is from the <a href=\"https://hybrid-robotics.berkeley.edu/\">Hybrid Robotics Lab at UC Berkeley</a>, led by <a href=\"https://me.berkeley.edu/people/koushil-sreenath/\">Koushil Sreenath</a>, and is being done by Katherine Poggensee, Albert Li, Daniel Sotsaikich, Bike Zhang, and Prasanth Kotaru.</p> \n<p>For a bit more detail, we spoke with Albert Li via email.</p> \n<figure class=\"rt med-lrg\" role=\"img\"> \n <img alt=\"Cassie Cal robot\" src=\"https://spectrum.ieee.org/image/MzQxNDYyNw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image:&nbsp;UC Berkeley \n  </figcaption> \n  <figcaption>\n    UC Berkeley’s Cassie Cal getting ready to juggle. \n  </figcaption> </img> \n</figure> \n<p><strong><em>IEEE Spectrum:</em> What would be involved in getting Cassie to juggle without relying on motion capture?</strong></p> \n<p><em>Albert Li: Our motivation for starting off with motion capture was to first address the control challenge of juggling on a biped without worrying about implementing the perception. We actually do have a ball detector working on a camera, which would mean we wouldn’t have to rely&nbsp;on the motion capture system. However, we need to mount the camera in a way that it would provide the&nbsp;best upwards field of view, and we&nbsp;also have develop a reliable estimator. The estimator is particularly important because when the ball gets close enough to the camera, we actually can’t track the ball and have to assume our dynamic models describe its motion accurately enough until it bounces back up.</em></p> \n<p><strong>What keeps Cassie from juggling indefinitely?</strong></p> \n<p><em>There are a few factors that affect how long Cassie can sustain a juggle. While in simulation the paddle exhibits homogeneous properties like its stiffness and damping, in reality every surface has anisotropic contact properties. So, there are parts of the paddle which may be better for juggling than others (and importantly, react differently than modeled). These differences in contact are also exacerbated due to how the paddle is cantilevered when mounted on Cassie. When the ball hits these areas, it leads to a larger than expected error in a juggle. Due to the small size of the paddle, the ball may then just hit the paddle’s edge and end the juggling run. Over a very long run, this is a likely occurrence. Additionally, some large juggling errors could cause Cassie’s feet to slip slightly, which ends up changing the stable standing position over time. Since this version of the controller assumes Cassie is stationary, this change in position eventually leads to poor juggles and failure.</em></p> \n<p><strong>Would Cassie be able to juggle while walking (or <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/cassie-on-hovershoes\">hovershoe-ing</a>)?</strong></p> \n<p><em>Walking (and <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/cassie-on-hovershoes\">hovershoe-ing</a>) while juggling is a far more challenging problem and is certainly a goal for future research. Some of these challenges include getting the paddle to precise poses to juggle the ball while also moving to avoid any destabilizing effects of stepping incorrectly.&nbsp;The number of juggles per step of walking could also vary and make the mathematics of the problem more challenging. The controller goal is also more involved. While the current goal of the juggling controller is to juggle the ball to a static apex position, with a walking juggling controller, we may instead want to hit the ball forwards and also walk forwards to bounce it, juggle the ball along a particular path, etc. Solving such challenges would be the main thrusts of the follow-up research.</em></p> \n<p><strong>Can you give an example of a practical task that would be made possible by using a controller like this?</strong></p> \n<p><em>Studying juggling means studying contact behavior and leveraging our models of it to achieve a known objective. Juggling could also be used to study predictable post-contact flight behavior.&nbsp;Consider the scenario where a robot is attempting to make a catch, but fails, letting the ball to bounce off of its hand, and then recovering the catch. This behavior could also be intentional: It is often easier to first execute a bounce to direct the target and then perform a subsequent action. For example, volleyball players could in principle directly hit a spiked ball back, but almost always bump the ball back up and then return it.</em></p> \n<p><em>Even beyond this motivating example, the kinds of models we employ to get juggling working are more generally applicable to any task that involves contact, which could include tasks besides bouncing like sliding and rolling. For example, clearing space on a desk by pushing objects to the side may be preferable than individually manipulating each and every object on it.</em></p> \n<p><strong>You mention collaborative juggling or juggling multiple balls—is that something you’ve tried yet? Can you talk a bit more about what you’re working on next?&nbsp;</strong></p> \n<p><em>We haven’t yet started working on collaborative or multi-ball juggling, but that’s also a goal for future work. Juggling multiple balls statically is probably the most reasonable next goal, but presents additional challenges. For instance, you have to encode a notion of juggling urgency (if the second ball isn’t hit hard enough, you have less time to get the first ball up before you get back to the second one).&nbsp;&nbsp;</em></p> \n<p><em>On the other hand, collaborative human-robot juggling requires a more advanced decision-making framework. To get robust multi-agent juggling, the robot will need to employ some sort of probabilistic model of the expected human behavior (are they likely to move somewhere? Are they trying to catch the ball high or low? Is it safe to hit the ball back?). In general, developing such human models is difficult since humans are fairly unpredictable and often don’t exhibit rational behavior. This will be a focus of future work.</em></p> \n<p>[ <a href=\"https://hybrid-robotics.berkeley.edu/\">Hybrid Robotics Lab</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-vdKHxTZE2E:fseR5PgFwgo:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-vdKHxTZE2E:fseR5PgFwgo:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-vdKHxTZE2E:fseR5PgFwgo:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-vdKHxTZE2E:fseR5PgFwgo:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-vdKHxTZE2E:fseR5PgFwgo:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-vdKHxTZE2E:fseR5PgFwgo:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-vdKHxTZE2E\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 18 Nov 2019 19:22:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxNDYzMA.jpeg","linkMd5":"ed36754ceb3d54691c9985ca6f20cf97","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn53@2020_5/2020/08/25/10-38-00-524_61b21a0e36df9d34.webp","destWidth":1240,"destHeight":878,"sourceBytes":210916,"destBytes":57420,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQxNDYyNw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn72@2020_1/2020/08/25/10-38-04-934_13d3e37a5014450b.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-vdKHxTZE2E:fseR5PgFwgo:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn48@2020_1/2020/08/25/10-38-04-127_5a1d3cb023cf4207.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-vdKHxTZE2E:fseR5PgFwgo:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn31@2020_5/2020/08/25/10-38-06-147_b420cb84ea9170fd.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-vdKHxTZE2E":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn80@2020_5/2020/08/25/10-38-03-453_97aa7a6681915bdf.webp"},"publishedOrCreatedDate":1598351859718},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Invasion of the Mini Cheetah Robots","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here's what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.darpa.mil/news-events/2019-11-12\">DARPA SubT Urban Circuit</a> –&nbsp;February 18-27, 2020 –&nbsp;Olympia, Wash., USA</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>There will be a <a href=\"https://robots.ieee.org/robots/minicheetah/?utm_source=spectrum\">Mini-Cheetah</a> Workshop (sponsored by Naver Labs) a year from now at IROS 2020 in Las Vegas. Mini-Cheetahs for everyone!</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/3ho4puvbGcU\" width=\"620\"></iframe></p> \n <p></p> \n <p>That’s just a rendering, of course, but this isn’t:</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/G6fMV1UPzkg?rel=0\" width=\"620\"></iframe></p> \n <p>[ <a href=\"https://www.naverlabs.com/mini-cheetah\">MCW</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>I was like 95 percent&nbsp;sure that the Urban Circuit of the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit\">DARPA SubT Challenge</a> was going to be in something very subway station-y. Oops!</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/cmyJJG4E8NA\" width=\"620\"></iframe></p> \n  <p></p> \n  <blockquote> \n   <p><em>In the Subterranean (SubT) Challenge, teams deploy autonomous ground and aerial systems to attempt to map, identify, and report artifacts along competition courses in underground environments. The artifacts represent items a first responder or service member may encounter in unknown underground sites. This video provides a preview of the Urban Circuit event location. The Urban Circuit is scheduled for February 18-27, 2020, at Satsop Business Park west of Olympia, Washington.</em></p> \n  </blockquote> \n  <p>[ <a href=\"https://www.darpa.mil/news-events/2019-11-12\">SubT</a> ]</p> \n  <p></p> \n  <p></p> \n  <p></p> \n  <hr> \n   <p></p> \n   <blockquote> \n    <p><em>Researchers at SEAS and the Wyss Institute for Biologically Inspired Engineering have developed a resilient RoboBee powered by soft artificial muscles that can crash into walls, fall onto the floor, and collide with other RoboBees without being damaged. It is the first microrobot powered by soft actuators to achieve controlled flight.</em></p> \n   </blockquote> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/o259kRrPM5A\" width=\"620\"></iframe></p> \n   <p></p> \n   <blockquote> \n    <p><em>To solve the problem of power density, the researchers built upon the electrically-driven soft actuators developed in the lab of David Clarke, the Extended Tarr Family Professor of Materials. These soft actuators are made using dielectric elastomers, soft materials with good insulating properties, that deform when an electric field is applied. By improving the electrode conductivity, the researchers were able to operate the actuator at 500 Hertz, on par with the rigid actuators used previously in similar robots.</em></p> \n    <p><em>Next, the researchers aim to increase the efficiency of the soft-powered robot, which still lags far behind more traditional flying robots.</em></p> \n   </blockquote> \n   <p>[ <a href=\"https://www.seas.harvard.edu/news/2019/11/robobee-powered-soft-muscles\">Harvard</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>We present a system for fast and robust handovers with a robot character, together with a user study investigating the effect of robot speed and reaction time on perceived interaction quality. The system can match and exceed human speeds and confirms that users prefer human-level timing.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/w1Ff4nqcUvk\" width=\"620\"></iframe></p> \n    <p></p> \n    <blockquote> \n     <p><em>In a 3×3 user study, we vary the speed of the robot and add variable sensorimotor delays. We evaluate the social perception of the robot using the Robot Social Attribute Scale (RoSAS). Inclusion of a small delay, mimicking the delay of the human sensorimotor system, leads to an improvement in perceived qualities over both no delay and long delay conditions. Specifically, with no delay the robot is perceived as more discomforting and with a long delay, it is perceived as less warm.</em></p> \n    </blockquote> \n    <p>[ <a href=\"https://la.disneyresearch.com/publication/fast-handovers-with-a-robot-character-small-sensorimotor-delays-improve-perceived-qualities/\">Disney Research</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>When cars are autonomous, they’re not going to be able to pump themselves full of gas. Or, more likely, electrons. Kuka has the solution.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/EHopesMka6I\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"https://www.kuka.com/carla-connect\">Kuka</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>This looks like fun, right?</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/vB6xImBRd5o\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://www.bec-rides.com/\">Robocoaster</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>NASA is leading the way in the use of On-orbit Servicing, Assembly, and Manufacturing to enable large, persistent, upgradable, and maintainable spacecraft. This video was developed by the Advanced Concepts Lab (ACL) at NASA Langley Research Center.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/xP4_Q7iIlb0\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://sspd.gsfc.nasa.gov/\">NASA</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>The noisiest workshop by far at Humanoids last month (<em>by far</em>) was Musical Interactions With Humanoids, the end result of which was this:</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hcVwnF3WIJo\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"http://lofarolabs.com/events/humanoids2019/music/\">Workshop</a> ]</p> \n        <hr> \n         <blockquote> \n          <p><em>IROS is an IEEE event, and in furthering the IEEE mission to benefit humanity through technological innovation, IROS is doing a great job. But don’t take it from us – we are joined by IEEE President-Elect Professor Toshio Fukuda to find out a bit more about the impact events like IROS can have, as well as examine some of the issues around intelligent robotics and systems - from privacy to transparency of the systems at play.</em></p> \n         </blockquote> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://www.youtube.com/embed/Er506BykGRo\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[&nbsp;<a href=\"https://www.iros2019.org/\">IROS</a>&nbsp;]</p> \n         <hr> \n          <p>Speaking of IROS, we hope you’ve been <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau\">enjoying our coverage</a>. We have already featured <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot\">Harvard’s strange sea-urchin-inspired robot</a> and a Japanese&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now\">quadruped that can climb vertical ladders</a>, with more stories to come over the next several weeks.</p> \n          <p>In the mean time, enjoy these 10 videos from the conference (as usual, we’re including the title, authors, and abstract for each—if you’d like more details about any of these projects, let us know and we’ll find out more for you).</p> \n          <hr> \n           <h3>\"A Passive Closing, Tendon Driven, Adaptive Robot Hand for Ultra-Fast, Aerial Grasping and Perching,\" by Andrew McLaren, Zak Fitzgerald, Geng Gao, and Minas Liarokapis from the University of Auckland, New Zealand.</h3> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/iRp5shPwatQ\" width=\"620\"></iframe></p> \n           <blockquote> \n            <p><em>Current grasping methods for aerial vehicles are slow, inaccurate and they cannot adapt to any target object. Thus, they do not allow for on-the-fly, ultra-fast grasping. In this paper, we present a passive closing, adaptive robot hand design that offers ultra-fast, aerial grasping for a wide range of everyday objects. We investigate alternative uses of structural compliance for the development of simple, adaptive robot grippers and hands and we propose an appropriate quick release mechanism that facilitates an instantaneous grasping execution. The quick release mechanism is triggered by a simple distance sensor. The proposed hand utilizes only two actuators to control multiple degrees of freedom over three fingers and it retains the superior grasping capabilities of adaptive grasping mechanisms, even under significant object pose or other environmental uncertainties. The hand achieves a grasping time of 96 ms, a maximum grasping force of 56 N and it is able to secure objects of various shapes at high speeds. The proposed hand can serve as the end-effector of grasping capable Unmanned Aerial Vehicle (UAV) platforms and it can offer perching capabilities, facilitating autonomous docking.</em></p> \n           </blockquote> \n           <hr> \n            <h3>\"Unstructured Terrain Navigation and Topographic Mapping With a Low-Cost Mobile Cuboid Robot,\" by Andrew S. Morgan, Robert L. Baines, Hayley McClintock, and Brian Scassellati from Yale University, USA.</h3> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/klm6kwu2DAk\" width=\"620\"></iframe></p> \n            <blockquote> \n             <p><em>Current robotic terrain mapping techniques require expensive sensor suites to construct an environmental representation. In this work, we present a cube-shaped robot that can roll through unstructured terrain and construct a detailed topographic map of the surface that it traverses in real time with low computational and monetary expense. Our approach devolves many of the complexities of locomotion and mapping to passive mechanical features. Namely, rolling movement is achieved by sequentially inflating latex bladders that are located on four sides of the robot to destabilize and tip it. Sensing is achieved via arrays of fine plastic pins that passively conform to the geometry of underlying terrain, retracting into the cube. We developed a topography by shade algorithm to process images of the displaced pins to reconstruct terrain contours and elevation. We experimentally validated the efficacy of the proposed robot through object mapping and terrain locomotion tasks. </em></p> \n            </blockquote> \n            <hr> \n             <h3>\"Toward a Ballbot for Physically Leading People: A Human-Centered Approach,\" by Zhongyu Li and Ralph Hollis from Carnegie Mellon University, USA.</h3> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/rFIGOfcDngA\" width=\"620\"></iframe></p> \n             <blockquote> \n              <p><em>This work presents a new human-centered method for indoor service robots to provide people with physical assistance and active guidance while traveling through congested and narrow spaces. As most previous work is robot-centered, this paper develops an end-to-end framework which includes a feedback path of the measured human positions. The framework combines a planning algorithm and a human-robot interaction module to guide the led person to a specified planned position. The approach is deployed on a person-size dynamically stable mobile robot, the CMU ballbot. Trials were conducted where the ballbot physically led a blindfolded person to safely navigate in a cluttered environment. </em></p> \n             </blockquote> \n             <hr> \n              <h3>\"Achievement of Online Agile Manipulation Task for Aerial Transformable Multilink Robot,\" by Fan Shi, Moju Zhao, Tomoki Anzai, Keita Ito, Xiangyu Chen, Kei Okada, and Masayuki Inaba from the University of Tokyo, Japan.</h3> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Qf5ADoHQ-_M\" width=\"620\"></iframe></p> \n              <blockquote> \n               <p><em>Transformable aerial robots are favorable in aerial manipulation tasks for their flexible ability to change configuration during the flight. By assuming robot keeping in the mild motion, the previous researches sacrifice aerial agility to simplify the complex non-linear system into a single rigid body with a linear controller. In this paper, we present a framework towards agile swing motion for the transformable multi-links aerial robot. We introduce a computational-efficient non-linear model predictive controller and joints motion primitive frame-work to achieve agile transforming motions and validate with a novel robot named HYRURS-X. Finally, we implement our framework under a table tennis task to validate the online and agile performance. </em></p> \n              </blockquote> \n              <hr> \n               <h3>\"Small-Scale Compliant Dual Arm With Tail for Winged Aerial Robots,\" by Alejandro Suarez, Manuel Perez, Guillermo Heredia, and Anibal Ollero from the University of Seville, Spain.</h3> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/C7Th6EcCnjk\" width=\"620\"></iframe></p> \n               <blockquote> \n                <p><em>Winged aerial robots represent an evolution of aerial manipulation robots, replacing the multirotor vehicles by fixed or flapping wing platforms. The development of this morphology is motivated in terms of efficiency, endurance and safety in some inspection operations where multirotor platforms may not be suitable. This paper presents a first prototype of compliant dual arm as preliminary step towards the realization of a winged aerial robot capable of perching and manipulating with the wings folded. The dual arm provides 6 DOF (degrees of freedom) for end effector positioning in a human-like kinematic configuration, with a reach of 25 cm (half-scale w.r.t. the human arm), and 0.2 kg weight. The prototype is built with micro metal gear motors, measuring the joint angles and the deflection with small potentiometers. The paper covers the design, electronics, modeling and control of the arms. Experimental results in test-bench validate the developed prototype and its functionalities, including joint position and torque control, bimanual grasping, the dynamic equilibrium with the tail, and the generation of 3D maps with laser sensors attached at the arms. </em></p> \n               </blockquote> \n               <hr> \n                <h3>\"A Novel Small-Scale Turtle-inspired Amphibious Spherical Robot,\" by Huiming Xing, Shuxiang Guo, Liwei Shi, Xihuan Hou, Yu Liu, Huikang Liu, Yao Hu, Debin Xia, and Zan Li from Beijing Institute of Technology, China.</h3> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/QZ5YyNiJJMA\" width=\"620\"></iframe></p> \n                <blockquote> \n                 <p><em>This paper describes a novel small-scale turtle-inspired Amphibious Spherical Robot (ASRobot) to accomplish exploration tasks in the restricted environment, such as amphibious areas and narrow underwater cave. A Legged, Multi-Vectored Water-Jet Composite Propulsion Mechanism (LMVWCPM) is designed with four legs, one of which contains three connecting rod parts, one water-jet thruster and three joints driven by digital servos. Using this mechanism, the robot is able to walk like amphibious turtles on various terrains and swim flexibly in submarine environment. A simplified kinematic model is established to analyze crawling gaits. With simulation of the crawling gait, the driving torques of different joints contributed to the choice of servos and the size of links of legs. Then we also modeled the robot in water and proposed several underwater locomotion. In order to assess the performance of the proposed robot, a series of experiments were carried out in the lab pool and on flat ground using the prototype robot. Experiments results verified the effectiveness of LMVWCPM and the amphibious control approaches.</em></p> \n                </blockquote> \n                <hr> \n                 <h3>\"Advanced Autonomy on a Low-Cost Educational Drone Platform,\" by Luke Eller, Theo Guerin, Baichuan Huang, Garrett Warren, Sophie Yang, Josh Roy, and Stefanie Tellex from Brown University, USA.</h3> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/r0XlUwYDNc8\" width=\"620\"></iframe></p> \n                 <blockquote> \n                  <p><em>PiDrone is a quadrotor platform created to accompany an introductory robotics course. Students build an autonomous flying robot from scratch and learn to program it through assignments and projects. Existing educational robots do not have significant autonomous capabilities, such as high-level planning and mapping. We present a hardware and software framework for an autonomous aerial robot, in which all software for autonomy can run onboard the drone, implemented in Python. We present an Unscented Kalman Filter (UKF) for accurate state estimation. Next, we present an implementation of Monte Carlo (MC) Localization and Fast-SLAM for Simultaneous Localization and Mapping (SLAM). The performance of UKF, localization, and SLAM is tested and compared to ground truth, provided by a motion-capture system. Our evaluation demonstrates that our autonomous educational framework runs quickly and accurately on a Raspberry Pi in Python, making it ideal for use in educational settings. </em></p> \n                 </blockquote> \n                 <hr> \n                  <h3>\"FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality,\" by Winter Guerra, Ezra Tal, Varun Murali, Gilhyun Ryou and Sertac Karaman from the Massachusetts Institute of Technology, USA.</h3> \n                  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/QCnU_M6DhYU\" width=\"620\"></iframe></p> \n                  <blockquote> \n                   <p><em>FlightGoggles is a photorealistic sensor simulator for perception-driven robotic vehicles. The key contributions of FlightGoggles are twofold. First, FlightGoggles provides photorealistic exteroceptive sensor simulation using graphics assets generated with photogrammetry. Second, it provides the ability to combine (i) synthetic exteroceptive measurements generated in silico in real time and (ii) vehicle dynamics and proprioceptive measurements generated in motio by vehicle(s) in flight in a motion-capture facility. FlightGoggles is capable of simulating a virtual-reality environment around autonomous vehicle(s) in flight. While a vehicle is in flight in the FlightGoggles virtual reality environment, exteroceptive sensors are rendered synthetically in real time while all complex dynamics are generated organically through natural interactions of the vehicle. The FlightGoggles framework allows for researchers to accelerate development by circumventing the need to estimate complex and hard-to-model interactions such as aerodynamics, motor mechanics, battery electrochemistry, and behavior of other agents. The ability to perform vehicle-in-the-loop experiments with photorealistic exteroceptive sensor simulation facilitates novel research directions involving, e.g., fast and agile autonomous flight in obstacle-rich environments, safe human interaction, and flexible sensor selection. FlightGoggles has been utilized as the main test for selecting nine teams that will advance in the AlphaPilot autonomous drone racing challenge. We survey approaches and results from the top AlphaPilot teams, which may be of independent interest. FlightGoggles is distributed as open-source software along with the photorealistic graphics assets for several simulation environments, under the MIT license at http://flightgoggles.mit.edu. </em></p> \n                  </blockquote> \n                  <hr> \n                   <h3>\"An Autonomous Quadrotor System for Robust High-Speed Flight Through Cluttered Environments Without GPS,\" by Marc Rigter, Benjamin Morrell, Robert G. Reid, Gene B. Merewether, Theodore Tzanetos, Vinay Rajur, KC Wong, and Larry H. Matthies from University of Sydney, Australia; NASA Jet Propulsion Laboratory, California Institute of Technology, USA; and Georgia Institute of Technology, USA.</h3> \n                   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/am1Tq4MXn7w\" width=\"620\"></iframe></p> \n                   <blockquote> \n                    <p><em>Robust autonomous flight without GPS is key to many emerging drone applications, such as delivery, search and rescue, and warehouse inspection. These and other appli- cations require accurate trajectory tracking through cluttered static environments, where GPS can be unreliable, while high- speed, agile, flight can increase efficiency. We describe the hardware and software of a quadrotor system that meets these requirements with onboard processing: a custom 300 mm wide quadrotor that uses two wide-field-of-view cameras for visual- inertial motion tracking and relocalization to a prior map. Collision-free trajectories are planned offline and tracked online with a custom tracking controller. This controller includes compensation for drag and variability in propeller performance, enabling accurate trajectory tracking, even at high speeds where aerodynamic effects are significant. We describe a system identification approach that identifies quadrotor-specific parameters via maximum likelihood estimation from flight data. Results from flight experiments are presented, which 1) validate the system identification method, 2) show that our controller with aerodynamic compensation reduces tracking error by more than 50% in both horizontal flights at up to 8.5 m/s and vertical flights at up to 3.1 m/s compared to the state-of-the-art, and 3) demonstrate our system tracking complex, aggressive, trajectories.</em></p> \n                   </blockquote> \n                   <hr> \n                    <h3>\"Morphing Structure for Changing Hydrodynamic Characteristics of a Soft Underwater Walking Robot,\" by Michael Ishida, Dylan Drotman, Benjamin Shih, Mark Hermes, Mitul Luhar, and Michael T. Tolley from the University of California, San Diego (UCSD)&nbsp;and University of Southern California, USA.</h3> \n                    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/y6n9bI5rzu8\" width=\"620\"></iframe></p> \n                    <blockquote> \n                     <p><em>Existing platforms for underwater exploration and inspection are often limited to traversing open water and must expend large amounts of energy to maintain a position in flow for long periods of time. Many benthic animals overcome these limitations using legged locomotion and have different hydrodynamic profiles dictated by different body morphologies. This work presents an underwater legged robot with soft legs and a soft inflatable morphing body that can change shape to influence its hydrodynamic characteristics. Flow over the morphing body separates behind the trailing edge of the inflated shape, so whether the protrusion is at the front, center, or back of the robot influences the amount of drag and lift. When the legged robot (2.87 N underwater weight) needs to remain stationary in flow, an asymmetrically inflated body resists sliding by reducing lift on the body by 40% (from 0.52 N to 0.31 N) at the highest flow rate tested while only increasing drag by 5.5% (from 1.75 N to 1.85 N). When the legged robot needs to walk with flow, a large inflated body is pushed along by the flow, causing the robot to walk 16% faster than it would with an uninflated body. The body shape significantly affects the ability of the robot to walk against flow as it is able to walk against 0.09 m/s flow with the uninflated body, but is pushed backwards with a large inflated body. We demonstrate that the robot can detect changes in flow velocity with a commercial force sensor and respond by morphing into a hydrodynamically preferable shape. </em></p> \n                    </blockquote> \n                    <hr> \n                     <div class=\"feedflare\"> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=LIToXQovbEs:w8wppRpFVAI:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=LIToXQovbEs:w8wppRpFVAI:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=LIToXQovbEs:w8wppRpFVAI:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=LIToXQovbEs:w8wppRpFVAI:-BTjWOF_DHI\" border=\"0\" /></a> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=LIToXQovbEs:w8wppRpFVAI:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=LIToXQovbEs:w8wppRpFVAI:gIN9vFwOqvQ\" border=\"0\" /></a> \n                     </div> \n                     <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/LIToXQovbEs\" height=\"1\" width=\"1\" alt=\"\" /> \n                    </hr> \n                   </hr> \n                  </hr> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 15 Nov 2019 22:55:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxMzkyMg.jpeg","linkMd5":"40500605e90a1fb25e13a0c4109cdc90","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn42@2020_2/2020/08/25/10-38-01-172_4d40cf32504dad3c.webp","destWidth":1280,"destHeight":720,"sourceBytes":229837,"destBytes":148672,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=LIToXQovbEs:w8wppRpFVAI:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn42@2020_4/2020/08/25/10-38-17-618_b4b657b6c5d3d42c.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=LIToXQovbEs:w8wppRpFVAI:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx/cdn28@2020_2/2020/08/25/10-38-02-319_259d796e79ae6ca6.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/LIToXQovbEs":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn9@2020_5/2020/08/25/10-38-02-081_5451384509fafbd5.webp"},"publishedOrCreatedDate":1598351859739},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Is it Time for Tech to Stop Moving Fast and Breaking Things?","link":"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things","description":"<p>On Monday, I attended the 2019 <a href=\"https://hai.stanford.edu/events/2019-fall-conference/overview\">Fall Conference</a> of Stanford’s <a href=\"https://hai.stanford.edu/about/welcome\">Institute for Human Centered Artificial Intelligence</a> (HAI). That same night I watched the Season 6 opener for the HBO TV show <em><a href=\"https://www.hbo.com/silicon-valley\">Silicon Valley</a></em>. And the debates featured in both surrounded the responsibility of tech companies for the societal effects of the technologies they produce. The two events have jumbled together in my mind, perhaps because I was in a bit of a brain fog, thanks to the nasty combination of a head cold and the smoke that descended on Silicon Valley from the northern California wildfires. But perhaps that mixture turned out to be a good thing.</p> \n<p>What is clear, in spite of the smoke, is that this issue&nbsp;is&nbsp;something a lot of people are talking about, inside and outside of Silicon Valley (witness the viral video of Rep.&nbsp;<a href=\"https://www.rollingstone.com/politics/politics-news/alexandria-ocasio-cortez-mark-zuckerberg-facebook-hearing-903057/\">Alexandria Ocasio-Cortez (D-NY) grilling Facebook CEO Mark Zuckerberg</a>).</p> \n<p>So, to add to that conversation, here’s my HBO <em>Silicon Valley</em>/Stanford HAI conference mashup.</p> \n<p></p> \n<p><em>Silicon Valley</em>’s&nbsp;fictional CEO&nbsp;Richard Hendriks, in the opening scene of the episode,&nbsp;<span>tells Congress&nbsp;</span>that <a href=\"http://www.facebook.com/\">Facebook</a>, <a href=\"http://www.google.com/\">Google</a>, and <a href=\"http://www.amazon.com/\">Amazon</a> only care about exploiting personal data for profit. He states:</p> \n<blockquote> \n <p>“These companies are kings, and they rule over kingdoms far larger than any nation in history.”</p> \n</blockquote> \n<p>Meanwhile Marietje Schaake, former member of the European Parliament and a fellow at HAI, told the conference audience of 900:</p> \n<blockquote> \n <p>“There is a lot of power in the hands of few actors—Facebook decides who is a news source, Microsoft will run the defense department’s cloud…. I believe we need a deeper debate about which tasks need to stay in the hands of the public.”</p> \n</blockquote> \n<p>Eric Schmidt, former CEO and executive chairman of Google, agreed. He says:&nbsp;</p> \n<blockquote> \n <p>“It is important that we debate now the ethics of what we are doing, and the impact of the technology that we are building.”</p> \n</blockquote> \n<p>&nbsp;Stanford Associate Professor <a href=\"https://spectrum.ieee.org/geek-life/profiles/ge-wang-the-iphones-music-man\">Ge Wang</a>, also speaking at the HAI conference, pointed out:</p> \n<blockquote> \n <p>“‘Doing no harm’&nbsp;is a vital goal, and it is not easy. But it is different from a proactive goal, to ‘do good.’”</p> \n</blockquote> \n<p>Had <em>Silicon Valley</em>’s Hendricks been there, he would have agreed. He said in the episode:</p> \n<blockquote> \n <p>“Just because it’s successful, doesn’t mean it’s good. Hiroshima was a successful implementation.”</p> \n</blockquote> \n<p>The speakers at the HAI conference discussed the implications of moving fast and breaking things, of putting untested and unregulated technology into the world now that we know that things like public trust and even democracy can be broken.</p> \n<p>Google’s Schmidt told the HAI audience:</p> \n<blockquote> \n <p>“I don’t think that everything that is possible should be put into the wild in society, we should answer the question, collectively, how much risk are we willing to take.</p> \n</blockquote> \n<p>And Silicon Valley denizens real and fictional no longer think it’s OK to just&nbsp;say sorry afterwards. Says Schmidt:</p> \n<blockquote> \n <p>“When you ask Facebook about various scandals, how can they still say ‘We are very sorry; we have a lot of learning to do.’ This kind of naiveté stands out of proportion to the power tech companies have. With great power should come great responsibility, or at least modesty.”</p> \n</blockquote> \n<p>Schaake argued:</p> \n<blockquote> \n <p>“We need more guarantees, institutions, and policies than stated good intentions. It’s about more than promises.”</p> \n</blockquote> \n<p>Fictional CEO Hendricks thinks saying sorry is a cop-out as well. In the episode, a developer admits that his app collected user data in spite of Hendricks assuring Congress that his company doesn’t do that:</p> \n<blockquote> \n <p>“You didn’t know at the time,” the developer says.&nbsp;“Don’t beat yourself up about it. But in the future, stop saying it. Or don’t; I don’t care. Maybe it will be like Google saying ‘Don’t be evil,’ or Facebook saying ‘I’m sorry, we’ll do better.’”</p> \n</blockquote> \n<p>Hendricks doesn’t buy it:</p> \n<blockquote> \n <p>“This stops now. I’m the boss, and this is over.”</p> \n</blockquote> \n<p>(Well, he is fictional.)</p> \n<p>How can government, the tech world, and the general public address this in a more comprehensive way? Out in the real world, the “what to do” discussion at Stanford HAI surrounded regulation—how much, what kind, and when.</p> \n<p>Says the European Parliament’s Schaake:</p> \n<blockquote> \n <p>“An often-heard argument is that government should refrain from regulating tech because [regulation] will stifle innovation. [That argument] implies that innovation is more important than democracy or the rule of law. Our problems don’t stem from over regulation, but under regulation of technologies.”</p> \n</blockquote> \n<p>But when should that regulation happen. Stanford provost emeritus John Etchemendy, speaking from the audience at the HAI conference, said:</p> \n<blockquote> \n <p>“I’ve been an advocate of not trying to regulate before you understand it. Like San Francisco banning of use of facial recognition is not a good example of regulation; there are uses of facial recognition that we should allow. We want regulations that are just right, that prevent the bad things and allow the good things. So we are going to get it wrong either way, if we regulate to soon or hold off, we will get some things wrong.”</p> \n</blockquote> \n<p>Schaake would opt for regulating sooner rather than later. She says that she often hears the argument that it is too early to regulate artificial intelligence—as well as the argument that it is too late to regulate ad-based political advertising, or online privacy. Neither, to her, makes sense. She told the HAI attendees:</p> \n<blockquote> \n <p>“We need more than guarantees&nbsp;than stated good intentions.”</p> \n</blockquote> \n<p>U.S. Chief Technology Officer <a href=\"https://www.whitehouse.gov/people/michael-kratsios/\">Michael Kratsios</a> would go with later rather than sooner. (And, yes, the country has a CTO. President Barack Obama created the position in 2009; Kratsios is the fourth to hold the office and the first under President Donald Trump. He was confirmed in August.) Also speaking at the HAI conference, Kratsios argued:</p> \n<blockquote> \n <p>“I don’t think we should be running to regulate anything. We are a leader [in technology] not because we had great regulations, but we have taken a free market approach. We have done great in driving innovation in technologies that are born free, like the Internet. Technologies born in captivity, like autonomous vehicles, lag behind.”</p> \n</blockquote> \n<p>In the fictional world of HBO’s <em>Silicon Valley</em>, startup founder Hendricks has a solution—a technical one of course:&nbsp; <a href=\"https://spectrum.ieee.org/view-from-the-valley/telecom/internet/hbos-silicon-valley-joins-the-push-for-a-decentralized-web\">the decentralized Internet</a>. He tells Congress:</p> \n<blockquote> \n <p>“The way we win is by creating a new, decentralized Internet, one where the behavior of companies like this will be impossible, forever. Where it is the users, not the kings, who have sovereign control over their data. I will help you build an Internet that is of the people, by the people, and for the people.”</p> \n</blockquote> \n<p>(This is not a fictional concept, though it is a long way from wide use. Also called the <a href=\"https://spectrum.ieee.org/view-from-the-valley/telecom/internet/brewster-kahle-on-whats-next-for-the-decentralized-web-movement\">decentralized Web</a>, the concept takes the content on today’s Web and fragments it, and then replicates and scatters those fragments to hosts around the world, increasing privacy and reducing the ability of governments to restrict access.)</p> \n<p>If neither regulation nor technology comes to make the world safe from the unforeseen effects of new technologies, there is one more hope, according to Schaake: the millennials and subsequent generations.</p> \n<p>Tech companies can no longer pursue growth at all costs, not if they want to keep attracting the talent they need, says Schaake. She noted that,&nbsp;“the young generation looks at the environment, at homeless on the streets,” and they expect their companies to tackle those and other issues and make the world a better place.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VszUWNQuPRA:_kdYL-vrJa4:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VszUWNQuPRA:_kdYL-vrJa4:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VszUWNQuPRA:_kdYL-vrJa4:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VszUWNQuPRA:_kdYL-vrJa4:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VszUWNQuPRA:_kdYL-vrJa4:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VszUWNQuPRA:_kdYL-vrJa4:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VszUWNQuPRA\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 01 Nov 2019 15:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNDMwOQ.jpeg","linkMd5":"daf121b367dccdb1c38a3b54e0c17b83","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn69@2020_3/2020/08/25/10-37-59-701_03c9c0f0f0e67746.webp","destWidth":1240,"destHeight":930,"sourceBytes":162147,"destBytes":69880,"author":"Tekla S. Perry","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VszUWNQuPRA:_kdYL-vrJa4:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn83@2020_1/2020/08/25/10-38-02-758_00e4778bd80ba076.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VszUWNQuPRA:_kdYL-vrJa4:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn60@2020_4/2020/08/25/10-38-10-541_99616023c1828a68.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VszUWNQuPRA":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn19@2020_1/2020/08/25/10-38-08-458_e15d2fe66f8344f4.webp"},"publishedOrCreatedDate":1598351859724},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Japanese Researchers Teaching Robots to Repair Themselves","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves","description":"<link href=\"/ns/interactive/0118race-to-5g/css/5g-logo-treatments.css\" rel=\"stylesheet\"> \n <style type=\"text/css\">aside.inlay.xlrg.XploreFree { font-family: \"Georgia\", serif; border-width: 4px 0; border-top: solid #888; border-bottom: solid #888; padding: 10px 0; font-size: 19px; font-weight: bold; text-align: center; } span.FreeRed { color: red; text-transform: uppercase; font-family: \"Theinhardt-Medium\", sans-serif; } span.XploreBlue { color: #03a6e3; font-family: \"Theinhardt-Medium\", sans-serif; } </style> \n <div class=\"mobileHide\"> \n  <div class=\"imgWrapper offsetLeft lt sm\"> \n   <a href=\"https://spectrum.ieee.org/static/journal-watch\"><img alt=\"Journal Watch report logo, link to report landing page\" src=\"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg\" /></a> \n  </div> \n </div> \n <p>When mobile manipulators eventually make it into our homes, self-repair is going to be a very important function. Hopefully, these robots will be durable enough that they won’t need to be repaired very often, but from time to time they’ll almost certainly need minor maintenance. At Humanoids 2019 in Toronto, researchers from the University of Tokyo showed how they taught a <a href=\"https://robots.ieee.org/robots/pr2/?utm_source=spectrum\">PR2</a> to perform simple repairs on itself by tightening its own screws. And using that skill, the robot was also able to augment itself, adding accessories like hooks to help it carry more stuff. Clever robot!</p> \n <!--nextpage--> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/47NjYRWVjLk\" width=\"620\"></iframe></p> \n <p>To keep things simple, the researchers provided the robot with CAD data that tells it exactly where all of its screws are.&nbsp;</p> \n <p>At the moment, the robot can’t directly detect on its own whether a particular screw needs tightening, although it can tell if its physical pose doesn’t match its digital model, which suggests that something has gone wonky. It can also check its screws autonomously from time to time, or rely on a human physically pointing out that it has a screw loose, using the human’s finger location to identify which screw it is. Another challenge is that most robots, like most humans, are limited in the areas on themselves that they can comfortably reach. So to tighten up everything, they might have to find themselves a robot friend to help, just like humans help each other put on sunblock.</p> \n <p>The actual tightening is either super easy or quite complicated, depending on the location and orientation of the screw. If the robot is lucky, it can just use its continuous wrist rotation for tightening, but if a screw is located in a tight position that requires an Allen wrench, the robot has to regrasp the tool over and over as it incrementally tightens the screw.&nbsp;</p> \n <figure class=\"xlrg\" role=\"img\"> \n  <img alt=\"PR2 augmenting itself by adding a hook\" src=\"https://spectrum.ieee.org/image/MzQyNDI4MQ.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Image: University of Tokyo \n   </figcaption> \n   <figcaption>\n     In one experiment, the researchers taught a PR2 robot to attach a hook to one of its shoulders. The robot uses one hand to grasp the hook and another hand to grasp a screwdriver. The researchers tested the hook by hanging a tote bag on it. \n   </figcaption> </img> \n </figure> \n <p>The other neat trick that a robot can do once it can tighten screws on its own body is to add new bits of hardware to itself. <a href=\"https://robots.ieee.org/robots/pr2/?utm_source=spectrum\">PR2</a> was thoughtfully designed with mounting points on its shoulders (or maybe technically its neck) and head, and it turns out that it can reach these points with its manipulators, allowing to modify itself, as the researchers explain:</p> \n <blockquote> \n  <p><em>When PR2 wants to have a lot of things, the only two hands are not enough to realize that. So we let PR2 to use a bag the same as we put it on our shoulder. PR2 started attaching the hook whose pose is calculated with self CAD data with a driver on his shoulder in order to put a bag on his shoulder. PR2 finished attaching the hook, and the people put a lot of cans in a tote bag and put it on PR2’s shoulder. </em></p> \n </blockquote> \n <h4>“Self-Repair and Self-Extension by Tightening Screws based on Precise Calculation of Screw Pose of Self-Body with CAD Data and Graph Search with Regrasping a Driver,” by Takayuki Murooka, Kei Okada, and Masayuki Inaba from the University of Tokyo, was presented at Humanoids 2019 in Toronto, Canada.</h4> \n <div class=\"mobileShow\"> \n  <h3 class=\"RptHdBackBarMobile\"><span class=\"BackArrowBlkBkgrd\">&lt;</span>&nbsp;<a href=\"https://spectrum.ieee.org/static/journal-watch\">Back to IEEE Journal Watch</a></h3> \n </div> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7B7ZQFeObi8:QLT2oDiNohE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7B7ZQFeObi8:QLT2oDiNohE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7B7ZQFeObi8:QLT2oDiNohE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7B7ZQFeObi8:QLT2oDiNohE:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7B7ZQFeObi8:QLT2oDiNohE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7B7ZQFeObi8:QLT2oDiNohE:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/7B7ZQFeObi8\" height=\"1\" width=\"1\" alt=\"\" /> \n</link>","descriptionType":"html","publishedDate":"Thu, 05 Dec 2019 22:01:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQyNDI4Mw.jpeg","linkMd5":"51766f8db6dedb5c20550ff3c3224e16","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn98@2020_6/2020/08/25/10-37-58-833_e0bf7973e0ba05b6.webp","destWidth":1240,"destHeight":704,"sourceBytes":446637,"destBytes":37192,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn28@2020_5/2020/08/25/10-38-09-369_964adaa282b1aac4.webp","https://spectrum.ieee.org/image/MzQyNDI4MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn22@2020_1/2020/08/25/10-38-12-912_09bd06bad6d0d8d3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7B7ZQFeObi8:QLT2oDiNohE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn75@2020_1/2020/08/25/10-38-04-933_aaa7b2b8372dc7fe.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7B7ZQFeObi8:QLT2oDiNohE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn68@2020_4/2020/08/25/10-38-05-093_c78bb44aac8261da.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/7B7ZQFeObi8":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn28@2020_5/2020/08/25/10-38-05-166_3ccd3b37c0353e90.webp"},"publishedOrCreatedDate":1598351859710},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"New Double 3 Robot Makes Telepresence Easier than Ever","link":"https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever","description":"<p>Today, Double Robotics is announcing <a href=\"https://www.doublerobotics.com/\">Double 3</a>, the latest major upgrade to its line of consumer(ish) telepresence robots. <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/double-2-review-telepresence-robot\">We had a (mostly) fantastic time testing out Double 2 back in 2016</a>.&nbsp;One of the things that we found out back then was that it takes a lot of practice to remotely drive the robot around. Double 3 solves this problem by leveraging the substantial advances in 3D sensing and computing that have taken place over the past few years, giving their new robot a level of intelligence that promises to make telepresence more accessible for everyone.</p> \n<!--nextpage--> \n<p>Double 2’s iPad has been replaced by “a fully integrated solution”—which is a fancy way of saying a dedicated 9.7-inch&nbsp;touchscreen and a whole bunch of other stuff. That other stuff includes an NVIDIA Jetson TX2 AI computing module, a beamforming six-microphone array, an 8-watt speaker, a pair of 13-megapixel cameras (wide angle and zoom) on a tilting mount, five ultrasonic rangefinders, and most excitingly, a pair of Intel RealSense D430 depth sensors.&nbsp;</p> \n<p>It’s those new depth sensors that really make Double 3 special. The D430 modules each uses a pair of stereo cameras with a pattern projector to generate 1280 x 720 depth data with a range of between 0.2 and 10 meters away. The Double 3 robot uses all of this high quality depth data to locate obstacles, but at this point, it&nbsp;still doesn’t&nbsp;drive&nbsp;completely autonomously. Instead, it presents the remote operator with a slick, augmented reality view of drivable areas in the form of a grid of dots.&nbsp;You&nbsp;just click where you want the robot to go, and it will skillfully take itself there while avoiding obstacles (including dynamic obstacles) and related mishaps along the way.&nbsp;</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/zSDL9pR0A6I\" width=\"620\"></iframe>This effectively offloads the most stressful part of telepresence—not running into stuff—from the remote user to the robot itself, which is the way it should be. That&nbsp;makes it that much easier to encourage people to utilize telepresence for the first time. The way the system is implemented through augmented reality is particularly impressive, I think.&nbsp;It looks like it’s intuitive enough for an inexperienced user without being restrictive, and is a clever way of mitigating even significant amounts of lag.&nbsp;</p> \n<p>Otherwise, Double 3’s mobility system is exactly the same as the one featured on Double 2. In fact, that you can stick a Double 3 head on a Double 2 body and it instantly becomes a Double 3. Double Robotics is thoughtfully offering this to current Double 2 owners as a significantly more affordable upgrade option than buying a whole new robot.</p> \n<p>For more details on all of Double 3's new features, we spoke with the co-founders of Double Robotics, Marc DeVidts and David Cann.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Double Robotics Marc DeVidts (left) and David Cann\" src=\"https://spectrum.ieee.org/image/MzM2MjU1OQ.jpeg\"> </img> \n</figure> \n<p><strong>IEEE Spectrum: Why use this augmented reality system instead of just letting the user click on a regular camera image? Why make things more visually complicated, especially for new users?</strong></p> \n<blockquote> \n <p><strong>Marc DeVidts and David Cann: </strong>One of the things that we realized about nine months ago when we got this whole thing working was that without the mixed reality for driving, it was really too magical of an experience for the customer. Even us—we had a hard time understanding whether the robot could really see obstacles and understand where the floor is and that kind of thing. So, we said “What would be the best way of communicating this information to the user?”&nbsp;And the right way to do it ended up drawing the graphics directly onto the scene. It’s really awesome—we have a full, real time 3D scene with the depth information drawn on top of it. We’re starting with some relatively simple graphics, and we’ll be adding more graphics in the future to help the user understand what the robot is seeing.</p> \n</blockquote> \n<p><strong>How robust is the vision system when it comes to obstacle detection and avoidance? Does it work with featureless surfaces, IR absorbent surfaces, in low light, in direct sunlight, etc?</strong></p> \n<blockquote> \n <p>We’ve looked at all of those cases, and one of the reasons that we’re going with the RealSense is the projector that helps us to see blank walls. We also found that having two sensors—one facing the floor and one facing forward—gives us a great coverage area. Having ultrasonic sensors in there as well helps us to detect anything that we can't see with the cameras. They're sort of a last safety measure, especially useful for detecting glass.&nbsp;</p> \n</blockquote> \n<p><strong>It seems like there’s a lot more that you could do with this sensing and mapping capability. What else are you working on?</strong></p> \n<blockquote> \n <p>We're starting with this semi-autonomous driving variant, and we're doing a private beta of full mapping. So, we’re going to do full SLAM of your environment that will be mapped by multiple robots at the same time while you're driving, and then you'll be able to zoom out to a map and click anywhere and it will drive there. That's where we're going with it, but we want to take baby steps to get there. It's the obvious next step, I think, and there are a lot more possibilities there.</p> \n</blockquote> \n<p><strong>Do you expect developers to be excited for this new mapping capability?</strong></p> \n<blockquote> \n <p>We're using a very powerful computer in the robot, a NVIDIA Jetson TX2 running Ubuntu. There's room to grow. It’s actually really exciting to be able to see, in real time, the 3D pose of the robot along with all of the depth data&nbsp;that gets transformed in real time into one view that gives you a full map. Having all of that data and just putting those pieces together and getting everything to work has been a huge feat in of itself.&nbsp;</p> \n <p>We have an extensive API for developers to do custom implementations, either for telepresence or other kinds of robotics research. Our system isn't running ROS, but we're going to be adding ROS adapters for all of our hardware components.</p> \n</blockquote> \n<p><strong>Telepresence robots depend heavily on wireless connectivity, which is usually not something that telepresence robotics companies like Double have direct control over. Have you found that connectivity has been getting significantly better since you first introduced Double?</strong></p> \n<blockquote> \n <p>When we started in 2013, we had a lot of customers that didn’t have WiFi in their hallways, just in the conference rooms. We very rarely hear about customers having WiFi connectivity issues these days. The bigger issue we see is when people are calling into the robot from home, where they don't have proper traffic management on their home network. The robot doesn't need a ton of bandwidth, but it does need consistent, low latency bandwidth.&nbsp;And so, if someone else in the house is watching Netflix or something like that, it’s going to saturate your connection. But for the most part, it’s gotten a lot better over the last few years, and it’s no longer a big problem for us.</p> \n</blockquote> \n<p><strong>Do you think 5G will make a significant difference to telepresence robots?</strong></p> \n<blockquote> \n <p>We’ll see. We like the low latency possibilities and the better bandwidth, but it's all going to be a matter of what kind of reception you get. LTE can be great, if you have good reception; it’s all about where the tower is. I’m pretty sure that WiFi is going to be the primary thing for at least the next few years.</p> \n</blockquote> \n<hr> \n <p>DeVidts also mentioned that an unfortunate side effect of the new depth sensors is that hanging a t-shirt on your Double to give it some personality will likely render it partially blind, so that's just something to keep in mind. To make up for this, you can switch around the colorful trim surrounding the screen, which is nowhere near as fun.</p> \n <p>When the Double 3 is ready for shipping in late September, US $2,000 will get you the new head with all the sensors and stuff, which seamlessly integrates with your Double 2 base. Buying Double 3 straight up (with&nbsp;<span>the included&nbsp;charging dock)</span> will run you $4,ooo. This is by no means an inexpensive robot, and my impression is that it’s not really designed for individual consumers. But for commercial, corporate, healthcare, or education applications, $4k for a robot as capable as the Double 3 is really quite a good deal—especially considering the kinds of use cases for which&nbsp;it’s ideal.</p> \n <p>[ Double Robotics ]</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5_MvFCYKYnM:EayMvXTOEek:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5_MvFCYKYnM:EayMvXTOEek:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5_MvFCYKYnM:EayMvXTOEek:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5_MvFCYKYnM:EayMvXTOEek:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5_MvFCYKYnM:EayMvXTOEek:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5_MvFCYKYnM:EayMvXTOEek:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/5_MvFCYKYnM\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Tue, 27 Aug 2019 17:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MjU0Mg.jpeg","linkMd5":"9811c27a8bfb20ca93a0926d39ae7e1e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn61@2020_4/2020/08/25/10-38-00-970_c4dcfbc79c6fe996.webp","destWidth":1240,"destHeight":930,"sourceBytes":178115,"destBytes":88388,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2MjU1OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn86@2020_6/2020/08/25/10-38-11-816_e76f28574abc8862.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5_MvFCYKYnM:EayMvXTOEek:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn13@2020_4/2020/08/25/10-38-02-373_f29891d9aeea81b1.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5_MvFCYKYnM:EayMvXTOEek:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn90@2020_4/2020/08/25/10-38-12-021_aa03ec4bdb47442d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/5_MvFCYKYnM":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn37@2020_5/2020/08/25/10-38-12-287_8b2a5106a9747295.webp"},"publishedOrCreatedDate":1598351859735},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Want a Really Hard Machine Learning Problem? Try Agriculture, Says John Deere Labs","link":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/want-a-really-hard-machine-learning-problem-try-agriculture-say-john-deere-labs-leaders","description":"<p>What’s the world’s hardest machine learning problem? Autonomous vehicles? Robots that can walk? Cancer detection?</p> \n<p>Nope, says <a href=\"https://johndeerejournal.com/2019/04/childhood-personality-form-sanchezs-innovative-style/\">Julian Sanchez</a>. It’s agriculture.</p> \n<p>Sanchez might be a little biased. He is the director of precision agriculture for <a href=\"https://www.deere.com/en/\">John Deere</a>, and is in charge of adding intelligence to traditional farm vehicles. But he does have a little perspective, having spent time working on software for both medical devices and air traffic control systems.</p> \n<p>I met with Sanchez and <a href=\"https://www.linkedin.com/in/arostaps/\">Alexey Rostapshov</a>, head of digital innovation at John Deere Labs, at the organization’s San Francisco offices last month. Labs launched in 2017 to take advantage of the area’s tech expertise, both to apply machine learning to in-house agricultural problems and to work with partners to build technologies that play nicely with Deere’s big green machines. Deere’s neighbors in San Francisco’s tech-heavy South of Market are <a href=\"https://www.linkedin.com/feed/\">LinkedIn</a>, <a href=\"https://www.salesforce.com/\">Salesforce</a>, and <a href=\"https://www.planet.com/\">Planet Labs</a>, which puts it in a good position for recruiting.</p> \n<!--nextpage--> \n<p>“We’ve literally had folks knock on the door and say, ‘What are you doing here?’” says Rostapshov, and some return to drop off resumes.</p> \n<p>Here’s why Sanchez believes agriculture is such a big challenge for artificial intelligence.</p> \n<p>“It’s not just about driving tractors around,” he says, although autonomous driving technologies are part of&nbsp;the mix. (John&nbsp;Deere is doing a lot of work with precision GPS to improve autonomous driving, for example, and allow&nbsp;tractors to plan their own routes around fields.)&nbsp;</p> \n<p>But more complex than the driving problem, says Sanchez, are the classification problems.</p> \n<p><strong>Corn: A Classic Classification Problem</strong></p> \n<p></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"corn \" src=\"https://spectrum.ieee.org/image/MzM4Nzg3Mw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Tekla Perry \n  </figcaption> </img>\n</figure> \n<p>One key effort, Sanchez&nbsp;says, are AI systems “that allow me to tell whether grain being harvested is good quality or low quality and to make automatic adjustment systems for the harvester.” The company <a href=\"https://www.deere.com/en/our-company/news-and-announcements/news-releases/2019/agriculture/2019may29-deere-enhances-my20-s700-series-combines/\">is already selling</a> an early version of this image analysis&nbsp;technology. But the many differences between grain types, and grains grown under different conditions, make this task a tough one for machine learning.</p> \n<p>“Take corn,” Sanchez says. “Let’s say we are building a deep learning algorithm to detect this corn. And we take lots of pictures of kernels to give it. Say we pick those kernels in central Illinois. But, one mile over, the farmer planted a slightly different hybrid which has slightly different coloration of yellow. Meanwhile, this other farm harvested three days later in a field five miles away; it’s the same hybrid, but it also looks different.</p> \n<p>“It’s an overwhelming classification challenge, and that’s just for corn. But you are not only doing it for corn, you have to add 20 more varieties of grain to the mix; and some, like canola, are almost microscopic.”</p> \n<p>Even the ground conditions vary dramatically—far more than road conditions, Sanchez points out.</p> \n<p>“Let’s say we are building a deep learning algorithm to detect how much residue is left on the soil after a harvest, including stubble and some chaff. Let’s drive 2,000 acres of fields in the Midwest looking at residue. That’s great, but I guarantee that if you go drive those the next year, it will look significantly different.</p> \n<p>“Deep learning is great at&nbsp;interpolating conditions between what it knows; it is not good at extrapolating to situations it hasn’t seen. And in agriculture, you always feel that there is a set of conditions that you haven’t yet classified.”</p> \n<p></p> \n<p><strong>A Flood of Big Data</strong></p> \n<p></p> \n<p>The scale of the data is also daunting, Rostapshov points out. “We are one of the largest users of cloud computing services in the world,” he says. “We are gathering 5 to 15 million measurements per second from 130,000 connected machines globally. We have over 150 million acres in our databases, using petabytes and petabytes [of storage]. We process more data than Twitter does.”</p> \n<p>Much of this information is so-called dirty data, that is, it doesn’t share&nbsp;the same format or structure, because it’s coming not only from a wide variety of John Deere machines, but also includes data from some 100 other companies that have access to the platform, including weather information, aerial imagery, and soil analyses.</p> \n<p>As a result, says Sanchez, Deere has had to make “tremendous investments in back-end data cleanup.”</p> \n<aside class=\"inlay pullquote lt med\">\n  Deep learning is great at interpolating conditions between what it knows; it is not good at extrapolating to situations it hasn’t seen.” \n <span class=\"pq-attrib\">—Julian Sanchez, John Deere</span> \n</aside> \n<p>“We have gotten progressively more skilled at that problem,” he says. “We started simply by cleaning up our own data. You’d think it would be nice and neat, since it’s coming from our own machines, but there is a wide variety of different models and different years. Then we started geospatially tagging the agronomic data—the information about where you are applying herbicides and fertilizer and the like—coming in from our vehicles. When we started bringing in other data, from drones, say, we were already good at cleaning it up.”</p> \n<p></p> \n<p></p> \n<p><strong>John Deere’s Hiring Pitch&nbsp; </strong></p> \n<p></p> \n<p>Hard problems can be a good thing&nbsp;to have for a company looking to hire machine learning engineers.</p> \n<p></p> \n<p>“Our opening line to potential recruits,” Sanchez says, “is ‘This stuff matters.’&nbsp;Then, if we get a chance to talk to them more, we follow up with ‘Not only does this stuff matter, but the problems are really hard and interesting.’ When we explain the variability in farming and how we have to apply all the latest tools to these problems, we get their attention.”</p> \n<p>Software engineers “know that feeding a growing population is a massive problem and are excited about the prospect of making a difference,” Rostapshov says.</p> \n<p></p> \n<p>Only 20 engineers work in the San Francisco labs right now, and that’s on a busy day—some of the researchers spend part of their time&nbsp;at <a href=\"http://www.bluerivertechnology.com/\">Blue River Technology</a>, a startup based in Sunnyvale that was acquired by Deere in 2017. About half of the researchers are focusing on AI. The Lab is&nbsp;in the process of doubling its office space (no word on staffing plans for that expansion yet).</p> \n<aside class=\"inlay pullquote rt med\">\n  “We are one of the largest users of cloud computing services in the world.” \n <span class=\"pq-attrib\">—Alexey Rostapshov, John Deere Labs</span> \n</aside> \n<p>Company-wide, Deere has thousands of software engineers, with many using AI and machine learning tools in their work, and about the same number of mechanical and electrical engineers, Sanchez reports. “If you look at our hiring 10 years ago,” he says, “it was heavily weighted to mechanical engineers. But if you look at those numbers now, it is by a large majority [engineers working] in the software space. We still need mechanical engineers—we do build green machines—but if you go by our footprint of tech talent, it is pretty safe to call John Deere a software company. And if you follow the key conversations that are happening in the company right now, 95 percent of them are software-related.”</p> \n<p>For now, these software engineers are focused on developing technologies that allow farmers to “do more with less,” Sanchez says. Meaning,&nbsp;to get more and better crops from less fuel, less seed, less fertilizer, less pesticide, and fewer workers, and putting together building blocks that, he says, could eventually lead to fully autonomous farm vehicles. The data Deere collects today, for the most part, stays in silos (the virtual kind), with AI algorithms that analyze specific sets of data to provide guidance to individual farmers. At some point, however, with tools to anonymize data&nbsp;and buy-in from farmers, aggregating data could provide some powerful insights.</p> \n<p>“We are not asking farmers for that yet,” Sanchez says. “We are not doing aggregation to look for patterns. We are focused on offering technology that allows an individual farmer to use less, on positioning ourselves to be in a neutral spot. We are not about selling you more seed or more fertilizer. So we are building up a good trust level. In the long term, we can have conversations about doing more with deep learning.”</p> \n<p></p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=dR0py1h-AZM:ZBThNqRB3fg:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=dR0py1h-AZM:ZBThNqRB3fg:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=dR0py1h-AZM:ZBThNqRB3fg:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=dR0py1h-AZM:ZBThNqRB3fg:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=dR0py1h-AZM:ZBThNqRB3fg:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=dR0py1h-AZM:ZBThNqRB3fg:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/dR0py1h-AZM\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 04 Oct 2019 14:52:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4NzgzMg.jpeg","linkMd5":"5f2d11cc893fe393f2105cffb5ccf143","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn72@2020_1/2020/08/25/10-38-00-217_de117fd7fec19bf4.webp","destWidth":1240,"destHeight":930,"sourceBytes":263572,"destBytes":152398,"author":"Tekla S. Perry","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM4Nzg3Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn20@2020_2/2020/08/25/10-38-03-813_e58a5435a92fb223.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=dR0py1h-AZM:ZBThNqRB3fg:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn96@2020_4/2020/08/25/10-38-06-928_7f61e53813db3ee0.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=dR0py1h-AZM:ZBThNqRB3fg:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn51@2020_2/2020/08/25/10-38-03-186_d991dce8b784a7bc.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/dR0py1h-AZM":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn6@2020_1/2020/08/25/10-38-17-526_bdeff458413ec2b9.webp"},"publishedOrCreatedDate":1598351859730},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Zipline Deploys Medical Delivery Drones with U.S. Military","link":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","description":"<p>We usually don’t toss around the word “disrupting” in a technology context without some serious eye roll.&nbsp;But <a href=\"https://flyzipline.com/\">Zipline</a> really has been disrupting medical supply delivery in Africa by using drones to bypass busy roads and hilly terrain to deliver medical supplies to hospitals and clinics in minutes rather than hours. <a href=\"https://spectrum.ieee.org/robotics/drones/in-the-air-with-ziplines-medical-delivery-drones\">We visited Zipline in Rwanda last year</a>, and the system it has for delivering blood, blood products, and medication is versatile, reliable, and even (in some cases) more affordable than any other delivery method available.&nbsp;</p> \n<p>It’s not at all surprising that the unique capabilities&nbsp;Zipline offers have caught the attention of the U.S. military, which (at least in terms of personnel ratios) is <a href=\"https://www.armyupress.army.mil/Portals/7/combat-studies-institute/csi-books/mcgrath_op23.pdf\">primarily a massive logistics and support organization</a> and secondarily a fighting force. For the past year or so, the Defense Department’s <a href=\"https://www.diu.mil/\">Defense Innovation Unit</a> (DIU) has been working with Zipline to evaluate how their technology could be used to help the U.S. Marines Corps. In July, Zipline deployed to Australia to participate in a joint military exercise to demonstrate “how its instant drone delivery capability could help save lives in austere and tactical emergency environments, which include&nbsp;live-fire artillery.”</p> \n<!--nextpage--> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Image of the Zipline team\" src=\"https://spectrum.ieee.org/image/MzM5NjQ5Mg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Zipline \n  </figcaption> \n  <figcaption>\n    The Zipline team \n  </figcaption> </img> \n</figure> \n<p>Here’s what Zipline managed to pull off while operating from a single fulfillment center it built in just a week as part of the exercise:</p> \n<ul> \n <li>461 sorties and 381 deliveries, many in zones with active artillery fire</li> \n <li>An increase of max payload&nbsp;to 2.044 kilograms</li> \n <li>Delivered 24 units of mock blood in under 1.5 hours&nbsp;to a notional mass casualty scenario</li> \n <li>Delivered 68 kg (150 lbs.) of product in under 3 hrs; and 50 kg (110 lbs.) of mock blood and water in under 4 hrs</li> \n <li>57 deliveries to 5 different locations in 6.3 hrs</li> \n <li>Night-time deliveries to units doing night operations</li> \n <li>Endurance flight deliveries with flight times over 80 minutes</li> \n</ul> \n<p>All of this grows directly out of Zipline’s existing medical delivery system in Africa. So, delivering at night, covering long distances, reaching multiple locations, and coordinating the movements of multiple drones in the air at once are all familiar challenges. Flying under artillery fire is probably new, I would guess, and we asked Zipline CEO Keller Rinaudo what else was different about working in a military context:</p> \n<blockquote> \n <p><em>We were operating in complex military airspace—with Ospreys flying overhead and live artillery fire. And it was in Australia, which has one of the most complex and regulated airspaces in the world.</em></p> \n <p><em>Under normal operating circumstances, we generate flight paths based on recommended keep-out zones from civilian ATC and then provide them with flight visibility and tracking ability. In order to operate during the multinational forces exercise in Australia, Zipline had to share the same space as other military air assets and around ground unit movements. We created flight paths to avoid military aircraft, holding areas, battle positions and helicopter landing zones. We selected [delivery] sites that supported ground units, and we carefully deconflicted with Air and Artillery officers before operations began, which included carefully calculating the exact location under the trace of fire where a Zip could fly.&nbsp;</em></p> \n <p><em>The DoD wanted to know if we could rapidly forward deploy, integrate with ground forces, deconflict with military aircraft and our own in real time, operate in a range of conditions, and demonstrate the capacity to swarm aid in mass casualty events. We did all those things.</em></p> \n</blockquote> \n<p>Essentially, Zipline is solving the exact same problem for the military that it’s solving for communities in Africa, says Andrew Musto, a program manager at the military’s DIU. “Right now, [medical] supplies are moved via helicopter or ground convoy. Both of these methods expose their crews and vehicles to risk, can be limited in their availability, and take time to plan and prepare for.” The military doesn’t really have a good solution for the problem of, “I need blood resupply 10 miles from here in a combat zone within the next 20 minutes,” and that’s what Zipline offers.</p> \n<figure class=\"rt med\" role=\"img\"> \n <img alt=\"Image of the Zipline package before deployment. \" src=\"https://spectrum.ieee.org/image/MzM5NjUwMg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Zipline \n  </figcaption> \n  <figcaption>\n    Boxed up medical supplies before deployment via drone.&nbsp; \n  </figcaption> </img> \n</figure> \n<p>Rinaudo says that he hopes to increase the work that Zipline does with the military by an order of magnitude next year. He&nbsp;points&nbsp;out that Zipline would be very valuable in many of the humanitarian and disaster relief contexts in which the military deploys, as opposed to just in combat.&nbsp;</p> \n<p>While Zipline has been focusing almost exclusively on medical deliveries, we should point out that its system is not fundamentally about delivering medical supplies. Despite the application for which it it’s best known, it’s&nbsp;a logistics company that offers very fast delivery of small items by drone.</p> \n<p>Medical supplies are perhaps the most obvious cargo that’s small enough, valuable enough, and urgently needed, but they could be delivering anything that’s small enough to fit in a Zip’s cargo compartment. It’s probably naive to think that the military isn’t interested in Zips as more of a general-purpose logistics system that could also carry other things, which could include batteries, spare parts, or cargoes that are somewhat less benign. I’m not suggesting that there’s necessarily anything wrong with this, and frankly, if Zipline doesn’t end up doing it, someone else (whether it’s another private company or the military itself) almost certainly will.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=mMsm0tCvg3c:6NEoQDAwefA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=mMsm0tCvg3c:6NEoQDAwefA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=mMsm0tCvg3c:6NEoQDAwefA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=mMsm0tCvg3c:6NEoQDAwefA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=mMsm0tCvg3c:6NEoQDAwefA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=mMsm0tCvg3c:6NEoQDAwefA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/mMsm0tCvg3c\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 22 Oct 2019 15:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NjQ0Nw.jpeg","linkMd5":"50bd446780e290d101b8680d1908ab28","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn97@2020_3/2020/08/25/10-38-01-127_febe66035ffb225f.webp","destWidth":1240,"destHeight":930,"sourceBytes":593587,"destBytes":26086,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5NjQ5Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn95@2020_4/2020/08/25/10-38-06-170_e91eca728e214375.webp","https://spectrum.ieee.org/image/MzM5NjUwMg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn72@2020_2/2020/08/25/10-38-09-268_1c4c760479549201.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=mMsm0tCvg3c:6NEoQDAwefA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn36@2020_5/2020/08/25/10-38-04-722_3623ed247c6282ab.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=mMsm0tCvg3c:6NEoQDAwefA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn66@2020_2/2020/08/25/10-38-12-504_24716e5eb026b3f7.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/mMsm0tCvg3c":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn7@2020_2/2020/08/25/10-38-09-216_b20b5ecdbc264217.webp"},"publishedOrCreatedDate":1598351859727},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Stochastic Robots Use Randomness to Achieve More Complex Goals","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/georgia-tech-smarticles-stochastic-robot-swarm","description":"<p>The idea behind swarm robots is to replace discrete, expensive, breakable uni-tasking components with a whole bunch of much simpler, cheaper, and replaceable robots that can work together to do the same sorts of tasks. Unfortunately, all of those swarm robots end up needing their own computing and communications and stuff if you want to get them to do what you want them to do.&nbsp;</p> \n<p>A different approach to swarm robotics is to use a swarm of much cheaper robots that are far less intelligent. In fact, they may not have to be intelligent at all, if you can rely on their physical characteristics to drive them instead. These swarms are “stochastic,” meaning that their motions are randomly determined, but if you’re clever and careful, you can still get them to do specific things.&nbsp; &nbsp;</p> \n<p>Georgia Tech has developed some little swarm robots called “smarticles” that can’t really do much at all on their own, but once you put them together into a jumble, their randomness can actually accomplish something.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GJyr2FlMF_E?rel=0\" width=\"620\"></iframe></p> \n<p>Honestly, calling these particle robots “smart” might be giving them a bit too much credit, because they’re actually kind of dumb and strictly speaking not capable of all that much on their own. A single smarticle weighs 35 grams, and consists of some little 3D-printed flappy bits attached to servos, plus an Arduino Pro Mini, a battery, and a light or sound sensor. When its little flappy bits are activated, each smarticle can move slightly, but a single one mostly just moves around in a square and then will gradually drift in a mostly random direction over time.</p> \n<p>It gets more interesting when you throw a whole bunch of smarticles into a constrained area. A small collection of five or 10 smarticles constrained together form a “supersmarticle,” but besides being in close proximity to one another, the smarticles within the supersmarticle aren’t communicating or anything like that. As far as each smarticle is concerned, they’re independent, but weirdly, a bumble of them can work together without working together.</p> \n<blockquote> \n <p><em>“These are very rudimentary robots whose behavior is dominated by mechanics and the laws of physics,” said Dan Goldman, a Dunn Family Professor in the School of Physics at the Georgia Institute of Technology.</em></p> \n <p><em>The researchers noticed that if one small robot stopped moving, perhaps because its battery died, the group of smarticles would begin moving in the direction of that stalled robot. Graduate student Ross Warkentin learned he could control the movement by adding photo sensors to the robots that halt the arm flapping when a strong beam of light hits one of them.</em></p> \n <p><em>“If you angle the flashlight just right, you can highlight the robot you want to be inactive, and that causes the ring to lurch toward or away from it, even though no robots are programmed to move toward the light,” Goldman said. “That allowed steering of the ensemble in a very rudimentary, stochastic way.”</em></p> \n</blockquote> \n<p>It turns out that it’s possible to model this behavior, and control a supersmarticle with enough fidelity to steer it through a maze. And while these particular smarticles aren’t all that small, strictly speaking, the idea is to develop techniques that will work when robots are scaled way way down to the point where you can't physically fit useful computing in there at all.&nbsp;</p> \n<p>The researchers are also working on some other concepts, like these:</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Stochastic robots\" src=\"https://spectrum.ieee.org/image/MzM3Njc2OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Science Robotics \n  </figcaption> \n  <figcaption>\n    The Georgia Tech researchers envision stochastic robot swarms that don’t have a perfectly defined shape or delineation but are capable of self-propulsion, relying on the ensemble-level behaviors that lead to collective locomotion. In such a robot, the researchers say, groups of largely generic agents may be able to achieve complex goals, as observed in biological collectives. \n  </figcaption> </img>\n</figure> \n<p>Er, yeah. I’m…not sure I really <em>want</em> there to be a bipedal humanoid robot built out of a bunch of tiny robots. Like, that seems creepy somehow, you know? I’m totally okay with slugs, but let’s not get crazy.&nbsp;</p> \n<h4>“A robot made of robots: Emergent transport and control of a smarticle ensemble, by William Savoie, Thomas A. Berrueta, Zachary Jackson, Ana Pervan, Ross Warkentin, Shengkai Li, Todd D. Murphey, Kurt Wiesenfeld, and Daniel I. Goldman” from the Georgia Institute of Technology, appears in the current issue of <em>Science Robotics</em>.</h4>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5urTYVBa2Lw:VJf9q2xFxfM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5urTYVBa2Lw:VJf9q2xFxfM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5urTYVBa2Lw:VJf9q2xFxfM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5urTYVBa2Lw:VJf9q2xFxfM:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=5urTYVBa2Lw:VJf9q2xFxfM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5urTYVBa2Lw:VJf9q2xFxfM:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/5urTYVBa2Lw\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 18 Sep 2019 18:25:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3NjczOA.jpeg","linkMd5":"3579544e8e9768028a4476e076f6e099","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn83@2020_3/2020/08/25/10-38-00-292_b36f4e23edcb5c83.webp","destWidth":1240,"destHeight":827,"sourceBytes":207546,"destBytes":75038,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3Njc2OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn3@2020_2/2020/08/25/10-38-06-845_6d2f003f94f36adf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5urTYVBa2Lw:VJf9q2xFxfM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn52@2020_2/2020/08/25/10-38-09-523_5c02ad0d50b26af3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5urTYVBa2Lw:VJf9q2xFxfM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn15@2020_6/2020/08/25/10-38-05-161_9eafc6539bf586ee.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/5urTYVBa2Lw":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn40@2020_2/2020/08/25/10-38-08-490_e42d6f9dfe0c6706.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"This \"Useless\" Social Robot Wants to Succeed Where Others Failed","link":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","description":"<p>The recent <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures\">high-profile failures</a> of <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/consumer-robotics-company-anki-abruptly-shuts-down\">some home</a> <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/mayfield-robotics-cancels-kuri-social-home-robot\">social</a> <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/jibo-is-probably-totally-dead-now\">robots</a> (<a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robotic-dreams-robotic-realities\">and the companies behind them</a>) have made it even more challenging than it was before to develop robots in that space. And it was challenging enough to begin with: Making a robot that can autonomously interact with random humans in their homes over a long period of time for a price that people can afford is extraordinarily difficult. However, the massive amount of initial interest in robots like <a href=\"https://robots.ieee.org/robots/jibo/?utm_source=spectrum\">Jibo</a>, <a href=\"https://robots.ieee.org/robots/kuri\\/?utm_source=spectrum\">Kuri</a>, <a href=\"https://robots.ieee.org/robots/vector/?utm_source=spectrum\">Vector</a>, and <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/buddy-the-social-robot-is-somehow-not-dead-yet\">Buddy</a> prove that people do want these things, or at least think they do, and while that’s the case, there’s incentive for other companies to give social home robots a try.</p> \n<p>One of those companies is <a href=\"https://www.kiki.ai/\">Zoetic</a>, founded in 2107 by <a href=\"https://twitter.com/mita\">Mita Yun</a> and Jitu Das, both ex-Googlers. Their robot, <a href=\"https://www.kiki.ai/\">Kiki</a>, is more or less exactly what you’d expect from a social home robot: It’s <a href=\"https://spectrum.ieee.org/tech-talk/robotics/home-robots/ces-2017-why-every-social-robot-at-ces-looks-alike\">cute, white, roundish</a>, has big eyes,&nbsp;promises that it will be your “robot sidekick,” and is not cheap: It’s&nbsp;<a href=\"https://www.kickstarter.com/projects/zoeticai/kiki/description\">on Kickstarter for US $800</a>. Kiki is among what appears to be a sort of tentative second wave of social home robots, where designers have (presumably) had a chance to take <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures\">everything that they learned from the social home robot pioneers</a> and use it to make things better this time around.</p> \n<!--nextpage--> \n<p>Kiki’s Kickstarter video is, again, more or less exactly what you’d expect from a social home robot crowdfunding campaign:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/CreyjtMbOzg?rel=0\" width=\"620\"></iframe></p> \n<p>We won’t get into all of the details on Kiki in this article (the Kickstarter page has tons of information), but a few distinguishing features:</p> \n<ul> \n <li>Each Kiki will develop its own personality over time through its daily interactions with its owner, other people, and other Kikis.</li> \n <li>Interacting with Kiki is more abstract than with most robots—it can understand some specific words and phrases, and will occasionally use a few specific words or two, but otherwise it’s mostly listening to your tone of voice and responding with sounds rather than speech.</li> \n <li>Kiki doesn’t move on its own, but it can operate for up to two hours away from its charging dock.</li> \n <li>Depending on how you&nbsp;treat Kiki, it can get depressed or neurotic. It also needs to be fed, which you can do by drawing different kinds of food in the app.</li> \n <li>Everything Kiki does runs on board the robot. It has Wi-Fi connectivity for updates, but it doesn’t rely on the cloud for anything in real time, meaning that your data stays on the robot and that the robot will continue to function even if its remote service shuts down.</li> \n</ul> \n<hr> \n <p>It’s hard to say whether features like these are unique enough to help Kiki be successful where other social home robots haven’t been, so we spoke with Zoetic cofounder <a href=\"https://twitter.com/mita\">Mita Yun</a> and asked her why she believes that Kiki is going to be the social home robot that makes it.</p> \n <p><strong><em>IEEE Spectrum: </em>What’s your background?</strong></p> \n <p><strong>Mita Yun:</strong> <em>I was an only child growing up, and so I always wanted something like Doraemon or Totoro. Something that when you come home it’s there to greet you, not just because it’s programmed to do that but because it’s actually actively happy to see you, and only you. I was so interested in this that I went to study robotics at CMU, and then after I graduated I joined Google and worked there for five years. I tended to go for the more risky and more fun projects, but they always got canceled—the first project I joined was called Android at Home, and then I joined Google Glass, and then I joined a team called Robots for Kids. That project was building educational robots, and then I just realized that when we’re adding technology to something, to a product, we’re actually taking the life away somehow, and the kids were more connected with stuffed animals compared to the educational robots we were building. That project was also canceled, and in 2017, I left with a coworker of mine (Jitu Das) to bring this dream into reality. And now we’re building Kiki.</em></p> \n <aside class=\"inlay pullquote xlrg\">\n   “Jibo was Alexa plus cuteness equals $800, and I feel like that equation doesn’t work for most people, and that eventually killed the company. So, for Kiki, we are actually building something very different. We’re building something that’s completely useless.” \n  <span class=\"pq-attrib\">—Mita Yun, Zoetic</span> \n </aside> \n <p><strong>You started working on Kiki in 2017, when things were already getting challenging for <a href=\"https://robots.ieee.org/robots/jibo/?utm_source=spectrum\">Jibo</a>. Why did you decide to start developing a social home robot at that point?</strong></p> \n <p><em>I thought Jibo was great. It had a special, magical way of moving, and it was such a new idea that you could have this robot with embodiment and it can actually be your assistant. The problem with Jibo, in my opinion, was that it took too long to fulfill the orders. It took them three to four years to actually manufacture, because it was a very complex piece of hardware, and then during that period of time Alexa and Google Home came out, and they started selling these voice systems for $30 and then you have Jibo for $800. Jibo was Alexa plus cuteness equals $800, and I feel like that equation doesn’t work for most people, and that eventually killed the company. So, for Kiki, we are actually building something very different. We’re building something that’s completely useless.&nbsp;</em></p> \n <p><strong>Can you elaborate on “completely useless”?</strong></p> \n <p><em>I feel like people are initially connected with robots because they remind them of a character. And it’s the closest we can get to a character other than an organic character like an animal. So we’re connected to a character like when we have a robot in a mall that’s roaming around—even if it looks really ugly, like if it doesn’t have eyes, people still take selfies with it. Why? Because they think it’s a character. And humans are just hardwired to love characters and love stories. With Kiki, we just wanted to build a character that’s alive, we don’t want to have a character do anything super useful.&nbsp;</em></p> \n <p><em>I understand why other robotics companies are adding Alexa integration to their robots, and I think that’s great. But the dream I had, and the understanding I have about robotics technology, is that for a consumer robot especially, it is very very difficult for the robot to justify its price through usefulness. And then there’s also research showing that the more useless something is, the easier it is to have an emotional connection, so that’s why we want to keep Kiki very useless.&nbsp;</em></p> \n <p><strong>What kind of character are you creating with Kiki?</strong></p> \n <p><em>The whole design principle around Kiki is we want to make it a very vulnerable character. In terms of its status at home, it’s not going to be higher or equal status as the owner, but slightly lower status than the human, and it’s vulnerable and needs you to take care of it in order to grow up into a good personality robot.</em></p> \n <p><em>We don’t let Kiki speak full English sentences, because whenever it does that, people are going to think it’s at least as intelligent as a baby, which is impossible for robots at this point. And we also don’t let it move around, because when you have it move around, people are going to think, “I’m going to call Kiki’s name, and then Kiki&nbsp;will come to me.” But that is actually very difficult to build. And then also we don’t have any voice integration, so it doesn’t tell you about the stock market price and so on.</em></p> \n <figure class=\"xlrg\" role=\"img\"> \n  <img alt=\"Kiki social robot\" src=\"https://spectrum.ieee.org/image/MzM3Nzk4Mw.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Zoetic \n   </figcaption> \n   <figcaption>\n     Kiki is designed to be&nbsp;“vulnerable,” and it&nbsp;needs you to take care of it so it can “grow up into a good personality robot,” according to its creators. \n   </figcaption> </img> \n </figure> \n <p><strong>That sounds similar to what Mayfield did with Kuri, emphasizing an emotional connection rather than specific functionality.</strong></p> \n <p><em>It is very similar, but one of the key differences from <a href=\"https://robots.ieee.org/robots/kuri/?utm_source=spectrum\">Kuri</a>, I think, is that Kuri started with a Kobuki base, and then it’s wrapped into a cute shell, and they added sounds. So Kuri started with utility in mind—navigation is an important part of Kuri, so they started with that challenge. For Kiki, we started with the eyes. The entire thing started with the character itself.</em></p> \n <p><strong>How will you be able to convince your customers to spend $800 on a robot that you’ve described as “useless” in some ways?</strong></p> \n <p><em>Because it’s useless, it’s actually easier to convince people, because it provides you with an emotional connection. I think Kiki is not a utility-driven product, so the adoption cycle is different. For a functional product, it’s very easy to pick up, because you can justify it by saying “I’m going to pay this much and then my life can become this much more efficient.” But it’s also very easy to be replaced and forgotten. For an emotionally driven product, it’s slower to pick up, but once people actually pick it up, they’re going to be hooked—they get connected with it, and they’re willing to invest more into taking care of the robot so it will grow up to be smarter.&nbsp;</em></p> \n <p><strong>Maintaining value over time has been another challenge for social home robots. How will you make sure that people don’t get bored with Kiki after a few weeks?</strong></p> \n <p><em>Of course Kiki has limits in what it can do. We can combine the eyes, the facial expression, the motors, and lights and sounds, but is it going to be constantly entertaining? So we think of this as, imagine if a human is actually puppeteering Kiki—can Kiki stay interesting if a human is puppeteering it and interacting with the owner?&nbsp;So I think what makes a robot interesting is not just in the physical expressions, but the part in between that and the robot conveying its intentions and emotions.</em></p> \n <p><em>For example, if you come into the room and then Kiki decides it will turn in the other direction, ignore you, and then you feel like, huh, why did the robot do that to me? Did I do something wrong? And then maybe you will come up to it and you will try to figure out why it did that. So, even though Kiki can only express&nbsp;four different dimensions, it can still make things very interesting, and then when its strategies change, it makes it feel like a new experience.</em></p> \n <p><em>There’s also an explore and exploit process going on. Kiki wants to make you smile, and it will try different things. It could try to chase its tail, and if you smile, Kiki learns that this works and will exploit it. But maybe after doing it three times, you no longer find it funny, because you’re bored of it, and then Kiki will observe your reactions and be motivated to explore a new strategy.</em></p> \n <figure class=\"xlrg\" role=\"img\"> \n  <img alt=\"Kiki social robot\" src=\"https://spectrum.ieee.org/image/MzM3NzkyNw.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Zoetic \n   </figcaption> \n   <figcaption>\n     Kiki’s creators are hoping that, with an emotionally engaging robot, it will be easier for people to get attached to it and willing to spend time taking care of it. \n   </figcaption> </img> \n </figure> \n <p><strong>A particular risk with crowdfunding a robot like this is setting expectations unreasonably high. The emphasis on personality and emotional engagement with Kiki seems like it may be very difficult for the robot to live up to in practice.</strong></p> \n <p><em>I think we invested more than most robotics companies into really building out Kiki’s personality, because that is the single most important thing to us. For <a href=\"https://robots.ieee.org/robots/jibo/?utm_source=spectrum\">Jibo</a> a lot of the focus was in the assistant, and for <a href=\"https://robots.ieee.org/robots/kuri/?utm_source=spectrum\">Kuri</a>, it’s more in the movement. For Kiki, it’s very much in the personality.&nbsp;</em></p> \n <p><em>I feel like when most people talk about personality, they’re mainly talking about expression. With Kiki, it’s not just in the expression itself, not just in the voice or the eyes or the output layer, it’s in the layer in between—when Kiki receives input, how will it make decisions about what to do? We actually don’t think the personality of Kiki is categorizable, which is why I feel like Kiki has a deeper implementation of how personalities should work. And you’re right, Kiki doesn’t really understand why you’re feeling a certain way, it just reads your facial expressions. It’s maybe not your best friend, but maybe closer to your little guinea pig robot.&nbsp;</em></p> \n <figure class=\"rt med-lrg\" role=\"img\"> \n  <img alt=\"Kiki social robot\" src=\"https://spectrum.ieee.org/image/MzM3NzkyNg.png\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Zoetic \n   </figcaption> \n   <figcaption>\n     The team behind Kiki paid particular attention to its eyes, and&nbsp;designed the robot to always face the person that it is interacting with. \n   </figcaption> </img> \n </figure> \n <p><strong>Is that where you’d put Kiki on the scale of human to pet?</strong></p> \n <p><em>Kiki is definitely not human;&nbsp;we want to keep it very far away from human. And it’s also not a dog or cat. When we were designing Kiki, we took inspiration from mammals because humans are deeply connected to mammals since we’re mammals ourselves. And specifically we’re connected to predator animals. With prey animals, their eyes are usually on the sides of their heads, because they need to see different angles. A predator animal needs to hunt, they need to focus. Cats and dogs are predator animals. So with Kiki, that’s why we made sure the eyes are on one side of the face and the head can actuate independently from the body and the body can turn so it’s always facing the person that it’s paying attention to.</em></p> \n <p><em>I feel like Kiki probably does more than a plant. It does more than a fish, because a fish doesn’t look you in the eyes. It’s not as smart as a cat or a dog, so I would just put it in this guinea pig kind of category.</em></p> \n <p><strong>What have you found so far when running user studies with Kiki?</strong></p> \n <p><em>When we were first designing Kiki we went through a whole series of prototypes. One of the earlier prototypes of Kiki looked like a CRT, like a very old monitor, and when we were testing that with people they didn’t even want to touch it. Kiki’s design inspiration actually came from an airplane, with a very angular, futuristic look, but based on user feedback we made it more round and more friendly to the touch. The lights were another feature request from the users, which adds another layer of expressivity to Kiki, and they wanted to see multiple Kikis working together with different personalities. Users also wanted different looks for Kiki, to make it look like a deer or a unicorn, for example, and we actually did take that into consideration because it doesn’t look like any particular mammal. In the future, you’ll be able to have different ears to make it look like completely different animals.</em></p> \n <p><em>There has been a lot of user feedback that we didn’t implement—I believe we should observe the users reactions and feedback but not listen to their advice. The users shouldn’t be our product designers, because if you test Kiki with 10 users, eight of them will tell you they want Alexa in it. But we’re never going to add Alexa integration to Kiki because that’s not what it’s meant to do.&nbsp;</em></p> \n <hr> \n  <p>While it’s far too early to tell whether Kiki will be a long-term success, the Kickstarter campaign is currently over 95 percent funded with 8 days&nbsp;to go, and 34 robots are still available for a May 2020 delivery.</p> \n  <p>[ <a href=\"https://www.kickstarter.com/projects/zoeticai/kiki\">Kickstarter</a> ]</p> \n  <div class=\"feedflare\"> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=X-v6WYhBbOk:6bxLlz5K92Q:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=X-v6WYhBbOk:6bxLlz5K92Q:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=X-v6WYhBbOk:6bxLlz5K92Q:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=X-v6WYhBbOk:6bxLlz5K92Q:-BTjWOF_DHI\" border=\"0\" /></a> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=X-v6WYhBbOk:6bxLlz5K92Q:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=X-v6WYhBbOk:6bxLlz5K92Q:gIN9vFwOqvQ\" border=\"0\" /></a> \n  </div> \n  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/X-v6WYhBbOk\" height=\"1\" width=\"1\" alt=\"\" /> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Thu, 19 Sep 2019 20:45:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3Nzg3NQ.jpeg","linkMd5":"83dc60de229d0234a169f3b14521767c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn1@2020_6/2020/08/25/10-38-01-220_be87524870fa3a8c.webp","destWidth":1240,"destHeight":680,"sourceBytes":515521,"destBytes":49396,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3Nzk4Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn6@2020_5/2020/08/25/10-38-16-285_0a5b3e836c487bb4.webp","https://spectrum.ieee.org/image/MzM3NzkyNw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn23@2020_3/2020/08/25/10-38-02-920_080c779d14bd616e.webp","https://spectrum.ieee.org/image/MzM3NzkyNg.png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn56@2020_1/2020/08/25/10-38-09-583_c37dc9d9af101bfc.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=X-v6WYhBbOk:6bxLlz5K92Q:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn23@2020_2/2020/08/25/10-38-11-225_156e769a1a9b5269.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=X-v6WYhBbOk:6bxLlz5K92Q:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn21@2020_2/2020/08/25/10-38-06-228_7ba20b8739d6c111.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/X-v6WYhBbOk":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn63@2020_4/2020/08/25/10-38-17-110_2c0ce5b3e116b1a2.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Microsoft's AI Research Draws Controversy Over Possible Disinformation Use","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/microsofts-ai-research-draws-controversy-over-possible-disinformation-use","description":"<p></p> \n<p></p> \n<p>AI capable of&nbsp;automatically posting relevant comments on news articles has raised concerns&nbsp;that the technology could empower online disinformation campaigns designed to influence public opinion and national elections. The AI research&nbsp;in question, conducted by Microsoft Research Asia and Beihang University in China, became the subject of controversy even prior to&nbsp;the paper’s scheduled presentation at a major AI conference this week.</p> \n<!--nextpage--> \n<p></p> \n<p><span>The “DeepCom” AI model developed by&nbsp;the Microsoft and Beihang University team showed that it could effectively mimic human behavior by reading and commenting on news articles written in&nbsp;English and Chinese. But t</span><span>he&nbsp;</span><a href=\"https://arxiv.org/pdf/1909.11974v1.pdf\" style=\"font-family: Georgia, serif; font-size: 18px;\">original paper</a><span>&nbsp;</span><span>uploaded to the arXiv preprint server on 26 September made&nbsp;no mention of ethical issues regarding possible misuse of the technology. The omission sparked&nbsp;a backlash that eventually prompted the research team to upload an&nbsp;<a href=\"https://arxiv.org/pdf/1909.11974.pdf\">updated paper</a>&nbsp;addressing those concerns.&nbsp;</span></p> \n<p>“A paper by Beijing researchers presents a new machine learning technique whose main uses seem to be trolling and disinformation,” wrote&nbsp;<a href=\"http://randomwalker.info/\" style=\"font-family: Georgia, serif; font-size: 18px;\">Arvind Narayanan</a><span>, a computer scientist at the Center for Information Technology Policy at Princeton University, in a <a href=\"https://twitter.com/random_walker/status/1178663474475483137\">Twitter post</a>. “</span>It's been accepted for publication at EMLNP [sic], one of the top 3 venues for Natural Language Processing research. Cool Cool Cool [sic].”</p> \n<p>The Microsoft and Beihang University paper has spurred discussion within the broader research community about whether&nbsp;machine learning researchers should follow stricter guidelines and more openly acknowledge the possible negative implications of certain AI applications.&nbsp;<span>The paper is currently scheduled for presentation at the&nbsp;2019&nbsp;</span><a href=\"https://www.emnlp-ijcnlp2019.org/program/schedule/\" style=\"font-family: Georgia, serif; font-size: 18px;\">Conference on Empirical Methods in Natural Language Processing</a><span>&nbsp;(EMNLP) in Hong Kong on 7 November.</span></p> \n<p>Both Narayanan and <a href=\"http://blog.otoro.net/\">David Ha</a>, a scientist at Google Brain Research, <a href=\"https://twitter.com/hardmaru/status/1178074819956334592?lang=en\">voiced their skepticism</a>&nbsp;of&nbsp;the original paper’s suggestion&nbsp;that&nbsp;<span>“automatic news comment generation is beneficial for real applications but has not attracted enough attention from the research community.” Ha sarcastically asked if there would be a follow-up paper about an AI model called <a href=\"https://twitter.com/hardmaru/status/1178076168894832640\">“DeepTroll” or “DeepWumao”</a>&nbsp;(“Wumao” is the name for Chinese Internet commentators paid by the Chinese Communist Party to help manipulate public opinion by making online comments.)&nbsp;</span></p> \n<p></p> \n<aside class=\"inlay pullquote lt med-lrg\">\n  “I think there's qualitative difference between research on fundamental problems that have the potential for misuse and applications which are specifically suited to, if not designed for, misuse,” \n <span class=\"pq-attrib\">—Alvin Grissom II, Ursinus College</span> \n</aside> \n<a href=\"https://jack-clark.net/\">Jack Clark</a>, a former journalist turned policy director for the OpenAI research organization, gave a more \n<a href=\"https://twitter.com/jackclarkSF/status/1178076882429808640\">blunt rebuttal</a>&nbsp;to the&nbsp;paper’s suggestion: “As a former journalist, I can tell you that this is a lie.” \n<p></p> \n<p><span>Researchers such as&nbsp;</span><a href=\"https://www.ursinus.edu/live/profiles/3125-alvin-grissom-ii\" style=\"font-family: Georgia, serif; font-size: 18px;\">Alvin Grissom II</a><span>, a computer scientist at Ursinus College in Collegeville, Penn., raised questions about what types of AI research deserve to be publicized by prominent research conferences such as EMNLP. “I think there's qualitative difference between research on fundamental problems that have the potential for misuse and applications which are specifically suited to, if not designed for, misuse,” said Grissom in a <a href=\"https://twitter.com/AlvinGrissomII/status/1178759063234064384\">Twitter post</a>.</span></p> \n<p></p> \n<p></p> \n<p>The Microsoft and Beihang University researchers’ <a href=\"https://arxiv.org/pdf/1909.11974.pdf\">updated paper</a>, which acknowledges some of the ethical concerns, was uploaded after Katyanna Quach reported on the controversy&nbsp;for&nbsp;<em><a href=\"https://www.theregister.co.uk/2019/10/02/ai_news_bot/\">The Register</a>. </em>The updated version also removed the original paper’s statement about how “automatic news generation is beneficial for real applications.”</p> \n<p><span>“We are aware of potential ethical issues with application of these methods to generate news commentary that is taken as human,” the researchers wrote in the updated paper’s conclusion.&nbsp;“We hope to stimulate discussion about best practices and controls on these methods around responsible uses of the technology.”</span></p> \n<aside class=\"inlay pullquote rt med-lrg\">\n  “Security conferences these days require submissions to describe ethical considerations and how the authors followed ethical principles. Machine learning conferences should consider doing this.” \n <span class=\"pq-attrib\">—Arvind Narayanan, Princeton University</span> \n</aside> \n<p>The updated paper’s conclusion about possible applications also specifically mentions that the team was “motivated to extend the capabilities of a popular chatbot.” That almost certainly refers to Microsoft’s China-based&nbsp;<a href=\"https://www.microsoft.com/en-us/research/publication/the-design-and-implementation-of-xiaoice-an-empathetic-social-chatbot/\">chatbot named Xiaoice</a>. It&nbsp;has more than 660 million users worldwide and has become a virtual celebrity in China. <a href=\"https://www.microsoft.com/en-us/research/people/wuwei/\">Wei Wu</a>, one of the coauthors on the DeepCom paper, holds the position of&nbsp;principal&nbsp;applied scientist for the Microsoft Xiaoice team at Microsoft Research Asia in Beijing.&nbsp;</p> \n<p>The Microsoft and&nbsp;Beihang University researchers did not provide much additional input&nbsp;when reached for comment. Instead, both Wu and a&nbsp;Microsoft representative referred to the updated version of the paper that acknowledges&nbsp;the ethical issues. But&nbsp;<span>the Microsoft representative was unable to refer&nbsp;</span><em>IEEE Spectrum</em><span>&nbsp;to even one&nbsp;source who could speak about the company’s research review process.</span></p> \n<p>“I'd like to hear from Microsoft if they had any ethical review process in place, and whether they plan to make any changes to their processes in the future in response to the concerns about this paper,” Narayanan wrote in an email to <em>IEEE Spectrum</em>. His prior work includes research on how AI can <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-learns-gender-and-racial-biases-from-language\">learn gender and racial biases</a>&nbsp;from language.</p> \n<p>Microsoft has previously staked out a position for itself as a leader in AI ethics with initiatives such as the company’s&nbsp;<a href=\"https://news.microsoft.com/2018/03/29/satya-nadella-email-to-employees-embracing-our-future-intelligent-cloud-and-intelligent-edge/\">AI and Ethics in Engineering and Research (AETHER)</a> Committee. That committee’s advice has supposedly led Microsoft to <a href=\"https://www.geekwire.com/2018/microsoft-cutting-off-sales-ai-ethics-top-researcher-eric-horvitz-says/\">reject certain sales</a> of its commercialized technology in the past.&nbsp;It’s&nbsp;less clear how much AETHER is involved in screening AI research collaborations prior to the AI application and commercialization stage.</p> \n<p>Meanwhile, Nayaranan and other researchers have also asked questions about the review process for accepting papers at the EMNLP conference being held in Hong Kong. <span>“Security conferences these days require submissions to describe ethical considerations and how the authors followed ethical principles,” Narayanan wrote in a&nbsp;</span><a href=\"https://twitter.com/random_walker/status/1178670853929738240\" style=\"font-family: Georgia, serif; font-size: 18px;\">Twitter post</a><span>.&nbsp;“Machine learning conferences should consider doing this.”</span></p> \n<p>Narayanan urged conference attendees to direct questions at both the paper’s authors and the program chairs for the conference.&nbsp;</p> \n<p>The organizers admit that authors and reviewers were not required to look at the ethical considerations or social impact of the technologies described in the papers that were submitted. But reviewers did flag a handful of papers based on ethical concerns, <span>says Jing Jiang, a computer scientist at Singapore Management University in Singapore who served on&nbsp;the conference</span><span>’s&nbsp; </span><span>organizing committee</span>. The committee decided that the authors of any flagged papers making the cut on technical merits would be asked to address ethical issues in a revised draft and undergo additional review.</p> \n<p>But here’s the rub: The DeepCom paper by Microsoft and Beihang University did not get flagged by any reviewers, says Jiang.&nbsp;Perhaps more formal guidelines for considering ethical issues in papers submitted to future conferences are in order.</p> \n<p><em>This article was updated on 5 November.</em></p> \n<p></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WyumTS8jes0:bDgdbPvpRZA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WyumTS8jes0:bDgdbPvpRZA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WyumTS8jes0:bDgdbPvpRZA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WyumTS8jes0:bDgdbPvpRZA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WyumTS8jes0:bDgdbPvpRZA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WyumTS8jes0:bDgdbPvpRZA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WyumTS8jes0\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 04 Nov 2019 21:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNTMxNg.jpeg","linkMd5":"5e83ca8ac583d87a19ee7990f07737a8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn42@2020_4/2020/08/25/10-38-01-655_9ade2f8a97dcc40d.webp","destWidth":1240,"destHeight":930,"sourceBytes":238780,"destBytes":43780,"author":"Jeremy Hsu","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WyumTS8jes0:bDgdbPvpRZA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn71@2020_1/2020/08/25/10-38-09-727_cde057126deb20d5.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WyumTS8jes0:bDgdbPvpRZA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn27@2020_4/2020/08/25/10-38-03-001_923e24f245237821.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WyumTS8jes0":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn3@2020_5/2020/08/25/10-38-04-292_c776c30bf23cbcec.webp"},"publishedOrCreatedDate":1598351859727},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Quadruped Robots Can Climb Ladders Now","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","description":"<p>When we look at <a href=\"https://spectrum.ieee.org/tag/quadruped+robots\">quadruped&nbsp;robots</a>, it’s impossible not to compare them to quadruped animals like dogs and cats. Over the last several years, such robots have begun to <a href=\"https://spectrum.ieee.org/preview/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale\">approach the capabilities of their biological counterparts</a> under just a few very specific situations, like walking without falling over. Biology provides a gold standard that robots are striving to reach, and it’s going to take us a very long time to make quadrupeds that can do everything that animals can.</p> \n<p>The cool thing about robots, though, is that they don’t have to be constrained by biology, meaning that there’s always the potential for them to learn new behaviors that animals simply aren’t designed for. At <a href=\"https://spectrum.ieee.org/tag/iros+2019\">IROS 2019</a> last&nbsp;week, we saw one such example, with a quadruped robot that’s able to climb vertical ladders.</p> \n<!--nextpage--> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Quadruped robot climbing vertical ladder\" src=\"https://spectrum.ieee.org/image/MzQxMjEyOA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Tokyo Metropolitan University \n  </figcaption> \n  <figcaption>\n    To generate the robot’s autonomous climbing behavior, the researchers used a recurrent neural network&nbsp;that trained it to ascend the ladder. The behavior was created for this specific ladder, but the researchers plan to generalize the system so that the robot can climb new ladders without prior training. \n  </figcaption> </img> \n</figure> \n<p>A casual Google search makes it seems like vertical ladder climbing is quite challenging for biological quadrupeds. Dogs can do it, although usually you see them climbing up ladders that are angled (leaning against something) rather than vertical. Cats are a bit better, but vertical ladders still look like a challenge for them, especially if they can’t use their claws to grip. The problem is as the steepness of a ladder increases to vertical, your center of mass moves farther and farther away from the rungs, and you have to support an increasing amount of your own weight by actively gripping rungs rather than just standing on them, which is a problem for animals that don’t have robust grasping systems.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Japanese quadruped robot\" src=\"https://spectrum.ieee.org/image/MzQxMjEyOQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Tokyo Metropolitan University \n  </figcaption> \n  <figcaption>\n    To climb the ladder, the quadruped robot is equipped with an inertial measurement unit (IMU), time-of-flight 3D camera on its face, and touch and force sensors on each claw. An Intel NUC computer acts as the main control system, with an Arduino used as a secondary controller to manage the input-output signals of the internal sensors (force, touch, and IMU). The robot has 23 degrees of freedom (DoF): 5 DoF in each leg, 2 DoF for the dual laser rangefinder sensors, and 1 DoF for the head. \n  </figcaption> </img> \n</figure> \n<p>Most robotic quadrupeds don’t have robust grasping systems either, but adding such a system to a robot seems like a promising idea to explore. Roboticists at Tokyo Metropolitan University have built a cute little (7 kilograms) quadruped with 5 degrees-of-freedom legs that include a sort of opposable thumb that turn its feet into grippers. It’s able to use those grippers to climb vertical handrail-free ladders fully autonomously.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/k-SdJ1clNMQ\" width=\"620\"></iframe></p> \n<p>That transition from the ladder to the upper surface seems quite tricky to perform, and it’s particularly clever how the robot uses its hind legs to grasp the top rung and use it to propel itself onto&nbsp;the platform.&nbsp;It’s also worth noting that the autonomous system was trained on this specific ladder, and that it took five tries to get it right, although the researchers say that the failures were due to lack of actuator torque rather than their overall approach. They plan to fix this in future work, as well as to generalize the system so that it can climb new ladders without prior training.</p> \n<h4>“A Novel Capability&nbsp;of Quadruped Robot Moving Through Vertical Ladder Without Handrail Support,” by Azhar Aulia Saputra, Yuichiro Toda, Naoyuki Takesue, and Naoyuki Kubota from Tokyo Metropolitan University and Okayama University, was presented at IROS 2019 in Macau.</h4> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RignCTneE9M:v2UFZRjM9PY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RignCTneE9M:v2UFZRjM9PY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RignCTneE9M:v2UFZRjM9PY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RignCTneE9M:v2UFZRjM9PY:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RignCTneE9M:v2UFZRjM9PY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RignCTneE9M:v2UFZRjM9PY:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/RignCTneE9M\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 13 Nov 2019 21:10:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxMjEwOQ.jpeg","linkMd5":"8be8e5cf657dd68e13509f254295323b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn35@2020_6/2020/08/25/10-37-59-877_3cdfdf5f06e20eee.webp","destWidth":1240,"destHeight":938,"sourceBytes":231618,"destBytes":52674,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQxMjEyOA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn13@2020_5/2020/08/25/10-38-16-442_35999cf9baacff97.webp","https://spectrum.ieee.org/image/MzQxMjEyOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn10@2020_2/2020/08/25/10-38-13-749_9ae56a3f256c383e.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RignCTneE9M:v2UFZRjM9PY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn96@2020_2/2020/08/25/10-38-11-101_46dab501fe2c8c72.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RignCTneE9M:v2UFZRjM9PY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx/cdn40@2020_3/2020/08/25/10-38-03-075_9e57bff312e42441.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/RignCTneE9M":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn24@2020_2/2020/08/25/10-38-10-178_493edee4740f344f.webp"},"publishedOrCreatedDate":1598351859720},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: This Robotic Thread Could One Day Travel Inside Your Brain","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>Eight engineering students from ETH Zurich are working on a year-long focus project to develop a multimodal robot called Dipper, which can fly, swim, dive underwater, and manage that difficult air-water transition:</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/q_9tSHTW1xE\" width=\"620\"></iframe></p> \n <p></p> \n <p>The robot uses one motor to selectively drive either a propeller or a marine screw depending on whether it’s in flight or not. We’re told that getting the robot to autonomously do the water to air transition is still a work in progress, but that within a few weeks things should be much smoother.</p> \n <p>[ <a href=\"https://dipper.ethz.ch/index_en.html\">Dipper</a> ]</p> \n <p><em>Thanks Simon!</em></p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Giving a jellyfish a hug without stressing them out is exactly as hard as you think, but Harvard’s robot will make sure that all jellyfish get the emotional (and physical) support that they need.</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/8Z6Z1OoCvxA\" width=\"620\"></iframe></p> \n  <p></p> \n  <blockquote> \n   <p><em>The gripper’s six “fingers” are composed of thin, flat strips of silicone with a hollow channel inside bonded to a layer of flexible but stiffer polymer nanofibers. The fingers are attached to a rectangular, 3D-printed plastic “palm” and, when their channels are filled with water, curl in the direction of the nanofiber-coated side. Each finger exerts an extremely low amount of pressure — about 0.0455 kPA, or less than one-tenth of the pressure of a human’s eyelid on their eye. By contrast, current state-of-the-art soft marine grippers, which are used to capture delicate but more robust animals than jellyfish, exert about 1 kPA.</em></p> \n   <p><em>The gripper was successfully able to trap each jellyfish against the palm of the device, and the jellyfish were unable to break free from the fingers’ grasp until the gripper was depressurized. The jellyfish showed no signs of stress or other adverse effects after being released, and the fingers were able to open and close roughly 100 times before showing signs of wear and tear.</em></p> \n  </blockquote> \n  <p>[ <a href=\"https://news.harvard.edu/gazette/story/2019/08/ultra-soft-underwater-grippers-reach-next-level-of-perfection/\">Harvard</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <blockquote> \n    <p><em>MIT engineers have developed a magnetically steerable, thread-like robot that can actively glide through narrow, winding pathways, such as the labyrinthine</em><em> vasculature of the brain. In the future, this robotic thread may be paired with existing endovascular technologies, enabling doctors to remotely guide the robot through a patient’s brain vessels to quickly treat blockages and lesions, such as those that occur in aneurysms and stroke.</em></p> \n   </blockquote> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/INSyV4dgqu8\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>[ <a href=\"https://news.mit.edu/2019/robot-brain-blood-vessels-0828\">MIT</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>See NASA’s next Mars rover quite literally coming together inside a clean room at the Jet Propulsion Laboratory. This behind-the-scenes look at what goes into building and preparing a rover for Mars, including extensive tests in simulated space environments, was captured from March to July 2019. The rover is expected to launch to the Red Planet in summer 2020 and touch down in February 2021.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/QnuLxzocuhY\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>The Mars 2020 rover doesn’t have a name yet, but you can give it one! As long as you’re not too old! Which you probably are!</p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/WHr3PeSafjU?rel=0\" width=\"620\"></iframe></p> \n    <p>[ <a href=\"https://mars.nasa.gov/mars2020/\">Mars 2020</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>I desperately wish that we could watch this next video at normal speed, not just slowed down, but it’s quite impressive anyway.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/XTE8xauGTuo\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>Here’s one more video from the Namiki Lab showing some high speed tracking with a pair of very enthusiastic robotic cameras:</p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/-xughkGuATc?rel=0\" width=\"620\"></iframe></p> \n     <p>[ <a href=\"http://www.em.eng.chiba-u.jp/~namiki/index-e.html\">Namiki Lab</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <blockquote> \n       <p><em>Normally, tedious modeling of mechanics, electronics, and information science is required to understand how insects’ or robots’ moving parts coordinate smoothly to take them places. But in a new study, biomechanics researchers at the Georgia Institute of Technology boiled down the sprints of cockroaches to handy principles and equations they then used to make a test robot amble about better.</em></p> \n      </blockquote> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/MeSFVvw2zwk\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"http://www.rh.gatech.edu/news/625040/scurrying-roaches-help-researchers-steady-staggering-robots\">Georgia Tech</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>More magical obstacle-dodging footage from Skydio’s still secret new drone.</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/mMnhIRk7psY\" width=\"620\"></iframe></p> \n       <p></p> \n       <blockquote> \n        <p><em>We’ve been hard at work extending the capabilities of our upcoming drone, giving you ways to get the control you want without the stress of crashing. The result is you can fly in ways, and get shots, that would simply be impossible any other way. How about flying through obstacles at full speed, backwards?</em></p> \n       </blockquote> \n       <p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>This is a cute demo with Misty:</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/fSOVi8XCraM\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://www.mistyrobotics.com/\">Misty Robotics</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>We’ve seen pieces of hardware like this before, but always made out of hard materials—a soft version is certainly something new.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/HiILkgaunUY\" width=\"620\"></iframe></p> \n         <p></p> \n         <blockquote> \n          <p><em>Utilizing vacuum power and soft material actuators, we have developed a soft reconfigurable surface (SRS) with multi-modal control and performance capabilities. The SRS is comprised of a square grid array of linear vacuum-powered soft pneumatic actuators (linear V-SPAs), built into plug-and-play modules which enable the arrangement, consolidation, and control of many DoF.</em></p> \n         </blockquote> \n         <p>[ <a href=\"https://www.epfl.ch/labs/rrl/\">RRL</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>The EksoVest is not really a robot, but it’ll make you a cyborg! With super strength!</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ugUGhy_g7mc\" width=\"620\"></iframe></p> \n          <p></p> \n          <blockquote> \n           <p><em>\"This is NOT intended to give you super strength but instead give you super endurance and reduce fatigue so that you have more energy and less soreness at the end of your shift.\"</em></p> \n          </blockquote> \n          <p>Drat!</p> \n          <p>[ <a href=\"https://eksobionics.com/eksoworks/eksovest/\">EksoVest</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>We have created a solution for parents, grandparents, and their children who are living separated. This is an amazing tool to stay connected from a distance through the intimacy that comes through interactive play with a child. For parents who travel for work, deployed military, and families spread across the country, the Cushybot One is much more than a toy; it is the opportunity for maintaining a deep connection with your young child from a distance.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hOqYpnS6BHw\" width=\"620\"></iframe></p> \n           <p></p> \n           <p><em>Hmm.</em></p> \n           <p>I think the concept here is great, but it’s going to be a serious challenge to successfully commercialize.</p> \n           <p>[ <a href=\"https://www.indiegogo.com/projects/cushybot-one-play-together-from-anywhere#/\">Indiegogo</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>What happens when you equip <a href=\"https://robots.ieee.org/robots/rvr/?utm_source=spectrum\">RVR</a> with a parachute and send it off a cliff? Watch this episode of RVR Launchpad to find out – then go Behind the Build to see how we (eventually) accomplished this high-flying feat.</em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/FQ1wPUxvj2o\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.sphero.com/rvr\">Sphero</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>These omnidirectional crawler robots aren’t new, but that doesn’t keep them from being fun to watch.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GrMAmZOAl_c\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://www.nedo.go.jp/news/press/AA5_101178.html\">NEDO</a> ] via [ <a href=\"https://pc.watch.impress.co.jp/docs/news/1203459.html\">Impress</a> ]</p> \n             <p></p> \n             <p></p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p>We’ll finish up the week with a couple of past ICRA and IROS keynote talks—one by Gill Pratt on The Reliability Challenges of Autonomous Driving, and the other from Peter Hart, on Making Shakey.</p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/9AIri8ejX90\" width=\"620\"></iframe></p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/t6B9wpMyKyc?rel=0\" width=\"620\"></iframe></p> \n              <p>[ <a href=\"https://www.youtube.com/channel/UCvmXaE8AFihVzN9rN5de0Dw\">IEEE RAS</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <div class=\"feedflare\"> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ypq5vl5-FKM:qbOc_szKWNw:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ypq5vl5-FKM:qbOc_szKWNw:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ypq5vl5-FKM:qbOc_szKWNw:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ypq5vl5-FKM:qbOc_szKWNw:-BTjWOF_DHI\" border=\"0\" /></a> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ypq5vl5-FKM:qbOc_szKWNw:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ypq5vl5-FKM:qbOc_szKWNw:gIN9vFwOqvQ\" border=\"0\" /></a> \n               </div> \n               <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ypq5vl5-FKM\" height=\"1\" width=\"1\" alt=\"\" /> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 30 Aug 2019 18:54:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2NDc1Mg.jpeg","linkMd5":"4bdb7374dde6559b940d866d23415332","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn59@2020_5/2020/08/25/10-38-00-270_01c86ef1c9aa25b4.webp","destWidth":1240,"destHeight":827,"sourceBytes":187017,"destBytes":47898,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ypq5vl5-FKM:qbOc_szKWNw:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn45@2020_6/2020/08/25/10-38-14-283_451c5eb60f5108a1.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ypq5vl5-FKM:qbOc_szKWNw:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn93@2020_1/2020/08/25/10-38-11-997_6372b0c7abe23ab9.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ypq5vl5-FKM":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn88@2020_2/2020/08/25/10-38-09-904_11b6339f09049b1e.webp"},"publishedOrCreatedDate":1598351859736},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Why People Demanded Privacy to Confide in the World’s First Chatbot","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","description":"<p><em>This is part four of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>.</em></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p>Between 1964 and 1966, <a href=\"https://en.wikipedia.org/wiki/Joseph_Weizenbaum\">Joseph Weizenbaum</a>, a German American computer scientist at MIT’s artificial intelligence lab, developed <a href=\"https://web.stanford.edu/class/linguist238/p36-weizenabaum.pdf\">the first-ever chatbot</a>&nbsp;[PDF].</p> \n<p>While there were already some rudimentary digital language generators in existence—programs that could spit out somewhat coherent lines of text—Weizenbaum’s program was the first designed explicitly for interactions with&nbsp;humans.&nbsp;The user could type in some statement or set of statements in their normal language, press enter, and receive a response from the machine. As Weizenbaum explained, his program made “certain kinds of natural-language conversation between man and computer possible.”</p> \n<p></p> \n<p>He named the program <a href=\"https://en.wikipedia.org/wiki/ELIZA\">Eliza</a> after Eliza Doolittle, the working-class hero of George Bernard Shaw’s <em><a href=\"https://en.wikipedia.org/wiki/Pygmalion_(play)\">Pygmalion</a></em> who learns how to talk with an upper-class accent. The new Eliza was written for the 36-bit <a href=\"https://en.wikipedia.org/wiki/IBM_7090#IBM_7094\">IBM 7094</a>, an early transistorized mainframe computer, in a programming language that Weizenbaum developed called MAD-SLIP. &nbsp;</p> \n<p></p> \n<p>Because computer time was a valuable resource, Eliza could only be run via a time-sharing system; the user interacted with the program remotely via an electric typewriter and printer. When the user typed in a sentence and pressed enter, a message was sent to the mainframe computer. Eliza scanned the message for the presence of a keyword and used it in a new sentence to form a response that was sent back, printed out, and read by the user.</p> \n<p></p> \n<!--nextpage--> \n<p>To encourage ongoing dialogue, Weizenbaum designed Eliza to simulate the type of conversational style used by a <a href=\"https://en.wikipedia.org/wiki/Person-centered_therapy\">Rogerian psychoanalyst</a>. The program would take something the user said and reflect it back with a question, as can be seen in the example below. (Notice how the program takes keywords like “boyfriend” and “depressed” and re-uses them.)</p> \n<p></p> \n<p>Human: Men are all alike.&nbsp;</p> \n<p></p> \n<p>Computer: IN WHAT WAY?&nbsp;</p> \n<p></p> \n<p>Human: They're always bugging us about something or other.&nbsp;</p> \n<p></p> \n<p>Computer: CAN YOU THINK OF A SPECIFIC EXAMPLE?&nbsp;</p> \n<p></p> \n<p>Human: Well, my boyfriend made me come here.&nbsp;</p> \n<p></p> \n<p>Computer: YOUR BOYFRIEND MADE YOU COME HERE&nbsp;</p> \n<p></p> \n<p>Human: He says I'm depressed much of the time.&nbsp;</p> \n<p></p> \n<p>Computer: I AM SORRY TO HEAR YOU ARE DEPRESSED&nbsp;</p> \n<p></p> \n<p>Human: It's true. I am unhappy.&nbsp;</p> \n<p></p> \n<p>Weizenbaum chose this mode of dialogue for Eliza because it gave the impression that the computer understood what was being said without having to offer anything new to the conversation. It created the illusion of comprehension and engagement in a mere 200 lines of code.</p> \n<p></p> \n<p>To test Eliza’s capacity to engage an interlocutor, Weizenbaum invited students and colleagues into his office and let them chat with the machine while he looked on. He noticed, with some concern, that during their brief interactions with Eliza, many users began forming emotional attachments to the algorithm. They would open up to the machine and confess problems they were facing in their lives and relationships.</p> \n<p></p> \n<aside class=\"inlay pullquote rt med\">\n  During their brief interactions with Eliza, many users began forming emotional attachments to the algorithm. \n</aside> \n<p>Even more surprising was that this sense of intimacy persisted even after Weizenbaum described how the machine worked and explained that it didn’t really understand anything that was being said. Weizenbaum was most troubled when his secretary, who had watched him build the program from scratch over many months, insisted that he leave the room so she could talk to Eliza in private.</p> \n<p></p> \n<p>For Weizenbaum, this experiment with Eliza made him question an idea that <a href=\"https://en.wikipedia.org/wiki/Alan_Turing\">Alan Turing</a> had&nbsp;proposed in 1950 about machine intelligence. In his paper, entitled&nbsp;“<a href=\"https://academic.oup.com/mind/article/LIX/236/433/986238\">Computing Machinery and Intelligence,</a>” Turing suggested that&nbsp;if a computer could conduct a convincingly human conversation in text, one could assume it was intelligent—an idea that became the basis of the famous <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing Test</a>. &nbsp;</p> \n<p></p> \n<p>But Eliza demonstrated that convincing communication between a human and a machine could take place even if comprehension only flowed from one side: The simulation of intelligence, rather than intelligence itself, was enough to fool people. Weizenbaum called this the Eliza effect, and believed it was a type of “delusional thinking” that humanity would collectively suffer from in the digital age. This insight was a profound shock for Weizenbaum, and one that came to define his intellectual trajectory over the next decade. &nbsp;</p> \n<p></p> \n<aside class=\"inlay pullquote lt med\">\n  The simulation of intelligence, rather than intelligence itself, was enough to fool people. \n</aside> \n<p>In 1976, he published <em><a href=\"http://blogs.evergreen.edu/cpat/files/2013/05/Computer-Power-and-Human-Reason.pdf\">Computing Power and Human Reason:&nbsp;From Judgment to Calculation</a>&nbsp;</em>[PDF], which offered a long meditation on why people are willing to believe that a simple machine might be able to understand their complex human emotions.</p> \n<p>In this book, he argues that the Eliza effect signifies a broader pathology afflicting “modern man.” In a world conquered by science, technology, and capitalism, people had grown accustomed to viewing themselves as isolated cogs in a large and uncaring machine. In such a diminished social world, Weizenbaum reasoned, people had grown so desperate for connection that they put aside their reason and judgment in order to believe that a program could care about their problems.</p> \n<p></p> \n<p>Weizenbaum spent the rest of his life <a href=\"https://www.youtube.com/watch?v=hAfpnkaZprE\">developing</a> this humanistic critique of&nbsp;artificial intelligence and digital technology. His mission was to remind people that their machines were not as smart as they were often said to be. And that even though it sometimes appeared as though they could talk, they were never really listening.</p> \n<p></p> \n<p><em>This is the fourth installment of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. Last week’s post described <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models\">Andrey Markov and Claude Shannon’s painstaking efforts</a> to create statistical models of language for text generation. Come back next Monday for part five, which describes the <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation\">Microsoft’s disastrous 2016 experiment</a> with a chatbot that learned the subtleties of language from Twitter.&nbsp;</em></p> \n<p></p> \n<p><em>You can also check out our prior series on the <a href=\"https://spectrum.ieee.org/tag/AI+history\">untold history of AI</a>.</em></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=r2-HEkba0No:-emDvS734jg:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=r2-HEkba0No:-emDvS734jg:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=r2-HEkba0No:-emDvS734jg:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=r2-HEkba0No:-emDvS734jg:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=r2-HEkba0No:-emDvS734jg:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=r2-HEkba0No:-emDvS734jg:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/r2-HEkba0No\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 18 Nov 2019 14:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxMTAzNA.jpeg","linkMd5":"ddd96fe231dfc465a52535e475a98b15","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn70@2020_5/2020/08/25/10-38-00-959_3e1672f9363a5fda.webp","destWidth":1240,"destHeight":930,"sourceBytes":318686,"destBytes":205706,"author":"Oscar Schwartz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=r2-HEkba0No:-emDvS734jg:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn36@2020_5/2020/08/25/10-38-03-193_1ca7d1ab61cc5266.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=r2-HEkba0No:-emDvS734jg:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn15@2020_4/2020/08/25/10-38-07-712_545b73adcbe745b9.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/r2-HEkba0No":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn37@2020_1/2020/08/25/10-38-14-012_ed2a0f2c65c0e9f6.webp"},"publishedOrCreatedDate":1598351859719},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Robot Teaches Kids Hand Washing Skills in Rural India","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india","description":"<p>Last Tuesday, October 15<sup>th</sup>, was Global Handwashing Day, which I’m sure you celebrated by washing your hands every time you were supposed to. Hand washing is a skill that can be difficult for kids to learn, because there usually aren’t immediate negative consequences to not washing your hands. The not immediate consequences can be severe, though—hand washing with soap can prevent 40 percent of diarrhea and respiratory infections, which kill about 1,300 young children around the world each day. Kids that don’t die can still get very sick, and about 443 million school days are lost globally each year due to water and sanitation related diseases in developing countries.</p> \n<p>In rural India, only about 18 percent of people wash their hands with soap, and the way to change this is to start teaching hand washing as a skill at a young age. It’s especially important to do this in schools, which are both places for teaching things as well as places where diseases get transmitted (although hopefully more the first thing than the second thing). But we don’t want teachers spending their time standing by the bathroom sink nagging their pupils, and we don’t have to, because robots can help with this.</p> \n<!--nextpage--> \n<p><a href=\"https://www.psy.gla.ac.uk/~amold/\">Amol Deshmukh</a>, a researcher at the University of Glasgow whose work (in collaboration with Amrita University in India)&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-research-in-the-wild-water-transport-in-rural-india\">using robots to transport water in rural villages in India we wrote about last year</a>, has presented some new research last week on the development of a social robot that can encourage hand washing in rural Indian schools. The robot’s name is Pepe, and Deshmukh hopes that by just hanging out by the bathroom sink and doing some polite reminding, Pepe will be able to save kids’ lives.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/DBFy3LI890s\" width=\"620\"></iframe></p> \n<p>Pepe is simple and inexpensive, as robots go. It’s made mostly out of acrylic since it’s cheap and looks shiny and clean. The bright green color was chosen because it’s associated with good health and the environment. The hand shape is self-explanatory, and the only moving parts are the two degree of freedom eyes, which are important because people are more likely to do things they know they should be doing when they think they’re being watched. They are being watched, of course, through a small front-facing camera that’s used for data collection.</p> \n<p>A phone provides the screen for Pepe’s animated mouth, and the box that holds everything includes a speaker as well. Pepe’s name was carefully chosen (in consultation with local language experts) to be easy to pronounce and remember, but unfamiliar enough that the kids wouldn’t associate a gender with it. Pepe is able to speak the same language that the kids do (Malayalam), but not dynamically—the things it says are pre-recorded and tweaked to make them sound like a child with an indeterminate gender, and the robot is operated remotely by a researcher who tells it what to say and when. The whole thing costs less than US $100, and seems like it could easily be made for even less in volume.</p> \n<p>Pepe’s job was to sit by the hand washing station at a school in rural India that hosts about 100 students ranging from 5 to 10 years old. Most of them (74 percent) had interacted with a smartphone before, but none of them had seen a robot. Pepe would remind the kids to wash their hands after using the restroom and before meals, talk them through the seven steps of proper hand washing (which you can view <a href=\"https://www.chp.gov.hk/files/pdf/guidelines_for_hand_hygiene.pdf\">here</a> if you’re trying to figure out what these seven steps could possibly be), and then say either “oh no” or “very good” depending on how the child did. It could also respond to a few other things, like&nbsp;“please don’t touch me” if a child got overly friendly.</p> \n<p>The robot was only on station for three days, but it managed to increase hand washing rates by nearly 40 percent, and hand washing quality increased as well. Six days after the robot left the school, the researchers checked back, and found that while many students had gone back to their old habits, there was still a significant improvement in hygiene overall.</p> \n<p>Part of what’s exciting about this research is seeing how humans and robots interact outside of the contexts that we’re used to. Here’s how the children responded when asked if they thought a different design might be good for the robot:</p> \n<blockquote> \n <p><em>The [students] mentioned they would like it to be a cat’s face. The reason was they could take it home and pet it. When the interviewers asked if a dog’s face would also be appropriate, many of the students responded saying that they would not like that because dogs are considered unclean in their religion. Many of the students also commented that they had cats at home. Students with other religious backgrounds also agreed with this suggestion. This underscores the need for social robot design to take in cultural perceptions of form and symbolism.</em></p> \n</blockquote> \n<p>A robot like Pepe seems like it could make a significant difference to hygiene in rural India and elsewhere, but the challenge will be to make it scalable. Having a human in the loop is obviously an issue if you want to deploy a whole bunch of these robots, so we asked Deshmukh why he didn’t start out with something more autonomous. “We wanted to pilot this robot as a proof-of-concept to investigate if such an application can be effective in this context and environment,” he said.&nbsp;“We did not know what results to expect but our research approach was systematic. We also wanted to identify the technical challenges for autonomous technology, for example computer vision in outdoor light sensitive and unstructured environments is still a technical challenge.” He added:&nbsp;“Our future goal is to build an autonomous system and deploy the robot in multiple schools, but this will require substantial funding and development effort.”</p> \n<p>In order for the case to be made that these robots are worth that funding and development effort, you have to compare them to other methods of encouraging hand washing, like the bathroom posters that I can vaguely remember not paying attention to in elementary school.</p> \n<aside class=\"inlay pullquote xlrg\">\n  “Our future goal is to build an autonomous system and deploy the robot in multiple schools, but this will require substantial funding and development effort.” \n <span class=\"pq-attrib\">—Amol Deshmukh, University of Glasgow</span> \n</aside> \n<p>Deshmukh says that the robot does a much better job. “Existing practices for influencing hand washing behavior in developing countries face three major challenges: 1. they are very resource intensive, 2. complex to measure/monitor compliance at scale due to unreliable self-reporting , and 3. they rarely are sustainable over long-term. Previous research on hand hygiene intervention suggests the disruption of the social and physical setting where the hand washing behavior should take place by placing eye-catching cues and visual reminders can lead to more successful interventions.” For that reason, he explained, “there is need for an interdisciplinary and innovative approach for influencing sustainable hand washing behavior. Social robots can, for example, encourage, remind, educate, give real-time feedback, observe, and monitor hand washing behavior, plus they are a distinct novelty for rural populations.”</p> \n<p>For more details, we spoke with Amol Deshmukh via email.</p> \n<p><strong><em>IEEE Spectrum:</em> What do you think was the most interesting part of your results?</strong></p> \n<p><strong>Amol Deshmukh:</strong> The most interesting part about the study was the level of response we got from the students not just in terms of increase in levels hand washing but their interest in getting to know the robot. From the questionnaires response, 72 percent of the students thought it was alive (referring to its ability to talk as the main reason). The students tried to talk back to the robot, even sing not knowing that the robot cannot understand them. We also used&nbsp;this opportunity to engage with students, for example after the study finished a demonstration of the working of the robot was provided for 3rd and 4th grade students to encourage interest in the STEM disciplines.</p> \n<p></p> \n<figure class=\"rt med\" role=\"img\"> \n <img alt=\"Pepe hand washing robot\" src=\"https://spectrum.ieee.org/image/MzM5NzM0OA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: University of Glasgow \n  </figcaption> </img> \n</figure> \n<p><strong>Novelty is always a concern for social robots like these. Do you think that if the robot was left at the school long term, it would continue to be as effective?</strong></p> \n<p>In response to the question “Would you like to see Pepe back in the school after vacation?”&nbsp;we found that 97&nbsp;<span>percent</span> of the (42) respondents indicated that they wanted to see the robot after a few months when their school re-opens. However traditionally human-robot interaction research has indicated users engagement levels with robots fades over a period of time. Our study was a short-term study in the future wen would like to deploy the robot over a long term period to investigate if the social robot will still be effective.</p> \n<p><strong>How could this robot be changed to make it more effective, especially long term?</strong></p> \n<p>In the long-term we would like to carry out research following a user centered design process, investigate which interactive strategies work better, for example use of gamification, if the children wash their hands effectively a time based graphic in relation to the hand washing steps completed can be shown on the screen (e.g. balloon bursts or a rocket fires) along with the robot showing a happy expression.</p> \n<p><strong>What are you working on next?</strong></p> \n<p>We want to develop the autonomous capabilities for the robot, by using machine learning to detect behavior cues—for example mutual gaze, hand washing activity along with effective verbal and non-verbal interaction strategies from the robot. Investigate the effectiveness of the social robot in influencing behavior change by objectively measuring hand washing activity such as quality, frequency, and duration using computer vision. We are seeking funding opportunities for carrying out this research effectively and deploy multiple robots at scale in rural schools in developing countries.</p> \n<hr> \n <p>Deshmukh presented two papers on Pepe at the IEEE RO-MAN conference in New Delhi, India this week, along with colleagues from <a href=\"http://ammachilabs.org/\">Ammachi Labs</a> at Amrita University, including Unnikrishnan R, Sooraj K Babu, Shanker Ramesh, Parameswar Anitha, and Rao R. Bhavani.</p> \n <p>[ <a href=\"https://www.gla.ac.uk/news/headline_678574_en.html\">University of Glasgow</a> ]</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=F9OLArjw1as:mj94lXbFh2A:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=F9OLArjw1as:mj94lXbFh2A:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=F9OLArjw1as:mj94lXbFh2A:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=F9OLArjw1as:mj94lXbFh2A:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=F9OLArjw1as:mj94lXbFh2A:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=F9OLArjw1as:mj94lXbFh2A:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/F9OLArjw1as\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Wed, 23 Oct 2019 14:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NzM1MA.jpeg","linkMd5":"645d9089aa580f603b8e05528f7302e7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn33@2020_4/2020/08/25/10-37-58-806_e317ffe2a5e8da89.webp","destWidth":1240,"destHeight":827,"sourceBytes":113230,"destBytes":33396,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5NzM0OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn77@2020_2/2020/08/25/10-38-15-414_f6d622475181e356.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=F9OLArjw1as:mj94lXbFh2A:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn74@2020_4/2020/08/25/10-38-16-346_410891ccb3e6e535.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=F9OLArjw1as:mj94lXbFh2A:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_2/2020/08/25/10-38-10-324_7df99b838c68940b.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/F9OLArjw1as":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn92@2020_6/2020/08/25/10-38-09-258_2bf7410ac3e1cc46.webp"},"publishedOrCreatedDate":1598351859727},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Kuka's Robutt Is a Robot Designed to Assess New Car Seats","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>Kuka’s&nbsp;“robutt”&nbsp;can, according to the company, simulate&nbsp;“thousands of butts in the pursuit of durability and comfort.”&nbsp;Two of the robots are&nbsp;used at a Ford&nbsp;development center in Germany to evaluate new car seats. The tests are quite exhaustive, consisting of around 25,000 simulated sitting motions for each new seat design.” Or as Kuka puts it, “Pleasing all the butts on the planet is serious business.”</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://www.youtube.com/embed/49KUx62-w7Y\" width=\"620\"></iframe></p> \n <p></p> \n <p>[&nbsp;<a href=\"https://www.blog.kuka.com/2019/04/30/apple-apricot-or-pear-the-kuka-robutt-in-test-use-at-ford/?lang=en\">Kuka</a>&nbsp;]</p> \n <hr> \n  <p>Here’s a clever idea: 3D printing manipulators, and then using the 3D printer head to move those manipulators around and do stuff with them:</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/KdNfX_TPKIg\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>[ <a href=\"https://dl.acm.org/citation.cfm?doid=3332165.3347885\">Paper</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <blockquote> \n    <p><em>Two former soldiers performed a series of tests to see if the ONYX Exoskeleton gave them extra strength and endurance in difficult environments.</em></p> \n   </blockquote> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SIQJKePalto\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>So when can I rent one of these to help me move furniture?</p> \n   <p>[ <a href=\"https://www.lockheedmartin.com/en-us/products/exoskeleton-technologies/military.html\">Lockheed</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>One of the defining characteristics of legged robots in general (and humanoid robots in particular) is the ability of walking on various types of terrain. In this video, we show our humanoid robot <a href=\"https://robots.ieee.org/robots/toro/?utm_source=spectrum\">TORO</a> walking dynamically over uneven (on grass outside the lab), rough (large gravel), and compliant terrain (a soft gym mattress). The robot can maintain its balance, even when the ground shifts rapidly under foot, such as when walking over gravel. This behaviour showcases the torque-control capability of quickly adapting the contact forces compared to position control methods.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Styw7Wk2lQk\" width=\"620\"></iframe></p> \n    <p></p> \n    <blockquote> \n     <p><em>An in-depth discussion of the current implementation is presented in the paper \"Dynamic Walking on Compliant and Uneven Terrain using DCM and Passivity-based Whole-body Control\".</em></p> \n    </blockquote> \n    <p>[ <a href=\"https://www.dlr.de/rmc/en/desktopdefault.aspx/tabid-11942/#gallery/28887\">DLR RMC</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Tsuki is a ROS-enabled quadruped designed and built by <a href=\"http://zlethic.com/\">Lingkang Zhang</a>. It’s completely position controlled, with no contact sensors on the feet, or even an IMU.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ulsR6pKh_N8\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>It can even do flips!</p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/K2UY5iTbnXo?rel=0\" width=\"620\"></iframe></p> \n     <p>[ <a href=\"http://zlethic.com/quadruped-tsuki/\">Tsuki</a> ]</p> \n     <p><em>Thanks Lingkang!</em></p> \n     <p></p> \n     <hr> \n      <p></p> \n      <blockquote> \n       <p><em>TRI CEO Dr. Gill Pratt presents TRI’s contributions to Toyota’s New \"LQ\" Concept Vehicle, which includes onboard artificial intelligence agent \"Yui\" and LQ’s automated driving technology.</em></p> \n      </blockquote> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/YSlg2h84ESM\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://www.tri.global/\">TRI</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>Hooman Hedayati wrote in to share some work (presented at HRI this year) on using augmented reality to make drone teleoperation more intuitive. Get a virtual drone to do what you want first, and then the real drone will follow.</p> \n       <p><iframe allow=\"autoplay; fullscreen\" allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://player.vimeo.com/video/314350943?title=0&amp;byline=0&amp;portrait=0\" width=\"620\"></iframe></p> \n       <p>[ <a href=\"https://ieeexplore.ieee.org/abstract/document/8673306\">Paper</a> ]</p> \n       <p><em>Thanks Hooman!</em></p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>You can now order a <a href=\"https://robots.ieee.org/robots/rvr/?utm_source=spectrum\">Sphero RVR</a>&nbsp;for $250. It’s very much <em>not</em> spherical, but it does other stuff, so we’ll give it a pass.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GtJXZzbfk1A\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://www.sphero.com/rvr\">Sphero</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>The AI Gamer Q56 robot is an expert at whatever this game is, using AI plus actual physical control manipulation. Watch until the end!</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/DL5PkhkKJHE\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://www.bandainamco-mirai.com/\">Bandai Namco</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <blockquote> \n           <p><em>We present a swarm of autonomous flying robots for the exploration of unknown environments. The tiny robots do not make maps of their environment, but deal with obstacles on the fly. In robotics, the algorithms for navigating like this are called \"bug algorithms\". The navigation of the robots involves them first flying away from the base station and later finding their way back with the help of a wireless beacon.</em></p> \n          </blockquote> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/IgMKiIEbfN8\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"http://mavlab.tudelft.nl/\">MAVLab</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>Okay Soft Robotics you successfully and disgustingly convinced us that vacuum grippers should never be used for food handling. Yuck!</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/6SyM1yfhzxA\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.softroboticsinc.com/\">Soft Robotics</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>Beyond the asteroid belt are \"fossils of planet formation\" known as the Trojan asteroids. These primitive bodies share Jupiter’s orbit in two vast swarms, and may hold clues to the formation and evolution of our solar system. Now, NASA is preparing to explore the Trojan asteroids for the first time. A mission called Lucy will launch in 2021 and visit seven asteroids over the course of twelve years - one in the main belt and six in Jupiter’s Trojan swarms.</em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/4ZHCwSaBzd8\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.nasa.gov/content/goddard/lucy-the-first-mission-to-jupiter-s-trojans\">NASA</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>I’m not all that impressed by this concept car from Lexus except that it includes some kind of super-thin autonomous luggage-carrying drone.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/-IvIlreVTLs\" width=\"620\"></iframe></p> \n             <p></p> \n             <blockquote> \n              <p><em>The LF-30 Electrified also carries the ‘Lexus Airporter’ drone-technology support vehicle. Using autonomous control, the Lexus Airporter is capable of such tasks as independently transporting baggage from a household doorstep to the vehicle’s luggage area.</em></p> \n             </blockquote> \n             <p>[ <a href=\"https://discoverlexus.com/models/premieres/lf-30\">Lexus</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p></p> \n              <p></p> \n              <blockquote> \n               <p><em>Vision 60 legged robot managing unstructured terrain without vision or force sensors in its legs. Using only high-transparency actuators and 2kHz algorithmic stability control... 4-limbs and 12-motors with only a velocity command.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/D1SwHm2JJYg\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://www.ghostrobotics.io/\">Ghost Robotics</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>Tech United Eindhoven is looking good for RoboCup@Home 2020.</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/AiuS5d112o0\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://www.techunited.nl/en/\">Tech United</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <blockquote> \n                 <p><em>Penn engineers participated in the Subterranean (SubT) Challenge hosted by DARPA, the Defense Advanced Research Projects Agency. The goal of this Challenge is for teams to develop automated systems that can work in underground environments so they could be deployed after natural disasters or on dangerous search-and-rescue missions.</em></p> \n                </blockquote> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/qGR60YAyK-U\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>[ <a href=\"https://pluto-subt.github.io/\">Team PLUTO</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <p>It’s <a href=\"https://spectrum.ieee.org/automaton/robotics/diy/new-beetlecam-wildlife-robotic-camera\">BeetleCam</a> vs White Rhinos in Kenya, and the White Rhinos don’t seem to mind at all.</p> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/jWsvS6DFvKM\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <p>[ <a href=\"http://blog.burrard-lucas.com/2019/10/photographing-rhinos/\">Will Burrard-Lucas</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <div class=\"feedflare\"> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WQZUBYIDfRg:y_chSDmQyhE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WQZUBYIDfRg:y_chSDmQyhE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WQZUBYIDfRg:y_chSDmQyhE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WQZUBYIDfRg:y_chSDmQyhE:-BTjWOF_DHI\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WQZUBYIDfRg:y_chSDmQyhE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WQZUBYIDfRg:y_chSDmQyhE:gIN9vFwOqvQ\" border=\"0\" /></a> \n                  </div> \n                  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WQZUBYIDfRg\" height=\"1\" width=\"1\" alt=\"\" /> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 25 Oct 2019 18:10:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwMDIwMA.jpeg","linkMd5":"f89d2d766c8d79193395fae8610e0f13","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn14@2020_2/2020/08/25/10-38-00-025_597022f229b3aa34.webp","destWidth":1240,"destHeight":742,"sourceBytes":340700,"destBytes":115194,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WQZUBYIDfRg:y_chSDmQyhE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn98@2020_3/2020/08/25/10-38-16-411_dbc87045d62f6c10.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WQZUBYIDfRg:y_chSDmQyhE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn58@2020_4/2020/08/25/10-38-16-132_1868c01ff72dd64d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WQZUBYIDfRg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_3/2020/08/25/10-38-11-108_8829968c6d83b9b0.webp"},"publishedOrCreatedDate":1598351859739},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"From Mainframes to PCs: What Robot Startups Can Learn From the Computer Revolution","link":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","description":"<p><em>This is a guest post. The views expressed here are solely those of the author and do not represent positions of&nbsp;</em>IEEE Spectrum<em>&nbsp;or the IEEE.</em></p> \n<p><span>Autonomous robots are coming around slowly. We already got autonomous vacuum cleaners, autonomous lawn mowers, toys that bleep and blink, and (maybe) soon autonomous cars. Yet, generation after generation, we keep waiting for the robots that we all know from movies and TV shows. Instead, businesses seem to get farther and farther away from the robots that are able to do a large variety of tasks using general-purpose, human anatomy-inspired hardware.</span></p> \n<p><span>Although these are the droids we have been looking for, anything that came close, such as</span><a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-software/willow-garage-to-shut-down\"> Willow Garage</a>’s <a href=\"https://robots.ieee.org/robots/pr2/?utm_source=spectrum\">PR2</a> or<a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/rethink-robotics-pioneer-of-collaborative-robots-shuts-down\"> Rethink Robotics</a>’ <a href=\"https://robots.ieee.org/robots/baxter/?utm_source=spectrum\">Baxter</a> has bitten the dust.&nbsp;With building a&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robotic-dreams-robotic-realities\">robotic company being particularly hard</a>, compounding business risk with technological risk, the trend goes from selling robots to selling actual services like mowing your lawn, provide taxi rides, fulfilling retail orders, or picking strawberries by the pound. Unfortunately for fans of R2-D2 and C-3PO, these kind of business models emphasize specialized, room- or fridge-sized hardware that is optimized for one very specific task, but does not contribute to a general-purpose robotic platform.</p> \n<!--nextpage--> \n<p></p> \n<p><span id=\"docs-internal-guid-d8378c9f-7fff-6f83-23cf-bea9696a2afe\">We have actually seen something very similar in the personal computer (PC) industry. In the 1950s, even though computers could be as big as an entire room and were only available to a selected few, the public already had a good idea of what computers would look like. A </span><a href=\"https://en.wikipedia.org/wiki/List_of_fictional_computers\">long list of fictional computers</a> started to populate mainstream entertainment during that time. In a <span>1962&nbsp;</span><em>New York Times</em><span>&nbsp;article titled “</span><a href=\"https://www.nytimes.com/1962/11/03/archives/pocket-computer-may-replace-shopping-list-inventor-says-device.html\" style=\"font-family: Georgia, serif; font-size: 18px;\">Pocket Computer to Replace Shopping List</a>,<span>”&nbsp;</span>visionary scientist&nbsp;<a href=\"https://en.wikipedia.org/wiki/John_Mauchly\">John Mauchly</a>&nbsp;stated that “there is no reason to suppose the average boy or girl cannot be master of a personal computer.”</p> \n<p>In 1968, <a href=\"https://en.wikipedia.org/wiki/Douglas_Engelbart\">Douglas Engelbart</a> gave us the “<a href=\"https://en.wikipedia.org/wiki/The_Mother_of_All_Demos\">mother of all demos</a>,”&nbsp;browsing hypertext on a graphical screen and a mouse, and other ideas that have become standard only decades later. Now that we have finally seen all of this, it might be helpful to examine what actually enabled the computing revolution to learn where robotics is really at and what we need to do next.</p> \n<h3>The parallels between computers and robots</h3> \n<p></p> \n<p><span id=\"docs-internal-guid-d8378c9f-7fff-6f83-23cf-bea9696a2afe\">In the 1970s, mainframes&nbsp;</span><a href=\"https://www.nytimes.com/1970/04/05/archives/maxi-computers-face-mini-conflict-mini-trend-reaching-computers.html\">were about to be replaced by the emerging class of mini-computers</a>, fridge-sized devices that cost less than US $25,000 ($165,000 in 2019 dollars). These computers did not use punch-cards, but could be programmed in Fortran and BASIC, dramatically expanding the ease with which potential applications could be created. Yet it was still unclear whether mini-computers could ever replace big mainframes in applications that require fast and efficient processing of large amounts of data, let alone enter every living room. This is&nbsp;very similar to the robotics industry right now, where large-scale factory robots (mainframes) that have <a href=\"https://robots.ieee.org/robots/unimate/?utm_source=spectrum\">existed since the 1960s</a>&nbsp;are seeing competition from a growing industry of collaborative robots that can safely work next to humans and can easily be installed and programmed (minicomputers). As in the ’70s, applications for these devices that reach system prices comparable to that of a luxury car are quite limited, and it is hard to see how they could ever become a consumer product.&nbsp;</p> \n<p><span id=\"docs-internal-guid-d8378c9f-7fff-6f83-23cf-bea9696a2afe\">Yet, as in the computer industry, successful architectures are quickly being cloned, driving prices down, and entirely new approaches on how to construct or program robotic arms are sprouting left and right. Arm makers are joined by manufacturers of autonomous carts,&nbsp;robotic grippers, and sensors. These components can be combined, paving the way for standard general purpose platforms that follow the model of the&nbsp;</span><a href=\"https://en.wikipedia.org/wiki/IBM_Personal_Computer\" style=\"font-size: 1em;\">IBM PC</a><span>, which built a capable,</span><a href=\"https://en.wikipedia.org/wiki/Open_architecture\" style=\"font-size: 1em;\"> open architecture</a><span> relying as much on commodity parts as possible.</span></p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/BH9IGpZ9tLg\" width=\"620\"></iframe></p> \n<p><span>General purpose robotic systems have not been successful for similar reasons that general purpose, also known as “personal,”&nbsp;computers took decades to emerge. Mainframes were custom-built for each application, while typewriters got smarter and smarter, not really leaving room for general purpose computers in between. Indeed, given the cost of hardware and the relatively little abilities of today’s autonomous robots, it is almost always smarter to build a special purpose machine than trying to make a collaborative mobile manipulator smart. </span></p> \n<p><span>A current example is e-commerce grocery fulfillment. The current trend is to&nbsp;</span><a href=\"https://www.forbes.com/sites/brittainladd/2019/02/01/crossing-the-rubicon-why-2018-was-the-point-of-no-return-for-online-grocery/\">reserve underutilized parts of a brick-and-mortar store for a micro-fulfillment center</a> that stores goods in little crates with an automated retrieval system and a (human) picker. A number of startups like <a href=\"https://www.alertinnovation.com/\">Alert Innovation</a>, <a href=\"https://getfabric.com/\">Fabric</a>, <a href=\"https://www.ocadotechnology.com/\">Ocado Technology</a>, <a href=\"https://www.takeoff.com/\">TakeOff Technologies</a>, and <a href=\"https://www.tompkinsrobotics.com/\">Tompkins Robotics</a>, to just name a few, have raised hundreds of millions of venture capital recently to build mainframe equivalents of robotic fulfillment centers. This is in contrast with a robotic picker, which would drive through the aisles to restock and pick from shelves. Such a robotic store clerk would come much closer to our vision of a general purpose robot, but would require many copies of itself that crowd the aisles to churn out hundreds of orders per hour as a microwarehouse could. Although eventually more efficient, the margins in retail are already low and make it unlikely that this industry will produce the technological jump that we need to get friendly C-3POs manning the aisles.&nbsp;</p> \n<aside class=\"inlay pullquote xlrg\">\n  Startups have raised hundreds of millions of venture capital recently to build mainframe equivalents of robotic fulfillment centers. This is in contrast with a robotic picker, which would drive through the aisles to restock and pick from shelves, and would come much closer to our vision of a general purpose robot. \n</aside> \n<p></p> \n<p><span id=\"docs-internal-guid-36a9af95-7fff-d85f-f792-e90cbe4616d5\">Mainframes were also attacked from the bottom. Fascination with the new digital technology has led to a hobbyist movement to create microcomputers that were sold via mail order or at RadioShack. Initially, a large number of small businesses was selling tens, at most hundreds, of&nbsp;devices, usually as a kit and with wooden enclosures. This trend culminated into the “<a href=\"https://en.wikipedia.org/wiki/History_of_personal_computers#1977_and_the_emergence_of_the_%22Trinity%22\">1977 Trinity</a>”&nbsp;in the form of the</span><a href=\"https://en.wikipedia.org/wiki/Apple_II\"> Apple II</a>, the<a href=\"https://en.wikipedia.org/wiki/Commodore_PET\"> Commodore PET</a>, and the Tandy&nbsp;<a href=\"https://spectrum.ieee.org/consumer-electronics/gadgets/the-consumer-electronics-hall-of-fame-tandyradioshack-trs80-model-1\">TRS-80</a>, complete computers that&nbsp;were sold for prices around $2500 (TRS) to $5000 (Apple) in today’s dollars. The main application of these computers was their programmability (in BASIC), which would enable consumers to “learn to chart your biorhythms, balance your checking account, or even control your home environment,” according to<a href=\"https://en.wikipedia.org/wiki/Apple_II#/media/File:Apple_II_advertisement_Dec_1977_page_2.jpg\"> an original Apple advertisement</a>. Similarly, there exists a myriad of gadgets that explore different aspects of robotics such as mobility, manipulation, and entertainment.</p> \n<p></p> \n<p><span id=\"docs-internal-guid-36a9af95-7fff-d85f-f792-e90cbe4616d5\">As in the fledgling personal computing industry, the advertised functionality was at best a model of the real deal. A now-famous milestone in entertainment robotics was the original&nbsp;<a href=\"https://robots.ieee.org/robots/aibo/?utm_source=spectrum\">Sony’s Aibo</a>, a robotic dog that was advertised to have many properties that a real dog has such as develop its own personality, play with a toy, and interact with its owner. Released in 1999, and <a href=\"https://robots.ieee.org/robots/aibo2018/?utm_source=spectrum\">re-launched in 2018</a>, the platform has a solid following among hobbyists and academics who like its programmability, but probably only very few users who accept the device as a pet stand-in.</span></p> \n<p><span id=\"docs-internal-guid-36a9af95-7fff-d85f-f792-e90cbe4616d5\">There also exist countless “build-your-own-robotic-arm” kits. One of the more successful examples is the </span><a href=\"https://www.ufactory.cc/#/\">uArm</a>, which sells for around $800, and is advertised to perform pick and place, assembly, 3D printing, laser engraving, and many other things that sound like high value applications. Using compelling videos of the robot actually doing these things in a constrained environment has led to two successful crowd-funding campaigns, and have established the robot as a successful educational tool.</p> \n<p>Finally, there exist platforms that allow hobbyist programmers to explore mobility to construct robots that patrol your house, deliver items, or provide their users with telepresence abilities. An example of that is the <a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty II</a>. Much like with the original Apple II, there remains a disconnect between the price of the hardware and the fidelity of the applications that were available.&nbsp;</p> \n<p></p> \n<p><span id=\"docs-internal-guid-36a9af95-7fff-d85f-f792-e90cbe4616d5\">For computers, this disconnect&nbsp;began to disappear with the invention of the first electronic spreadsheet software&nbsp;</span><a href=\"https://en.wikipedia.org/wiki/VisiCalc\">VisiCalc</a> that spun out of Harvard in 1979 and prompted many people to buy an entire microcomputer just to run the program. VisiCalc was soon joined by<a href=\"https://en.wikipedia.org/wiki/WordStar\"> WordStar</a>, a word processing application, that sold for close to $2000 in today’s dollars. WordStar, too, would entice many people to buy the entire hardware just to use the software. The two programs are early examples of what became known as “<a href=\"https://en.wikipedia.org/wiki/Killer_application\">killer application</a>.”</p> \n<p></p> \n<p><span id=\"docs-internal-guid-36a9af95-7fff-d85f-f792-e90cbe4616d5\">With factory automation being mature, and robots with the price tag of a minicomputer being capable of driving around and&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robots-getting-a-grip-on-general-manipulation\">autonomously carrying out many manipulation tasks</a>, the robotics industry is somewhere where the PC industry was between 1973—the release of the<a href=\"https://en.wikipedia.org/wiki/Xerox_Alto\"> Xerox Alto</a>, the first computer with a graphical user interface, mouse, and special software—and 1979—when microcomputers in the under $5000 category began to take off.</p> \n<h3>Killer apps for robots</h3> \n<p>So what would it take for robotics to continue to advance like computers did? The market itself already has done a good job distilling what the possible killer apps are. VCs and customers alike push companies who have set out with lofty goals to reduce their offering to a simple value proposition. As a result, companies that started at opposite ends often converge to mirror images of each other that offer very similar autonomous carts, (bin) picking, palletizing, depalletizing, or sorting solutions. Each of these companies usually serves a single application to a single vertical—for example bin-picking clothes, transporting warehouse goods, or picking strawberries by the pound. They are&nbsp;trying to prove that their specific technology works without spreading themselves too thin.</p> \n<p></p> \n<p><span id=\"docs-internal-guid-36a9af95-7fff-d85f-f792-e90cbe4616d5\">Very few of&nbsp;these companies have really&nbsp;taken off. One example is&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/amazon-acquires-kiva-systems-for-775-million\">Kiva Systems, which&nbsp;turned into the logistic robotics division of Amazon</a>. Kiva and others are structured around sound value propositions that are grounded in well-known user needs. As these solutions are very specialized, however, it is unlikely that they result into any economies of scale of the same magnitude that early computer users who bought both a spreadsheet </span>and a word processor application for their expensive minicomputer could enjoy. What would make these robotic solutions more interesting is when functionality becomes stackable. Instead of just being able to do bin picking, palletizing, and transportation with the same hardware, these three skills could be combined to model entire processes.</p> \n<p>A skill that is yet little addressed by startups and is historically owned by the mainframe equivalent of robotics is assembly of simple mechatronic devices. The ability to assemble mechatronic parts is equivalent to other tasks such as changing a light bulb, changing the batteries in a remote control, or tending machines like a lever-based espresso machine. These tasks would&nbsp;involve&nbsp;the autonomous execution of complete workflows possible using a single machine, eventually leading to an explosion of industrial productivity across all sectors.&nbsp; For example, picking up an item from a bin, arranging it on the robot, moving it elsewhere, and placing it into a shelf or a machine is a process that equally applies to a manufacturing environment, a retail store, or someone’s kitchen.</p> \n<figure class=\"rt med-lrg\" role=\"img\"> \n <img alt=\"Autonomous, vision and force-based assembly of the Siemens learning challenge.\" src=\"https://spectrum.ieee.org/image/MzM4OTg3OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Robotic Materials Inc. \n  </figcaption> \n  <figcaption>\n    Autonomous, vision and force-based assembly of the \n   <a href=\"https://new.siemens.com/us/en/company/fairs-events/robot-learning.html\">Siemens robot learning challenge</a>. \n  </figcaption> </img> \n</figure> \n<p><span>Even though many of the above applications are becoming possible, it is still very hard to&nbsp;</span><a href=\"https://techcrunch.com/2015/11/28/the-platform-paradox/\">get a platform off the ground</a> without added components that provide “killer app” value of their own. Interesting examples are Rethink Robotics or the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-software/wizards-of-ros-willow-garage-and-the-making-of-the-robot-operating-system\">Robot Operating System (ROS)</a>. Rethink Robotics’ <a href=\"https://robots.ieee.org/robots/baxter/?utm_source=spectrum\">Baxter</a> and <a href=\"https://robots.ieee.org/robots/sawyer/?utm_source=spectrum\">Sawyer</a> robots pioneered a great user experience (like the 1973 Xerox Alto, really the first PC), but its applications were difficult to extend beyond simple pick-and-place and palletizing and depalletizing items.</p> \n<p>ROS pioneered interprocess communication software that was adapted to robotic needs (multiple computers, different programming languages) and the idea of software modularity in robotics, but—in the absence of a common hardware platform—hasn’t yet delivered a single application, e.g. for navigation, path planning, or grasping, that performs beyond research-grade demonstration level and won’t get&nbsp;discarded once developers turn to production systems. At the same time, an increasing number of robotic devices, such as robot&nbsp;arms or 3D perception systems that&nbsp;offer intelligent functionality, provide other ways to wire them together that do not require an intermediary computer, while keeping close control over the real-time aspects of their hardware.</p> \n<p></p> \n<figure class=\"rt med-lrg\" role=\"img\"> \n <img alt=\"Robotic Materials GPR-1 combines a MIR-100 autonomous cart with an UR-5 collaborative robotic arm, an onRobot F/T sensor and Robotic Materials’ SmartHand to perform out-of-the box mobile assembly, bin picking, palletizing, and depalletizing jobs. \" src=\"https://spectrum.ieee.org/image/MzM4OTg4MA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image:&nbsp;Robotic Materials Inc. \n  </figcaption> \n  <figcaption> \n   <p>Robotic Materials GPR-1 combines a MIR-100 autonomous cart with an UR-5 collaborative robotic arm, an onRobot force/torque&nbsp;sensor and Robotic Materials’ SmartHand to perform out-of-the-box mobile assembly, bin picking, palletizing, and depalletizing tasks.&nbsp;</p> \n  </figcaption> </img> \n</figure> \n<p><span>At my company, Robotic Materials Inc.,&nbsp;we have made strides to identify a few applications such as bin picking and assembly, making them configurable with a single click by combining machine learning and optimization with an intuitive user interface. Here, users can define object classes and how to grasp them using a web browser, which then appear as first-class objects in a robot-specific graphical programming language. We have also done this for assembly, allowing users to stack perception-based picking and force-based assembly primitives&nbsp;</span><a href=\"https://youtu.be/_JNy3ZidQ6E\">by simply dragging and dropping appropriate commands</a> together.</p> \n<p><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\">While such an approach might answer the question of a killer app for robots priced in the “minicomputer” range, it is unclear how killer app-type value can be generated with robots in the less-than-$5000 category. A possible answer is two-fold: First, with low-cost arms, mobility platforms, and entertainment devices continuously improving, a confluence of technology readiness and user innovation, like with the Apple II and VisiCalc, will eventually happen. For example, there is not much innovation needed to turn </span><a href=\"https://www.mistyrobotics.com/\">Misty</a>&nbsp;into a home security system; the uArm into a low-cost bin-picking system; or an Aibo-like device into a therapeutic system for the elderly or children with autism.</p> \n<p>Second, robots and their components have to become dramatically cheaper. Indeed, computers have seen an exponential reduction in price accompanied by an exponential increase in computational power, thanks in great part to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Moore%27s_law\">Moore’s Law</a>.&nbsp;This development has helped robotics too, allowing us to reach breakthroughs in mobility and manipulation due to the ability to process massive amounts of image and depth data in real-time, and we can expect it to continue to do so.</p> \n<h3><span>Is there a Moore’s Law for robots?</span></h3> \n<p>One might ask, however, how a similar dynamics might be possible for robots as a whole, including all their motors and gears, and what a “Moore’s Law” would look like for the robotics industry. Here, it helps to remember that the perpetuation of&nbsp; Moore’s Law is not the reason, but the result of the PC revolution. Indeed, the first killer apps for bookkeeping, editing, and gaming were so good that they unleashed tremendous consumer demand, beating the benchmark on what was thought to be physically possible over and over again. (I vividly remember 56 kbps to be the absolute maximum data rate for copper phone lines until DSL appeared.)</p> \n<p>That these economies of scale are also applicable to mechatronics is impressively demonstrated by the car industry.&nbsp;A good example is the 2020 Prius Prime, a highly computerized plug-in hybrid, that is available for one third of the cost of my company’s&nbsp;<a href=\"http://roboticmaterials.com/rm/wp-content/uploads/2019/09/GPR-1-Data-Sheet.pdf\">GPR-1 mobile manipulator</a>&nbsp;while being orders of magnitude more complex, sporting an electrical motor, a combustion engine, and a myriad of sensors and computers. It is therefore very well conceivable to produce a mobile manipulator that retails at one tenth of the cost of a modern car, once robotics enjoy similar mass-market appeal. Given that these robots are part of the equation, actively lowering cost of production, this might happen as fast as never before in the history of industrialization.&nbsp;</p> \n<aside class=\"inlay pullquote rt med\">\n  It is therefore very well conceivable to produce a mobile manipulator that retails at one tenth of the cost of a modern car, once robotics enjoy similar mass-market appeal. \n</aside> \n<p><span>There is one more driver that might make robots exponentially more capable: the cloud. Once a general purpose robot has learned or was programmed with a new skill, it could share it with every other robot. At some point, a grocer who buys a robot could assume that it already knows how to recognize and handle 99 percent&nbsp;of the retail items in the store. Likewise, a manufacturer can assume that the robot can handle and assemble every item available from <a href=\"https://www.mcmaster.com/\">McMaster-Carr</a> and <a href=\"https://us.misumi-ec.com/\">Misumi</a>. Finally, families could expect a robot to know every kitchen item that Ikea and Pottery Barn is selling. Sounds like a labor intense problem, but probably more manageable than collecting footage for </span><a href=\"https://en.wikipedia.org/wiki/Google_Street_View\">Google’s Street View</a> using cars, tricycles, and snowmobiles, among other vehicles.</p> \n<h3>Strategies for robot startups</h3> \n<p><span>While we are waiting for these two trends—better and better applications and hardware with decreasing cost—to converge, we as a community have to keep exploring what the canonical robotic applications beyond mobility, bin picking, palletizing, depalletizing, and assembly are. We must also continue to solve the fundamental challenges that stand in the way of making these solutions truly general and robust. </span></p> \n<p><span>For both questions, it might help to look at the strategies that have been critical in the development of the personal computer, which might equally well apply to robotics:</span></p> \n<p></p> \n<ul style=\"list-style-type:disc\"> \n <li> <p><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\">Start with a solution to a problem your customers have. Unfortunately, their problem is almost never that they need your sensor, widget, or piece of code, but something that already costs them money or </span>negatively affects<span> them in some other way. Example: There are many more people who had a problem calculating their taxes (and wanted to buy VisiCalc) than writing their own solution in BASIC.</span></p> </li> \n</ul> \n<p></p> \n<ul style=\"list-style-type:disc\"> \n <li> <p><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\">Build as little of your own hardware as necessary. Your business model should be stronger than the margin you can make on the hardware. Why taking the risk? Example: Why build your own typewriter if you can write the best typewriting application that makes it worth buying a computer just for that?&nbsp;</span></p> </li> \n</ul> \n<p></p> \n<ul style=\"list-style-type:disc\"> \n <li> <p><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\">If your goal is a platform, make sure it comes with a killer application, which alone justifies the platform cost. Example: Microcomputer companies came and went until the “1977 Trinity” intersected with the killer apps spreadsheet and word processors. Corollary: You can also get lucky.&nbsp;</span></p> </li> \n</ul> \n<p></p> \n<ul style=\"list-style-type:disc\"> \n <li> <p><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\">Use an&nbsp;</span><a href=\"https://en.wikipedia.org/wiki/Open_architecture\">open architecture</a>, which creates an ecosystem where others compete on creating better components and peripherals, while allowing others to integrate your solution into their vertical and stack it with other devices. Example: Both the Apple II and the IBM PC were completely open architectures, enabling many clones, thereby growing the user and developer base.&nbsp;</p> </li> \n</ul> \n<p></p> \n<p><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\">It’s worthwhile pursuing this. With most business processes already being digitized, general purpose robots will allow us to fill in gaps in mobility and manipulation, increasing productivity at levels only limited by the amount of resources and energy that are available, possibly </span><a href=\"https://www.oreilly.com/ideas/machine-money-and-people-money\">creating a utopia in which creativity becomes the ultimate currency</a>. Maybe we’ll even get R2-D2.</p> \n<p></p> \n<h4><span id=\"docs-internal-guid-6ad4e7d6-7fff-94f6-b6ff-2a9aae4a145e\"><a href=\"http://correll.cs.colorado.edu/?page_id=19\">Nikolaus Correll</a> is an associate professor of computer science at the University of Colorado at Boulder where he works on mobile manipulation and other robotics applications.&nbsp;</span><span>He’s co-founder and CTO of <a href=\"https://roboticmaterials.com/\">Robotic Materials Inc.</a>, which is&nbsp;supported by the National Science Foundation and the National Institute of Standards and Technology via their Small Business Innovative Research (SBIR) programs. </span></h4> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=8qXUW4b1PHc:ZGA2YPIR5uc:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=8qXUW4b1PHc:ZGA2YPIR5uc:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=8qXUW4b1PHc:ZGA2YPIR5uc:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=8qXUW4b1PHc:ZGA2YPIR5uc:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=8qXUW4b1PHc:ZGA2YPIR5uc:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=8qXUW4b1PHc:ZGA2YPIR5uc:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/8qXUW4b1PHc\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 08 Oct 2019 21:18:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4OTg0NA.jpeg","linkMd5":"9778ce9d1c486affccf103eca762f153","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn14@2020_5/2020/08/25/10-37-58-876_612463ef1ba83719.webp","destWidth":2048,"destHeight":1536,"sourceBytes":162523,"destBytes":44522,"author":"Nikolaus Correll","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM4OTg3OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn32@2020_3/2020/08/25/10-38-08-647_11ba8337a3ff5223.webp","https://spectrum.ieee.org/image/MzM4OTg4MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn84@2020_4/2020/08/25/10-38-08-418_2d4e9d3a0e61f7ba.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=8qXUW4b1PHc:ZGA2YPIR5uc:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn4@2020_4/2020/08/25/10-38-06-152_eb334d95d9d75ccf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=8qXUW4b1PHc:ZGA2YPIR5uc:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn24@2020_4/2020/08/25/10-38-07-767_e33eb3365c620b07.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/8qXUW4b1PHc":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn49@2020_2/2020/08/25/10-38-13-130_60ad82f1e3fcea40.webp"},"publishedOrCreatedDate":1598351859730},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Trump CTO Addresses AI, Facial Recognition, Immigration, Tech Infrastructure, and More","link":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/trump-cto-addresses-ai-facial-recognition-immigration-tech-infrastructure-and-more","description":"<p><a href=\"https://www.whitehouse.gov/people/michael-kratsios/\">Michael Kratsios</a>, the Chief Technology Officer of the United States, took the stage at Stanford University last week to field questions from Stanford’s <a href=\"https://fsi.stanford.edu/people/eileen-donahoe\">Eileen Donahoe</a> and attendees at <a href=\"https://hai.stanford.edu/events/2019-fall-conference/overview\">the 2019 Fall Conference</a> of the <a href=\"https://hai.stanford.edu/\">Institute for Human-Centered Artificial Intelligence (HAI)</a>.</p> \n<p></p> \n<p>Kratsios, the fourth to hold the <a href=\"https://en.wikipedia.org/wiki/Chief_Technology_Officer_of_the_United_States\">U.S. CTO</a> position since its creation by President Barack Obama in 2009, was confirmed in August as President Donald Trump’s first CTO.&nbsp;Before joining the Trump administration, he was chief of staff at investment firm <a href=\"https://www.thielcapital.com/\">Thiel Capital</a> and chief financial officer of hedge fund <a href=\"https://www.clarium.com/\">Clarium Capital</a>. Donahoe is Executive Director of Stanford’s Global Digital Policy Incubator and served as the first U.S. Ambassador to the United Nations Human Rights Council during the Obama Administration.</p> \n<p>The conversation jumped around, hitting on both accomplishments and controversies. Kratsios touted the administration’s success in fixing policy around the use of drones,&nbsp;its memorandum on STEM education, and an increase in&nbsp;funding for basic research in AI—though the magnitude of that increase wasn’t specified. He pointed out that the Trump administration’s AI policy has been a continuation of <a href=\"https://hbr.org/2016/12/the-obama-administrations-roadmap-for-ai-policy\">the policies of the Obama administration</a>, and will continue to build on that foundation. As proof of this, he&nbsp;pointed&nbsp;to Trump’s signing of the <a href=\"https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/\">American AI Initiative</a> earlier this year.&nbsp;That executive order, Kratsios said, was intended to bring various government agencies together to coordinate their AI efforts and to push the idea that AI is a tool for the American worker. The AI Initiative, he noted, also took&nbsp;into consideration that AI will cause job displacement, and asked private companies to pledge to retrain workers.</p> \n<p>The administration, he said, is also looking to remove barriers to AI innovation.&nbsp;In service of that goal, the government will, in the next month or so, release a regulatory guidance memo instructing government agencies about “how they should think about AI technologies,” said&nbsp;Kratsios.</p> \n<p></p> \n<p><strong>U.S. vs China in AI</strong></p> \n<p></p> \n<p>A few of the exchanges between Kratsios and Donahoe hit on current hot topics, starting with the tension between the U.S. and China.</p> \n<p></p> \n<p>Donahoe:</p> \n<blockquote> \n <p>“You talk a lot about unique U.S. ecosystem. In which aspect of AI is the U.S. dominant, and where is China challenging us in dominance?</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“They are challenging us on machine vision. They have more data to work with, given that they have surveillance data.”</p> \n</blockquote> \n<p>Donahoe:</p> \n<blockquote> \n <p>“To what extent would you say the quantity of data collected and available will be a determining factor in AI dominance?”</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“It makes a big difference in the short term. But we do research on how we get over these data humps. There is a future where you don’t need as much data, a lot of federal grants are going to [research in] how you can train models using less data.”</p> \n</blockquote> \n<p>Donahoe turned the conversation to a different tension—that between <a href=\"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things\">innovation and values</a>.</p> \n<p>Donahoe:</p> \n<blockquote> \n <p>“A lot of conversation yesterday was about the tension between innovation and values, and how do you hold those things together and lead in both realms.”</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“We recognized that the U.S. hadn’t signed on to principles around developing AI. In May, we signed <a href=\"https://www.oecd.org/science/forty-two-countries-adopt-new-oecd-principles-on-artificial-intelligence.htm\">[the Organization for Economic Cooperation and Development Principles on Artificial Intelligence],</a> coming together with other Western democracies to say that these are values that we hold dear.</p> \n <p>[Meanwhile,] we have adversaries around the world using AI to surveil people, to suppress human rights. That is why American leadership is so critical: We want to come out with the next great product. And we want our values to underpin the use cases.”</p> \n</blockquote> \n<p>A member of the audience pushed further:</p> \n<blockquote> \n <p>“Maintaining U.S. leadership in AI might have costs in terms of individuals and society. What costs should individuals and society bear to maintain leadership?”</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“I don’t view the world that way. Our companies big and small do not hesitate to talk about the values that underpin their technology. [That is] markedly different from the way our adversaries think. The alternatives are so dire [that we] need to push efforts to bake the values that we hold dear into this technology.”</p> \n</blockquote> \n<p></p> \n<p><strong>Facial recognition</strong></p> \n<p></p> \n<p>And then the conversation turned to the use of AI for facial recognition, an&nbsp;application which (at least for&nbsp;police and other government agencies) was recently <a>banned in San Francisco</a>.</p> \n<p>Donahoe:</p> \n<blockquote> \n <p>“Some private sector companies have called for government regulation of facial recognition, and there already are some instances of local governments regulating it. Do you expect federal regulation of facial recognition anytime soon? If not, what ought the parameters be?”</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“A patchwork of regulation of technology is not beneficial for the country. We want to avoid that. Facial recognition has important roles—for example, finding lost or displaced children. There are use cases, but they need to be underpinned by values.”</p> \n</blockquote> \n<p>A member of the audience followed up on that topic, referring to some data presented earlier at the HAI conference on bias in AI:</p> \n<blockquote> \n <p>“Frequently the example of finding missing children is given as the example of why we should not restrict use of facial recognition. But we saw <a href=\"https://www.media.mit.edu/people/joyab/overview/\">Joy Buolamwini’s </a>&nbsp;presentation on <a href=\"https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212\">bias in data</a>. I would like to hear your thoughts about how government thinks we should use facial recognition, knowing about this bias.”</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“Fairness, accountability, and robustness are things we want to bake into any technology—not just facial recognition—as we build rules governing use cases.”</p> \n</blockquote> \n<p></p> \n<p><strong>Immigration and innovation</strong></p> \n<p></p> \n<p>A member of the audience brought up the issue of immigration:</p> \n<blockquote> \n <p>“One major pillar of innovation is immigration, does your office advocate for it?”</p> \n</blockquote> \n<blockquote> \n <p>Kratsios:</p> \n <p>“Our office pushes for best and brightest people from around the world to come to work here and study here. There are a few efforts we have made to move towards a more merit-based immigration system, without congressional action. [For example, in] the H1-B visa system, you go through two lotteries. We switched the order of them in order to get more people with advanced degrees through.”</p> \n</blockquote> \n<p><strong>The government’s tech infrastructure</strong></p> \n<p></p> \n<blockquote> \n <p></p> \n</blockquote> \n<p>Donahoe brought the conversation around to the tech infrastructure of the government itself:</p> \n<blockquote> \n <p>&nbsp;“We talk about the shiny object, AI, but the 80 percent is the unsexy stuff, at federal and state levels.&nbsp;We don’t have a modern digital infrastructure to enable all the services—like a research cloud. How do we create this digital infrastructure?”</p> \n</blockquote> \n<p>Kratsios:</p> \n<blockquote> \n <p>“I couldn’t agree more; the least partisan issue in Washington is about modernizing IT infrastructure. We spend like $85 billion a year on IT at the federal level, we can certainly do a better job of using those dollars.”</p> \n</blockquote> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=2xbdHXuXRMk:1qI5zxyxn7c:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=2xbdHXuXRMk:1qI5zxyxn7c:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=2xbdHXuXRMk:1qI5zxyxn7c:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=2xbdHXuXRMk:1qI5zxyxn7c:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=2xbdHXuXRMk:1qI5zxyxn7c:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=2xbdHXuXRMk:1qI5zxyxn7c:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/2xbdHXuXRMk\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 05 Nov 2019 15:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNTcxOA.jpeg","linkMd5":"0e1d29b76415f6a75c1bacb441cc9c0d","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn25@2020_3/2020/08/25/10-38-00-327_3dead94228bef8f0.webp","destWidth":1240,"destHeight":930,"sourceBytes":747309,"destBytes":109932,"author":"Tekla S. Perry","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=2xbdHXuXRMk:1qI5zxyxn7c:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn74@2020_4/2020/08/25/10-38-12-498_eaea85be62a1617f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=2xbdHXuXRMk:1qI5zxyxn7c:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn19@2020_5/2020/08/25/10-38-02-966_901a08df68e35324.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/2xbdHXuXRMk":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn69@2020_2/2020/08/25/10-38-12-406_e299dd90b9cfda73.webp"},"publishedOrCreatedDate":1598351859723},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"The Ultimate Optimization Problem: How to Best Use Every Square Meter of the Earth's Surface","link":"https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface","description":"<p></p> \n<p><a href=\"https://www.microsoft.com/en-us/research/people/lujoppa/\">Lucas Joppa</a> thinks big. Even while gazing down into his cup of tea in his modest office on Microsoft’s campus in Redmond, Washington, he seems to see the entire planet bobbing in there like a spherical tea bag.&nbsp;</p> \n<p>As Microsoft’s first chief environmental officer, Joppa came up with the company’s <a href=\"https://www.microsoft.com/en-us/ai/ai-for-earth\">AI for Earth</a> program, a five-year effort that’s spending US $50 million on AI-powered solutions to global environmental challenges.</p> \n<p>The program is not just about specific deliverables, though. It’s also about mindset, Joppa told <em>IEEE Spectrum</em> in an interview in July. “It’s a plea for people to think about the Earth in the same way they think about the technologies they’re developing,” he says. “You start with an objective. So what’s our objective function for Earth?” (In computer science, an objective function describes the parameter or parameters you are trying to maximize or minimize for optimal results.)</p> \n<figure class=\"rt med-sm\" role=\"img\"> \n <img alt=\"Microsoft's Lucas Joppa\" src=\"https://spectrum.ieee.org/image/MzM3OTA2MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Microsoft \n  </figcaption> \n  <figcaption>\n    Lucas Joppa \n  </figcaption> </img> \n</figure> \n<p>AI for Earth launched in December 2017, and Joppa’s team has since given grants to more than 400 organizations around the world. In addition to receiving funding, some grantees get help from Microsoft’s data scientists and access to the company’s computing resources.&nbsp;</p> \n<p>In a wide-ranging interview about the program, Joppa described his vision of the “ultimate optimization problem”—figuring out which parts of the planet should be used for farming, cities, wilderness reserves, energy production, and so on.&nbsp;</p> \n<p></p> \n<blockquote> \n <p>Every square meter of land and water on Earth has an infinite number&nbsp;of possible utility functions. It’s the job of <em>Homo sapiens</em> to describe our overall objective for the Earth. Then it’s the job of computers to produce optimization results that are aligned with the human-defined objective.</p> \n <p>I don’t think we’re close at all to being able to do this. I think we’re closer from a technology perspective—being able to run the model—than we are from a social perspective—being able to make decisions about what the objective should be. What do we want to do with the Earth’s surface?</p> \n</blockquote> \n<p></p> \n<!--nextpage--> \n<p>Such questions are increasingly urgent, as <a href=\"https://spectrum.ieee.org/static/special-report-blueprints-for-a-miracle\">climate change</a> has already begun reshaping our planet and our societies. Global sea and air surface temperatures have already risen by an average of 1 degree Celsius above preindustrial levels, <a href=\"https://www.ipcc.ch/sr15/\">according to the Intergovernmental Panel on Climate Change</a>.</p> \n<p>Today, people all around the world participated in a “<a href=\"https://globalclimatestrike.net/\">climate strike</a>,” with young people leading the charge and demanding a global transition to renewable energy. On Monday, world leaders will gather in New York for the <a href=\"https://www.un.org/en/climatechange/assets/pdf/Information_Note_Climate%20Summit_20Mar2019.pdf\">United Nations Climate Action Summit</a>, where they’re expected to present plans to limit warming to 1.5 degrees Celsius.&nbsp;</p> \n<p>Joppa says such summit discussions should aim&nbsp;for a truly holistic solution.&nbsp;&nbsp;</p> \n<blockquote> \n <p>We talk about how to solve climate change. There’s a higher-order question for society: What climate do we want? What output from nature do we want and desire? If we could agree on those things, we could put systems in place for optimizing our environment accordingly. Instead we have this scattered approach, where we try for local optimization. But the sum of local optimizations is never a global optimization.</p> \n</blockquote> \n<p>There’s increasing interest in using artificial intelligence to tackle global environmental problems. New sensing technologies enable scientists to collect unprecedented amounts of data about the planet and its denizens, and AI tools are becoming&nbsp;vital for interpreting all that data.</p> \n<p>The&nbsp;2018 report “<a href=\"https://www.pwc.com/gx/en/sustainability/assets/ai-for-the-earth-jan-2018.pdf\">Harnessing AI for the Earth</a>,” produced by the World Economic Forum and the consulting company PwC, discusses ways that AI can be used to address six of the world’s most pressing environmental challenges (climate change, biodiversity, and healthy oceans, water security, clean air, and disaster resilience).&nbsp;</p> \n<p>Many of the proposed applications involve better monitoring of human and natural systems, as well as modeling applications that would enable better predictions and more efficient use of natural resources.&nbsp;&nbsp;</p> \n<p>Joppa says that AI for Earth is taking a two-pronged approach, funding efforts to collect and interpret vast amounts of data alongside&nbsp;efforts that use that data to help humans make better decisions. And that’s where the global optimization engine would really come in handy.&nbsp;</p> \n<blockquote> \n <p>For any location on earth, you should be able to go and ask: What’s there, how much is there, and how is it changing? And more importantly: What should be there?</p> \n <p>On land, the data is really only interesting for the first few hundred feet. Whereas in the ocean, the depth dimension is really important.</p> \n <p>We need a planet with sensors, with roving agents, with remote sensing. Otherwise our decisions aren’t going to be any good.</p> \n</blockquote> \n<p></p> \n<p>AI for Earth isn’t going to create such an online portal within five years, Joppa stresses. But he hopes the projects that he’s funding will contribute to making such a portal possible—eventually.</p> \n<blockquote> \n <p>We’re asking ourselves: What are the fundamental missing layers in the tech stack that would allow people to build a global optimization engine? Some of them are clear, some are still opaque to me.</p> \n <p>By the end of five years, I’d like to have identified these missing layers, and have at least one example of each of the components.</p> \n</blockquote> \n<p>Some of the projects that AI for Earth has funded seem to fit that desire. Examples include <a href=\"https://silviaterra.com/bark/index.html\">SilviaTerra</a>, which used satellite imagery and AI to create a map of the 92 billion trees in forested areas across the United States. There’s also <a href=\"https://www.oceanmind.global/about/\">OceanMind</a>, a non-profit that detects illegal fishing and helps marine authorities enforce compliance. Platforms like <a href=\"http://www.wildbook.org/doku.php\">Wildbook</a> and <a href=\"https://www.inaturalist.org/\">iNaturalist</a> enable citizen scientists to upload pictures of animals and plants, aiding conservation efforts and research on biodiversity. And&nbsp;<a href=\"https://www.microsoft.com/en-us/research/project/farmbeats-iot-agriculture/\">FarmBeats</a> aims to enable data-driven agriculture with low-cost sensors, drones, and cloud services.&nbsp;&nbsp;</p> \n<p>It’s not impossible to imagine putting such services together into an optimization engine that knows everything about the land, the water, and the creatures who live on planet Earth. Then we’ll just have to tell that engine what we want to do about it.&nbsp;&nbsp;</p> \n<p><em>Editor’s note: This story is published in cooperation with more than 250 media organizations and independent journalists that have focused their coverage on climate change ahead of the <a href=\"https://www.un.org/en/climatechange/\">UN Climate Action Summit.</a> IEEE Spectrum’s participation in the <a href=\"https://www.cjr.org/covering_climate_now/covering-climate-now-170-outlets.php\">Covering Climate Now</a> partnership builds on <a href=\"https://spectrum.ieee.org/tag/climate+change\">our past reporting</a> about this global issue.</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UuITyw6-YW0:nxysFLk59TA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UuITyw6-YW0:nxysFLk59TA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UuITyw6-YW0:nxysFLk59TA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UuITyw6-YW0:nxysFLk59TA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UuITyw6-YW0:nxysFLk59TA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UuITyw6-YW0:nxysFLk59TA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/UuITyw6-YW0\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 20 Sep 2019 21:08:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3OTAxOQ.jpeg","linkMd5":"c521e6873fe26deaf6eefa6c6967cc9c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn33@2020_3/2020/08/25/10-38-01-419_9ce4826c1a7bd3b8.webp","destWidth":1240,"destHeight":930,"sourceBytes":391323,"destBytes":225650,"author":"Eliza Strickland","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3OTA2MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn94@2020_6/2020/08/25/10-38-12-930_3a92b8244b68a6ec.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UuITyw6-YW0:nxysFLk59TA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn79@2020_2/2020/08/25/10-38-07-427_1704728b44474cdc.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UuITyw6-YW0:nxysFLk59TA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn50@2020_3/2020/08/25/10-38-11-405_c9729e2fe6d9fb31.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/UuITyw6-YW0":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn98@2020_1/2020/08/25/10-38-12-683_e4fa77a16472962d.webp"},"publishedOrCreatedDate":1598351859731},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"The 3 Advantages of Assembled Cables","link":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","description":"<figure class=\"rt med-sm\" role=\"img\"> \n <img alt=\"Igus\" src=\"https://spectrum.ieee.org/image/MzM1ODE5Mg.png\"> </img>\n</figure> \n<p>Why waste time assembling your continuous-flex cables when you don’t have to?</p> \n<p></p> \n<p>The readycable® line of assembled motor, servo, signal and encoder cables comes equipped with standard industrial connectors that are ready to plug into your application the second they arrive. They’re expertly built similar to 24 manufacturers’ standards and are continuously tested under real-world conditions in the 41,200-square-foot igus® testing area. Our tests ensure each cable provides an extensive service life inside cable carriers, on robotic applications and more.</p> \n<figure class=\"ct lrg\" role=\"img\"> \n <img alt=\"Igus\" src=\"https://spectrum.ieee.org/image/MzM1ODE5Mw.png\"> </img>\n</figure> \n<p></p> \n<p><strong>Cable assembly with quality assurance</strong></p> \n<p></p> \n<p>At 12 state-of-the-art facilities throughout the world, including the U.S., highly-skilled technicians customize approximately 20,000 ready-to-connect chainflex® cables every week. Each cable is assembled similar to 24 manufacturers’ standards, including Siemens, Allen-Bradley and FANUC, and come equipped with industrial connectors from the biggest names in the industry, such as Intercontec, Harting and Yamaichi. Plus, igus® has created a 90-degree cable adapter called ibow® for connections in tight spaces.</p> \n<p></p> \n<p>Our technicians electrically test every cable, visually inspect every connection, and measure crimp heights and all critical dimensions precisely to ensure our assembled cables will function perfectly upon delivery and every day during the life of your machine. More than 1,200 cable types are available for customization.</p> \n<p></p> \n<p><strong>Cable design features for an extended service life</strong></p> \n<p></p> \n<p>Unlike traditional cables that tend to fail prematurely, chainflex® continuous-flex cables are engineered for optimal efficiency and can dramatically increase the service life and productivity of dynamic applications. chainflex® cables are designed with the following features:</p> \n<p></p> \n<ol> \n <li>A high-quality, high-tensile strength center element for strain relief</li> \n <li>A medium-to-fine conductor strand diameter to prevent the strand from kinking while being subjected to a high number of cycles</li> \n <li>The highest quality high-pressure extruded PVC or TPE materials that support the individual strands of the conductor and prevent the cable’s insulation from adhering to one another</li> \n <li>Individual conductors bundled into groups, which are cabled together in a single layer surrounding the cable core, enable pulling and compressive forces in the bending motion to balance and cancel out torsional forces</li> \n <li>A pressure-extruded inner jacket that ensures the insulated conductors are guided efficiently</li> \n <li>A high-quality braided shield that provides electromagnetic interference (EMI) protection for the cable</li> \n <li>A cost-effective outer jacket material that is resistant to UV radiation, abrasion, oils and chemicals</li> \n <li>CFRIP technology that allows users to open the cable’s jacket like a zipper, saving you time and eliminating the need for additional tools</li> \n</ol> \n<p></p> \n<p><strong>Extensively tested under real-world conditions</strong></p> \n<p></p> \n<p>Ideally, continuous-flex cables should be tested in configurations and environments that replicate real-world applications.</p> \n<p></p> \n<p>The 41,200-square-foot igus® testing area is the largest of its kind in the industry. Our engineers run 2 billion different bend tests to ensure cables designed for specific types of movement have the strength and durability to maintain their structure over millions of linear, tick-tock, torsional or multi-axis cycles. Engineers also use a climate chamber to test the long-term effects of extreme temperatures, chemicals and more.</p> \n<p></p> \n<p>Results show that in most cases, chainflex® cables can withstand temperatures between minus 40 C and 100 C. They are also flame retardant, UV resistant and highly oil resistant. All chainflex® cables have a predictable service life and come with a 36-month guarantee.</p> \n<p>For more information, watch the video <a href=\"http://toolbox.igus.com/youtube-all-videos/what-is-readycable\"><span>What is Readycable?</span></a></p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=M6oPcVu-H6M:Ky6uSH84PmI:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=M6oPcVu-H6M:Ky6uSH84PmI:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=M6oPcVu-H6M:Ky6uSH84PmI:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=M6oPcVu-H6M:Ky6uSH84PmI:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=M6oPcVu-H6M:Ky6uSH84PmI:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=M6oPcVu-H6M:Ky6uSH84PmI:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/M6oPcVu-H6M\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 20 Aug 2019 19:21:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1ODIyOQ.png","linkMd5":"c37b6ff6169ec7e6a055a9904de421c4","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn76@2020_6/2020/08/25/10-37-59-997_b6a7f1eca650a0f9.webp","destWidth":607,"destHeight":458,"sourceBytes":545084,"destBytes":39720,"author":"Igus","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1ODE5Mg.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn89@2020_4/2020/08/25/10-38-16-182_d5c2323f3f32169b.webp","https://spectrum.ieee.org/image/MzM1ODE5Mw.png":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn24@2020_1/2020/08/25/10-38-09-418_b918eb67e88bfb0c.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=M6oPcVu-H6M:Ky6uSH84PmI:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn22@2020_6/2020/08/25/10-38-17-590_9c31be7838ba8fe5.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=M6oPcVu-H6M:Ky6uSH84PmI:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn73@2020_2/2020/08/25/10-38-01-871_60065edb991267a7.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/M6oPcVu-H6M":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn84@2020_5/2020/08/25/10-38-09-907_fcb64f89cedb2119.webp"},"publishedOrCreatedDate":1598351859736},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"AI at the Speed of Light","link":"https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light","description":"<p>Neural networks shine for solving tough problems such as facial and <a href=\"https://spectrum.ieee.org/nanoclast/semiconductors/devices/nanoneurons-enable-neuromorphic-chips-for-voice-recognition\">voice recognition</a>, but conventional electronic versions are limited in speed and hungry for power. In theory, optics could beat digital electronic computers in the matrix calculations used in <a href=\"https://spectrum.ieee.org/tech-talk/computing/hardware/a-neural-net-based-on-light-could-best-digital-computers\">neural networks</a>. However, optics had&nbsp;been limited by their inability to do some complex calculations that had required electronics. Now new experiments show that all-optical neural networks can tackle those problems.&nbsp;</p> \n<p>The key attraction of neural networks is their massive interconnections among processors, comparable to the complex interconnections among neurons in the brain. This lets them perform many operations simultaneously, like the human brain does when looking at faces or listening to speech, making them more efficient for facial and voice&nbsp;recognition than traditional electronic computers that execute one instruction at a time.</p> \n<p>Today's electronic neural networks have reached <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/intels-neuromorphic-system-hits-8-million-neurons-100-million-coming-by-2020\">eight million neurons</a>, but their future use in artificial intelligence may be limited by their high power usage and limited parallelism in connections. Optical connections through lenses are inherently parallel. The lens in your eye simultaneously focuses light from across your field of view onto the retina in the back of your eye, where an array of light-detecting nerve cells detects the light. Each cell then relays the signal it receives to neurons in the brain that process the visual signals to show us an image.</p> \n<p>Glass lenses process optical signals by focusing light, which performs a complex mathematical operation called a Fourier transform that preserves the information in the original scene but rearranges is completely. One use of Fourier transforms is converting time variations in signal intensity into a plot of the frequencies present in the signal. The military used this trick in the 1950s to convert raw radar return signals recorded by an aircraft in flight into a three-dimensional image of the landscape viewed by the plane. Today that conversion is done electronically, but the vacuum-tube computers of the 1950s were not up to the task.</p> \n<p>Development of neural networks for artificial intelligence started with electronics, but their AI applications have been limited by their slow processing and need&nbsp;for extensive computing resources. Some researchers have developed hybrid neural networks, in which optics perform simple linear operations, but electronics perform more complex nonlinear calculations. Now two groups have demonstrated simple all-optical neural networks that do all processing with light.</p> \n<p>In May, <a href=\"https://www.uni-muenster.de/Physik.PI/Pernice/\">Wolfram Pernice</a> of the Institute of Physics at the University of Münster in Germany and colleagues reported testing an all-optical \"neuron\" in which signals change target materials between liquid and solid states, an effect that has been used for optical data storage. They demonstrated nonlinear processing, and produced output pulses like those from organic neurons. They then produced an integrated photonic circuit that incorporated four optical neurons operating at different wavelengths, each of which connected to 15 optical synapses. The photonic circuit contained more than 140 components and could recognize simple optical patterns. The group <a href=\"https://doi.org/10.1038/s41586-019-1157-8\">wrote that their device is scalable</a>, and that the technology promises \"access to the high speed and high bandwidth inherent to optical systems, thus enabling the direct processing of optical telecommunication and visual data.”&nbsp;</p> \n<p>Now a group at the Hong Kong University of Science and Technology reports in <em><a href=\"http://dx.doi.org/10.1364/OPTICA.6.001132\">Optica</a></em> that they have made an all-optical neural network based on a different process, electromagnetically induced transparency, in which incident light affects how atoms shift between quantum-mechanical energy levels. The process is nonlinear and can be triggered by very weak light signals, says <a href=\"https://people.phys.ust.hk/dusw/\">Shengwang Du</a>, a physics professor and coauthor of the paper.</p> \n<p>In their demonstration, they illuminated rubidium-85 atoms cooled by lasers to about 10 microKelvin (10 microdegrees above absolute zero). Although the technique may seem unusually complex, Du said the system was the most accessible one in the lab that could produce the desired effects. \"As a pure quantum atomic system [it] is ideal for this proof-of-principle experiment,\" he says.</p> \n<p>Next, they plan to scale&nbsp;up the demonstration using a hot atomic vapor center, which is less expensive, does not require time-consuming preparation of cold atoms, and can be integrated with photonic chips. Du says the major challenges are reducing cost of the nonlinear processing medium and increasing the scale of the all-optical neural network for more complex tasks.</p> \n<p>\"Their demonstration seems valid,\" says <a href=\"https://www.seas.gwu.edu/volker-j-sorger\">Volker Sorger</a>, an electrical engineer at George Washington University in Washington who was not involved in either demonstration. He says the all-optical approach is attractive because it offers very high parallelism, but the update rate is limited to about 100 hertz because of the liquid crystals used in their test, and he is not completely convinced their approach can be scaled error-free.&nbsp;</p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Q6sSAseS4hE:ZGUTfGSk7bY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Q6sSAseS4hE:ZGUTfGSk7bY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Q6sSAseS4hE:ZGUTfGSk7bY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Q6sSAseS4hE:ZGUTfGSk7bY:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Q6sSAseS4hE:ZGUTfGSk7bY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Q6sSAseS4hE:ZGUTfGSk7bY:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Q6sSAseS4hE\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 29 Aug 2019 15:34:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2Mzg1OQ.jpeg","linkMd5":"f1fb990643b446a2fb09abe6cb8c2668","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn4@2020_3/2020/08/25/10-38-00-606_020b802523e4b667.webp","destWidth":1240,"destHeight":930,"sourceBytes":178085,"destBytes":105214,"author":"Jeff Hecht","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Q6sSAseS4hE:ZGUTfGSk7bY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx/cdn32@2020_5/2020/08/25/10-38-10-295_3e23c6af9521bcda.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Q6sSAseS4hE:ZGUTfGSk7bY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn71@2020_5/2020/08/25/10-38-06-751_e887981c40b102d3.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Q6sSAseS4hE":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn24@2020_4/2020/08/25/10-38-05-164_7d16dd000eb2eef4.webp"},"publishedOrCreatedDate":1598351859734},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Next-Gen AR Glasses Will Require New Chip Designs","link":"https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality","description":"<p>What seems like a simple task—building a useful form of augmented reality into comfortable, reasonably stylish, eyeglasses—is going to need significant technology advances on many fronts, including displays, graphics, gesture tracking, and low-power processor design.</p> \n<p>That was the message of Sha Rabii, <a href=\"https://www.facebook.com/pg/facebook/about/\">Facebook</a>’s head of silicon and technology engineering. Rabii, speaking at <a href=\"https://www.armtechcon.com/\">Arm&nbsp;TechCon 2019</a> in San Jose, Calif., on Tuesday, described a future with AR glasses that enable wearers to see at night, improve overall eyesight, translate signs on the fly, prompt wearers with the names of&nbsp;people they meet, create shared whiteboards, encourage healthy food choices, and allow selective hearing in crowded rooms. This type of AR will be, he said, “an assistant, connected to the Internet, sitting on your shoulders, and feeding you useful information to your ears and eyes when you need it.”</p> \n<!--nextpage--> \n<p>This vision, he indicated, isn’t arriving anytime soon, but it is achievable. The biggest roadblock, he said, is lowering the energy consumption of the hardware, along with reducing the heat that today’s processors emit.</p> \n<p>“The low-power design community is uniquely positioned to take the mantle and create the tools that let us realize this vision,” he said.</p> \n<p>Rabii had a couple of suggestions for approaches that chip developers could take. For one, he said, chips have to be better tailored to their intended&nbsp;uses.</p> \n<p>“Our use case,” he said, speaking about AR glasses, “is moderate performance but high-power efficiency, form factors that support stylish and lightweight designs, [and chip designs that are] mindful of temperature for user comfort.”</p> \n<p>Designers also need to think more realistically about how chips use energy. Energy consumption, he says, is mostly “determined by memory access and data movement. Data transfer is far more expensive than compute.” For example, he indicated, “fetching 1 byte from DRAM takes 12,000 times more energy than performing an 8-bit addition; sending 1 byte wirelessly takes 300,000 times more energy.”</p> \n<p>Hardware designers need to keep these differences in mind in the way they implement AI, Rabii says. “The prevalent model is to have a monolithic accelerator as a discrete compute element, with all AI workloads transferred to this element,” he said. “But this is a data transfer intensive architecture, which has implications for power consumption.”</p> \n<p>Better, he suggested, would be to “treat AI as a deeply embedded function and distribute it across all the compute” in a system. This type of architecture, he said, brings compute to data, so data doesn’t have to move around as much, dramatically saving power.</p> \n<p>There are other ways AI can be designed to use less energy, Rabii says. “Not every AI function needs the same precision,” he says. “A large percentage of the computational effort is required for the last percents of accuracy,” so breaking up workloads and reducing precision when possible can make AI systems far more efficient.</p> \n<p>That’s what designers can do now. In the future, he said, Facebook is looking forward to improvements in semiconductor process technologies that will lead to better performance per watt, as well as specialized accelerators that focus on specific types of AI for higher performance and better energy efficiency. Some of those advances, he hopes, will come from <a href=\"https://www.arm.com/\">Arm Holdings</a>&nbsp;and the Arm ecosystem.</p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=EtQ_JVz0FMI:8EAx1saPp9A:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=EtQ_JVz0FMI:8EAx1saPp9A:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=EtQ_JVz0FMI:8EAx1saPp9A:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=EtQ_JVz0FMI:8EAx1saPp9A:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=EtQ_JVz0FMI:8EAx1saPp9A:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=EtQ_JVz0FMI:8EAx1saPp9A:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/EtQ_JVz0FMI\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 09 Oct 2019 17:26:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5MDMzNw.jpeg","linkMd5":"522bcd2fb6d5cf0d6950cfc6b4675a85","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn58@2020_2/2020/08/25/10-37-58-812_4c92b939334d08ef.webp","destWidth":1240,"destHeight":930,"sourceBytes":99777,"destBytes":35694,"author":"Tekla S. Perry","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=EtQ_JVz0FMI:8EAx1saPp9A:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn49@2020_4/2020/08/25/10-38-15-043_6f8775a7b1300911.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=EtQ_JVz0FMI:8EAx1saPp9A:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn33@2020_2/2020/08/25/10-38-11-230_2c6efeb6dacbfbf7.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/EtQ_JVz0FMI":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn78@2020_4/2020/08/25/10-38-01-674_1b5d89bcbe4796ee.webp"},"publishedOrCreatedDate":1598351859729},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Boston Dynamics' Atlas Robot Shows Off New Gymnastics Skills","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.robobusiness.com/\">RoboBusiness 2019</a> –&nbsp;October 1-3, 2019 –&nbsp;Santa Clara, Calif., USA</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>You’ve almost certainly seen the new <a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a> and Atlas videos from <a href=\"https://www.bostondynamics.com/\">Boston Dynamics</a>, if for no other reason than we <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale\">posted about Spot’s commercial availability</a>&nbsp;earlier this week. But what, are we supposed to NOT include them in Video Friday anyway? Psh! Here you go:</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/wlkCQXHEgjA\" width=\"620\"></iframe></p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_sBBaNYex3E?rel=0\" width=\"620\"></iframe></p> \n <p>[ <a href=\"https://www.bostondynamics.com/spot\">Boston Dynamics</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Eight deadly-looking&nbsp;robots. One Giant Nut trophy. Tonight is the BattleBots season finale, airing on Discovery, 8 p.m. ET, or check your <a href=\"https://battlebots.com/local/\">local channels</a>.</p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/wIObm_ZGjjI\" width=\"620\"></iframe></p> \n  <p>[ <a href=\"https://battlebots.com/event/ep15d-2/\">BattleBots</a> ]</p> \n  <p><em>Thanks Trey!</em></p> \n  <hr> \n   <p>Speaking of battling robots… Having <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/watch-the-worlds-first-giant-robot-fight\">giant robots fight each other</a> is one of those things that sounds really great in theory, but doesn’t work out so well in reality. And sadly, MegaBots is having to deal with reality, which means putting their <a href=\"https://www.ebay.com/itm/133181448480\">giant fighting robot up on eBay</a>.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/dyUVlRzC1G4\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>As of Friday afternoon, the current bid is just over $100,000 with a week to go.</p> \n   <p>[ <a href=\"https://www.megabots.com/\">MegaBots</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>Michigan Engineering has figured out the secret formula to getting 150,000 views on YouTube: drone plus <strong>nail gun</strong>.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GA445Flxkjo\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"https://news.umich.edu/roofing-drone-nails-down-shingles/\">Michigan Engineering</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Michael Burke from the University of Edinburgh writes:</p> \n     <blockquote> \n      <p><em>We’ve been learning to scoop grapefruit segments using a <a href=\"https://robots.ieee.org/robots/pr2/?utm_source=spectrum\">PR2</a>, by \"feeling\" the difference between peel and pulp. We use joint torque measurements to predict the probability that the knife is in the peel or pulp, and use this to apply feedback control to a nominal cutting trajectory learned from human demonstration, so that we remain in a position of maximum uncertainty about which medium we’re cutting. This means we slice along the boundary between the two mediums. It works pretty well!</em></p> \n     </blockquote> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/FNony5cUujA\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"https://arxiv.org/abs/1909.07247\">Paper</a> ] via [ <a href=\"http://rad.inf.ed.ac.uk/\">Robust Autonomy and Decisions Group</a> ]</p> \n     <p><em>Thanks Michael!</em></p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>Hey look, it’s Jan with eight <a href=\"https://robots.ieee.org/robots/emys/?utm_source=spectrum\">EMYS</a>&nbsp;robot heads. Hi, Jan! Hi, EMYSes!</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/NQyK6TnYY5A\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://emys.co/\">EMYS</a>&nbsp;]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>We’re putting the KRAKEN Arm through its paces, demonstrating that it can unfold from an Express Rack locker on the International Space Station and access neighboring lockers in NASA’s FabLab system to enable transfer of materials and parts between manufacturing, inspection, and storage stations. The KRAKEN arm will be able to change between multiple ’end effector’ tools such as grippers and inspection sensors - those are in development so they’re not shown in this video.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Ph7thfcO4P0\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"http://www.tethers.com/KRAKEN.html\">Tethers Unlimited</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <blockquote> \n         <p><em>UBTECH’s Alpha Mini Robot with Smart Robot’s “Maatje” software is offering healthcare service to children at Praktijk Intraverte Multidisciplinary Institution in Netherlands. </em></p> \n         <p><em>This institution is using Alpha Mini in counseling children’s behavior. Alpha Mini can move and talk to children and offers games and activities to stimulate and interact with them. Alpha Mini talks, helps and motivates children thereby becoming more flexible in society.</em></p> \n        </blockquote> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/zK7kD7q5f7M\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"http://www.ubtechedu.com/\">UBTECH</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>Some impressive work here from Anusha Nagabandi, Kurt Konoglie, Sergey Levine, Vikash Kumar at Google Brain, training a dexterous multi-fingered hand to do that thing with two balls that I’m really bad at.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/nlJUOn3O1Ew\" width=\"620\"></iframe></p> \n         <p></p> \n         <blockquote> \n          <p><em>Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in online model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills -- and that too, on a 24-DoF anthropomorphic hand in the real world, using just 2-4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects.</em></p> \n         </blockquote> \n         <p>[ <a href=\"https://sites.google.com/view/pddm/\">PDDM</a> ]</p> \n         <p><em>Thanks Vikash!</em></p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p><a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-cmu-two-armed-ballbot\">CMU’s Ballbot</a> has a deceptively light touch that’s ideal for leading people around.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tbdWu6k_NL8\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>A paper on this has been submitted to IROS 2019.</p> \n          <p>[ <a href=\"http://www.msl.ri.cmu.edu/projects/ballbot/\">CMU</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>The Autonomous Robots Lab at the University of Nevada is sharing some of the work they’ve done on path planning and exploration for aerial robots during the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit\">DARPA SubT Challenge</a>.</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ZvUedi5mzN8\" width=\"620\"></iframe></p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/yosb2CUbirM?rel=0\" width=\"620\"></iframe></p> \n           <p>[ <a href=\"https://www.autonomousrobotslab.com/\">Autonomous Robots Lab</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p>More proof that anything can be a drone if you staple some motors to it. Even 32 feet of styrofoam insulation.</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/C0qWfxvmm1E\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.youtube.com/watch?v=C0qWfxvmm1E\">YouTube</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>Whatever you think of military drones, we can all agree that they look cool.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/oW9V5tIMGzQ\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"http://www.boeing.com/defense/mq25/\">Boeing</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p>I appreciate the fact that <a href=\"https://robots.ieee.org/robots/icub/?utm_source=spectrum\">iCub</a> has eyelids, I really do, but sometimes, it ends up looking kinda sleepy in research videos.</p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/X-XwPxNrjFo\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://epfl-lasa.github.io/iCub-Assistant/\">EPFL LASA</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <blockquote> \n                <p><em>Video shows autonomous flight of a lightweight aerial vehicle outdoors and indoors on the campus of Carnegie Mellon University. The vehicle is equipped with limited onboard sensing from a front-facing camera and a proximity sensor. The aerial autonomy is enabled by utilizing a 3D prior map built in Step 1.</em></p> \n               </blockquote> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/INqwj8fNYNQ\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://frc.ri.cmu.edu/~zhangji/\">CMU</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <blockquote> \n                 <p><em>The Stanford Space Robotics Facility allows researchers to test innovative guidance and navigation algorithms on a realistic frictionless, underactuated system.</em></p> \n                </blockquote> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/nZ0hOH_UxMU\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>[ <a href=\"http://asl.stanford.edu/\">Stanford ASL</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <blockquote> \n                  <p><em>In this video, Ian and CP discuss <a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty</a>’s many capabilities including robust locomotion, obstacle avoidance, 3D mapping/SLAM, face detection and recognition, sound localization, hardware extensibility, photo and video capture, and programmable personality. They also talk about some of the skills he’s built using these capabilities (and others) and how those skills can be expanded upon by you. </em></p> \n                 </blockquote> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/wrm7bMJ0ml0\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <p>[ <a href=\"https://www.mistyrobotics.com/\">Misty Robotics</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <p>This week’s CMU RI Seminar comes from Aaron Parness at Caltech and NASA JPL, on “Robotic Grippers for Planetary Applications.”</p> \n                  <p></p> \n                  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/VBxzEkg_axs\" width=\"620\"></iframe></p> \n                  <p></p> \n                  <blockquote> \n                   <p><em>The previous generation of NASA missions to the outer solar system discovered salt water oceans on Europa and Enceladus, each with more liquid water than Earth – compelling targets to look for extraterrestrial life. Closer to home, JAXA and NASA have imaged sky-light entrances to lava tube caves on the Moon more than 100 m in diameter and ESA has characterized the incredibly varied and complex terrain of Comet 67P. While JPL has successfully landed and operated four rovers on the surface of Mars using a 6-wheeled rocker-bogie architecture, future missions will require new mobility architectures for these extreme environments. Unfortunately, the highest value science targets often lie in the terrain that is hardest to access. This talk will explore robotic grippers that enable missions to these extreme terrains through their ability to grip a wide variety of surfaces (shapes, sizes, and geotechnical properties). To prepare for use in space where repair or replacement is not possible, we field-test these grippers and robots in analog extreme terrain on Earth. Many of these systems are enabled by advances in autonomy. The talk will present a rapid overview of my work and a detailed case study of an underactuated rock gripper for deflecting asteroids.</em></p> \n                  </blockquote> \n                  <p>[ <a href=\"https://www.ri.cmu.edu/event/ri-seminar-aaron-parness-nasa-manager-robotic-climbers-grippers-group-2019-09-20/\">CMU</a> ]</p> \n                  <p></p> \n                  <hr> \n                   <p></p> \n                   <p><a href=\"https://rodneybrooks.com/\">Rod Brooks</a> gives some of the best robotics talks ever. He gave this one earlier this week at UC Berkeley, on “Steps Toward Super Intelligence and the Search for a New Path.”</p> \n                   <p></p> \n                   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/CcxG0IFssGg\" width=\"620\"></iframe></p> \n                   <p></p> \n                   <p>[ <a href=\"https://eecs.berkeley.edu/news\">UC Berkeley</a> ]</p> \n                   <p></p> \n                   <hr> \n                    <p></p> \n                    <div class=\"feedflare\"> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=osC6qndPrDk:KfkAlR-pyo0:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=osC6qndPrDk:KfkAlR-pyo0:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=osC6qndPrDk:KfkAlR-pyo0:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=osC6qndPrDk:KfkAlR-pyo0:-BTjWOF_DHI\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=osC6qndPrDk:KfkAlR-pyo0:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=osC6qndPrDk:KfkAlR-pyo0:gIN9vFwOqvQ\" border=\"0\" /></a> \n                    </div> \n                    <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/osC6qndPrDk\" height=\"1\" width=\"1\" alt=\"\" /> \n                   </hr> \n                  </hr> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 27 Sep 2019 20:20:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4NDI2OA.jpeg","linkMd5":"b050441ca360dcb3e9e825f306284a0b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn82@2020_5/2020/08/25/10-37-58-985_8fd194d8e4e3ec58.webp","destWidth":1240,"destHeight":698,"sourceBytes":166088,"destBytes":94580,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=osC6qndPrDk:KfkAlR-pyo0:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx/cdn70@2020_6/2020/08/25/10-38-13-297_8f87e48e85665a43.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=osC6qndPrDk:KfkAlR-pyo0:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn17@2020_4/2020/08/25/10-38-14-169_0f610a8a27030f59.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/osC6qndPrDk":"https://cdn.jsdelivr.net/gh/myreaderx/cdn64@2020_4/2020/08/25/10-38-05-003_a32755e96e8b1608.webp"},"publishedOrCreatedDate":1598351859731},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Electronic License Plates for Drones May Come Soon","link":"https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid","description":"<style type=\"text/css\"> \n <!--\n\t\t@page { margin: 0.79in }\n\t\tP { margin-bottom: 0.08in }\n\t\tA:link { so-language: zxx }\n\t--> </style> \n<p>Drones may soon carry electronic license plates, thanks to new guidelines for a <a href=\"https://www.faa.gov/uas/research_development/remote_id/\">remote ID</a> system for unmanned aircraft recently submitted for approval.</p> \n<p></p> \n<p><span>The newly proposed standard, </span><span lang=\"zxx\"><a href=\"https://www.astm.org/DATABASE.CART/WORKITEMS/WK65041.htm\"><span>ASTM WK65041</span></a></span><span>, sets up guidelines for how drones can identify themselves to remote observers, as well as for how to set up systems to read that data. Developed with input from civil aviation authorities such as the U.S. Federal Aviation Administration (FAA) and Transport Canada, as well as leaders in the drone industry, the would be standard was submitted to global standards organization ASTM International (originally the American Society for Testing and Materials) on 5 September and will be out for ballot on 9 September.</span></p> \n<p></p> \n<p><span>“We’re working with a lot of drone manufacturers on this standard—<a href=\"https://spectrum.ieee.org/automaton/aerospace/aviation/dji-promises-to-add-airsense-to-its-new-drones\">DJI</a>, for instance,”&nbsp;says Philip Kenul, chairman of ASTM International’s Committee F38 on Unmanned Aircraft Systems, which helped develop the newly proposed standard. “Many other companies are working with us as well, such as <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/wing-officially-launches-australian-drone-delivery-service\">Google Wing</a> and <a href=\"https://spectrum.ieee.org/view-from-the-valley/robotics/drones/drone-control-heres-how-amazon-thinks-drones-should-fit-into-us-airspace\">Amazon</a>, and will comply with the standard.”</span></p> \n<!--nextpage--> \n<p></p> \n<p><span>The standard’s developers envision drones continuously broadcasting ID data via WiFi or Bluetooth as part of the messages such technologies normally transmit to allow other devices to discover and link with the broadcasting device. These ID signals are readable from a distance of 350 to 450 meters. </span></p> \n<p></p> \n<p><span>“Intel has done studies showing that when Bluetooth 5 comes out, we could expect a range of up to a kilometer,”&nbsp;Kenul says.</span></p> \n<p></p> \n<p><span>Apps on smartphones or other devices can then connect to the Internet to look up data on the drone. The public will likely only be able to read the drone's ID number, which might be the drone's serial number, or a registration number from the FAA or other civil aviation authorities. Law enforcement can get more information about the drone from its ID data, such as its latitude, longitude, altitude, speed, direction, and takeoff point, plus its owner and operator data, and the stated purpose of the flight.</span></p> \n<p></p> \n<p>“You can just take a phone, point it up at a drone and get its electronic ID if you see it doing something dangerous. [And you can report it]&nbsp;just like reporting a car’s license plate number,”&nbsp;Kenul says.</p> \n<p></p> \n<p>The drone can also be identified if it is conn<span>ected to the Internet—say, over a cellular link through its operator. This option can prove helpful if an observer does not have the remote ID app, if the drone is out of range or moving too quickly for the app to acquire its Bluetooth or WiFi signals, or if high humidity in the air is degrading the Bluetooth or WiFi signals.</span></p> \n<p></p> \n<p>New drones can come automatically outfitted with such electronic license plates. Old drones can be retrofitted with ID chips that would plug into their USB port<span>s and might cost $7 to $12, Kenul says. When it comes to radio-controlled model aircraft, the remote pilots of these vehicles can use a smartphone app to report the ID of the aircraft and the location and time of operation to remote ID authorities.</span></p> \n<p></p> \n<p>Remote ID systems are key to the development of a UAS traffic management (<span lang=\"zxx\"><a href=\"https://spectrum.ieee.org/view-from-the-valley/robotics/drones/drone-control-heres-how-amazon-thinks-drones-should-fit-into-us-airspace\">UTM</a></span>) system, Kenul says. “Everyone knows UTM is important for drone applications, from package delivery to medical delivery, to urban area inspections, to operations after a hurricane,”&nbsp;he notes. “Remote ID is the first building block for UTM.”</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JTny_7tR99s:hOFx3iF1s6w:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JTny_7tR99s:hOFx3iF1s6w:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JTny_7tR99s:hOFx3iF1s6w:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JTny_7tR99s:hOFx3iF1s6w:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JTny_7tR99s:hOFx3iF1s6w:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JTny_7tR99s:hOFx3iF1s6w:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/JTny_7tR99s\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 10 Sep 2019 19:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3MDkwNw.png","linkMd5":"783ad2e174faa245c24d60ad66d167ad","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn2@2020_5/2020/08/25/10-37-59-098_c04232e391ce18d3.webp","destWidth":1240,"destHeight":930,"sourceBytes":1339469,"destBytes":106406,"author":"Charles Q. Choi","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JTny_7tR99s:hOFx3iF1s6w:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn64@2020_1/2020/08/25/10-38-07-985_da2a04d2969b5d82.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JTny_7tR99s:hOFx3iF1s6w:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn92@2020_2/2020/08/25/10-38-07-219_248dcc35caf86b97.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/JTny_7tR99s":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn11@2020_2/2020/08/25/10-38-05-198_199abf13df2695f4.webp"},"publishedOrCreatedDate":1598351859733},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Drones as Detectives: Surveying Crime Scenes for Evidence","link":"https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence","description":"<link href=\"/ns/interactive/0118race-to-5g/css/5g-logo-treatments.css\" rel=\"stylesheet\"> \n <style type=\"text/css\">aside.inlay.xlrg.XploreFree { font-family: \"Georgia\", serif; border-width: 4px 0; border-top: solid #888; border-bottom: solid #888; padding: 10px 0; font-size: 19px; font-weight: bold; text-align: center; } span.FreeRed { color: red; text-transform: uppercase; font-family: \"Theinhardt-Medium\", sans-serif; } span.XploreBlue { color: #03a6e3; font-family: \"Theinhardt-Medium\", sans-serif; } </style> \n <div class=\"mobileHide\"> \n  <div class=\"imgWrapper offsetLeft lt sm\"> \n   <a href=\"https://spectrum.ieee.org/static/journal-watch\"><img alt=\"Journal Watch report logo, link to report landing page\" src=\"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg\" /></a> \n  </div> \n </div> \n <p>When detectives and other forensics specialists arrive at a crime scene, there is a pressing need to survey the area quickly. Environmental disturbances such as wind or an incoming tide could ruin valuable evidence, and even the investigators themselves are at risk of contaminating the crime scene. Could a fleet of evidence-surveying drones be of help?</p> \n <p>Pompílio Araújo, a criminal expert for the Federal Police of Brazil, is responsible for recording crime scenes exactly as found. In his other role as a researcher at the Intelligent Vision Research Lab at Federal University of Bahia, he is trying to make his first job easier by developing drones that can—very quickly—home in on a piece of evidence and record it from multiple angles.</p> \n <!--nextpage--> \n <p>The drone system, dubbed AirCSI, <a href=\"https://www.youtube.com/watch?v=yAKYUTczc9k&amp;feature=youtu.be\">starts by scanning</a> a crime scene with one big sweep, using a stereo camera and a&nbsp;<a href=\"https://www.youtube.com/watch?v=y8kN7TeycC4\">visual self-localization and mapping</a> (SLAM) system to monitor the drone’s position.</p> \n <figure class=\"lt med\" role=\"img\"> \n  <img alt=\"The AirCSI drone system\" src=\"https://spectrum.ieee.org/image/MzM4ODc1Mw.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo:&nbsp;Pompilio Araujo \n   </figcaption> </img> \n </figure> \n <p>“Initially, the drone [flies] at a height that can take a broad view of the crime scene and detect some larger pieces of evidence,” explains Araújo, who published this preliminary stage of research in a previous <a href=\"https://ieeexplore.ieee.org/document/8010419\">study</a>. The drone system was first trained to detect guns, but could be trained to identify other types of weapons or evidence&nbsp;such as blood stains.</p> \n <p>In his <a href=\"https://ieeexplore.ieee.org/document/8848476\">latest work</a>, described in <em><a href=\"https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8859\">IEEE Geoscience and Remote Sensing Letters</a></em>, Araújo and his colleagues developed a secondary system for&nbsp;the drone, which includes a second camera, trained to image evidence from multiple angles. In this newer version, AirCSI <a href=\"https://www.youtube.com/watch?v=T1A0DhdWtP4&amp;feature=youtu.be\">calculates</a> a circular area for each piece of evidence that’s initially identified, considering its relevance and size. The drone system then calculates a zigzag trajectory and makes a series of additional sweeps in order&nbsp;to collect more detailed data on each piece of evidence.</p> \n <aside class=\"inlay XploreFree xlrg\"> \n  <a href=\"https://ieeexplore.ieee.org/document/8848476\">Read this article for <span class=\"FreeRed\">free</span> on <span class=\"XploreBlue\">IEEE <em>Xplore</em></span> until 3 November 2019</a> \n </aside> \n <p>“As a result, AirCSI provides a sketch with the localization of the evidences, as well as a detailed crime scene imagery,” says Araújo. His team used simulation software to test this newer version of AirCSI, and found that&nbsp;using multiple angles to detect evidence is up to 18 percent&nbsp;more effective than using only one angle.</p> \n <p>While the researchers have yet to test&nbsp;the new, multi-angle approach beyond simulations, they expect to try&nbsp;it out&nbsp;in a real environment by the end of this year&nbsp;or early next year.</p> \n <p>Meanwhile, they are working to overcome a few of the system’s limitations. For example, AirCSI can currently operate <span>only&nbsp;</span>in open environments with no obstacles. “In the future, we plan to do anti-collision routines and training for more criminal evidences,” Araújo says.</p> \n <p>He also plans on developing a way to completely reconstruct crime scenes using the drone footage, creating a virtual environment that investigators can explore indefinitely—or at least until the crime is solved.</p> \n <div class=\"mobileShow\"> \n  <h3 class=\"RptHdBackBarMobile\"><span class=\"BackArrowBlkBkgrd\">&lt;</span>&nbsp;<a href=\"https://spectrum.ieee.org/static/journal-watch\">Back to IEEE Journal Watch</a></h3> \n </div> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aO_Hjssz2eE:qUvcfntImLA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aO_Hjssz2eE:qUvcfntImLA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aO_Hjssz2eE:qUvcfntImLA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aO_Hjssz2eE:qUvcfntImLA:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aO_Hjssz2eE:qUvcfntImLA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aO_Hjssz2eE:qUvcfntImLA:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/aO_Hjssz2eE\" height=\"1\" width=\"1\" alt=\"\" /> \n</link>","descriptionType":"html","publishedDate":"Tue, 08 Oct 2019 16:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4ODc2MQ.jpeg","linkMd5":"b19d13bf54f71c1c40ce0f7d38f1a58b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn6@2020_6/2020/08/25/10-37-59-935_fddbd50ffa89c32b.webp","destWidth":1240,"destHeight":930,"sourceBytes":202779,"destBytes":110400,"author":"Michelle Hampson","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn28@2020_5/2020/08/25/10-38-09-369_964adaa282b1aac4.webp","https://spectrum.ieee.org/image/MzM4ODc1Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn34@2020_5/2020/08/25/10-38-16-602_8b71174fc7786a92.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aO_Hjssz2eE:qUvcfntImLA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn12@2020_6/2020/08/25/10-38-06-839_294634c6ed5f7bf4.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aO_Hjssz2eE:qUvcfntImLA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn80@2020_3/2020/08/25/10-38-06-652_d21e605947a48734.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/aO_Hjssz2eE":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn92@2020_2/2020/08/25/10-38-02-881_d01f013c443c7f87.webp"},"publishedOrCreatedDate":1598351859729},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"It Shouldn’t Be This Hard to Responsibly Fly a Drone","link":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","description":"<p>A few weeks ago, I went home to Oregon to visit my family. On a whim, I brought along my <a href=\"https://robots.ieee.org/robots/anafi/?utm_source=spectrum\">Parrot Anafi drone</a>—it’s <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone\">small and lightweight</a> and uses the same USB-C charger as my laptop, so it was easy to toss into my carry-on. Like you do in Oregon, our plans were to go river rafting, hiking, and kayaking, and I figured I’d pack the drone and try and find some interesting opportunities to fly it.</p> \n<p>I try hard to be a responsible drone owner. I don’t have my U.S.&nbsp;Federal Aviation Administration&nbsp;<a href=\"https://spectrum.ieee.org/aerospace/aviation/how-to-become-an-faaapproved-drone-pilot\">Part 107 certificate</a> yet, but my drone is registered and labeled, and I’m an <a href=\"https://www.modelaircraft.org/\">AMA</a> member, which includes insurance for recreational drone pilots. I also do my best to make sure that the places I fly are places that I’m allowed to fly, and fortunately, <a href=\"https://www.faa.gov/uas/recreational_fliers/where_can_i_fly/b4ufly/\">the FAA has a handy app</a> that is supposed to make that easy.</p> \n<p>And it would have been easy, if I didn’t bother to check whether what the app was telling me was accurate or not. But I did check, and as it turns out, the app is, in many situations, worse than useless.</p> \n<!--nextpage--> \n<p>Finding somewhere to legally and safely fly a drone for recreational purposes can be tricky. Most people who buy a drone to fly recreationally don’t want to have to worry about airspace restrictions and all that stuff. The FAA knows this, and the agency has invested what I can only assume is a lot of resources into educating new drone owners about how to be responsible pilots. It’s easy enough to just buy a cheap drone and ignore the FAA completely and fly it wherever you want, but setting aside willful ignorance, the FAA seems to understand that most folks are willing to obey common sense rules in the name of safety as long as it’s easy to do so.</p> \n<figure class=\"rt med-sm\" role=\"img\"> \n <img alt=\"FAA B4UFLY app\" src=\"https://spectrum.ieee.org/image/MzM2NTg5OQ.png\"> \n  <figcaption class=\"hi-cap\">\n    Image: Google Play \n  </figcaption> \n  <figcaption>\n    Screenshot of FAA’s B4UFLY app on Google&nbsp;Play store. \n  </figcaption> </img> \n</figure> \n<p>The problem is that the <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/a-weird-time-for-drone-operators\">rules that govern recreational drone operations often aren’t common sense and aren’t at all easy to access</a>. To help with this, <a href=\"https://www.faa.gov/uas/recreational_fliers/where_can_i_fly/b4ufly/\">the FAA has developed an app called B4UFLY</a> where you can tell it your location and it’ll let you know whether you’re “good to go” or whether there are flight restrictions in place. It’s a wonderful idea, and I imagine it would be very effective, if it didn’t have some serious issues.</p> \n<p>Don’t get me wrong—the app itself is relatively fast and easy to use and seems to function largely as intended. The issue is that it presents itself, explicitly, as a tool that can be used “<em><a href=\"https://www.faa.gov/uas/recreational_fliers/where_can_i_fly/b4ufly/\">so that recreational flyers know whether it is safe to fly their drone</a></em>” featuring “<em><a href=\"https://www.faa.gov/uas/recreational_fliers/where_can_i_fly/b4ufly/\">a clear ‘status’&nbsp;indicator that informs the operator whether it is safe to fly or not</a></em>.”</p> \n<p>But buried in a PDF FAQ (<a href=\"https://www.faa.gov/uas/b4ufly/media/UAS_B4UFLY_QandA.pdf\">now offline</a>) about the app is this:</p> \n<blockquote> \n <p><em>“Additionally, there may be local laws or ordinances about flying unmanned aircraft affecting your intended flight that are not reflected in this app. It is the responsibility of the operator to know the rules and fly safely at all times.”</em></p> \n</blockquote> \n<p>And oh boy is that a huge responsibility that the app itself doesn’t even mention, and that enormous loophole means that the B4UFLY app’s “good to go” indicator is not just meaningless but in fact giving you the wrong idea entirely.</p> \n<p>To illustrate the problem, here are some specific examples of places I wanted to fly my drone around Oregon. In each case, the FAA’s B4YFLY app (<a href=\"https://kittyhawk.io/b4ufly/\">powered by Kittyhawk</a>) was telling me that I was “good to go” when I really wasn’t. I’ve also included whether other drone safety mapping services (<a href=\"https://www.dji.com/flysafe/geo-map\">DJI FlySafe</a> and <a href=\"https://app.airmap.io/\">Airmap</a>) thought that it was safe to fly there, along with what it took to figure out the actual rules:</p> \n<h4><strong>City of Portland Public Parks</strong><br> •&nbsp;FAA B4UFly App:&nbsp;<span>Good to Go</span><br> •&nbsp;DJI FlySafe: No Advisories<br> •&nbsp;Airmap: <span>No Advisories</span><br> •&nbsp;Searching portlandoregon.gov for \"drone\": No info<br> •&nbsp;Searching portlandoregon.gov for regulations that include the word \"aircraft\" and then reading through them for something that <a href=\"http://https://www.portlandoregon.gov/citycode/article/147794\">might apply to drones</a>: Prohibited</br></br></br></br></br></h4> \n<h4><strong>Oregon State Parks</strong><br> •&nbsp;FAA B4UFly App: Good to Go<br> •&nbsp;DJI FlySafe: No Advisories<br> •&nbsp;Airmap: No Advisories<br> •&nbsp;Searching oregonstateparks.org for \"drone\": No info<br> •&nbsp;<a href=\"https://www.statesmanjournal.com/story/travel/outdoors/2017/04/11/oregon-state-parks-drones-elliott-state-forest-leslie-knope-lisa-sumption/100302996/\">Interview with Oregon Parks and Recreation Department director from 2017 where she says they’re working on a three-tier approach to drone use that will be published online</a>, and then searching for <em>that</em>: No info</br></br></br></br></br></h4> \n<figure class=\"rt med\" role=\"img\"> \n <img alt=\"B4UFLY\" src=\"https://spectrum.ieee.org/image/MzM2NTY3Nw.png\"> \n  <figcaption class=\"hi-cap\">\n    Image: FAA B4UFLY app \n  </figcaption> \n  <figcaption>\n    B4UFLY app showing “Good to go” over the Rogue River wilderness area. \n  </figcaption> </img> \n</figure> \n<h4><strong>Rogue River (Wilderness Area)</strong><br> •&nbsp;FAA B4UFly App: Good to Go<br> •&nbsp;DJI FlySafe: No Advisories<br> •&nbsp;Airmap: Prohibited<br> •&nbsp;Searching fs.fed.us for \"<a href=\"https://www.fs.fed.us/managing-land/fire/aviation/uas/responsible-use\">wilderness drone</a>\": Prohibited</br></br></br></br></h4> \n<h4><strong>Rogue River (Wild and Scenic Area)</strong><br> •&nbsp;FAA B4UFly App: Good to Go<br> •&nbsp;DJI FlySafe: No Advisories<br> •&nbsp;Airmap: No Advisories<br> •&nbsp;Searching fs.fed.us for \"wild and scenic drone\": No info<br> •&nbsp;Searching rivers.gov for \"drone\": No info<br> •&nbsp;<a href=\"https://www.fs.usda.gov/detail/hmnf/news-events/?cid=FSEPRD491278\">Huron-Manistee National Forest (in Michigan) press release that I found on Google that seems to apply to all Wild and Scenic Rivers</a>: Prohibited</br></br></br></br></br></br></h4> \n<p>The issue here is that for all of these places, the FAA’s B4UFLY app shows you a friendly green “good to go” message when in reality, flying in any of these areas is either explicitly prohibited or would require explicit permission. You could argue that this is <strong>worse</strong> than no app at all, because the app is actively giving you bad information. You are not, in fact, good to go, and if you’re already going, you should stop immediately.</p> \n<aside class=\"inlay pullquote xlrg\">\n  FAA’s B4UFLY app shows you a friendly green&nbsp;“good to go”&nbsp;message when in reality flying in these areas is either explicitly prohibited or would require explicit permission. You could argue that this is&nbsp;worse&nbsp;than no app at all, because the app is actively giving you bad information. \n</aside> \n<p>We asked the FAA for comment on this, and the agency sent us the following statement:</p> \n<blockquote> \n <p><em>The B4UFLY app provides information regarding airspace access for recreational drone flyers. It provides land use information for take offs and landings for National Park Service lands, but it does not provide information regarding take offs and landings are allowed on other federal, state, and locally managed lands and parks. Operators are expected to make themselves aware of any additional land use or police restrictions that may exist in the area where they wish to fly.</em>&nbsp;</p> \n</blockquote> \n<p>What the FAA is saying, I think, is that the B4UFLY app provides information about <em>airspace</em>, which is all that the FAA is allowed to regulate. The FAA can’t regulate anything on the ground, which is where most other drone regulations come in—when a city or state says “you can’t fly drones here,” what they’re actually saying is, “you can’t take off or land a drone from here because we own this land.”</p> \n<p>However, I have two issues with this. The first issue is that the FAA threw National Parks Service lands into the B4UFLY app, which (as they say in their comment above) is explicitly about “land use” regulations, not something the FAA has jurisdiction over, as far as I know. In fact, the National Parks Service itself acknowledges that <a href=\"https://www.nps.gov/articles/unmanned-aircraft-in-the-national-parks.htm\">it’s not technically illegal to fly drones over national parks, as long as you launch, operate, and land from outside the park boundary</a>. So, if the FAA is able to add national parks restrictions to the B4UFLY app, why not wilderness areas and other federal lands?</p> \n<p>The second issue here is a much more fundamental one—recreational drone pilots put their trust in the FAA to tell them where they can and cannot fly. When the FAA itself presents the B4UFLY app as a tool that can be used so that “recreational flyers know whether it is safe to fly their drone,” that’s exactly what it should do. Instead, the app provides only one very limited kind of information about recreational drone safety, without telling the user that it’s on them to somehow dig up all the rest of the information that may or may not affect their flight. And based on my experience in Oregon, it can be a frustrating process that’s frequently inconclusive.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Drone\" src=\"https://spectrum.ieee.org/image/MzM2NTY3Mg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman \n  </figcaption> \n  <figcaption>\n    Taken by a Parrot Anafi drone in Gifford Pinchot National Forest. \n  </figcaption> </img> \n</figure> \n<p>In general, there’s a difference between not being prohibited from flying somewhere, and having explicit permission to fly somewhere, and I feel like responsible drone owners should always try and obtain explicit permission to fly. By “explicit permission,” I mean something somewhere that says, “the folks in charge of this area say that it’s okay for you to fly here.” This may be inconvenient, but it’s the best way of making sure that what you’re doing is safe and respectful.</p> \n<p>Drones are so cheap and pervasive now that the only way to make sure that people fly them safely is to make the rules clear, concise, and easy to follow. The FAA seems to be trying to do this, but it’s simply not good enough. At the absolute minimum, the B4UFLY app should not tell users that they’re “good to go” unless they are flying from an area where drone use is explicitly permitted, like national forests. Anywhere else, users should be instructed to verify that their local laws allow drone use. Is that going to be a huge annoyance that drives users away from the app? Of course. But it’s the truth, and if the FAA doesn’t like that, they should work with local governments to put the necessary information into the app instead.</p> \n<p><em>A version of this post appears in the October 2019 print magazine as “Drone Pilots Beware.”</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=vedFlE8nubE:lU7iMmkn8mI:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=vedFlE8nubE:lU7iMmkn8mI:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=vedFlE8nubE:lU7iMmkn8mI:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=vedFlE8nubE:lU7iMmkn8mI:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=vedFlE8nubE:lU7iMmkn8mI:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=vedFlE8nubE:lU7iMmkn8mI:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/vedFlE8nubE\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 03 Sep 2019 20:44:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2NTYzOQ.jpeg","linkMd5":"da5f551943891d387e1f1c5f2532d096","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn77@2020_5/2020/08/25/10-38-01-032_635a012d3c9fd410.webp","destWidth":2000,"destHeight":1466,"sourceBytes":54847,"destBytes":36744,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2NTg5OQ.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn90@2020_4/2020/08/25/10-38-15-309_6d6ed66b3098be41.webp","https://spectrum.ieee.org/image/MzM2NTY3Nw.png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn7@2020_2/2020/08/25/10-38-03-995_9df673b238a7d0ee.webp","https://spectrum.ieee.org/image/MzM2NTY3Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_2/2020/08/25/10-38-17-132_7d054e7be221498a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=vedFlE8nubE:lU7iMmkn8mI:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn49@2020_4/2020/08/25/10-38-16-779_a839930022e4e739.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=vedFlE8nubE:lU7iMmkn8mI:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn38@2020_3/2020/08/25/10-38-20-611_c54559fc5b456489.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/vedFlE8nubE":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn48@2020_2/2020/08/25/10-38-07-103_bf8e4127f2922455.webp"},"publishedOrCreatedDate":1598351859734},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Labrador Systems Developing Affordable Assistive Robots for the Home","link":"https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home","description":"<p>Developing robots for the home is still a challenge, especially if you want those robots to interact with people and help them do practical, useful things. However, the potential markets for home robots are huge, and one of the most compelling markets is for home robots that can assist humans who need them. Today, <a href=\"https://labradorsystems.com/\">Labrador Systems</a>, a startup based in California, is announcing a pre-seed funding round of $2 million (led by <a href=\"https://sosv.com/\">SOSV</a>’s hardware accelerator <a href=\"https://hax.co\">HAX</a> with participation from <a href=\"https://developer.amazon.com/alexa-fund\">Amazon’s Alexa Fund</a> and <a href=\"https://www.irobot.com/about-irobot/company-information/ventures\">iRobot Ventures</a>, among others) with the goal of expanding development and conducting pilot studies of&nbsp; “a new [assistive robot] platform for supporting home health.”</p> \n<!--nextpage--> \n<p>Labrador was founded two years ago by <a href=\"https://www.linkedin.com/in/mike-dooley/\">Mike Dooley</a> and <a href=\"https://www.linkedin.com/in/nikolai-romanov-8852871/\">Nikolai Romanov</a>. Both Mike and Nikolai have backgrounds in consumer robotics at Evolution Robotics and iRobot, but as an ’80s gamer, Mike’s bio (or at least the parts of <a href=\"https://www.linkedin.com/in/mike-dooley/\">his bio on LinkedIn</a>) caught my attention: From 1995 to 1997, Mike worked at Brøderbund Software, helping to manage play testing for games like <em>Myst</em> and <em>Riven</em> and the <em>Where in the World is Carmen San Diego</em> series. He then spent three years at Lego as the product manager for <a href=\"https://robots.ieee.org/robots/mindstorms/?utm_source=spectrum\">MindStorms</a>. After doing some marginally less interesting things, Mike was the VP of product development at Evolution Robotics from 2006 to 2012, where he led the team that developed the Mint floor sweeping robot. <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/irobot-sweeps-up-evolution-robotics-for-74-million\">Evolution was acquired by iRobot in 2012</a>, and Mike ended up as the VP of product development over there until 2017, when he co-founded Labrador.</p> \n<p>I was pretty much sold at <em>Where in the World is Carmen San Diego</em> (the original version of which I played from a 5.25” floppy on my dad’s Apple IIe)<sup>*</sup>, but as you can see from all that other stuff, Mike knows what he’s doing in robotics as well.</p> \n<p>And according to Labrador’s press release, what they’re doing is this:</p> \n<blockquote> \n <p><em>Labrador Systems is an early stage technology company developing a new generation of assistive robots to help people live more independently. The company’s core focus is creating affordable solutions that address practical and physical needs at a fraction of the cost of commercial robots. … Labrador’s technology platform offers an affordable solution to improve the quality of care while promoting independence and successful aging.&nbsp;</em></p> \n</blockquote> \n<p>Labrador’s personal robot, the company’s first offering, will enter pilot studies in 2020.</p> \n<p>That’s about as light on detail as a press release gets, but there’s a bit more on Labrador’s website, including:</p> \n<ul> \n <li>Our core focus is creating affordable solutions that address practical and physical needs. (we are not a social robot company)</li> \n <li>By affordable, we mean products and technologies that will be available at less than 1/10th the cost of commercial robots.&nbsp;</li> \n <li>We achieve those low costs by fusing the latest technologies coming out of augmented reality with robotics to move things in the real world.</li> \n</ul> \n<p>The only hardware we’ve actually seen from Labrador at this point is a demo that they put together for <a href=\"https://labradorsystems.com/news/f/labrador-is-at-remars\">Amazon’s re:MARS conference</a>, which took place a few months ago, showing a “demonstration project” called Smart Walker:</p> \n<p><iframe allow=\"autoplay; fullscreen\" allowfullscreen=\"\" frameborder=\"0\" height=\"360\" src=\"https://player.vimeo.com/video/339546319?title=0&amp;byline=0&amp;portrait=0\" width=\"640\"></iframe></p> \n<p>This isn’t the home assistance robot that Labrador got its funding for, but rather a demonstration of some of their technology. So of course, the question is, what’s Labrador working on, then? It’s still a secret, but Mike Dooley was able to give us a few more details.</p> \n<p><strong><em>IEEE Spectrum:</em> Your website shows a smart walker concept—how is that related to the assistive robot that you’re working on?</strong></p> \n<p><strong>Mike Dooley:</strong> The smart walker was a request from a major senior living organization to have our robot (which is really good at navigation) guide residents from place to place within their communities. To test the idea with residents, it turned out to be much quicker to take the navigation system from the robot and put it on an existing <a href=\"https://en.wikipedia.org/wiki/Walker_(mobility)#Rollators\">rollator walker</a>. So when you see the clips of the technology in the smart walker video on our website, that’s actually the robot’s navigation system localizing in real time and path planning in an environment.</p> \n<p><strong>“Assistive robot” can cover a huge range of designs and capabilities—can you give us any more detail about your robot, and what it’ll be able to do?</strong></p> \n<p>One of the core features of our robot is to help people move things where they have difficulty moving themselves, particularly in the home setting. That may sound trivial, but to someone who has impaired mobility, it can be a major daily challenge and negatively impact their life and health in a number of ways. Some examples we repeatedly hear are people not staying hydrated or taking their medication on time simply because there is a distance between where they are and the items they need. Once we have those base capabilities, i.e. the ability to navigate around a home and move things within it, then the robot becomes a platform for a wider variety of applications.</p> \n<p><strong>What made you decide to develop assistive robots, and why are robots a good solution for seniors who want to live independently?</strong></p> \n<p>Supporting independent living has been seen as a massive opportunity in robotics for some time, but also as something off in the future. The turning point for me was watching my mother enter that stage in her life and seeing her transition to using a cane, then a walker, and eventually to a wheelchair. That made the problems very real for me. It also made things much clearer about how we could start addressing specific needs with the tools that are becoming available now.</p> \n<p>In terms of why robots can be a good solution, the basic answer is the level of need is so overwhelming that even helping with “basic” tasks can make an appreciable difference in the quality of someone’s daily life. It’s also very much about giving individuals a degree of control back over their environment. That applies to seniors as well as others whose world starts getting more complex to manage as their abilities become more impaired.</p> \n<p><strong>What are the particular challenges of developing assistive robots, and how are you addressing them? Why do you think there aren’t more robotics startups in this space?</strong></p> \n<p>The setting (operating in homes and personal spaces) and the core purpose of the product (aiding a wide variety of individuals) bring a lot of complexity to any capability you want to build into an assistive robot. Our approach is to put as much structure as we can into the system to make it functional, affordable, understandable and reliable.</p> \n<p>I think one of the reasons you don’t see more startups in the space is that a lot of roboticists want to skip ahead and do the fancy stuff, such as taking on human-level capabilities around things like manipulation. Those are very interesting research topics, but we think those are also very far away from being practical solutions you can productize for people to use in their homes.</p> \n<p><strong>How do you think assistive robots and human caregivers should work together?</strong></p> \n<p>The ideal scenario is allowing caregivers to focus more of their time on the high-touch, personal side of care. The robot can offload the more basic support tasks as well as extend the impact of the caregiver for the long hours of the day they can’t be with someone at their home. We see that applying to both paid care providers as well as the 40 million unpaid family members and friends that provide assistance.</p> \n<p>The robot is really there as a tool, both for individuals in need and the people that help them. What’s promising in the research discussions we’ve had so far, is that even when a caregiver is present, giving control back to the individual for simple things can mean a lot in the relationship between them and the caregiver.</p> \n<p><strong>What should we look forward to from Labrador in 2020?</strong></p> \n<p>Our big goal in 2020 is to start placing the next version of the robot with individuals with different types of needs to let them experience it naturally in their own homes and provide feedback on what they like, what don’t like and how we can make it better. We are currently reaching out to companies in the healthcare and home health fields to participate in those studies and test specific applications related to their services. We plan to share more detail about those studies and the robot itself as we get further into 2020.</p> \n<hr> \n <p>If you’re an organization (or individual) who wants to possibly try out Labrador’s prototype, the company encourages you to connect with them through their website. And as we learn more about what Labrador is up to, we’ll have updates for you, presumably in 2020.</p> \n <p>[ <a href=\"https://labradorsystems.com/\">Labrador Systems</a> ]</p> \n <p><sub>* I just lost an hour of my life after finding out that you can play <em>Where in the World is Carmen San Diego</em> <a href=\"https://archive.org/details/msdos_Where_in_the_World_is_Carmen_Sandiego_1985\">in your browser for free</a>.</sub></p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rgt5dqPWMbA:TjT0erajUXU:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rgt5dqPWMbA:TjT0erajUXU:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rgt5dqPWMbA:TjT0erajUXU:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rgt5dqPWMbA:TjT0erajUXU:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rgt5dqPWMbA:TjT0erajUXU:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rgt5dqPWMbA:TjT0erajUXU:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rgt5dqPWMbA\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Tue, 15 Oct 2019 14:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5MjkwNw.jpeg","linkMd5":"5f68fac164425c5c46c2a07001ed834c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn94@2020_3/2020/08/25/10-37-58-815_e72fabfa413eae06.webp","destWidth":1240,"destHeight":843,"sourceBytes":365014,"destBytes":95172,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rgt5dqPWMbA:TjT0erajUXU:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn68@2020_2/2020/08/25/10-38-13-393_a544e65fc7a4275f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rgt5dqPWMbA:TjT0erajUXU:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn68@2020_2/2020/08/25/10-38-10-584_d0b837a4d9c7122b.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rgt5dqPWMbA":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn7@2020_2/2020/08/25/10-38-07-705_f9bfd414f656657c.webp"},"publishedOrCreatedDate":1598351859728},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"This Startup Is Building Giant Robots to Grow Our Food","link":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food","description":"<p>We humans spend most of our time getting hungry or eating, which must be really inconvenient for the people who have to produce food for everyone. For a sustainable and tasty future, we’ll need to make the most of what we’ve got by growing more food with less effort, and that’s where the robots can help us out a little bit.</p> \n<p><a href=\"https://farmwise.io/\">FarmWise</a>, a California-based startup, is looking to enhance farming efficiency by automating everything from seeding to harvesting, starting with the worst task of all: weeding. And they’ve just raised US $14.5 million to do it.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/zYurqd7yUYs?rel=0\" width=\"620\"></iframe></p> \n<blockquote> \n <p><em>FarmWise’s autonomous, AI-enabled robots are designed to solve farmers’ most pressing challenges by performing a variety of farming functions - starting with weeding, and providing personalized care to every plant they touch. Using machine learning models, computer vision and high-precision mechanical tools, FarmWise’s sophisticated robots cleanly pick weeds from fields, leaving crops with the best opportunity to thrive while eliminating harmful chemical inputs. To date, FarmWise’s robots have efficiently removed weeds from more than 10 million plants.</em></p> \n</blockquote> \n<p>FarmWise is not the first company to work on large mobile farming robots. A few years ago, we wrote about <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/bosch-deepfield-robotics-weed-control\">DeepField Robotics and their giant weed-punching robot</a>. But considering how many humans there are, and how often we tend to get hungry, it certainly seems like there’s plenty of opportunity to go around.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"FarmWise robot\" src=\"https://spectrum.ieee.org/image/MzM3NTYzMg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: FarmWise \n  </figcaption> \n  <figcaption>\n    FarmWise is collecting massive amounts of data about every single plant in an entire field, which is something that hasn’t been possible before. Above, one of the robots at a farm in&nbsp;Salinas Valley, Calif. \n  </figcaption> </img> \n</figure> \n<p>Weeding is just one thing that farm robots are able to do. FarmWise is collecting massive amounts of data about every single plant in an entire field, practically on the per-leaf level, which is something that hasn’t been possible before. Data like this could be used for all sorts of things, but generally, the long-term hope is that robots could tend to every single plant individually—weeding them, fertilizing them, telling them what good plants they are, and then mercilessly yanking them out of the ground at absolute peak ripeness. It’s not realistic to do this with human labor, but it’s the sort of data-intensive and monotonous task that robots could be ideal for.&nbsp;</p> \n<p>The question with robots like this is not necessarily whether they can do the job that they were created for, because generally, they can—farms are structured enough environments that they lend themselves to autonomous robots, and the tasks are relatively well defined. The issue right now, I think, is whether robots are really time- and cost-effective for farmers. Capable robots are an expensive investment, and even if there is a shortage of human labor, will robots perform well enough to convince farmers to adopt the technology? That’s a solid maybe, and here’s hoping that FarmWise can figure out how to make it work.</p> \n<p>[ <a href=\"https://www.farmwise.io/\">FarmWise</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=1bD2VSOAgqs:Tu0j_hk2JoM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=1bD2VSOAgqs:Tu0j_hk2JoM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=1bD2VSOAgqs:Tu0j_hk2JoM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=1bD2VSOAgqs:Tu0j_hk2JoM:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=1bD2VSOAgqs:Tu0j_hk2JoM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=1bD2VSOAgqs:Tu0j_hk2JoM:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/1bD2VSOAgqs\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 17 Sep 2019 16:35:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3NjkxNQ.jpeg","linkMd5":"d8a32c6887301ef3975606092a06d695","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn63@2020_3/2020/08/25/10-38-00-247_d89b9a0963551bbc.webp","destWidth":2048,"destHeight":1431,"sourceBytes":661233,"destBytes":464868,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3NTYzMg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn83@2020_3/2020/08/25/10-38-06-639_58dbbb19ca518a25.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=1bD2VSOAgqs:Tu0j_hk2JoM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn85@2020_6/2020/08/25/10-38-16-208_5b4ba18e4f860c55.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=1bD2VSOAgqs:Tu0j_hk2JoM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn18@2020_6/2020/08/25/10-38-02-430_d05d29cdf18daebd.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/1bD2VSOAgqs":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn53@2020_4/2020/08/25/10-38-15-042_3146c91eae6a0cc9.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Water Jet Powered Drone Takes Off With Explosions","link":"https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions","description":"<p>At ICRA 2015, the <a href=\"https://www.imperial.ac.uk/aerial-robotics\">Aerial Robotics Lab</a> at the Imperial College London <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/novel-wings-and-jet-thrusters-for-swimming-flying-robots\">presented a concept for a multimodal flying swimming robot called AquaMAV</a>. The really difficult thing about a flying and swimming robot isn’t so much the transition from the first to the second, since you can manage that even if your robot is completely dead (thanks to gravity), but rather the other way: going from water to air, ideally in a stable and repetitive way. The AquaMAV concept solved this by basically just applying as much concentrated power as possible to the problem, using <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-squid-and-robot-scallop-showcase-bioinspired-underwater-propulsion\">a jet thruster to hurl the robot out of the water with quite a bit of velocity to spare</a>.</p> \n<p>In a paper appearing in <em>Science Robotics</em> this week, the roboticists behind AquaMAV present a fully operational robot that uses a solid-fuel powered chemical reaction to generate an explosion that powers the robot into the air.</p> \n<!--nextpage--> \n<p>The 2015 version of AquaMAV, which was mostly just some very vintage-looking computer renderings and a little bit of hardware, used a small cylinder of CO<sub>2</sub> to power its water jet thruster. This worked pretty well, but the mass and complexity of the storage and release mechanism for the compressed gas wasn’t all that practical for a flying robot designed for long-term autonomy. It’s a familiar challenge, especially for pneumatically powered soft robots—how do you efficiently generate gas on-demand, especially if you need a lot of pressure all at once?</p> \n<h3>An explosion propels the drone out of the water</h3> \n<p>There’s one obvious way of generating large amounts of pressurized gas all at once, and that’s explosions. <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/squishy-pink-robot-makes-explosive-jumps\">We’ve seen robots use explosive thrust for mobility before</a>, at a variety of scales, and it’s very effective as long as you can both properly harness the explosion and generate the fuel with a minimum of fuss, and this latest version of AquaMAV manages to do both:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/aJU8EL61NgA?rel=0\" width=\"620\"></iframe></p> \n<p>The water jet coming out the back of this robot aircraft is being propelled by a gas explosion. The gas comes from the reaction between a little bit of calcium carbide powder stored inside the robot, and water. Water is mixed with the powder one drop at a time, producing acetylene gas, which gets piped into a combustion chamber along with air and water. When ignited, the acetylene air mixture explodes, forcing the water out of the combustion chamber and providing up to 51 N of thrust, which is enough to launch the 160-gram robot 26 meters up and over the water at 11 m/s. It takes just 50 mg of calcium carbide (mixed with 3 drops of water) to generate enough acetylene for each explosion, and both air and water are of course readily available. With 0.2 g of calcium carbide powder on board, the robot has enough fuel for multiple jumps, and the jump is powerful enough that the robot can get airborne even under fairly aggressive sea conditions.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Water-air hybrid MAV\" src=\"https://spectrum.ieee.org/image/MzM3Mjk5OQ.png\"> \n  <figcaption class=\"hi-cap\">\n    Image: Science Robotics \n  </figcaption> \n  <figcaption>\n    The robot can transition from a floating state to an airborne jetting phase and back to floating (A). A 3D model render of the underside of the robot (B) shows the electronics capsule. The capsule contains the fuel tank (C), where calcium carbide reacts with air and&nbsp;water to propel the vehicle. \n  </figcaption> </img> \n</figure> \n<h3>Next step:&nbsp;<span>getting the robot to fly autonomously</span></h3> \n<p>Providing adequate thrust is just one problem that needs to be solved when attempting to conquer the water-air transition with a fixed-wing robot. The overall design of the robot itself is a challenge as well, because the optimal design and balance for the robot is quite different in each phase of operation, as the paper describes:</p> \n<blockquote> \n <p><em>For the vehicle to fly in a stable manner during the jetting phase, the center of mass must be a significant distance in front of the center of pressure of the vehicle. However, to maintain a stable floating position on the water surface and the desired angle during jetting, the center of mass must be located behind the center of buoyancy. For the gliding phase, a fine balance between the center of mass and the center of pressure must be struck to achieve static longitudinal flight stability passively. During gliding, the center of mass should be slightly forward from the wing’s center of pressure.&nbsp;</em></p> \n</blockquote> \n<p>The current version is mostly optimized for the jetting phase of flight, and doesn’t have any active flight control surfaces yet, but the researchers are optimistic that if they added some they’d have no problem getting the robot to fly autonomously. It’s just a glider at the moment, but a low-power propeller is the obvious step after that, and to get really fancy, a switchable gearbox could enable efficient movement on water as well as in the air. Long-term, the idea is that robots like these would be useful for tasks like autonomous water sampling over large areas, but I’d personally be satisfied with a remote controlled version that I could take to the beach.</p> \n<h4>“Consecutive aquatic jump-gliding with water-reactive fuel,” by R. Zufferey, A. Ortega Ancel, A. Farinha, R. Siddall, S. F. Armanini, M. Nasr, R. V. Brahmal, G. Kennedy, and M. Kovac from Imperial College in London, is published in the current issue of <em>Science Robotics</em>.</h4> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tbvl8qcvgVM:bcb_4nyhACk:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tbvl8qcvgVM:bcb_4nyhACk:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tbvl8qcvgVM:bcb_4nyhACk:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tbvl8qcvgVM:bcb_4nyhACk:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tbvl8qcvgVM:bcb_4nyhACk:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tbvl8qcvgVM:bcb_4nyhACk:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/tbvl8qcvgVM\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 12 Sep 2019 19:45:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3MzAzNQ.jpeg","linkMd5":"d38b2f1b0a483a0b8124b15d386f7c04","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn73@2020_5/2020/08/25/10-37-58-852_23982c065623be53.webp","destWidth":1240,"destHeight":857,"sourceBytes":728043,"destBytes":108186,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3Mjk5OQ.png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn95@2020_4/2020/08/25/10-38-08-318_07ef09cd60357624.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tbvl8qcvgVM:bcb_4nyhACk:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn39@2020_6/2020/08/25/10-38-03-889_bbd48ce76503c3c4.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tbvl8qcvgVM:bcb_4nyhACk:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn29@2020_1/2020/08/25/10-38-13-168_8f47435f0f325d73.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/tbvl8qcvgVM":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn56@2020_5/2020/08/25/10-38-06-326_3d1a49f95449594c.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"U.S. Energy Department is First Customer for World’s Biggest Chip","link":"https://spectrum.ieee.org/tech-talk/computing/hardware/us-energy-department-is-first-customer-for-worlds-biggest-chip","description":"<p><a href=\"https://www.anl.gov/\">Argonne National Laboratory</a> and <a href=\"https://www.llnl.gov/\">Lawrence Livermore National Laboratory</a> will be among the first organizations to install AI computers made from the <a href=\"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built\">largest silicon chip ever built</a>. Last month, <a href=\"https://www.cerebras.net/\">Cerebras Systems</a> unveiled a 46,225-square millimeter chip with 1.2 trillion transistors designed to speed the training of neural networks. Today, such training is often done in large data centers using GPU-based servers. Cerebras plans to begin selling computers based on the notebook-size chip in the 4th quarter of this year.</p> \n<!--nextpage--> \n<p>“The opportunity to incorporate the largest and fastest AI chip ever—the Cerebras WSE—into our advanced computing infrastructure will enable us to dramatically accelerate our deep learning research in science, engineering, and health” Rick Stevens, head of computing at Argonne National Laboratory, said in a press release. “It will allow us to invent and test more algorithms, to more rapidly explore ideas, and to more quickly identify opportunities for scientific progress.”</p> \n<p>Argonne and Lawrence Livermore are the first DOE entities to participate in what is expected to be a multi-year, multi-lab partnership. Cerebras plans to expand to other laboratories in the coming months.</p> \n<p><span>Cerebras computers will be integrated into existing supercomputers at the two DOE labs to act as AI accelerators for those machines. In 2021,&nbsp;</span>Argonne plans to&nbsp;become&nbsp;home to the United States’ first exascale computer, named Aurora; it will be&nbsp;capable of more than 1 billion billion calculations per second. Intel and Cray are the leaders on that $500 million project. The national laboratory is already home to <a href=\"https://www.top500.org/system/177718\">Mira, the 24th</a>-most powerful supercomputer in the world, and <a href=\"https://www.top500.org/system/178926\">Theta, the 28th</a>-most powerful. Lawrence Livermore is also on track to achieve exascale with <a href=\"https://computing.llnl.gov/newsroom/doennsa-lab-announce-partnership-cray-develop-nnsa%E2%80%99s-first-exascale-supercomputer\">El Capitan</a>, a $600-million, 1.5-exaflop machine set to go live in late 2022. The lab is also home to the <a href=\"https://www.top500.org/system/179398\">number-two-ranked Sierra supercomputer</a> and the <a href=\"https://www.top500.org/system/179567\">number-10-ranked Lassen</a>.</p> \n<p>The U.S. Energy Department established the <a href=\"https://www.energy.gov/articles/secretary-perry-stands-office-artificial-intelligence-and-technology\">Artificial Intelligence and Technology Office</a> earlier this month to better <a href=\"https://www.energy.gov/science-innovation/artificial-intelligence-and-technology-office\">take advantage of AI</a> for solving the kinds of&nbsp; problems the U.S. national laboratories tackle.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aU1T0j0-G7U:4jQqdmb0lgM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aU1T0j0-G7U:4jQqdmb0lgM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aU1T0j0-G7U:4jQqdmb0lgM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aU1T0j0-G7U:4jQqdmb0lgM:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=aU1T0j0-G7U:4jQqdmb0lgM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aU1T0j0-G7U:4jQqdmb0lgM:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/aU1T0j0-G7U\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 17 Sep 2019 15:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3NTE5Ng.jpeg","linkMd5":"f98c1554d57da1ae3c3120fbc73c502e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn86@2020_6/2020/08/25/10-38-00-448_4376dd538513a1aa.webp","destWidth":1240,"destHeight":930,"sourceBytes":696562,"destBytes":411318,"author":"Samuel K. Moore","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aU1T0j0-G7U:4jQqdmb0lgM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_3/2020/08/25/10-38-03-086_f186c3d87c8d1ee7.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aU1T0j0-G7U:4jQqdmb0lgM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_4/2020/08/25/10-38-08-155_fc3e6a972230359e.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/aU1T0j0-G7U":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn87@2020_1/2020/08/25/10-38-09-121_bb441d1060cfc2a1.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Boston Dynamics' Spot Robot Dog Goes on Sale","link":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","description":"<p><a href=\"https://www.bostondynamics.com/\">Boston Dynamics</a> is announcing this morning that <a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a>, its versatile quadruped robot, is now for sale. The machine’s animal-like behavior regularly electrifies crowds at tech conferences, and like other Boston Dynamics’ robots, Spot is a YouTube sensation whose <a href=\"https://www.youtube.com/user/BostonDynamics/videos\">videos</a> amass millions of views.</p> \n<p></p> \n<p>Now anyone interested in buying a Spot—or a pack of them—can go to the company’s <a href=\"https://www.bostondynamics.com/spot\">website</a> and submit an <a href=\"https://www.bostondynamics.com/spot-sales\">order form</a>. But don’t&nbsp;pull out your credit card just yet. Spot may cost as much as a luxury car, and it is not really available to consumers. The initial sale, described as an “early adopter program,” is targeting businesses. Boston Dynamics wants to find customers in select industries and help them deploy Spots in real-world scenarios.</p> \n<!--nextpage--> \n<p></p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/wlkCQXHEgjA\" width=\"620\"></iframe></p> \n<p>“What we’re doing is the productization of Spot,” Boston Dynamics CEO <a href=\"https://www.linkedin.com/in/marc-raibert-a520b9119/\">Marc Raibert</a> tells <em>IEEE Spectrum</em>. “It’s really a milestone for us going from robots that work in the lab to these that are hardened for work out in the field.”</p> \n<p></p> \n<p>Boston Dynamics has always been a secretive company, but last month, in preparation for launching Spot (formerly SpotMini), it allowed our photographers into its headquarters in Waltham, Mass., for a special shoot. In that session, we captured Spot and also&nbsp;<a href=\"https://robots.ieee.org/robots/atlas2016/\">Atlas</a>—the company’s highly dynamic humanoid—in action, walking, climbing, and jumping.</p> \n<p></p> \n<p>You can see <a href=\"https://robots.ieee.org/?utm_source=spectrum\">Spot’s photo interactives on our Robots Guide</a>. (The Atlas interactives will appear in coming weeks.)</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Animated gif of Boston Dynamics' Spot walking\" src=\"https://spectrum.ieee.org/image/MzM4MDc0Mw.gif\"> \n  <figcaption class=\"hi-cap\">\n    Gif: Bob O’Connor/Robots.ieee.org \n  </figcaption> </img> \n</figure> \n<p></p> \n<p>And if you’re in the market for a robot dog, here’s everything we know about Boston Dynamics’ plans for Spot.</p> \n<p></p> \n<h3>Who can buy a Spot?</h3> \n<p>If you’re interested in one, you should go to <a href=\"https://www.bostondynamics.com/spot\">Boston Dynamics’ website</a> and take a look at the information the company requires from potential buyers. Again, the focus is on businesses. Boston Dynamics says it wants to get Spots out to initial customers that “either have a compelling use case or a development team that we believe can do something really interesting with the robot,” says VP of business development <a href=\"https://www.linkedin.com/in/michael-patrick-perry-4160b11/\">Michael Perry</a>. “Just because of the scarcity of the robots that we have, we’re going to have to be selective about which partners we start working together with.”</p> \n<p></p> \n<p></p> \n<h3>What can Spot do?</h3> \n<p>As you’ve probably seen on the YouTube videos, Spot can <a href=\"https://robots.ieee.org/robots/spotmini/?gallery=interactive2\">walk</a>, trot, avoid obstacles, <a href=\"https://robots.ieee.org/robots/spotmini/?gallery=interactive4\">climb stairs</a>, and much more. The robot’s hardware is almost completely custom, with powerful compute boards for control, and five sensor modules located on every side of Spot’s body, allowing it to survey the space around itself from any direction. The legs are powered by 12 custom motors with a reduction, with a top speed of 1.6 meters per second. The robot can operate for 90 minutes on a charge. In addition to the basic configuration, you can integrate up to 14 kilograms of extra hardware to a payload interface. Among the payload packages Boston Dynamics plans to offer are a 6 degrees-of-freedom arm, a version of which can be seen in some of the YouTube <a href=\"https://robots.ieee.org/robots/spotmini/?gallery=video6\">videos</a>, and a ring of cameras called SpotCam that could be used to create Street View–type images inside buildings.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Boston Dynamics' Spot and its hardware payload\" src=\"https://spectrum.ieee.org/image/MzM4MDgxMQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Boston Dynamics \n  </figcaption> </img> \n</figure> \n<p></p> \n<p></p> \n<h3>How do you control Spot?</h3> \n<p>Learning to drive the robot using its gaming-style controller “takes 15 seconds,” says CEO Marc Raibert. He explains that while teleoperating Spot, you may not realize that the robot is doing a lot of the work. “You don’t really see what that is like until you’re operating the joystick and you go over a box and you don’t have to do anything,” he says. “You’re practically just thinking about what you want to do and the robot takes care of everything.” The control methods have evolved significantly since the company’s first quadruped robots, machines like <a href=\"https://robots.ieee.org/robots/bigdog/\">BigDog</a> and <a href=\"https://robots.ieee.org/robots/alphadog/\">LS3</a>. “The control in those days was much more monolithic, and now we have what we call a sequential composition controller,” Raibert says, “which lets the system have control of the dynamics in a much broader variety of situations.” That means that every time one of Spot’s feet touches or doesn’t touch the ground, this&nbsp;different state of the body affects the basic physical behavior of the robot, and the controller adjusts accordingly. “Our controller is designed to understand what that state is and have different controls depending upon the case,” he says.</p> \n<p></p> \n<p></p> \n<h3>How much does Spot cost?</h3> \n<p>Boston Dynamics would not give us specific details on prices, saying only that potential customers should contact them for a quote and that there is going to be a leasing&nbsp;option. It’s understandable: As with any expensive and complex product, prices can vary on a case by case basis and depend on factors such as configuration, availability, level of support, and so forth. When we pressed the company for at least an approximate base price, Perry answered: “Our general guidance is that the total cost of the early adopter program lease will be less than the price of a car—but how nice a car will depend on the number of Spots leased and how long the customer will be leasing the robot.”</p> \n<p></p> \n<p></p> \n<h3>Can Spot do mapping and SLAM out of the box?</h3> \n<p>The robot’s perception system includes cameras and 3D sensors (there is no lidar), used to avoid obstacles and sense the terrain so it can climb stairs and walk over rubble. It’s also used to create 3D maps. According to Boston Dynamics, the first software release will offer just teleoperation. But a second <span>release</span>, to be available in the next few weeks, will enable more autonomous behaviors. For example, it will be able to do mapping and autonomous navigation—similar to what the company demonstrated in a <a href=\"https://www.youtube.com/watch?v=Ve9kWX_KXus\">video</a> last year, showing how you can drive the robot through an environment, create a 3D point cloud of the environment, and then set waypoints within that map for Spot to go out and execute that mission. For customers that have their own autonomy stack and are interested in using those on Spot, Boston Dynamics made it “as plug and play as possible in terms of how third-party software integrates into Spot’s system,” Perry says. This is done mainly via an API.</p> \n<p></p> \n<p></p> \n<h3>How does Spot’s API works?</h3> \n<p>Boston Dynamics built an API so that customers can create application-level products with Spot without having to deal with low-level control processes. “Rather than going and building joint-level kinematic access to the robot,” Perry explains, “we created a high-level API and SDK that allows people who are used to Web app development or development of missions for drones to use that same scope, and they’ll be able to build applications for Spot.”</p> \n<p></p> \n<p></p> \n<h3>What applications should we see first?</h3> \n<p>Boston Dynamics envisions <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-spot-robots-pull-a-truck-and-more\">Spot as a platform</a>: a versatile mobile robot that companies can use to build applications based on their needs. What types of applications? The company says the best way to find out&nbsp;is to put Spot&nbsp;in the hands of as many users as possible and let them develop the applications. Some possibilities include performing remote data collection and light manipulation in construction sites; monitoring sensors and infrastructure at oil and gas sites; and carrying out dangerous missions such as bomb disposal and hazmat inspections. There are also other promising areas such as&nbsp;security, package delivery, and even entertainment. “We have some initial guesses about which markets could benefit most from this technology, and we’ve been engaging with customers doing proof-of-concept trials,” Perry says. “But at the end of the day, that value story is really going to be determined by people going out and exploring and pushing the limits of the robot.”</p> \n<p></p> \n<p></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Boston Dynamics' Spot posing\" src=\"https://spectrum.ieee.org/image/MzM4MDc3OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Bob O'Connor \n  </figcaption> </img> \n</figure> \n<h3>How many Spots have been produced?</h3> \n<p>Last June, Boston Dynamics said it was planning to build about a hundred&nbsp;Spots by the end of the year, eventually ramping up production to <a href=\"https://youtu.be/iZD6hkRwZKM?t=939\">a thousand units per year</a>&nbsp;by the middle of this year. The company admits that it is not quite there yet. It has built close to a hundred beta units, which it has used to test and refine the final design. This version is now being mass manufactured, but the company is still “in the early tens of robots,” Perry says.</p> \n<h3>How did Boston Dynamics test Spot?</h3> \n<p></p> \n<p></p> \n<p></p> \n<p>The company has tested the robots during&nbsp;proof-of-concept trials with customers, and at least one is already using Spot to survey construction sites. The company has also done reliability tests at its facility in Waltham, Mass. “We drive around, not quite day and night, but hundreds of miles a week, so that we can collect reliability data and find bugs,” Raibert says.</p> \n<p></p> \n<h3>What about competitors?</h3> \n<p>In recent years, there’s been a proliferation of quadruped robots that will compete in the same space as Spot. The most prominent of these is <a href=\"https://robots.ieee.org/robots/anymal/?utm_source=spectrum\">ANYmal</a>, from <a href=\"https://www.anybotics.com/\">ANYbotics</a>, a Swiss company that spun out of ETH Zurich. Other quadrupeds include <a href=\"https://www.ghostrobotics.io/robots\">Vision</a> from Ghost Robotics, used by <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt\">one of the teams in the DARPA Subterranean Challenge</a>; and <a href=\"https://robots.ieee.org/robots/laikago/?utm_source=spectrum\">Laikago</a> and Aliengo from <a href=\"http://www.unitree.cc\">Unitree Robotics</a>, a Chinese startup. Raibert views the competition as a positive thing. “We’re excited to see all these companies out there helping validate the space,” he says. “I think we’re more in competition with finding the right need [that robots can satisfy] than we are with the other people building the robots at this point.”</p> \n<p></p> \n<p></p> \n<h3>Why is Boston Dynamics selling Spot now?</h3> \n<p>Boston Dynamics has long been an R&amp;D-centric firm, with most of its early funding coming from <a href=\"https://robots.ieee.org/robots/petman/?utm_source=spectrum\">military</a> <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/atlas-drc-robot-is-75-percent-new-completely-unplugged\">programs</a>, but it says commercializing robots has always been a goal. Productizing its machines probably accelerated when the <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/boston-dynamics-now-belongs-to-google\">company was acquired by Google’s parent company, Alphabet</a>, which had an ambitious (and now apparently very dead) robotics program. The commercial focus likely continued after <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft\">Alphabet sold Boston Dynamics to SoftBank</a>, whose famed CEO, <a href=\"https://en.wikipedia.org/wiki/Masayoshi_Son\">Masayoshi Son</a>, is known for his love of robots—and profits.</p> \n<p></p> \n<p></p> \n<h3>Which should I buy,&nbsp;Spot or Aibo?</h3> \n<p>Don’t laugh. We’ve gotten emails from individuals interested in purchasing a Spot for personal use after seeing our stories on the robot. Alas, Spot is not a bigger, fancier <a href=\"https://robots.ieee.org/robots/aibo2018/?utm_source=spectrum\">Aibo</a> pet robot. It’s an expensive, industrial-grade machine that requires development and maintenance. If you’re maybe Jeff Bezos you could probably convince Boston Dynamics to <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/amazon-home-robots\">sell you one</a>, but otherwise the company will prioritize businesses.</p> \n<p></p> \n<p></p> \n<h3>What’s next for Boston Dynamics?</h3> \n<p>On the commercial side of things, other than Spot, Boston Dynamics is interested in the logistics space. Earlier this year it announced the <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-warehouse-robots-acquires-kinema-systems\">acquisition of Kinema Systems</a>, a startup that had&nbsp;developed&nbsp;vision sensors and deep-learning software to enable industrial robot arms to locate and move boxes.&nbsp;There’s also <a href=\"https://robots.ieee.org/robots/handle/?utm_source=spectrum\">Handle</a>, the mobile robot on whegs (wheels + legs), that can pick up and move packages. Boston Dynamics is <a href=\"https://www.bostondynamics.com/careers\">hiring</a> both in Waltham, Mass., and Mountain View, Calif., where Kinema was located.</p> \n<p></p> \n<p></p> \n<h3>Okay, can I watch a cool video now?</h3> \n<p>During our visit to Boston Dynamics’ headquarters last month, we saw Atlas and Spot performing some cool new tricks that we unfortunately are not allowed to tell you about. We hope that, although the company is putting a lot of energy and resources into its commercial programs, Boston Dynamics will still find plenty of time to improve its robots, build new ones, and of course, keep making videos. <em>[Update: The company has just released a new Spot video, which we’ve&nbsp;embedded at the top of the&nbsp;post.][Update 2: Boston Dynamics has just released a second video, this time of Atlas doing some of those tricks we saw during our visit&nbsp;and couldn’t tell you about. Enjoy!]</em></p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_sBBaNYex3E\" width=\"620\"></iframe></p> \n<p>[ <a href=\"https://www.bostondynamics.com/spot\">Boston Dynamics</a> ]</p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=qqJG5_zGl_w:vBCzePmXCC8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=qqJG5_zGl_w:vBCzePmXCC8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=qqJG5_zGl_w:vBCzePmXCC8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=qqJG5_zGl_w:vBCzePmXCC8:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=qqJG5_zGl_w:vBCzePmXCC8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=qqJG5_zGl_w:vBCzePmXCC8:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/qqJG5_zGl_w\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 24 Sep 2019 14:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4MDY0OQ.jpeg","linkMd5":"8651f5e29ba3de30c127e3e854f0dee5","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn87@2020_2/2020/08/25/10-38-00-108_96f1d359e1e449c2.webp","destWidth":2048,"destHeight":1536,"sourceBytes":356816,"destBytes":133450,"author":"Erico Guizzo","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM4MDc0Mw.gif":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn79@2020_1/2020/08/25/10-38-06-501_8b043de815673445.webp","https://spectrum.ieee.org/image/MzM4MDgxMQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn38@2020_5/2020/08/25/10-38-13-153_0002fa76d54781bc.webp","https://spectrum.ieee.org/image/MzM4MDc3OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn63@2020_5/2020/08/25/10-38-04-054_06170fef0befacb3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=qqJG5_zGl_w:vBCzePmXCC8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn21@2020_5/2020/08/25/10-38-16-617_c88b56f167f37ec3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=qqJG5_zGl_w:vBCzePmXCC8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn37@2020_5/2020/08/25/10-38-14-972_bdaa53b18355098f.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/qqJG5_zGl_w":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn60@2020_6/2020/08/25/10-38-04-843_7183d51123c04ca5.webp"},"publishedOrCreatedDate":1598351859741},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Five routes to competitive advantage with AI","link":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","description":"<p><span>Use AI to gain actionable insights and drive business benefits across all use cases. Learn how with this whitepaper from HPE OEM Solutions.</span></p> \n<p></p> \n<div class=\"imgWrapper ct med\"> \n <img alt=\"earth\" src=\"https://spectrum.ieee.org/image/MzM1MzE3OA.jpeg\"> </img>\n</div> \n<p><br> &nbsp;</br></p> \n<p></p> \n<p></p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=eWooJQd8zew:ZqKXuPKCKkE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=eWooJQd8zew:ZqKXuPKCKkE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=eWooJQd8zew:ZqKXuPKCKkE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=eWooJQd8zew:ZqKXuPKCKkE:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=eWooJQd8zew:ZqKXuPKCKkE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=eWooJQd8zew:ZqKXuPKCKkE:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/eWooJQd8zew\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 15 Aug 2019 14:00:00 +0000","feedId":8606,"bgimg":"http://feedproxy.google.com/image/MzM1MzE3OA.jpeg","linkMd5":"975c3b1b81e23539106f318d7f3e28f7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"author":"","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1MzE3OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn61@2020_6/2020/08/25/10-38-16-022_1b6a6cab8e40218d.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=eWooJQd8zew:ZqKXuPKCKkE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn60@2020_1/2020/08/25/10-38-09-721_5b3dd062474aa283.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=eWooJQd8zew:ZqKXuPKCKkE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn91@2020_2/2020/08/25/10-38-03-530_d46d14a8db8eb1c0.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/eWooJQd8zew":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn95@2020_1/2020/08/25/10-38-09-967_2453bb1648f6fdb8.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Natural Language Processing Dates Back to Kabbalist Mystics","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","description":"<p><em>This is part one of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. </em></p> \n<p></p> \n<p></p> \n<p></p> \n<p>We’re in the middle of a boom time for <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">natural language processing</a> (NLP), the field of computer science that focuses on linguistic interactions between humans and machines. Thanks to advances in machine learning over the past decade, we’ve seen vast improvements in speech recognition and machine translation software. Language generators are now good enough to write coherent news articles, and virtual agents like Siri and Alexa are becoming part of our daily lives.</p> \n<p></p> \n<p>Most trace the origins of this field back to the beginning of the computer age, when <a href=\"https://spectrum.ieee.org/tag/Alan+Turing\">Alan Turing</a>, writing in 1950, <a href=\"https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-why-alan-turing-wanted-ai-to-make-mistakes\">imagined a smart machine</a> that could interact fluently with a human via typed text on a screen. For this reason, machine-generated language is mostly understood as a digital phenomenon—and a central goal of artificial intelligence (AI) research.</p> \n<p></p> \n<p>This six-part series will challenge that common understanding of NLP. In fact, attempts to design formal rules and machines that can analyze, process, and generate language go back hundreds of years.</p> \n<aside class=\"inlay pullquote rt med\">\n  Attempts to design formal rules and machines that can analyze, process, and generate language go back hundreds of years. \n</aside> \n<p>While specific technologies have changed over time, the basic idea of treating language as a material that can be artificially manipulated by rule-based systems has been pursued by many people in many cultures and for many different reasons. These historical experiments reveal the promise and perils of attempting to simulate human language in non-human ways—and they hold lessons for today’s practitioners of cutting-edge NLP techniques.&nbsp;</p> \n<p></p> \n<p>The story begins in medieval Spain. In the late 1200s, a Jewish mystic by the name of <a href=\"https://en.wikipedia.org/wiki/Abraham_Abulafia\">Abraham Abulafia</a> sat down at a table in his small house in Barcelona, picked up a quill, dipped it in ink, and began combining the letters of the Hebrew alphabet in strange and seemingly random ways. Aleph with Bet, Bet with Gimmel, Gimmel with Aleph and Bet, and so on.</p> \n<p></p> \n<!--nextpage--> \n<p>Abulafia called this practice “<a href=\"https://books.google.com/books?id=9mv10jdWKswC&amp;pg=PA133&amp;lpg=PA133&amp;dq=The+Science+of+the+Combination+of+Letters+abulafia&amp;source=bl&amp;ots=pU1fP8a5sg&amp;sig=ACfU3U2VVGqGvxuRLlGfM41KTEHjzU-mpA&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjLvdG5urXkAhWHtVkKHZC1CmAQ6AEwBnoECAkQAQ#v=onepage&amp;q=The%20Science%20of%20the%20Combination%20of%20Letters%20abulafia&amp;f=false\">the science of the combination of letters</a>.” He wasn’t actually combining letters at random; instead he was carefully following a secret set of rules that he had devised while studying an ancient Kabbalistic text called the <a href=\"https://en.wikipedia.org/wiki/Sefer_Yetzirah\"><em>Sefer Yetsirah</em></a>. This book describes how God created “all that is formed and all that is spoken” by combining Hebrew letters according to sacred formulas. In one section, God exhausts all possible two-letter combinations of the 22 Hebrew letters.</p> \n<p></p> \n<p>By studying the <em>Sefer Yetsirah</em>, Abulafia gained the insight that linguistic symbols can be manipulated with formal rules in order to create new, interesting, insightful sentences. To this end, he spent months generating thousands of combinations of the 22 letters of the Hebrew alphabet and eventually emerged with a series of books that he claimed were endowed with prophetic wisdom.</p> \n<p></p> \n<p>For Abulafia, generating language according to divine rules offered insight into the sacred and the unknown, or as he put it, allowed him to “grasp things which by human tradition or by thyself thou would not be able to know.”</p> \n<p></p> \n<aside class=\"inlay pullquote lt med\">\n  Combining letters to generate language allows thou to “grasp things which by human tradition or by thyself thou would not be able to know.” \n <span class=\"pq-attrib\">—Abraham Abulafia, mystic&nbsp;</span> \n</aside> \n<p>But other Jewish scholars considered this rudimentary language generation a dangerous act that bordered on the profane. The Talmud tells stories of rabbis who, by the magical act of permuting language according to the formulas set out in the <em>Sefer Yetsirah</em>, created artificial creatures called <a href=\"http://biology.kenyon.edu/slonc/bio3/2001projects/Golems/golem_html2.html\">golems</a>. In these tales, rabbis manipulated the letters of the Hebrew alphabet to replicate God’s act of creation, using the sacred formulas to imbue inanimate objects with life.</p> \n<p></p> \n<p>In some of these myths, the rabbis used this skill for practical reasons, to make animals to eat when hungry or servants to help them with domestic duties. But many of these golem stories end badly. In one particularly well-known fable, <a href=\"https://en.wikipedia.org/wiki/Judah_Loew_ben_Bezalel\">Judah Loew ben Bezalel</a>, the 16th century rabbi of Prague, used the sacred practice of letter combinatorics to conjure a golem to protect the Jewish community from antisemitic attacks, only to see the golem turn violently on him instead.</p> \n<p></p> \n<p>This “science of the combination of letters” was a rudimentary form of natural language processing, as it involved combining letters of the Hebrew alphabet according to specific rules. For Kabbalists, it was a double-edged sword: a way to access new forms of knowledge and wisdom, but also an inherently dangerous practice that could bring about unintended consequences.</p> \n<p></p> \n<p>This tension reappears throughout the long history of language processing, and still echoes in discussions about the most cutting-edge NLP technology of our digital era.</p> \n<p></p> \n<p><em>This is the first installment of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. Come back next Monday for part two, which brings us to the Enlightenment, when&nbsp;Gottfried Wilhelm Leibniz&nbsp;dreamed of a <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas\">machine that could calculate ideas</a>.&nbsp;</em></p> \n<p><em>You can also check out our prior series on the <a href=\"https://spectrum.ieee.org/tag/AI+history\">untold history of AI</a>.&nbsp;</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rV5PxZEF_hs:AbMFTJ0QdHo:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rV5PxZEF_hs:AbMFTJ0QdHo:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rV5PxZEF_hs:AbMFTJ0QdHo:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rV5PxZEF_hs:AbMFTJ0QdHo:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rV5PxZEF_hs:AbMFTJ0QdHo:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rV5PxZEF_hs:AbMFTJ0QdHo:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rV5PxZEF_hs\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 28 Oct 2019 13:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5OTYyMA.jpeg","linkMd5":"4116b47e7bcae1aed0f1e1c85e917d83","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn49@2020_4/2020/08/25/10-38-00-453_417a3d5a740fc8e5.webp","destWidth":1240,"destHeight":930,"sourceBytes":312864,"destBytes":189692,"author":"Oscar Schwartz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rV5PxZEF_hs:AbMFTJ0QdHo:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn86@2020_4/2020/08/25/10-38-12-777_f2424c39eec89855.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rV5PxZEF_hs:AbMFTJ0QdHo:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn32@2020_1/2020/08/25/10-38-04-741_ef212499c794e3b2.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rV5PxZEF_hs":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn10@2020_2/2020/08/25/10-38-14-537_69e9b213f0cc6a32.webp"},"publishedOrCreatedDate":1598351859729},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Robot Made of Clay Can Sculpt Its Own Body","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","description":"<link href=\"/ns/interactive/0118race-to-5g/css/5g-logo-treatments.css\" rel=\"stylesheet\"> \n <style type=\"text/css\">aside.inlay.xlrg.XploreFree { font-family: \"Georgia\", serif; border-width: 4px 0; border-top: solid #888; border-bottom: solid #888; padding: 10px 0; font-size: 19px; font-weight: bold; text-align: center; } span.FreeRed { color: red; text-transform: uppercase; font-family: \"Theinhardt-Medium\", sans-serif; } span.XploreBlue { color: #03a6e3; font-family: \"Theinhardt-Medium\", sans-serif; } </style> \n <div class=\"mobileHide\"> \n  <div class=\"imgWrapper offsetLeft lt sm\"> \n   <a href=\"https://spectrum.ieee.org/static/journal-watch\"><img alt=\"Journal Watch report logo, link to report landing page\" src=\"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg\" /></a> \n  </div> \n </div> \n <p>We’re very familiar with a wide variety of transforming robots—whether for&nbsp;<a href=\"https://spectrum.ieee.org/robotics/humanoids/meet-aquanaut-the-underwater-transformer\">submarines</a>&nbsp;or&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/drones/spring-loaded-drone-collapses-midflight-to-zip-through-windows\">drones</a>, transformation is a way of making a single robot adaptable to different environments or tasks. Usually, these robots are restricted to a discrete number of configurations—perhaps two or three different forms—because of the constraints imposed by the rigid structures that robots are typically made of.</p> \n <p>Soft robotics has the potential to change all this, with robots that don’t have fixed forms but instead can transform themselves into whatever shape will enable them to do what they need to do. At <a href=\"https://spectrum.ieee.org/tag/icra+2019\">ICRA</a> in Montreal earlier this year, researchers from <a href=\"https://www.yale.edu/\">Yale University</a> demonstrated a creative approach toward&nbsp;a transforming robot powered by string and air, with a body made primarily out of clay.&nbsp;</p> \n <!--nextpage--> \n <figure class=\"lt med-sm\" role=\"img\"> \n  <img alt=\"Image of robot\" src=\"https://spectrum.ieee.org/image/MzM1MzQ4OQ.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Evan Ackerman \n   </figcaption> \n   <figcaption></figcaption> </img> \n </figure> \n <p>The robot is actuated by two different kinds of “skin,” one layered on top of another. There’s a locomotion skin, made of a pattern of pneumatic bladders that can roll the robot forward or backward when the bladders are&nbsp;inflated sequentially. On top of that is the morphing skin, which is cable-driven, and can sculpt the underlying material into a variety of shapes, including spheres, cylinders, and dumbbells. The robot itself consists of both of those skins wrapped around a chunk of clay, with the actuators driven by offboard power and control. Here it is in action:</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GVza4ojQAKs\" width=\"620\"></iframe></p> \n <p>The Yale researchers have been experimenting with morphing robots that use foams and tensegrity structures for their bodies, but that stuff provides a “restoring force,” springing back into its original shape once the actuation stops. Clay is different because it holds whatever shape it’s formed into, making the robot more energy efficient. And if the dumbbell shape stops being useful, the morphing layer can just squeeze it back into a cylinder or a sphere.</p> \n <p>While this robot, and the sample transformation shown in the video, are relatively simplistic, the researchers suggest some ways in which a more complex version could be used in the future:</p> \n <figure class=\"rt med\" role=\"img\"> \n  <img alt=\"The robot morphing skins.\" src=\"https://spectrum.ieee.org/image/MzM1MzQ3Nw.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: IEEE Xplore \n   </figcaption> \n   <figcaption> \n    <p>This robot’s morphing skin sculpts its clay body into different shapes.&nbsp;</p> \n   </figcaption> </img> \n </figure> \n <blockquote> \n  <p><em>Applications where morphing and locomotion might serve as complementary functions are abundant. For the example skins presented in this work, a search-and-rescue operation could use the clay as a medium to hold a payload such as sensors or transmitters. More broadly, applications include resource-limited conditions where supply chains for materiel are sparse. For example, the morphing sequence shown in Fig. 4 [above] could be used to transform from a rolling sphere to a pseudo-jointed robotic arm. With such a morphing system, it would be possible to robotically morph matter into different forms to perform different functions.</em></p> \n </blockquote> \n <aside class=\"inlay XploreFree xlrg\"> \n  <a href=\"https://ieeexplore.ieee.org/document/8653947\">Read this article for <span class=\"FreeRed\">free</span> on <span class=\"XploreBlue\">IEEE <em>Xplore</em></span> until 5 September 2019</a> \n </aside> \n <p><a href=\"https://ieeexplore.ieee.org/document/8653947\"><em>Morphing Robots Using Robotic Skins That Sculpt Clay</em></a>, by Dylan S. Shah, Michelle C. Yuen, Liana G. Tilton, Ellen J. Yang, and Rebecca Kramer-Bottiglio from Yale University, was presented at ICRA 2019 in Montreal.</p> \n <p>[ <a href=\"https://www.eng.yale.edu/faboratory/\">Yale Faboratory</a> ]</p> \n <div class=\"mobileShow\"> \n  <h3 class=\"RptHdBackBarMobile\"><span class=\"BackArrowBlkBkgrd\">&lt;</span>&nbsp;<a href=\"https://spectrum.ieee.org/static/journal-watch\">Back to IEEE Journal Watch</a></h3> \n </div> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_fF0aLR9nWY:U1o9ocBtuGY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_fF0aLR9nWY:U1o9ocBtuGY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_fF0aLR9nWY:U1o9ocBtuGY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_fF0aLR9nWY:U1o9ocBtuGY:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_fF0aLR9nWY:U1o9ocBtuGY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_fF0aLR9nWY:U1o9ocBtuGY:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_fF0aLR9nWY\" height=\"1\" width=\"1\" alt=\"\" /> \n</link>","descriptionType":"html","publishedDate":"Mon, 12 Aug 2019 18:27:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1MzQzNQ.jpeg","linkMd5":"affef744352a8c08e563bb642c76bc9c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn2@2020_4/2020/08/25/10-37-58-871_719094b708d94048.webp","destWidth":1240,"destHeight":930,"sourceBytes":134397,"destBytes":57690,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn28@2020_5/2020/08/25/10-38-09-369_964adaa282b1aac4.webp","https://spectrum.ieee.org/image/MzM1MzQ4OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn61@2020_4/2020/08/25/10-38-16-956_28ee415d5e45c174.webp","https://spectrum.ieee.org/image/MzM1MzQ3Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn39@2020_5/2020/08/25/10-38-05-409_5d8b3640165b0650.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_fF0aLR9nWY:U1o9ocBtuGY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn92@2020_3/2020/08/25/10-38-04-221_67218181903ee6ed.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_fF0aLR9nWY:U1o9ocBtuGY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn96@2020_6/2020/08/25/10-38-05-085_2c907a499d4dcf46.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_fF0aLR9nWY":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn93@2020_4/2020/08/25/10-38-14-399_931e22780bd2f18c.webp"},"publishedOrCreatedDate":1598351859738},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"RoMeLa's Newest Robot Is a Curiously Symmetrical Dynamic Quadruped","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped","description":"<p>A few years ago, we wrote about <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/nabiros-makes-us-wonder-why-we-all-dont-walk-sideways\">NABiRoS</a>, a bipedal robot from <a href=\"http://www.romela.org/\">Dennis Hong’s Robotics &amp; Mechanisms Laboratory (RoMeLa) at UCLA</a>. Unlike pretty much any other biped we’d ever seen, <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/nabiros-makes-us-wonder-why-we-all-dont-walk-sideways\">NABiRoS had a unique kinematic configuration that had it using its two legs to walk sideways</a>, which offered some surprising advantages.</p> \n<p>As it turns out, bipeds aren’t the only robots that can potentially benefit from a bit of a kinematic rethink. RoMeLa has redesigned quadrupedal robots too—rather than model them after a quadrupedal animal like a dog or a horse, RoMeLa’s ALPHRED robots use four legs arranged symmetrically around the body of the robot, allowing it to walk, run, hop, and jump, as well as manipulate&nbsp;and carry objects, karate chop through boards, and even roller skate on its butt. This robot can do it all.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/5xHtfD_Hewk?rel=0\" width=\"620\"></iframe></p> \n<p>Impressive, right? This is ALPHRED 2, and its predecessor,&nbsp;<a href=\"https://www.youtube.com/watch?v=Pf8XBXvWY38&amp;feature=youtu.be\">the original ALPHRED, was introduced at IROS 2018</a>. Both ALPHREDs are axisymmetric about the vertical axis, meaning that they don’t have a front or a back and are perfectly happy to walk in any direction you like. Traditional quadrupeds like <a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a> or <a href=\"https://robots.ieee.org/robots/laikago/?utm_source=spectrum\">Laikago</a>&nbsp;can also move sideways and backwards, but their leg arrangement makes them more efficient at moving in one particular direction, and also results in some curious compromises like a preference for going down stairs backwards. <a href=\"http://robots.ieee.org/robots/anymal/?utm_source=spectrum\">ANYmal</a> is a bit more flexible in that it can reverse its knees, but it’s still got that traditional quadrupedal two-by-two configuration.&nbsp;</p> \n<p>ALPHRED 2’s four symmetrical limbs can be used for a whole bunch of stuff. It can do quadrupedal walking and running, and it’s able to reach stable speeds of up to 1.5 m/s. If you want bipedal walking, it can do that NABiRoS-style, although it’s still a bit fragile at the moment. Using two legs for walking leaves two legs free, and those legs can turn into arms. A tripedal compromise configuration, with three legs and one arm, is more stable and allows the robot to do things like push buttons, open doors, and destroy property. And thanks to passive wheels under its body, ALPHRED 2 can use its limbs to quickly and efficiently skate around:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/di0bsv8xec4?rel=0\" width=\"620\"></iframe></p> \n<p>The impressive performance of the robot comes courtesy of a custom actuator&nbsp;that RoMeLa designed specifically for dynamic legged locomotion. They call it BEAR, or Back-Drivable Electromechanical Actuator for Robots. These are optionally liquid-cooled motors capable of proprioceptive sensing, consisting of a DC motor, a single stage 10:1 planetary gearbox, and channels through the back of the housing that coolant can be pumped through. The actuators have a peak torque of 32 Nm, and a continuous torque of about 8 Nm with passive air cooling. With liquid cooling, the continuous torque jumps to about 21 Nm. And in the videos above, ALPHRED 2 isn’t even running the liquid cooling system, suggesting that it’s capable of much higher sustained performance.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"RoMeLa's ALPHRED 2 robot\" src=\"https://spectrum.ieee.org/image/MzM3MDI4Ng.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: RoMeLa \n  </figcaption> \n  <figcaption>\n    Using two legs for walking leaves two legs free, and those legs can turn into arms.&nbsp; \n  </figcaption> </img> \n</figure> \n<p>RoMeLa has produced a bunch of very creative robots, and we appreciate that they also seem to produce a bunch of very creative demos showing why their unusual approaches are in fact (at least in some specific cases) somewhat practical. With the recent interest in highly dynamic robots that can be reliably useful in environments infested with humans, we can’t wait to see what kinds of exciting tricks the next (presumably liquid-cooled) version will be able to do.</p> \n<p>[ <a href=\"http://www.romela.org/\">RoMeLa</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Du61YeV7EjU:0OnRv_R-GvE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Du61YeV7EjU:0OnRv_R-GvE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Du61YeV7EjU:0OnRv_R-GvE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Du61YeV7EjU:0OnRv_R-GvE:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Du61YeV7EjU:0OnRv_R-GvE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Du61YeV7EjU:0OnRv_R-GvE:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Du61YeV7EjU\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 09 Sep 2019 19:45:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3MDI4OA.jpeg","linkMd5":"6d47e21e002c0841e6518b91f4e6bdab","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn38@2020_6/2020/08/25/10-38-01-661_52fb8c6dde7c5129.webp","destWidth":1240,"destHeight":957,"sourceBytes":263945,"destBytes":79996,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3MDI4Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn10@2020_2/2020/08/25/10-38-16-446_3e07b6fdaee77003.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Du61YeV7EjU:0OnRv_R-GvE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn48@2020_4/2020/08/25/10-38-08-742_20335e34f6198a63.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Du61YeV7EjU:0OnRv_R-GvE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn47@2020_1/2020/08/25/10-38-08-258_1f65ce545d5e4de8.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Du61YeV7EjU":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn24@2020_5/2020/08/25/10-38-03-882_99768cf861ea551e.webp"},"publishedOrCreatedDate":1598351859733},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Swappable Flying Batteries Keep Drones Aloft Almost Forever","link":"https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever","description":"<p>Battery power is a limiting factor for robots everywhere, but it’s particularly problematic for <a href=\"https://spectrum.ieee.org/robotics/drones\">drones</a>, which have to make an awkward tradeoff between the amount of battery they carry, the amount of other more useful stuff they carry, and how long they can spend in the air. Consumer drones seem to have settled around about a third of their overall mass in battery, resulting in flight times of 20 to 25 minutes at best, before you have to bring the drone back for a battery swap. And if whatever the drone was supposed to be doing depended on it staying in the air, then you’re pretty much out of luck.</p> \n<p>When much larger aircraft have this problem, and in particular military aircraft which sometimes need to stay on-station for long periods of time, the solution is mid-air refueling—why send an aircraft all the way back to its fuel source&nbsp;when you can instead bring the fuel source to the aircraft? It’s easier to do this with liquid fuel than it is with batteries, of course, but <a href=\"https://hiperlab.berkeley.edu/\">researchers at UC Berkeley</a> have come up with a clever solution: You just give the batteries wings. Or, in this case, rotors.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/P6XvhTe1Rdo?rel=0\" width=\"620\"></iframe></p> \n<p>The big quadrotor, which weighs 820 grams, is carrying its own 2.2 Ah lithium-polymer battery that by itself gives it a flight time of about 12 minutes. Each little quadrotor weighs 320 g, including its own 0.8 Ah battery plus a 1.5 Ah battery as cargo. The little ones can’t keep themselves aloft for all that long, but that’s okay, because as flying batteries their only job is to go from ground to the big quadrotor and back again.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Flying batteries for drones\" src=\"https://spectrum.ieee.org/image/MzM4MzAwMg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: UC Berkeley \n  </figcaption> \n  <figcaption>\n    The flying batteries land on a tray mounted atop the main drone and align their legs with electrical contacts. \n  </figcaption> </img> \n</figure> \n<h3>How the flying batteries work</h3> \n<p>As each flying battery approaches the main quadrotor, the smaller quadrotor takes a position about 30 centimeter above a passive docking tray mounted on top of the bigger drone. It then slowly descends to about 3 cm above, waits for its alignment to be just right, and then drops, landing on the tray which helps align its legs with electrical contacts. As soon as a connection is made, the main quadrotor is able to power itself completely from the smaller drone’s battery payload. Each flying battery can power the main quadrotor for about 5 minutes, and then it flies off and a new flying battery takes its place. If everything goes well, the main quadrotor only uses its primary battery during the undocking and docking phases, and in testing, this boosted its flight time from 12 minutes to nearly an hour.&nbsp;</p> \n<p>All of this happens in a motion-capture environment, which is a big constraint, and getting this precision(ish) docking maneuver to work outside, or when the primary drone is moving, is something that the researchers would like to figure out. There are potential applications in situations where continuous monitoring by a drone is important—you could argue that switching off two identical drones might be a simpler way of achieving that, but it also requires two (presumably fancy) drones as opposed to just one plus a bunch of relatively simple and inexpensive flying batteries.</p> \n<h4>“Flying Batteries: In-flight Battery Switching to Increase Multirotor Flight Time,” by Karan P. Jain and Mark W. Mueller from the <a href=\"https://hiperlab.berkeley.edu/\">High Performance Robotics Lab</a> at UC Berkeley, <a href=\"https://arxiv.org/abs/1909.10091\">is available on arXiv</a>.</h4> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4K0VX3LeIBo:fXrdncAkUXE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4K0VX3LeIBo:fXrdncAkUXE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4K0VX3LeIBo:fXrdncAkUXE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4K0VX3LeIBo:fXrdncAkUXE:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4K0VX3LeIBo:fXrdncAkUXE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4K0VX3LeIBo:fXrdncAkUXE:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/4K0VX3LeIBo\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 26 Sep 2019 19:57:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4MzA0Mw.jpeg","linkMd5":"019269ec580d7ee2f12ac1c56db02f28","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn29@2020_5/2020/08/25/10-37-58-837_830ff145b063714e.webp","destWidth":1072,"destHeight":739,"sourceBytes":355765,"destBytes":42380,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM4MzAwMg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn8@2020_5/2020/08/25/10-38-06-040_694582f0959001cb.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4K0VX3LeIBo:fXrdncAkUXE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn72@2020_1/2020/08/25/10-38-03-312_0f700900ce17f2cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4K0VX3LeIBo:fXrdncAkUXE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn38@2020_6/2020/08/25/10-38-11-250_073e45cdfcbeaaa0.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/4K0VX3LeIBo":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn40@2020_1/2020/08/25/10-38-07-835_f520ca76362099ab.webp"},"publishedOrCreatedDate":1598351859731},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"How Robotics Teams Prepared for DARPA's SubT Challenge","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","description":"<p>Eleven robotics teams from around the world are in Pittsburgh this week for the Tunnel Circuit event of the DARPA Subterranean Challenge. <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview\">Our other article is full of details about what the competition will be like</a>, based on what we've heard from DARPA and the videos we were able to find from STIX, the <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics\">SubT Integration Exercise</a> that took place in a Colorado mine in April. But STIX was a closed event, so to get a sense of how things really went, we needed to talk to some of the SubT systems track teams.</p> \n<!--nextpage--> \n<p>We&nbsp;spoke&nbsp;with a few teams (mostly those where we knew someone who'd be able to answer our questions) to learn about their experiences at STIX, and how they've been preparing for this week’s&nbsp;Tunnel Circuit. The comments below come from:</p> \n<p><a href=\"https://research.csiro.au/robotics/category/all-projects/\">Team CSIRO Data61</a> (Nicholas Hudson and Navinda Kottege)</p> \n<ul> \n <li>Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia</li> \n <li>Emesent, Australia</li> \n <li>Georgia Institute of Technology</li> \n</ul> \n<p><a href=\"https://www.grasp.upenn.edu/news/penn-engineers-send-robots-underground-darpa-subterranean-challenge\">Team PLUTO</a> (Camillo J. Taylor)</p> \n<ul> \n <li>University of Pennsylvania</li> \n <li>Exyn Technologies</li> \n <li>Ghost Robotics</li> \n</ul> \n<p><a href=\"https://subt.jpl.nasa.gov/\">Team COSTAR</a> (Joel Burdick)</p> \n<ul> \n <li>Jet Propulsion Laboratory</li> \n <li>California Institute of Technology</li> \n <li>Massachusetts Institute of Technology</li> \n <li>KAIST, South Korea</li> \n</ul> \n<p><a href=\"https://www.subt-cerberus.org/\">Team CERBERUS</a> (Kostas Alexis)</p> \n<ul> \n <li>University of Nevada, Reno</li> \n <li>ETH Zurich, Switzerland</li> \n <li>University of California, Berkeley</li> \n <li>Sierra Nevada Corporation</li> \n <li>Flyability, Switzerland</li> \n</ul> \n<p><a href=\"https://www.subt-explorer.com/\">Team Explorer</a> (Sebastian Scherer)</p> \n<ul> \n <li>Carnegie Mellon University</li> \n <li>Oregon State University</li> \n</ul> \n<h2>What was it like for you and your team to participate in STIX?</h2> \n<p><strong>Team CERBERUS: </strong>Participation in STIX was a defining moment for our team as intensive research activities were merged and integrated toward&nbsp;a fully functional system-of-systems approach on subterranean robotics. Team CERBERUS builds on the collaboration of walking and flying robots and associated perception and autonomy algorithms, which we believe have the potential to allow robotic systems to conquer the underground world. However, as always with research, enabling the field-readiness of a novel solution is a challenging—yet exciting—process! Preparing and participating at STIX was all about our preparation as a team and the readiness to perform.&nbsp;</p> \n<p><strong>Team CSIRO Data61:</strong> STIX was seven months into the project, and it was really the first time we had all our team (CSIRO, Emesent, and Georgia Tech) testing together. Transportation and logistics were a challenge, not just integrating systems together in this time frame&nbsp;but preparing them for overseas travel as well. It was a great experience, both seeing the scale and expectations from DARPA, and talking to other teams (CERBERUS and PLUTO), sharing experiences as well as things like tarps and battery safety bags. We achieved what we needed to at STIX: localizing and scoring an object against the DARPA servers, flying the drone, and navigating the robots in their environments.&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Gost\" src=\"https://spectrum.ieee.org/image/MzM1NDMxOQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: ANYbotics \n  </figcaption> \n  <figcaption>\n    I like Team CERBERUS’s ANYmal with the headlight eyeballs. \n  </figcaption> </img> \n</figure> \n<h2>What were some specific challenges that surprised you at STIX?</h2> \n<p><strong>Team CERBERUS: </strong>During STIX, DARPA showed us a bit of what it takes&nbsp;to be part of such a competition—to (try to) be DARPA ready! We followed a realistic approach and intentionally did not try to push too many uncertain boundaries at STIX. Nevertheless, what we found very challenging, both during our robotic explorations and based on our broader understanding of the Edgar mine, was the combination of a) narrow settings, b) very challenging communications, and c) sensor degradation. Furthermore, the ability to perform as a well-synchronized system-of-systems (and team) within 60 minutes requires absolute readiness and field hardening. STIX gave us a feeling of what level of challenges we will have to deal with and enhanced our excitement and dedication to the project.&nbsp;</p> \n<p><strong>Team PLUTO: </strong>One thing that we found interesting was the extent to which even in a mine, which is a human-built environment, the ground surface can be pitted and rutted with train tracks, rubble, and miscellaneous debris, which pose serious trip and snag hazards for ground-based robots.</p> \n<p><strong>Team Explorer:&nbsp;</strong>One thing was dust, especially for flying. Some mines are relatively wet, but the mine in Colorado was quite dry, so we had some big dust clouds when we were flying, and that confused some of the lidars a bit. That's something that we had to accommodate. Some other things<span>—</span>the places where DARPA hid the objects, some of them were very difficult. And then that DARPA put up fog machines was quite surprising.</p> \n<p><strong>Team CoSTAR: </strong>Because our team had already been testing in local mines (not nearly as big as the STIX or Tunnel Circuit mines), we knew several things to expect. Wireless communication is really tough in caves. We knew that, but we were really taken aback by how many problems we and other teams had with this issue. Moreover, DARPA really showed up with their A-game&nbsp;for STIX—they put some of the scoring items in tough places. The tunnel was also more complicated, in terms of its branching structure, than we had prepared for, and we weren't ready as a team for the vast size of the Edgar mine.</p> \n<p><strong>Team CSIRO Data61:</strong> There were narrow tunnels, many unstructured spaces, and one part of the tunnel system was the dustiest mine our drone had been in. There was also a smoke machine which made it difficult and disorientating for even people to walk through. The aggressive terrain pushed our UGV platform reliability. Being expected to find a cellphone&nbsp;a few meters under a metal grate really highlighted the expectations from DARPA.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"NCTU\" src=\"https://spectrum.ieee.org/image/MzM1NDMyMA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: NCTU \n  </figcaption> \n  <figcaption>\n    Team NCTU is bringing a blimp! \n  </figcaption> </img> \n</figure> \n<h2>How have you been preparing for the SubT Tunnel Event?</h2> \n<p><strong>Team CoSTAR: </strong>The main preparations for SubT? Lots of long hours by core team members. Lots of testing and “gaming,”&nbsp;as we call it (setting up mock competitions). Many hackathons&nbsp;to effect subsystem integration. And many trips to the Eagle Gold mine in Julian, Calif. (still a 3-hour drive from us) for testing and shakeout.</p> \n<p><strong>Team PLUTO: </strong>In preparation for the tunnel circuit in August, our team has been testing in a mine facility in Lansford, Penn., around 2 hours from our home base in Philadelphia. This mine has been a really interesting test site for us since it was continuously operated between 1855 and 1972, so the tunnels and shafts reflect the history of mining in the area. Because of its age and the nature of the mining operation, this facility has several unusual features, such as tight, slanted passageways and drainage ditches that pose unique challenges.</p> \n<p><strong>Team Explorer:&nbsp;</strong>The last couple weeks, we've been testing a lot at local mines to iron out all the issues and get the system robust. Some of the things we're explicitly looking at is how to cope with the fog and the dust, and we've spent a lot of time on that to make sure we can successfully navigate through those conditions. Our strategy for communications has been to lay out a network of mesh Wi-Fi as the robots go along, but the robots can also go out of communication and then come back and report what they found.</p> \n<p><strong>Team CSIRO Data61:</strong> The first thing we did after STIX was work with BIA5 (a local Brisbane company) to procure a Titan platform, which is far more robust than&nbsp;the tracked platform we were using. We also completed building and qualified our hexapod platform, rebuilt our common sensing payload to be IP rated, and created a system to drop mesh network communication nodes.</p> \n<p>The whole team (CSIRO, Georgia Tech, and Emesent) has also put considerable effort into autonomous exploration—finding 3D “frontier points” in a shared map, and beyond-communication-range behaviors to navigate to these frontiers and then return to the communications backhaul network. All the platforms are capable of onboard object detection, and we have focused on verifying the end-to-end system accuracy: from the pose of the object with respect to the robot, to the robot’s localization using multiagent SLAM, exploiting tunnel loop closures.</p> \n<p><strong>Team CERBERUS:</strong> In preparation for the Tunnel Circuit, we have a dual approach on a) fundamental research to solve the broad and general challenges of subterranean robotic autonomy, while simultaneously b) maximizing our competition readiness&nbsp;given the current state of our technology. This also implies multiple underground field deployments (in mines and tunnels), system integration activities across our teams, developing of specific modules related to competition needs and challenges (e.g., automated artifact detection and reporting), and of course, lots of team training. Last but not least: It is also a lot about the spirit of the team, the fun of the research, and the excitement of the competition. We are thrilled to be members of this effort and the collective journey of the robotics community!</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Ghost\" src=\"https://spectrum.ieee.org/image/MzM1NDMyMQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Ghost Robotics \n  </figcaption> \n  <figcaption>\n    Team PLUTO has a whole, um, herd of Ghost Robotics' Vision 60 quadrupeds. \n  </figcaption> </img> \n</figure> \n<hr> \n <p>There are a bunch of other teams involved in the systems track as well, and we're looking forward to meeting all of them in person. The best way to keep up with the competition is probably to <a href=\"https://twitter.com/search?q=%23SubTChallenge\">follow the hashtag #SubTChallenge on Twitter</a>.</p> \n <p></p> \n <p>[ <a href=\"https://www.subtchallenge.com/\">DARPA Subterranean Challenge</a> ]</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_SP9ysuBirA:u8fjM8CPmQg:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_SP9ysuBirA:u8fjM8CPmQg:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_SP9ysuBirA:u8fjM8CPmQg:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_SP9ysuBirA:u8fjM8CPmQg:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_SP9ysuBirA:u8fjM8CPmQg:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_SP9ysuBirA:u8fjM8CPmQg:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_SP9ysuBirA\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Wed, 14 Aug 2019 18:05:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1NDM5NQ.jpeg","linkMd5":"f42d642fe58f167022efac8e946848ef","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn18@2020_3/2020/08/25/10-37-58-883_1da9fa18b44fa5dd.webp","destWidth":1240,"destHeight":930,"sourceBytes":291254,"destBytes":168448,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1NDMxOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn93@2020_3/2020/08/25/10-38-04-150_95fcf6efb19558ea.webp","https://spectrum.ieee.org/image/MzM1NDMyMA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn49@2020_5/2020/08/25/10-38-16-329_0872930ff356618f.webp","https://spectrum.ieee.org/image/MzM1NDMyMQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn98@2020_6/2020/08/25/10-38-02-922_a6e7819d981e22aa.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_SP9ysuBirA:u8fjM8CPmQg:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn5@2020_5/2020/08/25/10-38-12-968_6faee1bfcce30e00.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_SP9ysuBirA:u8fjM8CPmQg:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn4@2020_5/2020/08/25/10-38-05-086_18c47ecdf123c004.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_SP9ysuBirA":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn63@2020_6/2020/08/25/10-38-07-321_2869a6aa06834613.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Cerebras Unveils First Installation of Its AI Supercomputer at Argonne National Labs","link":"https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation","description":"<p>At <a href=\"https://sc19.supercomputing.org/\">Supercomputing 2019</a> in Denver, Colo., <a href=\"https://www.cerebras.net/\">Cerebras Systems</a> unveiled the computer powered by the world’s biggest chip. Cerebras says the computer, the CS-1, has the equivalent machine learning capabilities of hundreds of racks worth of GPU-based computers consuming hundreds of kilowatts, but it takes up only one-third of a standard rack and consumes about 17 kW. <a href=\"https://www.anl.gov/\">Argonne National Labs</a>, future home of what’s expected to be the United States’ first exascale supercomputer, says it has already deployed a CS-1. Argonne is one of two announced U.S. National Laboratories customers for Cerebras, the other being Lawrence Livermore National Laboratory.</p> \n<!--nextpage--> \n<p>The system&nbsp;“is the fastest AI computer,” says CEO and cofounder Andrew Feldman. He compared it with&nbsp;Google's TPU clusters (the 2<span>nd</span>&nbsp;of three generations of that company’s AI computers), noting that one of those “takes 10 racks and over 100 kilowatts to deliver a third of the performance of a single [CS-1] box.”</p> \n<p>The CS-1 is&nbsp;designed to speed the training of novel and large neural networks, a process that can take weeks or longer. Powered by a 400,000-core, 1-trillion-transistor wafer-scale processor chip, the CS-1 should collapse that task to minutes or even seconds. However, Cerebras did not provide data showing this performance in terms of&nbsp;standard AI benchmarks such as the new <a href=\"https://mlperf.org/training-results-0-6\">MLPerf standards</a>. Instead it has been wooing potential customers by having them train their own neural network models on machines at Cerebras.</p> \n<p>This approach is not unusual, according to analysts. “Everybody runs their own models that they developed for their own business,” says <a href=\"http://www.moorinsightsstrategy.com/karl-freund-biography/\">Karl Freund, an AI analyst at Moor Insights &amp; Strategies</a>. “That’s the only thing that matters to buyers.”</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"A blowout of the CS-1 shows that most of the system is devoted to powering and cooling the Wafer Scale Engine chip at the back left.\" src=\"https://spectrum.ieee.org/image/MzQxNDUyNw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Cerebras Systems \n  </figcaption> \n  <figcaption>\n    A blowout of the CS-1 shows that most of the system is devoted to powering and cooling the Wafer Scale Engine chip at the back left. \n  </figcaption> </img> \n</figure> \n<p>Cerebras also unveiled some details of the software side of the system. The software allows users to write their machine learning models using standard frameworks such as <a href=\"https://pytorch.org/\">Pytorch</a> and <a href=\"https://www.tensorflow.org/\">Tensorflow</a>. It then sets about devoting variously-sized portions of the wafer-scale engine to layers of the neural network. How&nbsp;does it do this? By solving an optimization problem in order to ensure&nbsp;that the layers all complete their work at roughly the same pace and are contiguous with their neighbors. The result: Information can flow through the network without any holdups.</p> \n<p>The software can perform that optimization problem across multiple computers, allowing a cluster of computers to act as one big machine. Cerebras has linked as many as 32 CS-1s together to get a roughly 32-fold performance increase. This is in contrast with the behavior of GPU-based clusters, says Feldman. “Today, when you cluster GPUs, you don't get the behavior of one big machine. You get the behavior of lots of little machines.”</p> \n<p>Argonne has been working with Cerebras for two years, said Rick Stevens, its director for computing, in a press release. “By deploying the CS-1, we have dramatically shrunk training time across neural networks, allowing our researchers to be vastly more productive to make strong advances across deep learning research in cancer, traumatic brain injury, and other areas important to society today and in the years to come.”</p> \n<p>The CS-1’s first application is in predicting cancer drug response as part of a U.S. Department of Energy&nbsp;and National Cancer Institute collaboration. It is also being used to help understand the <a href=\"https://arxiv.org/abs/1903.01998\">behavior of colliding black holes</a> and the <a href=\"https://spectrum.ieee.org/tag/gravitational+waves\">gravitational waves</a> they produce. A previous instance of that problem required 1024 out of 4392 nodes of the <a href=\"https://www.alcf.anl.gov/theta\">Theta supercomputer</a>.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-xqOx5HkcOE:UY5txkCnYhQ:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-xqOx5HkcOE:UY5txkCnYhQ:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-xqOx5HkcOE:UY5txkCnYhQ:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-xqOx5HkcOE:UY5txkCnYhQ:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-xqOx5HkcOE:UY5txkCnYhQ:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-xqOx5HkcOE:UY5txkCnYhQ:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-xqOx5HkcOE\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 19 Nov 2019 13:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxNDUyOQ.jpeg","linkMd5":"026bc4b5af35d78854aaaae5d60ee79a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn43@2020_2/2020/08/25/10-37-59-814_988d77132adc0f25.webp","destWidth":1240,"destHeight":930,"sourceBytes":250127,"destBytes":145944,"author":"Samuel K. Moore","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQxNDUyNw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn80@2020_2/2020/08/25/10-38-05-974_76307efff0167b5d.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-xqOx5HkcOE:UY5txkCnYhQ:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn42@2020_5/2020/08/25/10-38-11-231_75338573ec4d6239.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-xqOx5HkcOE:UY5txkCnYhQ:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx/cdn94@2020_2/2020/08/25/10-38-15-368_2ac08e67610b1823.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-xqOx5HkcOE":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn68@2020_6/2020/08/25/10-38-03-277_91e696edddfa8b24.webp"},"publishedOrCreatedDate":1598351859718},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Soft Exosuit Makes Walking and Running Easier Than Ever","link":"https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever","description":"<p>Researchers at Harvard’s Wyss Institute have been testing a flexible, lightweight exosuit that can <a href=\"https://science.sciencemag.org/cgi/doi/10.1126/science.aav7536\">improve your metabolic efficiency by 4&nbsp;to 10&nbsp;percent while walking and running</a>. This is very important because, according to a press release from Harvard, the suit&nbsp;can help you be faster and&nbsp;more efficient, whether you’re “walking at a leisurely pace,” or “running for your life.” Great!</p> \n<!--nextpage--> \n<p>Making humans better at running for their lives is something that we don’t put nearly enough research effort into, I think. The problem&nbsp;may not come up very often, but when it does, it’s super important (because, bears). So, sign me up for anything that we can do to make our desperate flights faster or more efficient—especially if it’s a lightweight, wearable exosuit that’s soft, flexible, and comfortable to wear.</p> \n<p>This is the same sort of exosuit that was part of a DARPA program that we wrote about a few years ago, which was&nbsp;<a href=\"https://spectrum.ieee.org/video/robotics/military-robots/darpa-tests-batterypowered-exoskeletons-on-real-soldiers\">designed to make it easier for soldiers to carry heavy loads for long distances</a>.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Views of the exosuit\" src=\"https://spectrum.ieee.org/image/MzM1NTI1MA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photos: Wyss Institute at Harvard University \n  </figcaption> </img> \n</figure> \n<p>The system uses two waist-mounted electrical motors&nbsp;connected with cables to thigh straps that run down around your butt. The motors pull on the cables at the same time that your muscles actuate, helping them out and reducing the amount of work that your muscles put in without decreasing the amount of force they exert on your legs. The entire suit (batteries included) weighs 5&nbsp;kilograms (11 pounds).</p> \n<p>In order for the cables to actuate at the right time, the suit tracks your gait with two inertial measurement units&nbsp;(IMUs) on the thighs and one on the waist, and then adjusts its actuation profile accordingly. It works well, too, with measurable increases in performance:</p> \n<blockquote> \n <p><em>We show that a portable exosuit that assists hip extension can reduce the metabolic rate of treadmill walking at 1.5 meters per second by 9.3 percent&nbsp;and that of running at 2.5 meters per second by 4.0 percent&nbsp;compared with locomotion without the exosuit. These reduction magnitudes are comparable to the effects of taking off 7.4 and 5.7 kilograms during walking and running, respectively, and are in a range that has shown meaningful athletic performance changes.</em></p> \n</blockquote> \n<p>By increasing your efficiency, you can think of the suit as being able to make you walk or run faster, or farther, or carry a heavier load, all while spending the same amount of energy (or less), which could be just enough to outrun the bear that’s chasing you. Plus, it doesn’t appear to be&nbsp;uncomfortable to wear, and doesn’t require the user to do anything differently, which means that (unlike most robotics things) it’s maybe actually somewhat practical for real-world use—whether you’re indoors or outdoors, or walking or running, or being chased by a bear or not.</p> \n<p>Sadly, I have no idea when you might be able to buy one of these things. But the researchers are looking for ways to make the suit&nbsp;even easier to use, while also reducing the weight and making&nbsp;the efficiency increase more pronounced. Harvard’s <a href=\"https://www.seas.harvard.edu/directory/walsh\">Conor Walsh</a> says they’re “excited to continue to apply it to a range of applications, including assisting those with gait impairments, industry workers at risk of injury performing physically strenuous tasks, or recreational weekend warriors.” As a weekend warrior who is not entirely sure whether he can outrun a bear, I’m excited for this.</p> \n<p><em>Reducing the metabolic rate of walking and running with a versatile, portable exosuit</em>, by Jinsoo Kim, Giuk Lee, Roman Heimgartner, Dheepak Arumukhom Revi, Nikos Karavas, Danielle Nathanson, Ignacio Galiana, Asa Eckert-Erdheim, Patrick Murphy, David Perry, Nicolas Menard, Dabin Kim Choe, Philippe Malcolm, and Conor J. Walsh from the Wyss Institute for Biologically Inspired Engineering at Harvard University, <a href=\"https://science.sciencemag.org/cgi/doi/10.1126/science.aav7536\">appears in the current issue of <em>Science</em></a>.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZSXSV_e5cKI:BHPW6pvdnAY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZSXSV_e5cKI:BHPW6pvdnAY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZSXSV_e5cKI:BHPW6pvdnAY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZSXSV_e5cKI:BHPW6pvdnAY:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZSXSV_e5cKI:BHPW6pvdnAY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZSXSV_e5cKI:BHPW6pvdnAY:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ZSXSV_e5cKI\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 15 Aug 2019 18:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1NTE5Ng.jpeg","linkMd5":"7798fbe18b39cdcbed2b1084de9fdb63","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn58@2020_2/2020/08/25/10-38-01-826_0542b9d4493d8f0b.webp","destWidth":1240,"destHeight":930,"sourceBytes":377148,"destBytes":223916,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1NTI1MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn51@2020_1/2020/08/25/10-38-09-135_57078d60b7c528e3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZSXSV_e5cKI:BHPW6pvdnAY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn90@2020_4/2020/08/25/10-38-17-213_e77cb42fc963bc22.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZSXSV_e5cKI:BHPW6pvdnAY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn26@2020_5/2020/08/25/10-38-16-567_b22aeb0f76f151ec.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ZSXSV_e5cKI":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn53@2020_3/2020/08/25/10-38-13-158_25722e3a33193ce4.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: DJI's Mavic Mini Is a $400 Palm-Sized Foldable Drone","link":"https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p><a href=\"https://www.dji.com/\">DJI</a>’s new Mavic Mini looks like a pretty great drone for US $400 ($500 for a combo with more accessories): It’s tiny, flies for 30 minutes, and will do what you need as far as pictures and video (although not a whole lot more).</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hS_OjalEpZI\" width=\"620\"></iframe></p> \n <p></p> \n <p>DJI seems to have put a bunch of effort into making the drone 249 grams, <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/faa-task-force-drone-registration\">1 gram under what’s required for FAA registration</a>. That means you save $5 and a few minutes of your time, but that does <em>not</em> mean you don’t have to follow&nbsp;the FAA’s rules and regulations governing drone use.</p> \n <p>[ <a href=\"https://www.dji.com/mavic-mini\">DJI</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Don’t panic, but <a href=\"https://clearpathrobotics.com/\">Clearpath</a>&nbsp;and <a href=\"https://www.hebirobotics.com/\">HEBI Robotics</a> have armed the Jackal:</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/YfYvEC764iE\" width=\"620\"></iframe></p> \n  <p></p> \n  <blockquote> \n   <p><em>After locking eyes across a crowded room at ICRA 2019, Clearpath Robotics and HEBI Robotics basked in that warm and fuzzy feeling that comes with starting a new and exciting relationship. Over a conference hall coffee, they learned that the two companies have many overlapping interests. The most compelling was the realization that customers across a variety of industries are hunting for an elusive true love of their own – a robust but compact robotic platform combined with a long reach manipulator for remote inspection tasks.</em></p> \n   <p><em>After ICRA concluded, Arron Griffiths, Application Engineer at Clearpath, and Matthew Tesch, Software Engineer at HEBI, kept in touch and decided there had been enough magic in the air to warrant further exploration. A couple of months later, Matthew arrived at Clearpath to formally introduce the HEBI’s X-Series Arm to Clearpath’s Jackal UGV. It was love.</em></p> \n  </blockquote> \n  <p>[ <a href=\"https://clearpathrobotics.com/blog/2019/10/hebi-robotics-and-clearpath-robotics-join-forces-to-create-the-cutting-edge-hebi-jackal/\">Clearpath</a> ]</p> \n  <p><em>Thanks Dave!</em></p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>I’m really not a fan of the people-carrying drones, but heavy lift cargo drones seem like a more okay idea.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/2C5YDnR2EGw\" width=\"620\"></iframe></p> \n   <p></p> \n   <blockquote> \n    <p><em>Volocopter, the pioneer in Urban Air Mobility, presented the demonstrator of its VoloDrone. This marks Volocopters expansion into the logistics, agriculture, infrastructure and public services industry. The VoloDrone is an unmanned, fully electric, heavy-lift utility drone capable of carrying a payload of 200 kg (440 lbs) up to 40 km (25 miles). With a standardized payload attachment, VoloDrone can serve a great variety of purposes from transporting boxes, to liquids, to equipment and beyond. It can be remotely piloted or flown in automated mode on pre-set routes.</em></p> \n   </blockquote> \n   <p>[ <a href=\"https://www.volocopter.com/en/volodrone/\">Volocopter</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>JAY is a mobile service robot that projects a display on the floor and plays sound with its speaker. By playing sounds and videos, it provides visual and audio entertainment in various places such as exhibition halls, airports, hotels, department stores and more.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/TBTKn_5ah00\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"http://www.rainbow-robotics.com/new/\">Rainbow Robotics</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>The <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit\">DARPA Subterranean Challenge</a> Virtual Tunnel Circuit concluded this week—it was the same idea as the physical challenge that took place in August, just with a lot less IRL dirt.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/x8LM1j2TmZc\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>The awards ceremony and team presentations are in this next video, and we’ll have more on this once we get back from IROS.</p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/JxwL2eGQQs8\" width=\"620\"></iframe></p> \n     <p>[ <a href=\"https://www.subtchallenge.com/\">DARPA SubT</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <blockquote> \n       <p><em>NASA is sending a mobile robot to the south pole of the Moon to get a close-up view of the location and concentration of water ice in the region and for the first time ever, actually sample the water ice at the same pole where the first woman and next man will land in 2024 under the Artemis program. </em></p> \n      </blockquote> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ROWPoRXLvo4\" width=\"620\"></iframe></p> \n      <p></p> \n      <blockquote> \n       <p><em>About the size of a golf cart, the Volatiles Investigating Polar Exploration Rover, or VIPER, will roam several miles, using its four science instruments — including a 1-meter drill — to sample various soil environments. Planned for delivery in December 2022, VIPER will collect about 100 days of data that will be used to inform development of the first global water resource maps of the Moon.</em></p> \n      </blockquote> \n      <p>[ <a href=\"https://www.nasa.gov/feature/new-viper-lunar-rover-to-map-water-ice-on-the-moon\">NASA</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>Happy Halloween from <a href=\"https://robots.ieee.org/robots/daisy/\">HEBI Robotics</a>!</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_ZevC2uAoVQ\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://www.hebirobotics.com/\">HEBI</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>Happy Halloween from Soft Robotics!</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/9josmO7QiUk\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://www.softroboticsinc.com/\">Soft Robotics</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>Halloween must be really, really confusing for autonomous cars.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/1mSX7TKfPL0\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://waymo.com/\">Waymo</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <blockquote> \n           <p><em>Once a year at Halloween, hardworking JPL engineers put their skills to the test in a highly competitive pumpkin carving contest. The result: A pumpkin gently landed on the Moon, its retrorockets smoldering, while across the room a Nemo-inspired pumpkin explored the sub-surface ocean of Jupiter moon Europa. Suffice to say that when the scientists and engineers at NASA’s Jet Propulsion Laboratory compete in a pumpkin-carving contest, the solar system’s the limit. Take a look at some of the masterpieces from 2019. </em></p> \n          </blockquote> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/J_E37dF3oCA\" width=\"620\"></iframe></p> \n          <p></p> \n          <blockquote> \n           <p><em>Now in its ninth year, the contest gives teams only one hour to carve and decorate their pumpkin though they can prepare non-pumpkin materials - like backgrounds, sound effects and motorized parts - ahead of time.</em></p> \n          </blockquote> \n          <p>[ <a href=\"https://www.flickr.com/photos/nasa-jpl/sets/72157672207667603/\">JPL</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>The online autonomous navigation and semantic mapping experiment presented [below] is conducted with the Cassie Blue bipedal robot at the University of Michigan. The sensors attached to the robot include an IMU, a 32-beam LiDAR and an RGB-D camera. The whole online process runs in real-time on a Jetson Xavier and a laptop with an i7 processor.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/4OUr2DspYoo\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.biped.solutions/\">BPL</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p><a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty II</a> is now available to anyone who wants one, and she’s on sale for a mere $2900.</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/HmE-oIJ7UIU\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://mistyrobotics.com\">Misty</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <blockquote> \n              <p><em>We leveraged LIDAR-based slam, in conjunction with our specialized relative localization sensor UVDAR to perform a de-centralized, communication-free swarm flight without the units knowing their absolute locations. The swarming and obstacle avoidance control is based on a modified Boids-like algorithm, while the whole swarm is controlled by directing a selected leader unit.</em></p> \n             </blockquote> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/nSIqyV0AlXU\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"http://mrs.felk.cvut.cz/\">MRS</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>The MallARD robot is an autonomous surface vehicle (ASV), designed for the monitoring and inspection of wet storage facilities for example spent fuel pools or wet silos. The MallARD is holonomic, uses a LiDAR for localisation and features a robust trajectory tracking controller. </em></p> \n               <p><em>The University of Manchester’s researcher Dr Keir Groves designed and built the autonomous surface vehicle (ASV) for the challenge which came in the top three of the second round in Nov 2017. The MallARD went on to compete in a final 3rd round where it was deployed in a spent fuel pond at a nuclear power plant in Finland by the IAEA, along with two other entries. The MallARD came second overall, in November 2018.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Vxx_i1M5Y4U\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://nuclearrobots.org/\">RNE</a> ]</p> \n              <p><em>Thanks Jennifer!</em></p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>I sometimes get the sense that in the robotic grasping and manipulation world, suction cups are kinda seen as cheating at times. But, their nature allows you to do some pretty interesting things.</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/eK77vK8wkUE\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>More clever octopus footage please.</p> \n               <p>[ <a href=\"http://mlab.ri.cmu.edu/\">CMU</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <blockquote> \n                 <p><em>A Personal, At-Home Teacher For Playful Learning: From academic topics to child-friendly news bulletins, fun facts and more, Miko 2 is packed with relevant and freshly updated content specially designed by educationists and child-specialists. Your little one won’t even realize they’re learning.</em></p> \n                </blockquote> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/EyoOj-9P54w\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>As we point out pretty much every time we post a video like this, keep in mind that you’re seeing a heavily edited version of a hypothetical best case scenario for how this robot can function. And things like \"creating a relationship that they can then learn how to form with their peers\" is almost certainly overselling things. But at $300 (shipping included), this may be a decent robot as long as your expectations are appropriately calibrated.</p> \n                <p>[ <a href=\"https://miko.ai/\">Miko</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <p>ICRA 2018 plenary talk by <a href=\"https://rodneybrooks.com/\">Rodney Brooks</a>: “Robots and People: the Research Challenge.”</p> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_12Kq5ud7Dc\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <p>[ <a href=\"https://www.ieee-ras.org/\">IEEE RAS</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <p>ICRA-X 2018 talk by <a href=\"https://www.cc.gatech.edu/aimosaic/faculty/arkin/\">Ron Arkin</a>: “Lethal Autonomous Robots and the Plight of the Noncombatant.”</p> \n                  <p></p> \n                  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/woIh4szKP-k\" width=\"620\"></iframe></p> \n                  <p></p> \n                  <p>[ <a href=\"https://www.ieee-ras.org/\">IEEE RAS</a> ]</p> \n                  <p></p> \n                  <hr> \n                   <p></p> \n                   <p>On the most recent episode of the AI Podcast, Lex Fridman interviews Garry Kasparov.</p> \n                   <p></p> \n                   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/8RVa0THWUWw\" width=\"620\"></iframe></p> \n                   <p></p> \n                   <p>[ <a href=\"https://lexfridman.com/ai/\">AI Podcast</a> ]</p> \n                   <p></p> \n                   <hr> \n                    <p></p> \n                    <div class=\"feedflare\"> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=06nw6wEy23U:6leYwiyLyss:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=06nw6wEy23U:6leYwiyLyss:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=06nw6wEy23U:6leYwiyLyss:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=06nw6wEy23U:6leYwiyLyss:-BTjWOF_DHI\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=06nw6wEy23U:6leYwiyLyss:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=06nw6wEy23U:6leYwiyLyss:gIN9vFwOqvQ\" border=\"0\" /></a> \n                    </div> \n                    <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/06nw6wEy23U\" height=\"1\" width=\"1\" alt=\"\" /> \n                   </hr> \n                  </hr> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 01 Nov 2019 17:44:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNDQ5NQ.jpeg","linkMd5":"a2d95510787b8bacb7f69ee68b845ae0","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn19@2020_1/2020/08/25/10-38-00-157_6fc6d4e291470b2f.webp","destWidth":1240,"destHeight":762,"sourceBytes":97264,"destBytes":37356,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=06nw6wEy23U:6leYwiyLyss:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn23@2020_1/2020/08/25/10-38-04-573_83efcba348e01ecd.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=06nw6wEy23U:6leYwiyLyss:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn25@2020_2/2020/08/25/10-38-13-198_59ef8a619db59085.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/06nw6wEy23U":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn1@2020_4/2020/08/25/10-38-12-702_4c6b61e4f398b5ce.webp"},"publishedOrCreatedDate":1598351859726},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Caltech and JPL Firing Quadrotors Out of Cannons","link":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","description":"<p>As useful as <a href=\"https://spectrum.ieee.org/robotics/drones\">drones</a> are up in the air, the process of getting them there tends to be annoying at best and dangerous at worst. Consider what it takes to launch something as simple as a <a href=\"https://robots.ieee.org/robots/mavic/?utm_source=spectrum\">DJI Mavic</a> or a <a href=\"https://robots.ieee.org/robots/anafi/?utm_source=spectrum\">Parrot Anafi</a>— you need to find a flat spot free of debris or obstructions, unfold the thing and let it boot up and calibrate and whatnot, stand somewhere safe(ish), and then get it airborne and high enough quick enough to avoid hitting any people or things that you care about.</p> \n<p>I’m obviously being a little bit dramatic here, but ground launching drones is certainly both time consuming and risky, and there are occasions where getting a drone into the air as quickly and as safely as possible is a priority. <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau\">At IROS in Macau</a> earlier this month, researchers from <a href=\"https://www.caltech.edu/\">Caltech</a> and <a href=\"https://www.jpl.nasa.gov/\">NASA’s Jet Propulsion Laboratory (JPL)</a> presented a prototype for a ballistically launched drone—a football-shaped foldable quadrotor that gets fired out of a cannon, unfolds itself, and then flies off.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/N_BVCG4Tugs?rel=0\" width=\"620\"></iframe></p> \n<p>Test launching the SQUID (Streamlined Quick Unfolding Investigation Drone) from a truck as shown in the video effectively demonstrates why this is more than a novelty: It would otherwise be very difficult to conventionally launch a quadrotor from a vehicle moving that fast. You can imagine how useful this would be for first responders, ships dealing with waves, or even other aircraft in flight.</p> \n<figure class=\"rt lrg\" role=\"img\"> \n <img alt=\"SQUID, a foldable quadrotor that gets fired out of a cannon created by JPL and Caltech\" src=\"https://spectrum.ieee.org/image/MzQxNjMwMw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Caltech &amp; NASA JPL \n  </figcaption> \n  <figcaption>\n    A CAD model of the SQUID system showing (from left): ballistic configuration, multirotor configuration, and section view with a closer look of a hinge. \n  </figcaption> </img> \n</figure> \n<p>The prototype SQUID shown here weighs 530 grams and is about 27 centimeters long. Folded up, it’s just over 8 cm in diameter. SQUID gets its initial boost of 15 meters per second (referred to as “muzzle velocity” in the paper) from a pneumatic baseball pitching machine, which gives the drone an apex of about 10 m. Immediately after the drone exits the launcher, a nichrome wire heats up and burns through a monofilament line holding the arms in place. Driven by springs, the arms snap out in just 70 ms, while the aerodynamic body of the drone passively orients it into the airstream.</p> \n<p>As soon as the motors spin up (after about 200 ms), SQUID automatically orients itself into a hovering attitude, and it can be controlled just like a normal quadrotor within less than 1 second of launch. Landing is a bit of a challenge, although apparently “it can safely land if the bottom touches the ground first at a low speed,” after which it’ll gently topple over.</p> \n<p>SQUID’s design is easily scalable, and the researchers are currently working on both smaller (2-inch diameter) and larger (6-inch diameter) prototypes. The 6-inch SQUID would be large enough to carry a battery and payload enabling significantly more autonomy, including vision-based ballistic stabilization.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"SQUID, a foldable quadrotor that gets fired out of a cannon, launched from a moving vehicle\" src=\"https://spectrum.ieee.org/image/MzQxNjMwNA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Caltech &amp; NASA JPL \n  </figcaption> \n  <figcaption>\n    To test the drone, the researchers launched it from a moving vehicle up to 50 mph. \n  </figcaption> </img> \n</figure> \n<p>We should mention that ballistically launched drones aren’t a completely new idea—we’ve seen a couple of examples in the past, like <a href=\"https://www.youtube.com/watch?v=8FukTsKmXOo\">this very satisfying-sounding system from Raytheon</a>. But getting something similar to work for a quadrotor is a bit more difficult, and as far as we know, nobody else has thought about applying this launch technique to drones <strong>for planetary exploration</strong>:</p> \n<blockquote> \n <p><em>While the SQUID prototype, as outlined in this paper, has been designed for operation on Earth, the same concept is potentially adaptable to other planetary bodies, in particular Mars and Titan. The Mars helicopter, planned to deploy from the Mars 2020 rover, will provide a proof-of-concept for powered rotorcraft flight on the planet, despite the thin atmosphere. A rotorcraft greatly expands the data collection range of a rover, and allows access to sites that a rover would find impassible. However, the current deployment method for the Mars Helicopter from the underbelly of the rover reduces ground clearance, resulting in stricter terrain constraints. Additionally, the rover must move a significant distance away from the helicopter drop site before the helicopter can safely take off. The addition of a ballistic, deterministic launch system for future rovers or entry vehicles would isolate small rotorcraft from the primary mission asset, as well as enable deployment at longer distances or over steep terrain features.</em></p> \n</blockquote> \n<h4>“Design of a Ballistically-Launched Foldable Multirotor,” by Daniel Pastor, Jacob Izraelevitz, Paul Nadan, Amanda Bouman, Joel Burdick, and Brett Kennedy, from Caltech, JPL, and Olin College, was presented at IROS 2019 in Macau.</h4> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=FMSyH0YMiNw:MiUVkrzeNOc:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=FMSyH0YMiNw:MiUVkrzeNOc:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=FMSyH0YMiNw:MiUVkrzeNOc:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=FMSyH0YMiNw:MiUVkrzeNOc:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=FMSyH0YMiNw:MiUVkrzeNOc:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=FMSyH0YMiNw:MiUVkrzeNOc:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/FMSyH0YMiNw\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 20 Nov 2019 16:50:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxNjMwNg.jpeg","linkMd5":"2c1d20e7c96f50d4e36fa6c0bb86c46f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn11@2020_4/2020/08/25/10-38-00-315_ebabe405998644e7.webp","destWidth":1240,"destHeight":900,"sourceBytes":223611,"destBytes":35112,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQxNjMwMw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn36@2020_4/2020/08/25/10-38-02-377_2a9ffa2671921c94.webp","https://spectrum.ieee.org/image/MzQxNjMwNA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn32@2020_4/2020/08/25/10-38-03-405_b555a323edc2f2ac.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=FMSyH0YMiNw:MiUVkrzeNOc:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn43@2020_1/2020/08/25/10-38-07-899_85a321625f09a1a6.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=FMSyH0YMiNw:MiUVkrzeNOc:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn44@2020_6/2020/08/25/10-38-05-456_0698a280ba446832.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/FMSyH0YMiNw":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_4/2020/08/25/10-38-04-828_e63ce3a861cd45e5.webp"},"publishedOrCreatedDate":1598351859718},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Let’s Build Robots That Are as Smart as Babies","link":"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies","description":"<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"illustration\" src=\"https://spectrum.ieee.org/image/MzM5OTY1MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Illustration:&nbsp;Nicholas Little \n  </figcaption> </img> \n</figure> \n<p><strong>Let’s face it: Robots are dumb.</strong> At best they are idiot savants, capable of doing one thing really well. In general, even those robots require specialized environments in which to do their one thing really well. This is why autonomous cars or robots for home health care are so difficult to build. They’ll need to react to an uncountable number of situations, and they’ll need a generalized understanding of the world in order to navigate them all.</p> \n<p>Babies as young as two months <a href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.157\">already understand</a> that an unsupported object will fall, while five-month-old babies know materials like sand and water will pour from a container rather than plop out as a single chunk. Robots lack these understandings, which hinders them as they try to navigate the world without a prescribed task and movement.</p> \n<p>But we could see robots with a generalized understanding of the world (and the processing power required to wield it) thanks to the video-game industry. Researchers <a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/nvidia-brings-robot-simulation-closer-to-reality-by-making-humans-redundant\">are bringing physics engines</a>—the software that provides real-time physical interactions in complex video-game worlds—to robotics. The goal is to develop robots’ understanding in order to learn about the world in the same way babies do.</p> \n<p>Giving robots a baby’s sense of physics helps them navigate the real world and can even save on computing power, according to <a href=\"https://www.linkedin.com/in/lochlainn-wilson-se4/\">Lochlainn Wilson</a>, the CEO of <a href=\"https://www.se4.space/\">SE4</a>, a Japanese company building robots that could operate on Mars. SE4 plans to avoid the problems of latency caused by distance from Earth to Mars by building robots that can operate independently for a few hours before receiving more instructions from Earth.</p> \n<p>Wilson says that his company uses simple physics engines such as PhysX to help build more-independent robots. He adds that if you can tie a physics engine to a coprocessor on the robot, the real-time basic physics intuitions won’t take compute cycles away from the robot’s primary processor, which will often be focused on a more complicated task.</p> \n<p>Wilson’s firm occasionally still turns to a traditional graphics engine, such as Unity or the Unreal Engine, to handle the demands of a robot’s movement. In certain cases, however, such as a robot accounting for friction or understanding force, you really need a robust physics engine, Wilson says, not a graphics engine that simply simulates a virtual environment. For his projects, he often turns to the open-source Bullet Physics engine built by <a href=\"https://ai.google/research/people/ErwinCoumans/\">Erwin Coumans</a>, who is now an employee at Google.</p> \n<p><a href=\"https://github.com/bulletphysics/bullet3\">Bullet</a> is a popular physics-engine option, but it isn’t the only one out there. <a href=\"https://www.nvidia.com/en-us/\">Nvidia Corp.</a>, for example, has realized that its gaming and physics engines are well-placed to handle the computing demands required by robots. In a <a href=\"https://news.developer.nvidia.com/nvidia-opens-robotics-research-lab-in-seattle/\">lab in Seattle</a>, Nvidia is working with teams from the University of Washington to build kitchen robots, fully articulated robot hands and more, all equipped with Nvidia’s tech.</p> \n<p>When I visited the lab, I watched a robot arm move boxes of food from counters to cabinets. That’s fairly straightforward, but that same robot arm could avoid my body if I got in its way, and it could adapt if I moved a box of food or dropped it onto the floor.</p> \n<p>The robot could also understand that less pressure is needed to grasp something like a cardboard box of Cheez-It crackers versus something more durable like an aluminum can of tomato soup.</p> \n<p>Nvidia’s silicon has already helped advance the fields of artificial intelligence and computer vision by making it possible to process multiple decisions in parallel. It’s possible that the company’s new focus on virtual worlds will help advance the field of robotics and teach robots to think like babies.</p> \n<p><em>This article appears in the November 2019 print issue as “Robots as Smart as Babies.”</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=J1wRiTEvVSs:d30liMidOY4:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=J1wRiTEvVSs:d30liMidOY4:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=J1wRiTEvVSs:d30liMidOY4:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=J1wRiTEvVSs:d30liMidOY4:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=J1wRiTEvVSs:d30liMidOY4:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=J1wRiTEvVSs:d30liMidOY4:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/J1wRiTEvVSs\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 24 Oct 2019 19:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NDU1OA.jpeg","linkMd5":"f06a1c762bc020e3eb9c056139e79fb8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn73@2020_5/2020/08/25/10-38-01-000_9e8f4c5d4f76a3ef.webp","destWidth":1240,"destHeight":930,"sourceBytes":392774,"destBytes":37070,"author":"Stacey Higginbotham","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5OTY1MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn13@2020_1/2020/08/25/10-38-12-976_541edd09b114255b.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=J1wRiTEvVSs:d30liMidOY4:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn100@2020_1/2020/08/25/10-38-11-098_6506c6fb57832a6b.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=J1wRiTEvVSs:d30liMidOY4:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn48@2020_4/2020/08/25/10-38-03-199_dfb011f0f085d008.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/J1wRiTEvVSs":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn52@2020_6/2020/08/25/10-38-08-622_bce2d6ea67862f9d.webp"},"publishedOrCreatedDate":1598351859738},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Misty Robotics Begins Shipping Its Programmable Personal Robot","link":"https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"https://www.robobusiness.com/\">RoboBusiness 2019</a> –&nbsp;October 1-3, 2019 –&nbsp;Santa Clara, CA, USA</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>Even though I’m sure this counts as robot dog abuse, here’s some very impressive mostly not falling over from a “blind” Ghost Robotics Vision 60.</p> \n <p></p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sZZFsUM7Kxk\" width=\"620\"></iframe></p> \n <p></p> \n <p></p> \n <p>[ <a href=\"https://www.ghostrobotics.io/\">Ghost Robotics</a> ]</p> \n <p></p> \n <hr> \n  <p>Evan Rachel Wood from “<a href=\"https://www.hbo.com/westworld\">Westworld</a><span>” runs into </span><a href=\"https://robots.ieee.org/robots/sophia/?utm_source=spectrum\" style=\"font-family: Georgia, serif; font-size: 18px;\">Sophia</a><span> the robot, who pitches her a new show: “SophiaWorld.”</span></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/7QAAnvnseaM\" width=\"620\"></iframe></p> \n  <p>[ <a href=\"https://futurism.com/videos/futurism-studios-sophiaworld\">Futurism</a> ]</p> \n  <p></p> \n  <hr> \n   <p>This anthropomorphic robot hand from the <a href=\"http://divf.eng.cam.ac.uk/birl/\">Biologically Inspired Robotics Laboratory</a> at the University of Cambridge in the U.K.&nbsp;has soft fingers with adjustable stiffness, which I guess is important for playing the piano.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/IN8cnPKf3Eg\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>I play the bagpipes, where every note is super loud, so this makes no sense to me.</p> \n   <p>[ <a href=\"http://divf.eng.cam.ac.uk/birl/\">BIRL</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>RobotAI is a startup that makes 6 DoF detection of objects possible with unmatched speed and cost. This is could be a game changer for many applications solving tasks like object manipulation, bin picking, and indoor navigation. This video demonstrates the use of a simple web cam and a universal robot to see and grasp difficult objects that are reflective and small (less than 0.8 cm) as well as indoor navigation in different surroundings.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/d_k785DDmZE\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"https://www.robotai.info/\">RobotAI</a> ]</p> \n    <p><em>Thanks Agmon!</em></p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Now that DARPA’s not running an <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit\">underground robotics challenge</a>, they’ve had a chance to put together some videos about the whole thing.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ySDXnvPQILY\" width=\"620\"></iframe></p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sAv47y2T5eM?rel=0\" width=\"620\"></iframe></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/4atC1iLGy9c?rel=0\" width=\"620\"></iframe></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/pbTCTRNLk1M?rel=0\" width=\"620\"></iframe></p> \n     <p>[ <a href=\"https://www.subtchallenge.com/\">SubT</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>I like this system for picking and placing objects with an assistive robot—it’s a good combination of autonomy and simple but effective user control.</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ZimZlsJTaTU\" width=\"620\"></iframe></p> \n      <p></p> \n      <blockquote> \n       <p><em>We present a robot system comprised of a robotic arm and a mobility scooter that provides pick-and-place functionality for open world environments. A laser pointer is used to directly select an object in the world. Feedback is provided to the user via projecting an interface into the world.</em></p> \n      </blockquote> \n      <p>[ <a href=\"https://pointw.github.io/scooter-page/\">Northeastern</a> ]</p> \n      <p><em>Thanks Dian!</em></p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>Tencent Robotics X released the world’s first Intelligent Inspection and Manipulation Robot in the Shanghai World AI Conference (WAIC2019). Basic functions of the robot includes: automated meters reading based on AI algorithms, equipment status inspection, temperature monitoring, dexterous manipulation of different object, operation of the wheel valve and switches, 5G enabled remote robot teleoperation. The robot is targeting for different applications in hazard environment such as oil &amp; gas industry and other disaster areas.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/yGDirh88NwI\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://ai.tencent.com/ailab/en/index/\">Tencent</a> ]</p> \n       <p><em>Thanks Conghui!</em></p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>The Trifo Max robot vacuum is notable in that it acts as a home surveillance device as well as an autonomous floor cleaner.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/HlYrqG8H-QI\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>It’s $450 and being announced at IFA. It seems inevitable that robot vacuums will end up in this role, but we’re still concerned about the privacy implications.</p> \n        <p>[ <a href=\"https://trifo.com/max/\">Trifo Max</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p><a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty II</a> is now shipping to backers!</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/z2I2e5zkHyg\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>Over the next few weeks, all 500 backer <a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty II</a>s will ship. And if you’re not a backer, you can pre-order a production Misty II for $2400.</p> \n         <p>[ <a href=\"https://www.mistyrobotics.com/blog/the-misty-ii-robot-begins-shipping-to-crowdfunding-backers/\">Misty Robotics</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>This robot is pure evil.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/OAPqOeKbDxc\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://www.shadowrobot.com/telerobots/\">Shadow Robotics</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>Quadrupedal robot play time!</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/5v7hr6-_oxg\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"http://www.unitree.cc/\">Unitree</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>We document the reliably repeatable dynamical mounting and dismounting of wheeled stools and carts, and of fixed ledges, by the Minitaur robot. Because these tasks span a range of length scales that preclude quasi-static execution, we use a hybrid dynamical systems framework to variously compose and thereby systematically reuse a small lexicon of templates (low degree of freedom behavioral primitives). The resulting behaviors comprise the key competences beyond mere locomotion required for robust implementation on a legged mobile manipulator of a simple version of the warehouseman’s problem.</em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/7WGUW4oMyKE\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://kodlab.seas.upenn.edu/\">Kodlab</a> ]</p> \n            <p></p> \n            <p></p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>It’s not everyday you see an autonomous drone being tested in downtown Singapore.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/PvWQFym4vRI\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://epd.sutd.edu.sg/\">SUTD</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p>Well, this looks painful.</p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/RZlsu4cwUYc\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://asl.ethz.ch/\">ETH Zurich</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <blockquote> \n                <p><em>The Government of Rwanda, in partnership with the World Bank, UKAid/DfID, Danida, the Republic of Korea, the World Economic Forum, UNICEF, World Food Programme, other development partners and African drone associations will host the 2020 African Drone Forum: Symposium, Expo and Flying Competitions.</em></p> \n                <p><em>The African Drone Forum will take place in Kigali in February 2020, with a symposium of global drone policy experts and an expo of drone equipment. The event will include the flying competitions on Lake Kivu to demonstrate the latest breakthroughs in drone technology.</em></p> \n               </blockquote> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/u60s3r4O36M\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://www.lakevictoriachallenge.org/\">LVC</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <p>Five worthwhile minutes of <a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning\">Yann LeCun</a> talking about AI embodiment and what he thinks of <a href=\"https://robots.ieee.org/robots/sophia/?utm_source=spectrum\">Sophia</a>&nbsp;the robot.</p> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/UQAHz_JoGJ0\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>The rest of the hour-long interview with Yann from MIT’s Lex Fridman is at the link below.</p> \n                <p>[ <a href=\"https://www.youtube.com/watch?v=SGSOCuByo24\">AI Podcast</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <div class=\"feedflare\"> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Za6B4U3ZdIk:TjeXp1c4B1Q:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Za6B4U3ZdIk:TjeXp1c4B1Q:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Za6B4U3ZdIk:TjeXp1c4B1Q:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Za6B4U3ZdIk:TjeXp1c4B1Q:-BTjWOF_DHI\" border=\"0\" /></a> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Za6B4U3ZdIk:TjeXp1c4B1Q:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Za6B4U3ZdIk:TjeXp1c4B1Q:gIN9vFwOqvQ\" border=\"0\" /></a> \n                 </div> \n                 <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Za6B4U3ZdIk\" height=\"1\" width=\"1\" alt=\"\" /> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 06 Sep 2019 21:55:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2OTI5Mg.jpeg","linkMd5":"fb732aaba2fa77f47db578dd8a2e2c4e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn94@2020_2/2020/08/25/10-37-58-811_edc7fa982b7c63be.webp","destWidth":1240,"destHeight":827,"sourceBytes":371519,"destBytes":56100,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Za6B4U3ZdIk:TjeXp1c4B1Q:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx/cdn46@2020_5/2020/08/25/10-38-11-516_d70ea0567d08111e.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Za6B4U3ZdIk:TjeXp1c4B1Q:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn72@2020_5/2020/08/25/10-38-05-830_098f9bfa220ad51e.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Za6B4U3ZdIk":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn67@2020_6/2020/08/25/10-38-07-970_3a2f652279921085.webp"},"publishedOrCreatedDate":1598351859734},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"AI and the Future of Work: The Economic Impacts of Artificial Intelligence","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence","description":"<p>This week at MIT, academics and industry officials compared notes, studies, and predictions about AI and the future of work. During the discussions, an insurance company executive shared details about one AI program that rolled out at his firm earlier this year. A chatbot the company introduced, the executive said, now handles 150,000 calls per month.</p> \n<p></p> \n<p>Later in the day, a panelist—<a href=\"https://www.pbs.org/wgbh/frontline/about-us/executive-producer-david-fannin/\">David Fanning</a>, founder of PBS’s <em>Frontline</em>—remarked that this statistic is emblematic of broader fears he saw when reporting a new <a href=\"https://www.pbs.org/wgbh/frontline/film/in-the-age-of-ai/\"><em>Frontline</em> documentary about AI</a>. “People are scared,” Fanning said of the public’s AI anxiety.</p> \n<p></p> \n<p>Fanning was part of a <a href=\"https://workofthefuturecongress.mit.edu/\">daylong symposium</a> <span>&nbsp;about AI’s economic consequences—good, bad, and otherwise—</span>convened by <a href=\"https://workofthefuture.mit.edu/\">MIT’s Task Force on the Work of the Future</a>.</p> \n<!--nextpage--> \n<p></p> \n<p>“Dig into every industry, and you’ll find AI changing the nature of work,” said <a href=\"https://www.csail.mit.edu/person/daniela-rus\">Daniela Rus</a>, director of <a href=\"https://www.csail.mit.edu/\">MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL)</a>. She cited recent <a href=\"https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/four-fundamentals-of-workplace-automation\">McKinsey research</a> that found 45 percent of the work people are paid to do today can be automated with currently available technologies. Those activities, McKinsey found, represent some US $2 trillion in wages.</p> \n<p></p> \n<p>However, the threat of automation—whether by AI or other technologies—isn’t as new as technologists on America’s coasts seem to believe, said panelist Fred Goff, CEO of <a href=\"https://www.jobcase.com/\">Jobcase, Inc</a>.</p> \n<p></p> \n<p>“If you live in Detroit or Toledo, where I come from, technology has been displacing jobs for the last half-century,” Goff said. “I don’t think that most people in this country have the&nbsp;<em>increased</em> anxiety that the coasts do, because they’ve been living this.”</p> \n<p></p> \n<p>Goff added that the challenge AI poses for the workforce is not, as he put it, “getting coal miners to code.” Rather, he said, as AI automates some jobs, it will also open opportunities for “reskilling” that may have nothing to do with AI or automation. He touted trade schools—teaching skills like welding, plumbing, and electrical work—and certification programs for sales industry software packages like Salesforce.&nbsp;</p> \n<p></p> \n<p>On the other hand, a documentarian who&nbsp;reported <a href=\"https://www.youtube.com/watch?v=_iaKHeCKcq4\">another recent program on AI</a>—<a href=\"https://www.imdb.com/name/nm8153605/\">Krishna Andavolu</a>, senior correspondent for Vice Media—said “reskilling” may not be an easy answer.</p> \n<p></p> \n<p>“People in rooms like this … don’t realize that a lot of people don’t want to work that much,” Andavolu said. “They’re not driven by passion for their career, they’re driven by passion for life. We’re telling a lot of these workers that they need to reskill. But to a lot of people that sounds like, ‘I’ve got to work twice as hard for what I have now.’ That sounds scary. We underestimate that at our peril.”</p> \n<p></p> \n<p>Part of the problem with&nbsp;<a href=\"https://spectrum.ieee.org/view-from-the-valley/robotics/industrial-robots/robotics-company-prepares-to-take-responsibility-for-displaced-workers\">“reskilling</a><a href=\"https://spectrum.ieee.org/view-from-the-valley/robotics/industrial-robots/robotics-company-prepares-to-take-responsibility-for-displaced-workers\">,”</a> Andavolu said, is that some high-growth industries involve caregiving for seniors and in medical facilities—roles which are traditionally considered “feminized” careers. Destigmatizing these jobs, and increasing the pay to match the salaries of displaced jobs like long-haul truck drivers, is another challenge.</p> \n<p></p> \n<p><a href=\"https://economics.mit.edu/faculty/acemoglu\">Daron Acemoglu</a>, MIT Institute Professor of Economics, faulted the comparably slim funding of academic research into AI.</p> \n<p></p> \n<p>“There is nothing preordained about the progress of technology,” he said. Computers, the Internet, antibiotics, and sensors all grew out of government and academic research programs. What he called the “blue-sky thinking” of non-corporate AI research can also develop applications that are not purely focused on maximizing profits.</p> \n<p></p> \n<p>American companies, Acemoglu said, get tax breaks for capital R&amp;D—but not for developing new technologies for their employees. “We turn around and [tell companies], ‘Use your technologies to empower workers,’” he said. “But why should they do that? Hiring workers is expensive in many ways. And we’re subsidizing capital.”</p> \n<p></p> \n<p>Said <a href=\"https://www.fordfoundation.org/the-latest/news/ford-foundation-appoints-sarita-gupta-as-director-of-future-of-work-ers/\">Sarita Gupta</a>, director of the Ford Foundation’s Future of Work(ers) Program, “Low and middle income workers have for over 30 years been experiencing stagnant and declining pay, shrinking benefits, and less power on the job. Now technology is brilliant at enabling scale. But the question we sit with is—how do we make sure that we’re not scaling these longstanding problems?”</p> \n<p></p> \n<p><a href=\"https://andrewmcafee.org/\">Andrew McAfee</a>, co-director of MIT’s Initiative on the Digital Economy, said AI may not reduce the number of jobs available in the workplace today. But the quality of those jobs is another story. He cited the Dutch economist <a href=\"https://en.wikipedia.org/wiki/Jan_Tinbergen\">Jan Tinbergen</a> who decades ago said that “Inequality is a race between technology and education.”</p> \n<p></p> \n<p>McAfee said, ultimately, the time to solve the economic problems AI poses for workers in the United States&nbsp;is when the U.S. economy is doing well—like right now.</p> \n<p></p> \n<p>“We do have the wind at our backs,” said <a href=\"https://workofthefuture.mit.edu/people/elisabeth-b-reynolds\">Elisabeth Reynolds</a>, executive director of MIT’s Task Force on the Work of the Future.</p> \n<p></p> \n<p>“We have some breathing room right now,” McAfee agreed. “Economic growth has been pretty good. Unemployment is pretty low. Interest rates are very, very low. We might not have that war chest in the future.”</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tD3Hqeyz2og:4-p-HsTg3jA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tD3Hqeyz2og:4-p-HsTg3jA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tD3Hqeyz2og:4-p-HsTg3jA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tD3Hqeyz2og:4-p-HsTg3jA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=tD3Hqeyz2og:4-p-HsTg3jA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tD3Hqeyz2og:4-p-HsTg3jA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/tD3Hqeyz2og\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 22 Nov 2019 22:03:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxODUyOQ.jpeg","linkMd5":"5cb62d3fcb55be58473c9653072f3b07","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn18@2020_2/2020/08/25/10-38-01-983_725e65c2eccb3bc7.webp","destWidth":1240,"destHeight":930,"sourceBytes":240257,"destBytes":127052,"author":"Mark Anderson","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tD3Hqeyz2og:4-p-HsTg3jA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_3/2020/08/25/10-38-07-621_e2592d42094b999a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tD3Hqeyz2og:4-p-HsTg3jA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn43@2020_5/2020/08/25/10-38-06-324_6497e94b852709c2.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/tD3Hqeyz2og":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn10@2020_3/2020/08/25/10-38-11-891_7e9ca800712766d2.webp"},"publishedOrCreatedDate":1598351859717},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Disney Research Makes Dynamic Robots Less Wiggly, More Lifelike","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/disney-research-dynamic-robotic-characters","description":"<p>It’s an unfortunate fact that the laws of physics make it difficult to design and program robots that can move both smoothly and dynamically. The problem is that when a robotic part moves dynamically, all of the stuff that it’s attached to ends up bending and flexing. The bending and flexing may be just a little bit, if your robot is really bulky and stiff, but if you have a lightweight, compliant robot that’s designed to work around humans, the bending and flexing can be so much that it ultimately disrupts whatever motion the robot was trying to make in the first place.</p> \n<p><a href=\"https://www.disneyresearch.com/\">Disney Research</a>, which has an understandable interest in developing lightweight and dynamic robot characters, has presented a paper at <a href=\"https://s2019.siggraph.org/\">SIGGRAPH 2019</a> demonstrating an effective vibration damping method for robots that would otherwise be very, very wiggly.</p> \n<p></p> \n<!--nextpage--> \n<p>This fascinating&nbsp;video shows&nbsp;a sort of worst-case scenario for wiggly robots—very low stiffness structures making dynamic motions with high mass limbs.&nbsp;</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Z1jgaEO9aRs?rel=0\" width=\"620\"></iframe></p> \n<p>What you’re seeing here is not some kind of dynamic vibration damping system. In other words, if you give any of these robots a shove, they’re going to bounce all over the place. Rather, the specific motions that the robots are making (which are designed by animators) have been optimized to suppress vibrations by a computational tool described in Disney’s paper. You can’t see this happen in real time, but the tool is using a model of the robot to predict how it will vibrate, and then instruct the motors to make the very slight (but very exact) additional motions necessary to cancel out those vibrations while still making the robot move the way the animator wants it to.</p> \n<aside class=\"inlay pullquote xlrg\">\n  You can’t see this happen in real time, but Disney’s computational tool is using a model of the robot to predict how it will vibrate, and then instruct the motors to make the very slight additional motions necessary to cancel out those vibrations while still making the robot move the way the animator wants it to \n</aside> \n<p>This technique does require simulation and computation in advance, and its effectiveness depends in large part on how good your model of the robot is. It gets harder and harder to do efficiently as your robot gets more complicated, and as all of the parts that can flex increases—essentially, each point of flexure introduces another degree of freedom into the mix, and since one part flexing can cause another part to flex, it quickly turns into a huge mess. Part of how the researchers tackle this is to prioritize damping out large amplitude vibrations that are the most visible. And it works very well, even on relatively complex robotic characters:</p> \n<blockquote> \n <p><em>In a final demonstration, we retarget a boxing animation to the same 13-DOF full-body character, replacing the two hands with boxing gloves on both our simulation model and our physical system. Unlike the drumming sequence, the boxing motion contains faster motions with abrupt stops. The naïve retargeting causes excessive vibrations, especially when the character dodges and moves his upper body backwards and forwards. With the same objective and optimization parameters as for our Drummer, our optimized motor controls lead to deviations smaller than 1.5 cm (compared to 9 cm before optimization) while preserving the input animation without noticeable visual differences.</em></p> \n</blockquote> \n<h4>“Vibration-Minimizing Motion Retargeting for Robotic Characters,” by Shayan Hoshyari, Hongyi Xu, Espen Knoop, Stelian Coros, and Moritz Bächer from Disney Research, was presented at SIGGRAPH 2019 in Los Angeles.</h4> \n<p>[ <a href=\"https://la.disneyresearch.com/publication/publication-process-vibration-minimizing-motion-retargeting-for-robotic-characters/\">Disney Research</a> ]</p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JL1R_ghrql8:CdEDgfmEzKU:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JL1R_ghrql8:CdEDgfmEzKU:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JL1R_ghrql8:CdEDgfmEzKU:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JL1R_ghrql8:CdEDgfmEzKU:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=JL1R_ghrql8:CdEDgfmEzKU:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JL1R_ghrql8:CdEDgfmEzKU:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/JL1R_ghrql8\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 08 Aug 2019 21:58:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1MjQ1Nw.jpeg","linkMd5":"de6c237869321c3674d763f513eace56","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn66@2020_6/2020/08/25/10-38-01-552_713ff00546a33655.webp","destWidth":1240,"destHeight":794,"sourceBytes":226702,"destBytes":41032,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JL1R_ghrql8:CdEDgfmEzKU:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn66@2020_5/2020/08/25/10-38-17-063_c896ed55cd7bdf58.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JL1R_ghrql8:CdEDgfmEzKU:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn97@2020_5/2020/08/25/10-38-17-215_67e557ab6c9afb0d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/JL1R_ghrql8":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn24@2020_6/2020/08/25/10-38-06-993_f53bd6f9894c6375.webp"},"publishedOrCreatedDate":1598351859738},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Agility Robotics Unveils Upgraded Digit Walking Robot","link":"https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot","description":"<p>Last time we saw Agility Robotics’ <a href=\"https://robots.ieee.org/robots/digit/?utm_source=spectrum\">Digit</a> biped, it was <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/ford-self-driving-vans-will-use-legged-robots-to-make-deliveries\">picking up a box from a Ford delivery van and autonomously dropping it off on a porch</a>, while at the same time managing to not trip over stairs, grass, or small children. As a demo, it was pretty impressive, but of course there’s an enormous gap between making a video of a robot doing a successful autonomous delivery and letting that robot out into the semi-structured world and expecting it to reliably do a good job.</p> \n<p><a href=\"http://www.agilityrobotics.com/\">Agility Robotics</a> is aware of this, of course, and over the last six months they’ve been making substantial improvements to <a href=\"https://spectrum.ieee.org/robotics/humanoids/building-robots-that-can-go-where-we-go\">Digit to make it more capable and robust</a>. A new video posted today shows what’s new with the latest version of Digit—Digit v2.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/LSk8uCHN5CY?rel=0\" width=\"620\"></iframe></p> \n<p>We appreciate Agility Robotics foregoing music in the video, which lets us hear exactly what Digit sounds like in operation. The most noticeable changes are in Digit’s feet, torso, and arms, and I was particularly impressed to see Digit reposition&nbsp;the box on the table before grasping it to make sure that it could get a good grip. Otherwise, it’s hard to tell what’s new, so we asked Agility Robotics’ CEO <a href=\"https://www.linkedin.com/in/damion-shelton-1b5b90a/\">Damion Shelton</a> to get us up to speed.</p> \n<p><strong><em>IEEE Spectrum:</em> Can you summarize the differences between Digit v1 and v2? We’re particularly interested in the new feet.</strong></p> \n<p><strong>Damion Shelton:</strong> The feet now include a roll degree of freedom, so that Digit can resist lateral forces without needing to side step. This allows Digit v2 to balance on one foot statically, which Digit v1 and <a href=\"https://robots.ieee.org/robots/cassie/?utm_source=spectrum\">Cassie</a> could not do. The larger foot also dramatically decreases load per unit area, for improved performance on very soft surfaces like sand.</p> \n<p>The perception stack includes four Intel RealSense cameras used for obstacle detection and pick/place, plus the lidar. In Digit v1, the perception systems were brought up incrementally over time for development purposes. In Digit v2, all perception systems are active from the beginning and tied to a dedicated computer. The perception system is used for a number of additional things beyond manipulation, which we’ll start to show in the next few weeks.</p> \n<p>The torso changes are a bit more behind-the-scenes. All of the electronics in it are now fully custom, thermally managed, and environmentally sealed. We’ve also included power and ethernet to a payload bay that can fit either a <a href=\"https://en.wikipedia.org/wiki/Next_Unit_of_Computing\">NUC</a> or <a href=\"https://www.nvidia.com/en-us/autonomous-machines/jetson-store/\">Jetson</a> module (or other customer payload).</p> \n<p><strong>What exactly are we seeing in the video in terms of Digit’s autonomous capabilities?</strong></p> \n<p>At the moment this is a demonstration of shared autonomy. Picking and placing the box is fully autonomous. Balance and footstep placement are fully autonomous, but guidance and obstacle avoidance are under local teleop. It’s no longer a radio controller as in early videos; we’re not ready to reveal our current controller design but it’s a reasonably significant upgrade. This is v2 hardware, so there’s one more full version in development prior to the 2020 launch, which will expand the autonomy envelope significantly.</p> \n<aside class=\"inlay pullquote xlrg\">\n  “This is a demonstration of shared autonomy. Picking and placing the box is fully autonomous. Balance and footstep placement are fully autonomous, but guidance and obstacle avoidance are under local teleop. It’s no longer a radio controller as in early videos; we’re not ready to reveal our current controller design but it’s a reasonably significant upgrade” \n <span class=\"pq-attrib\">—Damion Shelton, Agility Robotics</span> \n</aside> \n<p><strong>What are some unique features or capabilities of Digit v2 that might not be obvious from the video?</strong></p> \n<p>For those who’ve used <a href=\"https://robots.ieee.org/robots/cassie/?utm_source=spectrum\">Cassie</a> robots, the power-up and power-down ergonomics are a lot more user friendly. Digit can be disassembled into carry-on luggage sized pieces (give or take) in under 5 minutes for easy transport. The battery charges in-situ using a normal laptop-style charger.</p> \n<p><strong>I’m curious about this “stompy” sort of gait that we see in Digit and many other bipedal robots—are there significant challenges or drawbacks to implementing a more human-like (and presumably quieter) heel-toe gait?</strong></p> \n<p>There are no drawbacks other than increased complexity in controls and foot design. With Digit v2, the larger surface area helps with the noise, and v2 has <a href=\"https://spectrum.ieee.org/robotics/humanoids/building-robots-that-can-go-where-we-go\">similar or better passive-dynamic performance as compared to Cassie or Digit v1</a>. The foot design is brand new, and new behaviors like heel-toe are an active area of development.</p> \n<p><strong>How close is Digit v2 to a system that you’d be comfortable operating commercially?</strong></p> \n<p>We’re on track for a 2020 launch for Digit v3. Changes from v2 to v3 are mostly bug-fix in nature, with a few regulatory upgrades like full battery certification. Safety is a major concern for us, and we have launch customers that will be operating Digit in a safe environment, with a phased approach to relaxing operational constraints. Digit operates almost exclusively under force control (as with cobots more generally), but at the moment we’ll err on the side of caution during operation until we have the stats to back up safety and reliability. The legged robot industry has too much potential for us to screw it up by behaving irresponsibly.</p> \n<p>It will be a while before Digit (or any other humanoid robot) is operating fully autonomously in crowds of people, but there are so many large market opportunities (think indoor factory/warehouse environments) to address prior to that point that we expect to mature the operational safety side of things well in advance of having saturated the more robot-tolerant markets.</p> \n<p>[ <a href=\"http://www.agilityrobotics.com/\">Agility Robotics</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WYorWvFk5Jk:OlhVHrX5MDI:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WYorWvFk5Jk:OlhVHrX5MDI:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WYorWvFk5Jk:OlhVHrX5MDI:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WYorWvFk5Jk:OlhVHrX5MDI:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=WYorWvFk5Jk:OlhVHrX5MDI:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WYorWvFk5Jk:OlhVHrX5MDI:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WYorWvFk5Jk\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 14 Oct 2019 15:59:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5MjE3Ng.jpeg","linkMd5":"507ca1aa50869b0a0bb43058028bc0d9","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn50@2020_4/2020/08/25/10-38-01-575_d65d69e996dcea5d.webp","destWidth":1240,"destHeight":868,"sourceBytes":297134,"destBytes":82122,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WYorWvFk5Jk:OlhVHrX5MDI:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn55@2020_4/2020/08/25/10-38-08-690_fd59797452acd1ef.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WYorWvFk5Jk:OlhVHrX5MDI:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn59@2020_2/2020/08/25/10-38-05-751_803ad8789614aec4.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WYorWvFk5Jk":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn12@2020_2/2020/08/25/10-38-05-959_b49f5e644244be62.webp"},"publishedOrCreatedDate":1598351859729},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"AI Chemist Finds the Best Recipe and Stirs Up Molecules From Scratch","link":"https://spectrum.ieee.org/tech-talk/computing/software/aipowered-robotic-system-stirs-up-molecules-from-scratch","description":"<p>The chemistry lab bench just took a step into the automated future. A team of engineers in the United States and Belgium has&nbsp;pioneered a new AI-powered, robotic system for developing molecules. The system uses smart algorithms to comb through millions of known reactions to discover the best way to create a pharmaceutical molecule, polymer, or other compound.</p> \n<p>A robotic arm and automated system of pumps, valves, and vessels then takes over to synthesize the molecule without the aid of human hands. Humans are, however, still “in the loop,” the researchers note. No AI system can yet match the guidance of a good chemical engineer in&nbsp;deciding which&nbsp;reactions to avoid. (Wildly exothermic reactions that go “boom” being just one of many undesirable outcomes that a human chemist can help prevent.)</p> \n<p>“[We] have a system that can learn what’s been done before and use that to instruct the robot to actually execute the synthesis,” says <a href=\"https://cheme.mit.edu/profile/klavs-f-jensen/\">Klavs Jensen</a>, professor of chemical engineering and materials science at MIT.&nbsp;</p> \n<!--nextpage--> \n<p>Jensen says that using a robotic arm and a standardized system of reactor vessels, and hoses to connect them, simplifies&nbsp;the process. A human operator running a comparable system with the same broad range of possible chemicals and reactions would need to operate some 77 valves—each of which would need to be monitored for clogging or leaking.</p> \n<p>This robotic chemistry synthesis system, Jensen and his co-authors say, promises to remove some of the drudge work from the chemical R&amp;D process. Their system \"reliev[es] expert chemists of manual tasks so that they may focus on new ideas,\" the researchers report in a&nbsp;<a href=\"https://science.sciencemag.org/content/365/6453/eaax1566/tab-pdf\" target=\"_blank\">recent issue</a>&nbsp;of the journal&nbsp;<em>Science.</em></p> \n<p><a href=\"http://web.mit.edu/chemistry/jamison/Group/TFJ/TFJ.html\">Tim Jamison</a>, co-author and MIT professor of chemistry, says this open-source robotic synthesis system also sets a precedent.</p> \n<p>“Every new molecule that I can think of in the history of mankind was initially made by someone in the laboratory using glassware—or something very close to that,” Jamison says. Making molecules with a robotic system, he says, is something different.</p> \n<p>Jamison noted that their research did not synthesize any new molecules. That was, in fact, partly the point.</p> \n<p>“The aim of the project was not necessarily to make something better, faster, cheaper,” he says. “It was to develop a system that would have a higher degree of automation. The system did not invent any molecules. Every single [one of the] target molecules we made were all known compounds.”</p> \n<p>The group’s paper reports on the robotic platform’s success in synthesizing 15 pharmaceutical compounds, including&nbsp;aspirin and&nbsp;the blood pressure medication quinapril, as well as&nbsp;the anesthetic lidocaine and the benzodiazepine diazepam.</p> \n<p>Pharmaceuticals are not the only compounds this robotic system can synthesize, though. “It could also have been used to make dyes for LEDs,” Jensen says. “It could have been used to make quantum dot materials, though we would have worked to get to higher temperatures. But I think we could have done that. I think perovskites, you could synthesize. There’s also a lot of interest in polymers for electronic and mechanical devices. Those often require unique molecules. So they’re thinking about making new monomers for those applications—and you could make those monomers with this system.”</p> \n<p>Jamison says he’s already fielded requests from labs and companies for this as-yet-unnamed new robotic chemical synthesis platform.</p> \n<p>“I’ve received emails with questions along the lines of ‘Looks great, I want one,’” he says. “Unfortunately we’re not able to provide systems to those who are interested. But it certainly has captured the imagination of a number of people.”</p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QGOM8XaI22U:GfwBL-DH35Q:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QGOM8XaI22U:GfwBL-DH35Q:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QGOM8XaI22U:GfwBL-DH35Q:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QGOM8XaI22U:GfwBL-DH35Q:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QGOM8XaI22U:GfwBL-DH35Q:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QGOM8XaI22U:GfwBL-DH35Q:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/QGOM8XaI22U\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 22 Aug 2019 15:54:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MDAxNA.jpeg","linkMd5":"1fbe4fd76aaa4013dc648f063ffe877a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn97@2020_5/2020/08/25/10-37-58-826_57e4fa566227a65e.webp","destWidth":1240,"destHeight":930,"sourceBytes":191999,"destBytes":107250,"author":"Mark Anderson","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QGOM8XaI22U:GfwBL-DH35Q:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn76@2020_3/2020/08/25/10-38-10-734_9eae0ce37e6e4e39.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QGOM8XaI22U:GfwBL-DH35Q:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn71@2020_3/2020/08/25/10-38-08-035_e6b411b852dc3d5f.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/QGOM8XaI22U":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn40@2020_4/2020/08/25/10-38-06-288_300b6f90e36e4055.webp"},"publishedOrCreatedDate":1598351859736},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"AI and the Future of Work: The Prospects for Tomorrow’s Jobs","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-prospects-for-tomorrows-jobs","description":"<p>AI experts gathered at MIT last week, with the aim of predicting the role artificial intelligence will play in the future of work. Will it be the enemy of the human worker? Will it prove to be a savior? Or will it be just another innovation—like electricity or the internet?</p> \n<p></p> \n<p>As <em>IEEE Spectrum</em>&nbsp;<a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence\" target=\"_blank\">previously reported</a>, this conference (“<a href=\"https://workofthefuturecongress.mit.edu/\" target=\"_blank\">AI and the Future of Work Congress</a>”), held at MIT’s Kresge Auditorium, offered sometimes pessimistic outlooks on the job- and industry-destroying path that AI and automation seems to be taking: Self-driving technology will put truck drivers out of work; smart law clerk algorithms will put paralegals out of work; robots will (continue to) put factory and warehouse workers out of work.</p> \n<p>Andrew McAfee, co-director of MIT’s Initiative on the Digital Economy, said even just in the past couple years, he’s noticed a shift in the public’s perception of AI. “I remember from previous versions of this conference, it felt like we had to make the case that we’re living in a period of accelerating change and that AI’s going to have a big impact,” he said. “Nobody had to make that case today.”</p> \n<p><a href=\"https://workofthefuture.mit.edu/people/elisabeth-b-reynolds\" target=\"_blank\">Elisabeth Reynolds</a>, executive director of MIT’s Task Force on the Work of the Future, noted that following the path of least resistance is not a viable way forward. “If we do nothing, we’re in trouble,” she said. “The future will not take care of itself. We have to do something about it.”</p> \n<p>Panelists and speakers spoke about championing productive uses of AI in the workplace, which ultimately benefit both employees and customers.</p> \n<p>As one example,&nbsp;<a href=\"https://mitsloan.mit.edu/faculty/directory/zeynep-ton\" target=\"_blank\">Zeynep Ton</a>, professor at MIT Sloan School of Management, highlighted retailer Sam’s Club’s recent rollout of a program called&nbsp;<a href=\"https://corporate.samsclub.com/newsroom/2019/06/11/sams-clubs-newest-app-cuts-tire-shopping-time-to-less-than-5-minutes\" target=\"_blank\">Sam’s Garage</a>. Previously customers shopping for tires for their car spent somewhere between 30 and 45 minutes with a Sam’s Club associate paging through manuals and looking up specs on websites.</p> \n<p>But with an AI algorithm, they were able to cut that spec hunting time down to 2.2 minutes. “Now instead of wasting their time trying to figure out the different tires, they can field the different options and talk about which one would work best [for the customer],” she said. “This is a great example of solving a real problem, including [enhancing] the experience of the associate as well as the customer.”</p> \n<p>“We think of it as an AI-first world that’s coming,” said Scott Prevost, VP of engineering at Adobe. Prevost said AI agents in Adobe’s software will behave something like a creative assistant or intern who will take care of more mundane tasks for you.</p> \n<aside class=\"inlay pullquote lt lrg\">\n  “We need a mindset change. That it is not just about minimizing costs or maximizing tax benefits, but really worrying about what kind of society we’re creating and what kind of environment we’re creating if we keep on just automating and [eliminating] good jobs.” \n <span class=\"pq-attrib\">—Daron Acemoglu, MIT Institute Professor of Economics</span> \n</aside> \n<p>Prevost cited an internal survey of Adobe customers that found 74 percent of respondents’ time was spent doing repetitive work—the kind that might be automated by an AI script or smart agent.</p> \n<p>“It used to be you’d have the resources to work on three ideas [for a creative pitch or presentation],” Prevost said. “But if the AI can do a lot of the production work, then you can have 10 or 100. Which means you can actually explore some of the further out ideas. It’s also lowering the bar for everyday people to create really compelling output.”</p> \n<p>In addition to changing the nature of work, noted a number of speakers at the event, AI is also directly transforming the workforce.</p> \n<p>Jacob Hsu, CEO of the computer career placement company Catalyte spoke about using AI for matching recruits to software developer jobs. Prospects who sign up with Catalyte take a battery of tests. The company’s AI algorithms then match each prospect’s skills with the field best suited for their talents.</p> \n<p>“We want to be like the Harry Potter Sorting Hat,” Hsu said.</p> \n<p>Guillermo Miranda, IBM’s global head of corporate social responsibility, said IBM has increasingly been hiring based not on credentials but on skills. For instance, he said, as much as 15&nbsp;percent of the company’s new hires in some divisions do not have a traditional four-year college degree. “As a company, we need to be much more clear about hiring by skills,” he said. “It takes discipline. It takes conviction. It takes a little bit of enforcing with H.R. by the business leaders. But if you hire by skills, it works.”</p> \n<p>Ardine Williams, Amazon’s VP of workforce development, said the e-commerce giant has been experimenting with developing skills of the employees at its warehouses (a.k.a. fulfillment centers) with an eye toward putting them in a position to get higher-paying work with other companies.</p> \n<p>She described an agreement Amazon had made in its Dallas fulfillment center with aircraft maker Sikorsky, which had been experiencing a shortage of skilled workers for its nearby factory. So Amazon offered to its employees a free certification training to seek higher-paying work at Sikorsky.</p> \n<p>“I do that because now I have an attraction mechanism—like a G.I. Bill,” Williams said. The program is also only available for employees who have worked at least a year with Amazon. So their program offers medium-term job retention, while ultimately moving workers up the wage ladder.</p> \n<p>Radha Basu, CEO of AI data company&nbsp;<a href=\"https://imerit.net/\" target=\"_blank\">iMerit</a>, said her firm aggressively hires from the pool of women and under-resourced minority communities in the U.S. and India. The company specializes in turning unstructured data (e.g. video or audio feeds) into tagged and annotated data for machine learning, natural language processing, or computer vision applications.</p> \n<p>“There is a motivation with these young people to learn these things,” she said. “It comes with no baggage.”</p> \n<p>Alastair Fitzpayne, executive director of The Aspen Institute’s&nbsp;<a href=\"https://www.aspeninstitute.org/programs/future-of-work/\" target=\"_blank\">Future of Work Initiative</a>, said the future of work ultimately means, in bottom-line terms, the future of human capital. “We have an R&amp;D tax credit,” he said. “We’ve had it for decades. It provides credit for companies that make new investment in research and development. But we have nothing on the human capital side that’s analogous.”</p> \n<p>So a company that’s making a big investment in worker training does it on their own dime, without any of the tax benefits that they might accrue if they, say, spent it on new equipment or new technology. Fitzpayne said a simple tweak to the R&amp;D tax credit could make a big difference by incentivizing&nbsp;<em>new</em>&nbsp;investment programs in worker training. Which still means Amazon’s pre-existing worker training programs—for a company that already&nbsp;<a href=\"https://www.washingtonpost.com/us-policy/2019/02/16/amazon-paid-no-federal-taxes-billion-profits-last-year/\" target=\"_blank\">famously pays no taxes</a>—would not count.</p> \n<p>“We need a different way of developing new technologies,” said Daron Acemoglu, MIT Institute Professor of Economics. He pointed to the clean energy sector as an example. First a consensus around the problem needs to emerge. Then a broadly agreed-upon set of goals and measurements needs to be developed (e.g., that AI and automation would, for instance, create at least X new jobs for every Y jobs that it eliminates).</p> \n<p>Then it just needs to be implemented.</p> \n<p>“We need to build a consensus that, along the path we’re following at the moment, there are going to be increasing problems for labor,” Acemoglu said. “We need a mindset change. That it is not just about minimizing costs or maximizing tax benefits, but really worrying about what kind of society we’re creating and what kind of environment we’re creating if we keep on just automating and [eliminating] good jobs.”</p> \n<p><em>This post was updated on 4 December 2019.</em>&nbsp;</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=v9Z7p-VLUvo:1XNgDJQjgWo:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=v9Z7p-VLUvo:1XNgDJQjgWo:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=v9Z7p-VLUvo:1XNgDJQjgWo:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=v9Z7p-VLUvo:1XNgDJQjgWo:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=v9Z7p-VLUvo:1XNgDJQjgWo:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=v9Z7p-VLUvo:1XNgDJQjgWo:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/v9Z7p-VLUvo\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 03 Dec 2019 18:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQyMjg5NA.jpeg","linkMd5":"8e5ebdad5680d32249cb5e9715f5ba1f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn93@2020_3/2020/08/25/10-38-01-195_c243fbbb43d6018e.webp","destWidth":1240,"destHeight":930,"sourceBytes":131415,"destBytes":65548,"author":"Mark Anderson","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=v9Z7p-VLUvo:1XNgDJQjgWo:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn35@2020_4/2020/08/25/10-38-03-875_f1c86a481bbb7261.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=v9Z7p-VLUvo:1XNgDJQjgWo:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn21@2020_5/2020/08/25/10-38-14-197_c1be9a330ef6bc57.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/v9Z7p-VLUvo":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn57@2020_6/2020/08/25/10-38-16-854_9deb8ac3ff6a8e3f.webp"},"publishedOrCreatedDate":1598351859710},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"What Is the Uncanny Valley?","link":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","description":"<p>Have you ever encountered a lifelike humanoid robot or a realistic computer-generated face that seem a bit off or unsettling, though you can’t quite explain why?</p> \n<p>Take for instance <a href=\"https://www.youtube.com/watch?v=mKZ84O1666g\">AVA</a>, one of the “digital humans” created by New Zealand tech startup <a href=\"https://www.soulmachines.com/\">Soul Machines</a> as an <a href=\"https://ava.autodesk.com/\">on-screen avatar for Autodesk</a>. Watching a lifelike digital being such as AVA can be both fascinating and disconcerting. AVA expresses empathy through her demeanor and movements: slightly raised brows, a tilt of the head, a nod.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/mKZ84O1666g?rel=0\" width=\"620\"></iframe></p> \n<p>By meticulously rendering every lash and line in its avatars, Soul Machines aimed to create a digital human that is virtually undistinguishable from a real one. But to many, rather than looking natural, AVA actually looks creepy. There’s something about it being almost human but not quite that can make people uneasy.</p> \n<p>Like AVA, many other ultra-realistic avatars, androids, and animated characters appear stuck in a disturbing in-between world: They are so lifelike and yet they are not “right.” This void of strangeness is known as the uncanny valley.</p> \n<!--nextpage--> \n<h2>Uncanny Valley: Definition and History</h2> \n<p>The <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/explain-the-uncanny-valley-in-less-than-1-minute-go\">uncanny valley</a> is a concept first introduced in the 1970s by <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley-revisited-a-tribute-to-masahiro-mori\">Masahiro Mori</a>, then a professor at the <a href=\"https://www.titech.ac.jp/english/?\">Tokyo Institute of Technology</a>. Mori coined the term “uncanny valley” to&nbsp;describe his observation that as robots appear more humanlike, they become more appealing—but only up to a certain point. Upon reaching the uncanny valley, our affinity descends into a feeling of strangeness, a sense of unease, and a tendency to be scared or freaked out. So the uncanny valley can be defined as people’s negative reaction to certain lifelike robots.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Uncanny Valley graph\" src=\"https://spectrum.ieee.org/image/MzQwNjE2MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Masahiro Mori \n  </figcaption> \n  <figcaption>\n    The uncanny valley graph created by Masahiro Mori: As a robot’s human likeness [horizontal axis] increases, our affinity towards the robot [vertical axis] increases too, but only up to a certain point. For some lifelike robots, our response to them plunges, and they appear repulsive or creepy. That’s the uncanny valley. \n  </figcaption> </img> \n</figure> \n<p>In his seminal <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley\">essay</a> for Japanese journal <em>Energy</em>, Mori wrote:</p> \n<blockquote> \n <p><em>I have noticed that, in climbing toward the goal of making robots appear human, our affinity for them increases until we come to a valley, which I call the </em>uncanny valley<em>.</em></p> \n</blockquote> \n<p>Later in the essay, Mori describes the uncanny valley by using an example—the first prosthetic hands:</p> \n<blockquote> \n <p><em>One might say that the prosthetic hand has achieved a degree of resemblance to the human form, perhaps on a par with false teeth. However, when we realize the hand, which at first site looked real, is in fact artificial, we experience an eerie sensation. For example, we could be startled during a handshake by its limp boneless grip together with its texture and coldness. When this happens, we lose our sense of affinity, and the hand becomes uncanny.</em></p> \n</blockquote> \n<p>In an <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley\">interview with </a><a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley\"><em>IEEE Spectrum</em></a>, Mori explained how he came up with the idea for the uncanny valley:</p> \n<blockquote> \n <p>“Since I was a child, I have never liked looking at wax figures. They looked somewhat creepy to me. At that time, electronic prosthetic hands were being developed, and they triggered in me the same kind of sensation. These experiences had made me start thinking about robots in general, which led me to write that essay. The uncanny valley was my intuition. It was one of my ideas.”</p> \n</blockquote> \n<h2>Uncanny Valley Examples</h2> \n<p>To better illustrate how the uncanny valley works, here are some examples of the phenomenon. Prepare to be freaked out.</p> \n<p><strong>1.&nbsp;Telenoid</strong></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Telenoid Robot\" src=\"https://spectrum.ieee.org/image/MzQwNTY1Mg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Hiroshi Ishiguro/Osaka University/ATR \n  </figcaption> </img> \n</figure> \n<p>Taking the top spot in the <a href=\"https://robots.ieee.org/robots/?t=rankings-creepiest-robots\">“creepiest” rankings of </a><a href=\"https://robots.ieee.org/robots/?t=rankings-creepiest-robots\"><em>IEEE Spectrum</em></a><a href=\"https://robots.ieee.org/robots/?t=rankings-creepiest-robots\">’s Robots</a> Guide, <a href=\"https://robots.ieee.org/robots/telenoid/?utm_source=spectrum\">Telenoid</a> is a robotic communication device designed by Japanese roboticist <a href=\"http://www.irc.atr.jp/~ishiguro/\">Hiroshi Ishiguro</a>. Its bald head, lifeless face, and lack of limbs make it seem <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/telenoid-r1-hiroshi-ishiguro-newest-and-strangest-android\">more alien than human</a>.</p> \n<p><strong>2.&nbsp;Diego-san</strong></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"University of California San Diego's Diego-San\" src=\"https://spectrum.ieee.org/image/MzQwNTY1Mw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Andrew Oh/Javier Movellan/Calit2 \n  </figcaption> </img> \n</figure> \n<p>Engineers and roboticists at the University of California San Diego’s <a href=\"https://inc.ucsd.edu/mplab/index.php\">Machine Perception Lab</a> developed this robot baby to help parents better communicate with their infants. At 1.2 meters (4 feet) tall and weighing 30 kilograms (66 pounds), <a href=\"https://robots.ieee.org/robots/diegosan/?utm_source=spectrum\">Diego-san</a> is a big baby—bigger than an average 1-year-old child.</p> \n<p>“Even though the facial expression is sophisticated and intuitive in this infant robot, I still perceive a false smile when I’m expecting the baby to appear happy,” says <a href=\"https://uk.linkedin.com/in/angela-tinwell-48151a8a\">Angela Tinwell</a>, a senior lecturer at the University of Bolton in the U.K. and author of <a href=\"https://www.amazon.com/Uncanny-Valley-Games-Animation/dp/146658694X\"><em>The Uncanny Valley in Games and Animation</em></a>. “This, along with a lack of detail in the eyes and forehead, can make the baby appear vacant and creepy, so I would want to avoid those ‘dead eyes’ rather than interacting with <a href=\"http://robots.ieee.org/robots/diegosan/?utm_source=spectrum\">Diego-san</a>.”</p> \n<p><strong>​3.&nbsp;Geminoid HI</strong></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Hiroshi Ishiguro's robot replica Geminoid HI\" src=\"https://spectrum.ieee.org/image/MzQwNTY3MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Osaka University/ATR/Kokoro \n  </figcaption> </img> \n</figure> \n<p>Another one of Ishiguro’s creations, <a href=\"https://robots.ieee.org/robots/geminoidhi1/?utm_source=spectrum\">Geminoid HI</a> is his <a href=\"https://spectrum.ieee.org/robotics/humanoids/hiroshi-ishiguro-the-man-who-made-a-copy-of-himself\">android replica</a>. He even took hair from his own scalp to put onto his robot twin. Ishiguro says he created <a href=\"https://robots.ieee.org/robots/geminoidhi1/?utm_source=spectrum\">Geminoid HI</a> to better understand what it means to be human.</p> \n<p><strong>4.&nbsp;Sophia</strong></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Hanson Robotics' humanoid robot Sophia\" src=\"https://spectrum.ieee.org/image/MzQwNTY1MA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo:&nbsp;Mikhail Tereshchenko/TASS/Getty Images \n  </figcaption> </img> \n</figure> \n<p>Designed by <a href=\"http://www.hansonrobotics.com/\">David Hanson</a> of Hanson Robotics, <a href=\"https://robots.ieee.org/robots/sophia/?utm_source=spectrum\">Sophia</a> is one of the most famous humanoid robots. Like Soul Machines’ AVA, Sophia displays a range of emotional expressions and is equipped with natural language processing capabilities.</p> \n<p><strong>5. Anthropomorphized felines</strong></p> \n<p>The uncanny valley doesn’t only happen with robots that adopt a human form. The 2019 live-action versions of the animated film <em>The Lion King</em> and the musical <em>Cats </em>brought the uncanny valley to the forefront of pop culture. To some fans, the photorealistic computer animations of talking lions and singing cats that mimic human movements were just creepy.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/7TavVZMewpY?rel=0\" width=\"620\"></iframe></p> \n<p>Are you feeling that eerie sensation yet?</p> \n<h2>Uncanny Valley: Science or Pseudoscience?</h2> \n<p>Despite our continued fascination with the uncanny valley, its validity as a scientific concept is highly debated. The uncanny valley wasn’t actually proposed as a scientific concept, yet has often been criticized in that light.</p> \n<p>Mori himself said in <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley\">his </a><a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley\"><em>IEEE Spectrum</em></a><a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley\"> interview</a> that he didn’t explore the concept from a rigorous scientific perspective but as more of a guideline for robot designers:</p> \n<blockquote> \n <p><em>Pointing out the existence of the uncanny valley was more of a piece of advice from me to people who design robots rather than a scientific statement.</em></p> \n</blockquote> \n<p><a href=\"http://www.macdorman.com/\">Karl MacDorman</a>, an associate professor of human-computer interaction at Indiana University who has long studied the uncanny valley, interprets the classic graph not as expressing Mori’s theory but as a heuristic for learning the concept and organizing observations.</p> \n<p>“I believe his theory is instead expressed by his examples, which show that a mismatch in the human likeness of appearance and touch or appearance and motion can elicit a feeling of eeriness,” MacDorman says. “In my own experiments, I have consistently reproduced this effect within and across sense modalities. For example, a mismatch in the human realism of the features of a face heightens eeriness; a robot with a human voice or a human with a robotic voice is eerie.”</p> \n<h2>How to Avoid the Uncanny Valley</h2> \n<p>Unless you intend to create creepy characters or evoke a feeling of unease, you can follow certain design principles to avoid the uncanny valley. “The effect can be reduced by not creating robots or computer-animated characters that combine features on different sides of a boundary—for example, human and nonhuman, living and nonliving, or real and artificial,” MacDorman says.</p> \n<p>To make a robot or avatar more realistic and move it beyond the valley, Tinwell says to ensure that a character’s facial expressions match its emotive tones of speech, and that its body movements are responsive and reflect its hypothetical emotional state. Special attention must also be paid to facial elements such as the forehead, eyes, and mouth, which depict the complexities of emotion and thought. “The mouth must be modeled and animated correctly so the character doesn’t appear aggressive or portray a ‘false smile’ when they should be genuinely happy,” she says.</p> \n<p>For <a href=\"http://www.bartneck.de/\">Christoph Bartneck</a>, an associate professor at the University of Canterbury in New Zealand, the goal is not to avoid the uncanny valley, but to avoid bad character animations or behaviors, stressing the importance of matching the appearance of a robot with its ability. “We’re trained to spot even the slightest divergence from ‘normal’ human movements or behavior,” he says. “Hence, we often fail in creating highly realistic, humanlike characters.”</p> \n<p>But he warns that the uncanny valley appears to be more of an uncanny cliff. “We find the likability to increase and then crash once robots become humanlike,” he says. “But we have never observed them ever coming out of the valley. You fall off and that’s it.”</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=KiyPhDu-vcA:sD8uwKJroUg:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=KiyPhDu-vcA:sD8uwKJroUg:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=KiyPhDu-vcA:sD8uwKJroUg:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=KiyPhDu-vcA:sD8uwKJroUg:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=KiyPhDu-vcA:sD8uwKJroUg:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=KiyPhDu-vcA:sD8uwKJroUg:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/KiyPhDu-vcA\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 06 Nov 2019 14:05:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNTc2MQ.jpeg","linkMd5":"cf05796d1ea712358de9c6a3708aa80c","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn10@2020_3/2020/08/25/10-38-01-354_d0a908ec9026c56b.webp","destWidth":1240,"destHeight":930,"sourceBytes":406720,"destBytes":87488,"author":"Rina Diane Caballar","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQwNjE2MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn74@2020_4/2020/08/25/10-38-11-613_0cd3327ac488aef4.webp","https://spectrum.ieee.org/image/MzQwNTY1Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn46@2020_1/2020/08/25/10-38-16-654_9a7704e40b5b6aca.webp","https://spectrum.ieee.org/image/MzQwNTY1Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn29@2020_2/2020/08/25/10-38-11-301_55f16f28d8d398ce.webp","https://spectrum.ieee.org/image/MzQwNTY3MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn88@2020_2/2020/08/25/10-38-05-159_0e8510e32f0a3b92.webp","https://spectrum.ieee.org/image/MzQwNTY1MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn18@2020_1/2020/08/25/10-38-14-577_a8781bce75271f55.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=KiyPhDu-vcA:sD8uwKJroUg:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn19@2020_6/2020/08/25/10-38-04-730_cfca997a8bd4560e.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=KiyPhDu-vcA:sD8uwKJroUg:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn9@2020_4/2020/08/25/10-38-12-793_d89b99629fcf0399.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/KiyPhDu-vcA":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn93@2020_6/2020/08/25/10-38-17-076_c8633878f9bdac57.webp"},"publishedOrCreatedDate":1598351859722},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Blue Frog Robotics Answers (Some of) Our Questions About Its Delayed Social Robot Buddy","link":"https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo","description":"<p>In September of 2015, <a href=\"https://buddytherobot.com/en/buddy-the-emotional-robot/\">Buddy the social home robot</a> <a href=\"https://www.indiegogo.com/projects/buddy-your-family-s-companion-robot#/\">closed its Indiegogo crowdfunding campaign more than&nbsp;600 percent over its funding goal</a>. A thousand people pledged for a robot originally scheduled to be delivered in December of 2016. But nearly three years later, the future of Buddy is still unclear. Last May, Blue Frog Robotics <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/buddy-the-social-robot-is-somehow-not-dead-yet\">asked for forgiveness from its backers and announced the launch of an “equity crowdfunding campaign”</a> to <a href=\"https://www.fundable.com/blue-frog-robotics-inc\">try to raise the additional funding</a> necessary to deliver the robot in April of 2020.</p> \n<p>By the time the crowdfunding campaign launched in August, the delivery date had slipped again, to September 2020, even as Blue Frog attempted to draw investors by estimating that sales of Buddy would “increase from 2000 robots in 2020 to 20,000 in 2023.” Blue Frog’s most recent communication with backers, in September, mentions a new CTO and a North American office, but does little to reassure backers of Buddy that they’ll ever be receiving their robot.&nbsp;</p> \n<p>Backers of the robot are understandably concerned about the future of Buddy, so we sent a series of questions to the founder and CEO of <a href=\"http://www.bluefrogrobotics.com/\">Blue Frog Robotics</a>, Rodolphe Hasselvander.</p> \n<!--nextpage--> \n<p>We’ve edited this interview slightly for clarity, but we should also note that Hasselvander was unable to provide answers to every question. In particular, we asked for some basic information about Blue Frog’s near-term financial plans, on which the entire future of Buddy seems to depend. We’ve left those questions in the interview anyway, along with Hasselvander’s response.</p> \n<p><strong>1. At this point, how much additional funding is necessary to deliver Buddy to backers?<br> 2. Assuming funding is successful, when can backers expect to receive Buddy?<br> 3. What happens if the fundraising goal is not met?<br> 4. You estimate that sales of Buddy will increase 10x over three years. What is this estimate based on?</br></br></br></strong></p> \n<p><em>Rodolphe Hasselvander:&nbsp;Regarding the questions 1-4, unfortunately, as we are fundraising in a <a href=\"https://en.wikipedia.org/wiki/Regulation_D_(SEC)\">Regulation D</a>, we do not comment on prospect, customer data, sales forecasts, or figures. Please refer to our press release <a href=\"https://www.buddytherobot.com/Mailling/2019/07_FUNDABLE/Invest%20in%20the%20Robotics%20Revolution%20with%20Blue%20Frog%20Robotics.pdf\">here</a> to have information about the fundraising.</em></p> \n<p><strong>5. Do you feel that you are currently being transparent enough about this process to satisfy backers?&nbsp;<br> 6. Buddy’s launch date has moved from April 2020 to September 2020 over the last four months. Why should backers remain confident about Buddy’s schedule?</br></strong></p> \n<p><em>Since the last newsletter, we haven’t changed our communication, the backers will be the first to receive their Buddy, and we plan an official launch in September 2020.</em></p> \n<p><strong>7. What is the goal of <a href=\"https://www.mybuddyworld.com/\">My Buddy World</a>?</strong></p> \n<p><em>At Blue Frog, we think that matching a great product with a big market can only happen through continual experimentation, iteration and incorporation of customer feedback. That’s why we created the forum My Buddy World. It has been designed for our Buddy Community to join us, discuss the world’s first emotional robot, and create with us. The objective is to deepen our conversation with Buddy’s fans and users, stay agile in testing our hypothesis and validate our product-market fit. We trust the value of collaboration. Behind Buddy, there is a team of roboticists, engineers, and programmers that are eager to know more about our consumers’&nbsp;needs and are excited to work with them to create the perfect human/robot experience.&nbsp;&nbsp;</em></p> \n<p><strong>8. How is the current version of Buddy different from the 2015 version that backers pledged for during the successful crowdfunding campaign, in both hardware and software?</strong></p> \n<p><em>We have completely revised some parts of Buddy as well as replaced and/or added more accurate and reliable components to ensure we fully satisfy our customers’ requirements for a mature and high-quality robot from day one. We sourced more innovative components to make sure that Buddy has the most up-to-date technologies such as adding four microphones, a high def thermal matrix, a 3D camera, an 8-megapixel RGB camera, time-of-flight sensors, and touch sensors.&nbsp;</em><br> &nbsp;<br> <em>If you want more info, we just posted&nbsp;an article about what is Buddy&nbsp;<a href=\"https://www.linkedin.com/pulse/what-robot-21st-century-rodolphe-hasselvander/\">here</a>.</em></br></br></p> \n<p><strong>9. Will the version of Buddy that ships to backers in 2020 do everything that that was shown in the original crowdfunding video?</strong></p> \n<p><em>Concerning the capabilities of Buddy regarding the video published on YouTube, I confirm that Buddy will be able to do everything you can see, like&nbsp;patrol autonomously and secure your home, telepresence, mathematics applications, interactive stories for children, IoT/smart home management, face recognition, alarm clock, reminder, message/photo sharing, music, hands free call, people following, games like hide and seek (and more). In addition, everyone will be able to create their own apps thanks to the “BuddyLab” application.</em></p> \n<p><strong>10. What makes you confident that Buddy will be successful when <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures\">Jibo, Kuri, and other social robots have not</a>?</strong></p> \n<p><em>Consumer robotics is a new market. Some people think it is a tough one. But we, at Blue Frog Robotics, believe it is a path of learning, understanding, and finding new ways to serve consumers. Here are the five key factors that will make Buddy successful.</em></p> \n<p><em>1) A market-fit robot</em></p> \n<p><em>Blue Frog Robotics is a consumer-centric company. We know that a successful business model and a compelling fit to market Buddy must come up from solving consumers’ frustrations and problems in a way that’s new and exciting. We started from there.&nbsp;&nbsp;</em></p> \n<p><em>By leveraged existing research and syndicated consumer data sets to understand our customers’ needs and aspirations, we get that creating a robot is not about the best tech innovation and features, but always about how well technology becomes a service to one’s basic human needs and assets: convenience, connection, security, fun, self-improvement, and time. To answer to these consumers’ needs and wants, we designed an all-in-one robot with four vital capabilities: intelligence, emotionality, mobility, and customization.</em></p> \n<p><em>With his multi-purpose brain, he addresses a broad range of needs in modern-day life, from securing homes to carrying out his owners’ daily activities, from helping people with disabilities to educating children, from entertaining to just becoming a robot friend.</em></p> \n<p><em>Buddy is a disruptive innovative robot that is about to transform the way we live, learn, utilize information, play, and even care about our health.<br> &nbsp;<br> 2) Endless possibilities</br></br></em></p> \n<p><em>One of the major advantages of Buddy is his adaptability. Beyond to be adorable, playful, talkative, and to accompany anyone in their daily life at home whether you are comfortable with technology or not, he offers via his platform applications to engage his owners in a wide range of activities. From fitness to cooking, from health monitoring to education, from games to meditation, the combination of intelligence, sensors, mobility, multi-touch panel opens endless possibilities for consumers and organizations to adapt their Buddy to their own needs.&nbsp;<br> &nbsp;<br> 3) An affordable price</br></br></em></p> \n<p><em>Buddy will be the first robot combining smart, social, and mobile capabilities and a developed platform with a personality to enter the U.S. market at affordable price.&nbsp;</em></p> \n<p><em>Our competitors are social or assistant robots but rarely both. Competitors differentiate themselves by features: mobile, non-mobile; by shapes: humanoid or not; by skills: social versus smart; targeting a specific domain like entertainment, retail assistant, eldercare, or education for children; and by price. Regarding our six competitors: Moorebot, Elli-Q, and Olly are not mobile; Lynx and <a href=\"https://robots.ieee.org/robots/nao/?utm_source=spectrum\">Nao</a> are in toy category; <a href=\"https://robots.ieee.org/robots/pepper/?utm_source=spectrum\">Pepper</a> is above $10k&nbsp;targeting B2B market; and finally, <a href=\"https://robots.ieee.org/robots/temi/?utm_source=spectrum\">Temi</a> can’t be considered an emotional robot.&nbsp;<br> Buddy remains highly differentiated as an all-in-one, best of his class experience, covering the needs for social interactions and assistance of his owners at each stage of their life at an affordable price.</br></em></p> \n<p><em>The price range of Buddy will be between US $1700 and $2000.&nbsp;</em></p> \n<p><em>4) A winning business model&nbsp;</em></p> \n<p><em>Buddy’s great business model combines hardware, software, and services, and provides game-changing convenience for consumers, organizations, and developers.</em></p> \n<p><em>Buddy offers a multi-sided value proposition focused on three vertical markets: direct consumers, corporations (healthcare, education, hospitality), and developers. The model creates engagement and sustained usage and produces stable and diverse cash flow.<br> &nbsp;<br> 5) A Passion for people and technology</br></br></em></p> \n<p><em>From day one, we have always believed in the power of our dream: To bring the services and the fun of an emotional robot in every house, every hospital, in every care house. Each day, we refuse to think that we are stuck or limited; we work hard to make Buddy a reality that will help people all over the world and make them smile.</em></p> \n<hr> \n <p>While we certainly appreciate Hasselvander’s consistent optimism and obvious enthusiasm, we’re obligated to point out that some of our most important questions were not directly answered. We haven’t learned anything that makes us all that much more confident that Blue Frog will be able to successfully deliver Buddy this time. Hasselvander also didn’t address our specific question about whether he feels like Blue Frog’s communication strategy with backers has been adequate, which is particularly relevant considering that over the four months between the last two newsletters, Buddy’s launch date slipped by six months.&nbsp;</p> \n <p>At this point, all we can do is hope that the strategy Blue Frog has chosen will be successful. We’ll let you know if as soon as we learn more.</p> \n <p>[ <a href=\"http://buddytherobot.com\">Buddy</a> ]</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hHQ_Cl_97H4:rFENhMTkafk:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hHQ_Cl_97H4:rFENhMTkafk:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hHQ_Cl_97H4:rFENhMTkafk:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hHQ_Cl_97H4:rFENhMTkafk:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hHQ_Cl_97H4:rFENhMTkafk:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hHQ_Cl_97H4:rFENhMTkafk:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/hHQ_Cl_97H4\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Mon, 28 Oct 2019 18:55:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwMDk5NQ.jpeg","linkMd5":"9959391ccbf704954bde8fcaaf54f5ba","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_3/2020/08/25/10-38-00-229_4f4a51bf3c70886b.webp","destWidth":1240,"destHeight":846,"sourceBytes":471517,"destBytes":46824,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hHQ_Cl_97H4:rFENhMTkafk:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn57@2020_1/2020/08/25/10-38-11-388_6ef947e6b0fa6c7e.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hHQ_Cl_97H4:rFENhMTkafk:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn5@2020_6/2020/08/25/10-38-02-092_2bf39581fb9da42a.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/hHQ_Cl_97H4":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_1/2020/08/25/10-38-03-911_1c48f5e419bbaf6c.webp"},"publishedOrCreatedDate":1598351859739},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Blue Ocean Robotics Acquires Beam Telepresence Robot From Suitable Technologies","link":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot","description":"<p>Today, <a href=\"https://www.blue-ocean-robotics.com/\">Blue Ocean Robotics</a>, a Danish robotics company,&nbsp;is announcing the acquisition of <a href=\"https://suitabletech.com/\">Suitable Technologies</a>’ <a href=\"https://robots.ieee.org/robots/beam/?utm_source=spectrum\">Beam telepresence robot</a> business. Blue Ocean has been a Beam partner for five years, but now they’re taking things over completely.</p> \n<!--nextpage--> \n<p>The <a href=\"https://robots.ieee.org/robots/beam/?utm_source=spectrum\">Beam robot</a> began its life as <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-software/052710-how-i-became-a-texai-robot-and-went-partying\">an internal project within Willow Garage</a>. It was spun out in 2012 as Suitable Technologies, which produced a couple different versions of the Beam. As telepresence platforms go, Beam is on the powerful and expensive side, designed primarily for commercial and enterprise customers.&nbsp;</p> \n<p>The most recent news from Suitable was the introduction of the <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/ces-2018-suitable-tech-introduces-beampro-2-telepresence-platform\">BeamPro 2</a>, which was announced over a year ago at CES 2018. The Suitable Tech website still lists it as “coming soon,” and our guess is that it’s now up to Blue Ocean to decide whether to go forward with this new version. Blue Ocean calls itself a “robot venture factory.” I’m&nbsp;not entirely sure what a “robot venture factory”&nbsp;is but Blue Ocean describes itself thusly:</p> \n<blockquote> \n <p><em>The company is known for developing professional service robots from the problem, idea and design phase to the development, commercialization and scaling phase. Every robot is placed in its own subsidiary which is responsible for scaling sales, customer service, support and everything else oriented towards global markets and customers. The parent company handles all development and production of robots across the organization.&nbsp;</em></p> \n</blockquote> \n<p>Ah, that explains it! Blue Ocean does already have a couple portfolio companies making very specific robots, including a UV disinfection robot for hospitals and a sort of mobile patient lift also for hospitals. They’re working on some kind of agriculture robot, too. I’d love to be able to tell you more, but the press release doesn’t offer much:</p> \n<blockquote> \n <p><em>With the acquisition, Blue Ocean Robotics sees an opportunity to generate additional synergy: “Our development of robots is based on our own in-house created toolbox with reusable technology components. This means that we can build all of our robots fast, inexpensively, and better than others,” says Blue Ocean Robotics’ CTO John Erland Østergaard. “Some of our robots, for example the UVD disinfection robot, are already equipped with remote controls. With the Beam technology being a big seller in the healthcare sector, we can continue to grow our business within this industry by having our distributors present both UVD and Beam when they visit customers.”</em></p> \n</blockquote> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Scott Hassan (left), one of the founders of Suitable Technologies, and Claus Risager, CEO of Blue Ocean Robotics.\" src=\"https://spectrum.ieee.org/image/MzM2MjMwNQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Blue Ocean Robotics \n  </figcaption> \n  <figcaption>\n    Scott Hassan (left), one of the founders of Suitable Technologies, and Claus Risager, CEO of Blue Ocean Robotics. \n  </figcaption> </img> \n</figure> \n<p>The press release is very specific that Blue Ocean isn’t acquiring Suitable Technologies itself—they’re acquiring the “assets and rights associated with the robot Beam” from Suitable, which I guess means that Suitable is still around somehow. But it’s really not clear what Suitable is without Beam, which (as far as we can make out) is the entirety of what the company does.</p> \n<p>Anyway, we’re glad that there’s enough interest in high-end telepresence robots to support this acquisition, and we hope that Blue Ocean will be investing in BeamPro 2 and further generations of the robot. It’s come a long way from the original <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-software/052710-how-i-became-a-texai-robot-and-went-partying\">Texai robot from Willow Garage</a>, and still has a lot of potential. For more information, visit the <a href=\"https://www.beam-now.com/\">new Beam website</a> that Blue Ocean has just launched.</p> \n<p>[ <a href=\"https://www.beam-now.com/\">Beam</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=605d7BLBLdY:xc88_2dnWUA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=605d7BLBLdY:xc88_2dnWUA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=605d7BLBLdY:xc88_2dnWUA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=605d7BLBLdY:xc88_2dnWUA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=605d7BLBLdY:xc88_2dnWUA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=605d7BLBLdY:xc88_2dnWUA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/605d7BLBLdY\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 27 Aug 2019 04:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MjI4Mg.jpeg","linkMd5":"3a355f69e17aa2bb77552519184504ec","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn67@2020_5/2020/08/25/10-37-59-985_95de98088918a975.webp","destWidth":1240,"destHeight":930,"sourceBytes":178722,"destBytes":25574,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2MjMwNQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn20@2020_4/2020/08/25/10-38-09-387_591c634711ade6a3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=605d7BLBLdY:xc88_2dnWUA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn43@2020_6/2020/08/25/10-38-04-959_26d77631b6d9e169.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=605d7BLBLdY:xc88_2dnWUA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn19@2020_6/2020/08/25/10-38-05-193_6240f354af9b7380.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/605d7BLBLdY":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn98@2020_1/2020/08/25/10-38-14-399_1ef54d2d1d2d720d.webp"},"publishedOrCreatedDate":1598351859735},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: This Wearable Robotic Tail Will Improve Your Balance","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.subtchallenge.com/\">DARPA SubT Tunnel Circuit</a> –&nbsp;August 15-22, 2019 –&nbsp;Pittsburgh, Pa., USA</h5> \n<h5><a href=\"https://clawar.org/clawar2019/\">CLAWAR 2019</a> –&nbsp;August 26-28, 2019 –&nbsp;Kuala Lumpur, Malaysia</h5> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>Lakshmi Nair from Georgia Tech describes some fascinating research towards robots that can create their own tools, as presented at ICRA this year:</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ZSObU3AFbhU\" width=\"620\"></iframe></p> \n <p></p> \n <blockquote> \n  <p><em>Using a novel capability to reason about shape, function, and attachment of unrelated parts, researchers have for the first time successfully trained an intelligent agent to create basic tools by combining objects.</em></p> \n  <p><em>The breakthrough comes from Georgia Tech’s Robot Autonomy and Interactive Learning (RAIL) research lab and is a significant step toward enabling intelligent agents to devise more advanced tools that could prove useful in hazardous – and potentially life-threatening – environments.</em></p> \n </blockquote> \n <p>[ <a href=\"https://sites.google.com/site/lvnair93/\">Lakshmi Nair</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Victor Barasuol, from the Dynamic Legged Systems Lab at IIT, wrote in to share some new research on their <a href=\"https://robots.ieee.org/robots/hyq/?utm_source=spectrum\">HyQ</a> quadruped that enables sensorless shin collision detection. This helps the robot navigate unstructured environments, and also mitigates all those painful shin strikes, because ouch.</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/vHc5ZpkCy8Y\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>This will be presented later this month at the International Conference on Climbing and Walking Robots (CLAWAR) in Kuala Lumpur, Malaysia.</p> \n  <p>[ <a href=\"https://dls.iit.it/\">IIT</a> ]</p> \n  <p><em>Thanks Victor!</em></p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>You used to have a tail, you know—as an embryo, about a month in to your development. All mammals used to have tails, and now we just have useless tailbones, which don’t help us with balancing even a little bit. BRING BACK THE TAIL!</p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Tr1-IhEhXYQ?rel=0\" width=\"620\"></iframe></p> \n   <p>The tail, created by Junichi Nabeshima, <a href=\"https://www.kmd.keio.ac.jp/faculty/kouta-minamizawa\">Kouta Minamizawa</a>, and <a href=\"https://myamens.com/\">MHD Yamen Saraiji</a> from Keio University’s Graduate School of Media Design, was presented at <span>SIGGRAPH 2019 Emerging Technologies</span>.</p> \n   <p>[ <a href=\"https://dl.acm.org/citation.cfm?id=3338573\">Paper</a> ] via [ <a href=\"https://gizmodo.com/a-wearable-robotic-tail-turns-anyone-into-a-furry-with-1836974428\">Gizmodo</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>The noises in this video are fantastic.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/HTEIcjHlAes\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"http://www.esa.int/Our_Activities/Space_Engineering_Technology/Jumping_space_robot_flies_like_a_spacecraft\">ESA</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Apparently the industrial revolution wasn’t a thorough enough beatdown of human knitting, because the robots are at it again.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/q1lsnE5ANuM\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"http://knitskel.csail.mit.edu/\">MIT CSAIL</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p><a href=\"https://robots.ieee.org/robots/skydior1/?utm_source=spectrum\">Skydio’s drones</a> just keep getting more and more impressive. Now if only they’d make one that I can afford...</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GQEFAi7JGBo\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>The only thing more fun than watching robots is watching people react to robots.</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/S4iwxRfW9Ng\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://www.fkv.de/en/exhibition/takayuki-todo-empathic-systems/\">SEER</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>There aren’t any robots in this video, but it’s robotics-related research, and very soothing to watch.</p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ZCzefYkTFAM\" width=\"620\"></iframe></p> \n        <p>[ <a href=\"https://news.stanford.edu/2019/08/06/can-robots-land-like-birds/\">Stanford</a> ]</p> \n        <p></p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>#autonomousicecreamtricycle</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/C8GMbD1G5OE\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>In case it wasn’t clear, which it wasn’t, this is a <a href=\"https://robots.ieee.org/robots/roboy/?utm_source=spectrum\">Roboy</a> project. And if you didn’t understand that first video, you definitely won’t understand this second one:</p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/J69CDIImYWw?rel=0\" width=\"620\"></iframe></p> \n         <p>Whatever that t-shirt is at the end (<a href=\"https://robots.ieee.org/robots/roboy/?utm_source=spectrum\">Roboy</a> in sunglasses puking rainbows...?) I need one.</p> \n         <p>[ <a href=\"https://roboy.org/\">Roboy</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <blockquote> \n           <p><em>By adding electronics and computation technology to a simple cane that has been around since ancient times, a team of researchers at Columbia Engineering have transformed it into a 21st century robotic device that can provide light-touch assistance in walking to the aged and others with impaired mobility.</em></p> \n          </blockquote> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sH16I1jw--o\" width=\"620\"></iframe></p> \n          <p></p> \n          <blockquote> \n           <p><em>The light-touch robotic cane, called CANINE, acts as a cane-like mobile assistant. The device improves the individual’s proprioception, or self-awareness in space, during walking, which in turn improves stability and balance.</em></p> \n          </blockquote> \n          <p>[ <a href=\"https://roar.me.columbia.edu/\">ROAR Lab</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>During the second field experiment for DARPA’s OFFensive Swarm-Enabled Tactics (OFFSET) program, which took place at Fort Benning, Georgia, teams of autonomous air and ground robots tested tactics on a mission to isolate an urban objective. Similar to the way a firefighting crew establishes a boundary around a burning building, they first identified locations of interest and then created a perimeter around the focal point.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ruWC10AW87E\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.darpa.mil/program/offensive-swarm-enabled-tactics\">DARPA</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p>I think there’s a bit of new footage here of Ghost Robotics’ Vision 60 quadruped walking around without sensors on unstructured terrain.</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/MntLRm8-1uU\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.ghostrobotics.io/\">Ghost Robotics</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>If you’re as tired of passenger drone hype as I am, there’s absolutely no need to watch this video of NEC’s latest hover test.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/cmdilp9LM0E\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://apnews.com/2d4ce8955bc04032928afdf97ed42818\">AP</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>As researchers teach robots to perform more and more complex tasks, the need for realistic simulation environments is growing. Existing techniques for closing the reality gap by approximating real-world physics often require extensive real world data and/or thousands of simulation samples. This paper presents TuneNet, a new machine learning-based method to directly tune the parameters of one model to match another using an iterative residual tuning technique. TuneNet estimates the parameter difference between two models using a single observation from the target and minimal simulation, allowing rapid, accurate and sample-efficient parameter estimation.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Ux2pSmZx3uE\" width=\"620\"></iframe></p> \n              <p></p> \n              <blockquote> \n               <p><em>The system can be trained via supervised learning over an auto-generated simulated dataset. We show that TuneNet can perform system identification, even when the true parameter values lie well outside the distribution seen during training, and demonstrate that simulators tuned with TuneNet outperform existing techniques for predicting rigid body motion. Finally, we show that our method can estimate real-world parameter values, allowing a robot to perform sim-to-real task transfer on a dynamic manipulation task unseen during training. We are also making a baseline implementation of our code available online.</em></p> \n              </blockquote> \n              <p>[ <a href=\"https://arxiv.org/abs/1907.11200\">Paper</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>Here’s an update on what <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/gitai-partners-with-jaxa-to-send-telepresence-robots-to-space\">GITAI has been up to with their telepresence astronaut-replacement robot</a>.</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/70AjPB7lwVA\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"http://gitai.tech/\">GITAI</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <blockquote> \n                 <p><em>Curiosity captured this 360-degree panorama of a location on Mars called “Teal Ridge” on June 18, 2019. This location is part of a larger region the rover has been exploring called the “clay-bearing unit” on the side of Mount Sharp, which is inside Gale Crater. The scene is presented with a color adjustment that approximates white balancing to resemble how the rocks and sand would appear under daytime lighting conditions on Earth.</em></p> \n                </blockquote> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/NJzDNgs7Db8\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>[ <a href=\"https://mars.nasa.gov/msl/\">MSL</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <p>Some updates (in English) on ROS from ROSCon France. The first is a keynote from Brian Gerkey:</p> \n                 <p><iframe allow=\"autoplay; fullscreen\" allowfullscreen=\"\" frameborder=\"0\" height=\"360\" src=\"https://player.vimeo.com/video/343427625?title=0&amp;byline=0&amp;portrait=0\" width=\"640\"></iframe></p> \n                 <p>And this second video is from Omri Ben-Bassat, about how to keep your <a href=\"https://robots.ieee.org/robots/vector/?utm_source=spectrum\">Anki Vector</a> alive using ROS:</p> \n                 <p><iframe allow=\"autoplay; fullscreen\" allowfullscreen=\"\" frameborder=\"0\" height=\"360\" src=\"https://player.vimeo.com/video/344256871?title=0&amp;byline=0&amp;portrait=0\" width=\"640\"></iframe></p> \n                 <p>All of the ROSCon FR talks are available on Vimeo.</p> \n                 <p>[ <a href=\"https://roscon.fr/\">ROSCon FR</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <div class=\"feedflare\"> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=R0msYVZ-Qm4:fR7STip5Pt8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=R0msYVZ-Qm4:fR7STip5Pt8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=R0msYVZ-Qm4:fR7STip5Pt8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=R0msYVZ-Qm4:fR7STip5Pt8:-BTjWOF_DHI\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=R0msYVZ-Qm4:fR7STip5Pt8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=R0msYVZ-Qm4:fR7STip5Pt8:gIN9vFwOqvQ\" border=\"0\" /></a> \n                  </div> \n                  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/R0msYVZ-Qm4\" height=\"1\" width=\"1\" alt=\"\" /> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 09 Aug 2019 13:43:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1MjU4Mw.jpeg","linkMd5":"2b5ea3f72dcd3a0879d0e273a51ddf91","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_6/2020/08/25/10-38-01-193_82711b7ee6c2607e.webp","destWidth":1800,"destHeight":1200,"sourceBytes":290474,"destBytes":104954,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=R0msYVZ-Qm4:fR7STip5Pt8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn42@2020_3/2020/08/25/10-38-16-704_765bcf9a5d6b6652.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=R0msYVZ-Qm4:fR7STip5Pt8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn18@2020_5/2020/08/25/10-38-28-419_d722d42f60373eeb.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/R0msYVZ-Qm4":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_5/2020/08/25/10-38-11-812_a0b90310900b276a.webp"},"publishedOrCreatedDate":1598351859741},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"A Path Towards Reasonable Autonomous Weapons Regulation","link":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation","description":"<p><em><strong>Editor’s Note:</strong> The debate on autonomous weapons systems has been escalating over the past several years as the underlying technologies evolve to the point where their deployment in a military context seems inevitable. </em>IEEE Spectrum<em> has published <a href=\"https://spectrum.ieee.org/tag/autonomous+weapons\">a variety of perspectives on this issue</a>. In summary, while there is a compelling argument to be made that autonomous weapons are inherently unethical and should be banned, there is also a compelling argument to be made that autonomous weapons could potentially make conflicts less harmful, especially to non-combatants. Despite an increasing amount of international attention (<a href=\"https://www.unog.ch/80256EDD006B8954/(httpAssets)/5497DF9B01E5D9CFC125845E00308E44/$file/CCW_GGE.1_2019_CRP.1_Rev2.pdf\">including from the United Nations</a>), progress towards consensus, much less regulatory action, has been slow. The following <a href=\"https://www.cc.gatech.edu/ai/robot-lab/online-publications/AWS.pdf\">workshop paper on autonomous weapons systems policy</a> is remarkable because it was authored by a group of experts with very different (and in some cases divergent) views on the issue. Even so, they were able to reach consensus on a roadmap that all agreed was worth considering. It’s collaborations like this that could be the best way to establish a reasonable path forward on such a contentious issue, and with the permission of the authors, we’re excited to be able to share this paper (originally <a href=\"https://www.cc.gatech.edu/ai/robot-lab/online-publications/AWS.pdf\">posted</a> on Georgia Tech’s Mobile Robot Lab website) with you in its entirety.</em></p> \n<!--nextpage--> \n<h2>Autonomous Weapon Systems: A Roadmapping Exercise</h2> \n<p><a id=\"footnote-1-back\" name=\"footnote-1-back\"></a>Over the past several years, there has been growing awareness and discussion surrounding the possibility of future lethal autonomous weapon systems that could fundamentally alter humanity’s relationship with violence in war. Lethal autonomous weapons present a host of legal, ethical, moral, and strategic challenges. At the same time, artificial intelligence (AI) technology could be used in ways that improve compliance with the laws of war and reduce non-combatant harm. Since 2014, states have come together annually at the United Nations to discuss lethal autonomous weapons systems<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-1\"><sup>1</sup></a>. Additionally, a growing number&nbsp;of individuals and non-governmental organizations have become active in discussions surrounding autonomous weapons, contributing to a rapidly expanding intellectual field working to better understand these issues. While a wide range of regulatory options have been proposed for dealing with the challenge of lethal autonomous weapons, ranging from a preemptive, legally binding international treaty to reinforcing compliance with existing laws of war, there is as yet no international consensus on a way forward.</p> \n<p>The lack of an international policy consensus, whether codified in a formal document or&nbsp;otherwise, poses real risks. States could fall victim to a security dilemma in which they deploy untested or unsafe weapons that pose risks to civilians or international stability. Widespread proliferation could enable illicit uses by terrorists, criminals, or rogue states. Alternatively, a lack of guidance on which uses of autonomy are acceptable could stifle valuable research that could reduce the risk of non-combatant harm.</p> \n<p>International debate thus far has predominantly centered around whether or not states&nbsp;should adopt a preemptive, legally-binding treaty that would ban lethal autonomous weapons before they can be built. Some of the authors of this document have called for such a treaty and would heartily support it, if states were to adopt it. Other authors of this document have argued an overly expansive treaty would foreclose the possibility of using AI to mitigate civilian harm. Options for international action are not binary, however, and there are a range of policy options that states should consider between adopting a comprehensive treaty or doing nothing.</p> \n<p><a id=\"footnote-2-back\" name=\"footnote-2-back\"></a><a id=\"footnote-3-back\" name=\"footnote-3-back\"></a>The purpose of this paper is to explore the possibility of a middle road. If a roadmap could&nbsp;garner sufficient stakeholder support to have significant beneficial impact, then what elements could it contain? The exercise whose results are presented below was not to identify recommendations that the authors each prefer individually (the authors hold a broad spectrum of views), but instead to identify those components of a roadmap that the authors are all willing to entertain<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-2\"><sup>2</sup></a>. We, the authors, invite policymakers to consider these&nbsp;components as they weigh possible actions to address concerns surrounding autonomous weapons<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-3\"><sup>3</sup></a>.</p> \n<p><strong>Summary of Issues Surrounding Autonomous Weapons</strong></p> \n<p>There are a variety of issues that autonomous weapons raise, which might lend themselves to different approaches. A non-exhaustive list of issues includes:</p> \n<ul> \n <li>The potential for beneficial uses of AI and autonomy that could improve precision and reliability in the use of force and reduce non-combatant harm.</li> \n <li>Uncertainty about the path of future technology and the likelihood of autonomous weapons being used in compliance with the laws of war, or international humanitarian law (IHL), in different settings and on various timelines.</li> \n <li>A desire for some degree of human involvement in the use of force. This has been expressed repeatedly in UN discussions on lethal autonomous weapon systems in different ways.</li> \n <li>Particular risks surrounding lethal autonomous weapons specifically targeting personnel as opposed to vehicles or materiel.</li> \n <li>Risks regarding international stability.</li> \n <li>Risk of proliferation to terrorists, criminals, or rogue states.</li> \n <li>Risk that autonomous systems that have been verified to be acceptable can be made unacceptable through software changes.</li> \n <li>The potential for autonomous weapons to be used as scalable weapons enabling a small number of individuals to inflict very large-scale casualties at low cost, either intentionally or accidentally.</li> \n</ul> \n<div></div> \n<p><strong><a id=\"footnote-4-back\" name=\"footnote-4-back\"></a>Summary of Components</strong></p> \n<ol> \n <li>A time-limited moratorium on the development, deployment, transfer, and use of anti-personnel lethal autonomous weapon systems<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-4\"><sup>4</sup></a>. Such a moratorium could include exceptions for certain classes of weapons.</li> \n <li>Define guiding principles for human involvement in the use of force.</li> \n <li>Develop protocols and/or technological means to mitigate the risk of unintentional escalation due to autonomous systems.</li> \n <li>Develop strategies for preventing proliferation to illicit uses, such as by criminals, terrorists, or rogue states.</li> \n <li>Conduct research to improve technologies and human-machine systems to reduce non-combatant harm and ensure IHL compliance in the use of future weapons.</li> \n</ol> \n<p><strong>Component 1:</strong></p> \n<p><a id=\"footnote-5-back\" name=\"footnote-5-back\"></a>States should consider adopting a five-year, renewable moratorium on the development, deployment, transfer, and use of anti-personnel lethal autonomous weapon systems. Anti-personnel lethal autonomous weapon systems are defined as weapons systems that, once activated, can select and engage dismounted human targets without further intervention by a human operator, possibly excluding systems such as:</p> \n<ul> \n <li>Fixed-point defensive systems with human supervisory control to defend human-occupied bases or installations</li> \n <li>Limited, proportional, automated counter-fire systems that return fire in order to provide immediate, local defense of humans</li> \n <li>Time-limited pursuit deterrent munitions or systems</li> \n <li>Autonomous weapon systems with size above a specified explosive weight limit that select as targets hand-held weapons, such as rifles, machine guns, anti-tank weapons, or man-portable air defense systems, provided there is adequate protection for non-combatants and ensuring IHL compliance<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-5\"><sup>5</sup></a></li> \n</ul> \n<div></div> \n<p>The moratorium would not apply to:</p> \n<ul> \n <li>Anti-vehicle or anti-materiel weapons</li> \n <li>Non-lethal anti-personnel weapons</li> \n <li>Research on ways of improving autonomous weapon technology to reduce non-combatant harm in future anti-personnel lethal autonomous weapon systems</li> \n <li>Weapons that find, track, and engage specific individuals whom a human has decided should be engaged within a limited predetermined period of time and geographic region</li> \n</ul> \n<p><strong>Motivation:</strong></p> \n<p>This moratorium would pause development and deployment of anti-personnel lethal autonomous weapons systems to allow states to better understand the systemic risks of their use and to perform research that improves their safety, understandability, and effectiveness. Particular objectives could be to:</p> \n<ul> \n <li>ensure that, prior to deployment, anti-personnel lethal autonomous weapons can be used in ways that are equal to or outperform humans in their compliance with IHL (other conditions may also apply prior to deployment being acceptable);</li> \n <li>lay the groundwork for a potentially legally binding diplomatic instrument; and</li> \n <li>decrease the geopolitical pressure on countries to deploy anti-personnel lethal autonomous weapons before they are reliable and well-understood.</li> \n</ul> \n<p><strong>Compliance Verification:</strong></p> \n<p>As part of a moratorium, states could consider various approaches to compliance verification. Potential approaches include:</p> \n<ul> \n <li>Developing an industry cooperation regime analogous to that mandated under the Chemical Weapons Convention, whereby manufacturers must know their customers and report suspicious purchases of significant quantities of items such as fixed-wing drones, quadcopters, and other weaponizable robots.</li> \n <li>Encouraging states to declare inventories of autonomous weapons for the purposes of transparency and confidence-building.</li> \n <li>Facilitating scientific exchanges and military-to-military contacts to increase trust, transparency, and mutual understanding on topics such as compliance verification and safe operation of autonomous systems.</li> \n <li>Designing control systems to require operator identity authentication and unalterable records of operation; enabling post-hoc compliance checks in case of plausible evidence of non-compliant autonomous weapon attacks.</li> \n <li>Relating the quantity of weapons to corresponding capacities for human-in-the-loop operation of those weapons.</li> \n <li>Designing weapons with air-gapped firing authorization circuits that are connected to the remote human operator but not to the on-board automated control system.</li> \n <li>More generally, avoiding weapon designs that enable conversion from compliant to non-compliant categories or missions solely by software updates.</li> \n <li>Designing weapons with formal proofs of relevant properties—e.g., the property that the weapon is unable to initiate an attack without human authorization. Proofs can, in principle, be provided using cryptographic techniques that allow the proofs to be checked by a third party without revealing any details of the underlying software.</li> \n <li>Facilitate access to (non-classified) AI resources (software, data, methods for ensuring safe operation) to all states that remain in compliance and participate in transparency activities.</li> \n</ul> \n<p><strong>Component 2:</strong></p> \n<p>Define and universalize guiding principles for human involvement in the use of force.</p> \n<ul> \n <li>Humans, not machines, are legal and moral agents in military operations.</li> \n <li>It is a human responsibility to ensure that any attack, including one involving autonomous weapons, complies with the laws of war.</li> \n <li>Humans responsible for initiating an attack must have sufficient understanding of the weapons, the targets, the environment and the context for use to determine whether that particular attack is lawful.</li> \n <li>The attack must be bounded in space, time, target class, and means of attack in order for the determination about the lawfulness of that attack to be meaningful.</li> \n <li>Militaries must invest in training, education, doctrine, policies, system design, and human-machine interfaces to ensure that humans remain responsible for attacks.</li> \n</ul> \n<p><strong>Component 3:</strong></p> \n<p>Develop protocols and/or technological means to mitigate the risk of unintentional escalation due to autonomous systems.</p> \n<p>Specific potential measures include:</p> \n<ul> \n <li>Developing safe rules for autonomous system behavior when in proximity to adversarial forces to avoid unintentional escalation or signaling. Examples include: \n  <ul> \n   <li>No-first-fire policy, so that autonomous weapons do not initiate hostilities without explicit human authorization.</li> \n   <li>A human must always be responsible for providing the mission for an autonomous system.</li> \n   <li>Taking steps to clearly distinguish exercises, patrols, reconnaissance, or other peacetime military operations from attacks in order to limit the possibility of reactions from adversary autonomous systems, such as autonomous air or coastal defenses.</li> \n  </ul> </li> \n <li>Developing resilient communications links to ensure recallability of autonomous systems. Additionally, militaries should refrain from jamming others’ ability to recall their autonomous systems in order to afford the possibility of human correction in the event of unauthorized behavior.</li> \n</ul> \n<p><strong>Component 4:</strong></p> \n<p>Develop strategies for preventing proliferation to illicit uses, such as by criminals, terrorists, or rogue states:</p> \n<ul> \n <li>Targeted multilateral controls to prevent large-scale sale and transfer of weaponizable robots and related military-specific components for illicit use.</li> \n <li>Employ measures to render weaponizable robots less harmful (e.g., geofencing; hard-wired kill switch; onboard control systems largely implemented in unalterable, non-reprogrammable hardware such as application-specific integrated circuits).</li> \n</ul> \n<p><strong>Component 5:</strong></p> \n<p>Conduct research to improve technologies and human-machine systems to reduce non-combatant harm and ensure IHL-compliance in the use of future weapons, including:</p> \n<ul> \n <li>Strategies to promote human moral engagement in decisions about the use of force</li> \n <li>Risk assessment for autonomous weapon systems, including the potential for large-scale effects, geopolitical destabilization, accidental escalation, increased instability due to uncertainty about the relative military balance of power, and lowering thresholds to initiating conflict and for violence within conflict</li> \n <li>Methodologies for ensuring the reliability and security of autonomous weapon systems</li> \n <li>New techniques for verification, validation, explainability, characterization of failure conditions, and behavioral specifications.</li> \n</ul> \n<p><strong>About the Authors</strong> <em>(in alphabetical order)</em></p> \n<p><a href=\"https://www.cc.gatech.edu/aimosaic/faculty/arkin/\">Ronald Arkin</a> directs the <a href=\"https://www.cc.gatech.edu/ai/robot-lab/\">Mobile Robot Laboratory</a> at Georgia Tech.</p> \n<p><a href=\"https://people.csail.mit.edu/lpk/\">Leslie Kaelbling</a> is co-director of&nbsp;the <a href=\"http://lis.csail.mit.edu/new/\">Learning and Intelligent Systems Group</a> at MIT.</p> \n<p><a href=\"https://people.eecs.berkeley.edu/~russell/\">Stuart Russell</a> is a professor of computer science and engineering at <a href=\"https://www.berkeley.edu/\">UC Berkeley</a>.</p> \n<p><a href=\"https://profiles.stanford.edu/dorsa-sadigh\">Dorsa Sadigh</a> is an assistant professor of computer science and of electrical engineering at <a href=\"https://www.stanford.edu/\">Stanford</a>.</p> \n<p><a href=\"https://www.cnas.org/people/paul-scharre\">Paul Scharre</a> directs the Technology and National Security Program at the <a href=\"https://www.cnas.org\">Center for a New American Security (CNAS)</a>.</p> \n<p><a href=\"http://www.cs.cornell.edu/selman/\">Bart Selman</a> is a professor of computer science at <a href=\"https://www.cornell.edu/\">Cornell</a>.</p> \n<p><a href=\"http://www.cse.unsw.edu.au/~tw/\">Toby Walsh</a> is a professor of artificial intelligence at the&nbsp;<a href=\"https://www.unsw.edu.au\">University of New South Wales (UNSW) Sydney</a>.</p> \n<p><em>The authors would like to thank Max Tegmark for organizing the three-day meeting from which this document was produced.</em></p> \n<hr> \n <div> \n  <p><sub><a id=\"footnote-1\" name=\"footnote-1\"></a><sup>1</sup>&nbsp;Autonomous Weapons System (AWS): A weapon system that, once activated, can select and engage targets without further intervention by a human operator. <a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-1-back\" style=\"color: gray;\">BACK TO TEXT↑</a></sub></p> \n  <p><sub><a id=\"footnote-2\" name=\"footnote-2\"></a><sup>2</sup>&nbsp;There is no implication that some authors would not personally support stronger recommendations.&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-2-back\" style=\"color: gray;\">BACK TO TEXT↑</a></sub></p> \n  <p><sub><a id=\"footnote-3\" name=\"footnote-3\"></a><sup>3</sup>&nbsp;For ease of use, this working paper will frequently shorten “autonomous weapon system” to “autonomous weapon.” The terms should be treated as synonymous, with the understanding that “weapon” refers to the entire system: sensor, decision-making element, and munition<span>.&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-3-back\" style=\"color: gray;\">BACK TO TEXT↑</a></sub></p> \n  <p><sub><a id=\"footnote-4\" name=\"footnote-4\"></a><sup>4</sup>&nbsp;Anti-personnel lethal autonomous weapon system: A weapon system that, once activated, can select and engage dismounted human targets with lethal force and without further intervention by a human operator<span>.&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-4-back\" style=\"color: gray;\">BACK TO TEXT↑</a></sub></p> \n  <p><sub><a id=\"footnote-5\" name=\"footnote-5\"></a><sup>5</sup>&nbsp;The authors are not unanimous about this item because of concerns about ease of repurposing for mass-casualty missions targeting unarmed humans. The purpose of the lower limit on explosive payload weight would be to minimize the risk of such repurposing. There is precedent for using explosive weight limit as a mechanism of delineating between anti-personnel and anti-materiel weapons, such as the <a href=\"https://en.wikipedia.org/wiki/Saint_Petersburg_Declaration_of_1868\">1868 St. Petersburg Declaration Renouncing the Use, in Time of War, of Explosive Projectiles Under 400 Grammes Weight</a><span>.&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-5-back\" style=\"color: gray;\">BACK TO TEXT↑</a></sub></p> \n </div> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RNa1KqrBSZw:_EaQTOBiM-E:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RNa1KqrBSZw:_EaQTOBiM-E:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RNa1KqrBSZw:_EaQTOBiM-E:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RNa1KqrBSZw:_EaQTOBiM-E:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=RNa1KqrBSZw:_EaQTOBiM-E:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RNa1KqrBSZw:_EaQTOBiM-E:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/RNa1KqrBSZw\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Mon, 21 Oct 2019 15:50:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NTcxMA.jpeg","linkMd5":"8ba894e1b44b55f874fed65b911e3953","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn61@2020_3/2020/08/25/10-38-01-889_eceddb2223f6c649.webp","destWidth":1240,"destHeight":930,"sourceBytes":255635,"destBytes":146912,"author":"Ronald Arkin","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RNa1KqrBSZw:_EaQTOBiM-E:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn51@2020_4/2020/08/25/10-38-10-628_b6e92550230580db.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RNa1KqrBSZw:_EaQTOBiM-E:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn68@2020_1/2020/08/25/10-38-02-649_72ebbe547f3f23cc.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/RNa1KqrBSZw":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn81@2020_1/2020/08/25/10-38-15-286_adc0ba404c07968d.webp"},"publishedOrCreatedDate":1598351859728},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"AI Agents Startle Researchers With Unexpected Hide-and-Seek Strategies","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","description":"<p>After 25&nbsp;million games, the AI agents playing hide-and-seek with each other had mastered four basic game strategies. The researchers expected that part.</p> \n<p>After a total of 380 million games, the AI players developed strategies that the researchers didn’t know were <em>possible</em> in the game environment—which the researchers had themselves created. That was the part that surprised the team at <a href=\"https://openai.com/\">OpenAI</a>, a research company based in San Francisco.</p> \n<p>The AI players learned everything via a machine learning technique known as <a href=\"https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\">reinforcement learning</a>. In this learning method, AI agents start out by taking random actions. Sometimes those random actions produce desired results, which earn them rewards. Via trial-and-error on a massive scale, they can learn sophisticated strategies.</p> \n<p>In the context of games, this process can be abetted by having the AI play against another version of itself, ensuring that the opponents will be evenly matched. It also locks the AI into a process of one-upmanship, where any new strategy that emerges forces the opponent to search for a countermeasure. Over time, this “self-play” amounted to what the researchers call an “auto-curriculum.”&nbsp;</p> \n<p>According to OpenAI researcher <a href=\"https://openai.com/blog/authors/igor/\">Igor Mordatch</a>, this experiment shows that self-play “is enough for the agents to learn surprising behaviors on their own—it’s like children playing with each other.”</p> \n<!--nextpage--> \n<iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/kopoLzvh5jY\" width=\"560\"></iframe> \n<p></p> \n<p>Reinforcement is a hot field of AI research right now. OpenAI’s researchers used the technique when they trained a <a href=\"https://openai.com/five/\">team of bots</a> to play the video game Dota 2, which <a href=\"https://www.theverge.com/2019/4/13/18309459/openai-five-dota-2-finals-ai-bot-competition-og-e-sports-the-international-champion\">squashed a world-champion human team</a> last April. The Alphabet subsidiary <a href=\"https://deepmind.com/\">DeepMind</a> has used it to triumph in the <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/mb\">ancient board game Go</a> and the <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/deepminds-ai-shows-itself-to-be-a-worldbeating-world-builder\">video game <em>StarCraft</em></a>.&nbsp;</p> \n<p><a href=\"https://allenai.org/team/anik/\">Aniruddha Kembhavi</a>, a researcher at the <a href=\"https://allenai.org/\">Allen Institute for Artificial Intelligence</a> (AI2) in Seattle, says games such as hide-and-seek offer a good way for AI agents to learn “foundational skills.” He worked on a team that taught their AllenAI to <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/pictionary-playing-ai-sketches-the-future-of-human-machine-collaborations\">play Pictionary with humans</a>, viewing the gameplay as a way for the AI to work on common sense reasoning and communication. “We are, however, quite far away from being able to translate these preliminary findings in highly simplified environments into the real world,” says Kembhavi.&nbsp;</p> \n<figure class=\"rt med\" role=\"img\"> \n <img alt=\"AI agents building fort.\" src=\"https://spectrum.ieee.org/image/MzM3NTUxMQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Illustration: OpenAI \n  </figcaption> \n  <figcaption>\n    AI agents construct a fort during a hide-and-seek game developed by OpenAI. \n  </figcaption> </img> \n</figure> \n<p>In OpenAI’s game of hide-and-seek, both the hiders and the seekers received a reward only if they won the game, leaving the AI players to develop their own strategies. Within a simple 3D environment containing walls, blocks, and ramps, the players first learned to run around and chase each other (strategy 1). The hiders next learned to move the blocks around to build forts (2), and then the seekers learned to move the ramps (3), enabling them to jump inside the forts. Then the hiders learned to move all the ramps into their forts before the seekers could use them (4).&nbsp;</p> \n<p>The two strategies that surprised the researchers came next. First the seekers learned that they could jump onto a box and “surf” it over to a fort (5), allowing them to jump in—a maneuver that the researchers hadn’t realized was physically possible in the game environment. So as a final countermeasure, the hiders learned to lock all the boxes into place (6) so they weren’t available for use as surfboards.&nbsp;</p> \n<figure class=\"lt med\" role=\"img\"> \n <img alt=\"Box surfing\" src=\"https://spectrum.ieee.org/image/MzM3NTUzNA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Illustration: OpenAI \n  </figcaption> \n  <figcaption>\n    An AI agent uses a nearby box to surf its way into a competitor’s fort. \n  </figcaption> </img> \n</figure> \n<p>In this circumstance, having AI agents behave in an unexpected way wasn’t a problem: They found different paths to their rewards, but didn’t cause any trouble. However, you can imagine situations in which the outcome would be rather serious. Robots acting in the real world could do real damage. And then there’s <a href=\"https://nickbostrom.com/\">Nick Bostrom</a>’s famous example of a paper clip factory run by an AI, whose goal is to make as many paper clips as possible. As <a href=\"https://spectrum.ieee.org/podcast/robotics/artificial-intelligence/nick-bostrom-says-we-should-trust-our-future-robot-overlords\">Bostrom told <em>IEEE Spectrum</em> back in 2014</a>, the AI might realize that “human bodies consist of atoms, and those atoms could be used to make some very nice paper clips.”</p> \n<p></p> \n<p><a href=\"https://www.linkedin.com/in/bowen-baker-59b48a65/\">Bowen Baker</a>, another member of the OpenAI research team, notes that it’s hard to predict all the ways an AI agent will act inside an environment—even a simple one. “Building these environments is hard,” he says. “The agents will come up with these unexpected behaviors, which will be a safety problem down the road when you put them in more complex environments.”&nbsp;</p> \n<p>AI researcher <a href=\"https://www.microsoft.com/en-us/research/people/kahofman/\" style=\"box-sizing: border-box; background-color: rgb(255, 255, 255); font-family: Georgia, serif; font-size: 18px;\">Katja Hofmann</a> at Microsoft Research Cambridge, in England, has seen a lot of gameplay by AI agents:&nbsp;She&nbsp;started a competition that <a href=\"https://spectrum.ieee.org/robotics/artificial-intelligence/in-the-quest-for-general-intelligence-ais-are-chasing-chickens-in-minecraft\">uses Minecraft as the playing field</a>. She says the emergent behavior seen in this game, and in prior experiments by other researchers, shows that games can be a useful for studies of safe and responsible AI.</p> \n<p>“I find demonstrations like this, in games and game-like settings, a great way to explore the capabilities and limitations of existing approaches in a safe environment,” says Hofmann. “Results like these will help us develop a better understanding on how to validate and debug reinforcement learning systems–a crucial step on the path towards real-world applications.”</p> \n<p>Baker says there’s also a hopeful takeaway from the surprises in the hide-and-seek experiment. “If you put these agents into a rich enough environment they will find strategies that we never knew were possible,” he says. “Maybe they can solve problems that we can’t imagine solutions to.”</p> \n<p><em>A version of this post appears in the November 2019 print issue as “AI Agents Play Hide-and-Seek.”</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=yy1rq30DNiI:ZPKbepRAZAE:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=yy1rq30DNiI:ZPKbepRAZAE:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=yy1rq30DNiI:ZPKbepRAZAE:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=yy1rq30DNiI:ZPKbepRAZAE:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=yy1rq30DNiI:ZPKbepRAZAE:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=yy1rq30DNiI:ZPKbepRAZAE:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/yy1rq30DNiI\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 17 Sep 2019 16:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3NTQ2NQ.jpeg","linkMd5":"ade9b8fe86da931a7835aee397c88af5","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn30@2020_1/2020/08/25/10-38-01-433_ded525da14056bcd.webp","destWidth":1240,"destHeight":930,"sourceBytes":107064,"destBytes":63040,"author":"Eliza Strickland","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM3NTUxMQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn27@2020_4/2020/08/25/10-38-06-124_5ef51988192fb6c4.webp","https://spectrum.ieee.org/image/MzM3NTUzNA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn43@2020_5/2020/08/25/10-38-10-389_0a51c416d3b156bb.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=yy1rq30DNiI:ZPKbepRAZAE:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn44@2020_6/2020/08/25/10-38-03-952_1c208efb99ff9f91.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=yy1rq30DNiI:ZPKbepRAZAE:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn66@2020_6/2020/08/25/10-38-15-218_3a336f2190e535ba.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/yy1rq30DNiI":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn56@2020_5/2020/08/25/10-38-02-400_c69e50949c11a6cd.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Caltech's Drone With Legs Takes First Steps","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>Caltech has been making progress on LEONARDO (<span>LEg ON Aerial Robotic DrOne</span>), their <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/caltech-building-agile-humanoid-robot-by-combining-legs-with-thrusters\">leggy thruster powered humanoid-thing</a>. It can now balance and walk, which is quite impressive to see.</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/NekTN9bTSS8\" width=\"620\"></iframe></p> \n <p></p> \n <p>We’ll circle back again when they’ve got it jumping and floating around.</p> \n <p>[ <a href=\"http://cast.caltech.edu/\">Caltech</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Turn the subtitles on to learn how robots became experts at slicing bubbly, melty, delicious cheese.</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/w0in0Lr0FbU\" width=\"620\"></iframe></p> \n  <p></p> \n  <blockquote> \n   <p><em>These robots learned how to do the traditional Swiss raclette from demonstration. The Robot Learning &amp; Interaction group at the Idiap Research Institute has developed an imitation learning technique allowing the robot to acquire new skills by considering position and force information, with an automatic adaptation to new situations. The range of applications is wide, including industrial robots, service robots, and assistive robots.</em></p> \n  </blockquote> \n  <p>[ <a href=\"https://www.idiap.ch/en/allnews/when-a-robot-learns-how-to-make-traditional-swiss-raclette-thanks-to-ai\">Idiap</a> ]</p> \n  <p><em>Thanks Sylvain!</em></p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>Some amazing news this week from Skydio, with the announcement of their better in every single way <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio 2 autonomous drone</a>. <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Read our full article for details</a>, but here’s a getting started video that gives you an overview of what the drone can do.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/11knl5PuUgE\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>The first batch sold out in 36 hours, but you can put down a $100 deposit to reserve the $999 drone for 2020 delivery.</p> \n   <p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p><a href=\"https://www.ubtrobot.com/pages/about-ubtech?ls=en\">UBTECH</a> is introducing a couple new robot kits for the holidays: ChampBot and FireBot.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/6lUplV78oPc\" width=\"620\"></iframe></p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/yo6QUuoeY5w?rel=0\" width=\"620\"></iframe></p> \n    <p>$130 each, available on October 20.</p> \n    <p>[ <a href=\"https://www.businesswire.com/news/home/20191001005535/en/Connecting-Kids-Brighter-Future-UBTECH-Unveils-Holiday\">Ubtech</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <blockquote> \n      <p><em>NASA’s InSight lander on Mars is trying to use its robotic arm to get the mission’s heat flow probe, or mole, digging again. InSight team engineer Ashitey Trebbi-Ollennu, based at NASA’s Jet Propulsion Laboratory in Pasadena, California, explains what has been attempted and the game plan for the coming weeks. The next tactic they’ll try will be \"pinning\" the mole against the hole it’s in.</em></p> \n     </blockquote> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/uuJoyZ0IuLk\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"https://mars.nasa.gov/insight/\">NASA</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <blockquote> \n       <p><em>We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions.</em></p> \n      </blockquote> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/cwPaof0kKdM\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"http://ryosuzuki.org/shapebots/\">Ryo Suzuki</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>Robot abuse!</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tQsLauQWp8M\" width=\"620\"></iframe></p> \n       <p></p> \n       <blockquote> \n        <p><em>Vision 60 legged robot managing unstructured terrain without vision or force sensors in its legs. Using only high-transparency actuators and 2kHz algorithmic stability control... 4-limbs and 12-motors with only a velocity command.</em></p> \n       </blockquote> \n       <p>[ <a href=\"https://ghostrobotics.io\">Ghost Robotics</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <blockquote> \n         <p><em>We asked real people to bring in real products they needed picked for their application. In MINUTES, we assembled the right tool.</em></p> \n        </blockquote> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SKTsgDqjYiI\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>This is a cool idea, but for a real challenge they should try it outside a supermarket. Or a pet store.</p> \n        <p>[ <a href=\"https://www.softroboticsinc.com/\">Soft Robotics</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <blockquote> \n          <p><em>Good water quality is important to humans and to nature. In a country with as much water as the Netherlands has, ensuring water quality is a very labour-intensive undertaking. To address this issue, researchers from TU Delft have developed a ‘pelican drone’: a drone capable of taking water samples quickly, in combination with a measuring instrument that immediately analyses the water quality. The drone was tested this week at the new Marker Wadden nature area ‘Living Lab’.</em></p> \n         </blockquote> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/wASteV6Rsyk\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"http://mavlab.tudelft.nl/pelican-drone-ensures-super-quick-water-sampling/\">MAVLab</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <blockquote> \n           <p><em>In an international collaboration led by scientists in Switzerland, three amputees merge with their bionic prosthetic legs as they climb over various obstacles without having to look. The amputees report using and feeling their bionic leg as part of their own body, thanks to sensory feedback from the prosthetic leg that is delivered to nerves in the leg’s stump.</em></p> \n          </blockquote> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hOvXsUKEewk\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://actu.epfl.ch/news/amputees-feel-as-though-their-prosthetic-limb-belo/\">EPFL</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>It’s a little hard to see, but this is one way of testing out asteroid imaging spacecraft without actually going into space: a fake asteroid and a 2D microgravity simulator.</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/LfucRFrYdDA\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"http://aerospacerobotics.caltech.edu/\">Caltech</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p>Drones can help filmmakers do the kinds of shots that would be otherwise impossible.</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/qa6QQYiZT58\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.dji.com/\">DJI</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>Two long interviews this week from Lex Fridman’s AI Podcast, and both of them are worth watching: <a href=\"http://garymarcus.com/\">Gary Marcus</a>, and <a href=\"http://norvig.com/\">Peter Norvig</a>.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/vNOTDn3D_RI\" width=\"620\"></iframe></p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_VPxEcT_Adc?rel=0\" width=\"620\"></iframe></p> \n             <p>[ <a href=\"https://lexfridman.com/ai/\">AI Podcast</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p>This week’s CMU RI Seminar comes from Tucker Hermans at the University of Utah, on “Improving Multi-fingered Robot Manipulation by Unifying Learning and Planning.”</p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/xgijLJIKgds\" width=\"620\"></iframe></p> \n              <p></p> \n              <blockquote> \n               <p><em>Multi-fingered hands offer autonomous robots increased dexterity, versatility, and stability over simple two-fingered grippers. Naturally, this increased ability comes with increased complexity in planning and executing manipulation actions. As such, I propose combining model-based planning with learned components to improve over purely data-driven or purely-model based approaches to manipulation. This talk examines multi-fingered autonomous manipulation when the robot has only partial knowledge of the object of interest. I will first present results on planning multi-fingered grasps for novel objects using a learned neural network. I will then present our approach to planning in-hand manipulation tasks when dynamic properties of objects are not known. I will conclude with a discussion of our ongoing and future research to further unify these two approaches.</em></p> \n              </blockquote> \n              <p>[ <a href=\"https://www.ri.cmu.edu/event/ri-seminar-tucker-hermans-university-of-utah-assistant-professor-2019-09-27/\">CMU RI</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <div class=\"feedflare\"> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rH4V7CL1C5g:iaetgA_Jprc:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rH4V7CL1C5g:iaetgA_Jprc:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rH4V7CL1C5g:iaetgA_Jprc:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rH4V7CL1C5g:iaetgA_Jprc:-BTjWOF_DHI\" border=\"0\" /></a> \n                <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=rH4V7CL1C5g:iaetgA_Jprc:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rH4V7CL1C5g:iaetgA_Jprc:gIN9vFwOqvQ\" border=\"0\" /></a> \n               </div> \n               <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rH4V7CL1C5g\" height=\"1\" width=\"1\" alt=\"\" /> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 04 Oct 2019 15:20:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4Nzk2NA.jpeg","linkMd5":"dd075dc0d056fad8777afa408a86be08","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn57@2020_1/2020/08/25/10-38-01-105_6270000a8ce60780.webp","destWidth":1240,"destHeight":800,"sourceBytes":360000,"destBytes":110864,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rH4V7CL1C5g:iaetgA_Jprc:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn1@2020_3/2020/08/25/10-38-15-514_098886294508d459.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rH4V7CL1C5g:iaetgA_Jprc:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn38@2020_1/2020/08/25/10-38-16-786_e30f5dd98d607b59.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rH4V7CL1C5g":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn96@2020_3/2020/08/25/10-38-15-273_a5989649c809c708.webp"},"publishedOrCreatedDate":1598351859730},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"All of the Winners in the DARPA Subterranean Challenge Tunnel Circuit","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","description":"<p>The first competitive event in the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt\">DARPA Subterranean Challenge</a> concluded last week—hopefully you were able to follow along on the livestream, on Twitter, or with some of the <a href=\"https://spectrum.ieee.org/tag/subt\">articles that we’ve posted about the event</a>. We’ll have plenty more to say about how things went for the SubT teams, but while they take a bit of a (well earned) rest, we can take a look at the winning teams as well as who won DARPA’s special superlative awards for the competition.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sBujR2XJSSY?rel=0\" width=\"620\"></iframe></p> \n<h2>First Place: Team Explorer (25/40 artifacts found)</h2> \n<p>With their rugged, reliable robots featuring giant wheels and the ability to drop communications nodes, Team Explorer was in the lead from day 1, scoring in double digits on every single run.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/emuiSC1InA4?rel=0\" width=\"620\"></iframe></p> \n<h2>Second Place: Team CoSTAR (11/40 artifacts found)</h2> \n<p>Team CoSTAR had one of the more diverse lineups of robots, and they switched up which robots they decided to send into the mine as they learned more about the course.&nbsp;</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/CilqPrFhczI?rel=0\" width=\"620\"></iframe></p> \n<h2>Third Place: Team CTU-CRAS (10/40 artifacts found)</h2> \n<p>While many teams came to SubT with DARPA funding, Team CTU-CRAS was self-funded, making them eligible for a special $200,000 Tunnel Circuit prize.</p> \n<p></p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/DupMrKxxbE4?rel=0\" width=\"620\"></iframe></p> \n<hr> \n <p>DARPA also awarded a bunch of “superlative awards” after SubT:</p> \n <h2>Most Accurate Artifact: Team Explorer</h2> \n <div class=\"imgWrapper xlrg\"> \n  <img alt=\"Explorer\" src=\"https://spectrum.ieee.org/image/MzM2MjA0Mg.png\"> </img> \n </div> \n <p>To score a point, teams had to submit the location of an artifact that was correct to within 5 meters of the artifact itself. However, DARPA was tracking the artifact locations with much higher precision—for example, the “zero” point on the backpack artifact was the center of the label on the front, which DARPA tracked to the millimeter. Team Explorer managed to return the location of a backpack with an error of just 0.18 meter, which is kind of amazing.</p> \n <h2>Down to the Wire: Team CSIRO Data61</h2> \n <div class=\"imgWrapper xlrg\"> \n  <img alt=\"CSIRO\" src=\"https://spectrum.ieee.org/image/MzM2MjA0Mw.png\"> </img> \n </div> \n <p>With just an hour to find as many artifacts as possible, teams had to find the right balance between sending robots off to explore and bringing them back into communication range to download artifact locations. Team CSIRO Data61 cut their last point pretty close, sliding their final point in with a mere 22 seconds to spare.&nbsp;</p> \n <h2>Most Distinctive Robots: Team Robotika</h2> \n <div class=\"imgWrapper xlrg\"> \n  <img alt=\"Robotika\" src=\"https://spectrum.ieee.org/image/MzM2MjA0NA.png\"> </img> \n </div> \n <p>Team Robotika had some of the quirkiest and most recognizable robots, which DARPA recognized with the “Most Distinctive” award. Robotika told us that part of the reason for that distinctiveness was practical—having a robot that was effectively in two parts meant that they could disassemble it so that it would fit in the baggage compartment of an airplane, very important for a team based in the Czech Republic.</p> \n <h2>Most Robots Per Person: Team Coordinated Robotics</h2> \n <div class=\"imgWrapper xlrg\"> \n  <img alt=\"Coordinated\" src=\"https://spectrum.ieee.org/image/MzM2MjA0NQ.png\"> </img> \n </div> \n <p>Kevin Knoedler, <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-software/coordinated-robotics-winner-nasa-space-robotics-challenge\">who won NASA’s Space Robotics Challenge entirely by himself</a>, brought his own personal swarm of drones to SubT. With a ratio of seven robots to one human, Kevin was almost certainly the hardest working single human at the challenge.</p> \n <h2>Fan Favorite: Team NCTU</h2> \n <figure class=\"xlrg\" role=\"img\"> \n  <img alt=\"Blimp\" src=\"https://spectrum.ieee.org/image/MzM2MjA0Ng.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Evan Ackerman/IEEE Spectrum \n   </figcaption> </img> \n </figure> \n <p>The Fan Favorite award went to the team that was most popular on Twitter (with the #SubTChallenge hashtag), and it may or may not be the case that I personally tweeted enough about Team NCTU’s blimp to win them this award. It’s also true that whenever we asked anyone on other teams what their favorite robot was (besides their own, of course), the blimp was overwhelmingly popular. So either way, the award is well deserved.&nbsp;</p> \n <p>DARPA shared this little behind-the-scenes clip of the blimp in action (sort of), showing what happened to the poor thing when the mine ventilation system was turned on between runs and DARPA staff had to chase it down and rescue it:</p> \n <div class=\"imgWrapper ct lrg\"> \n  <img alt=\"Blimp\" src=\"https://spectrum.ieee.org/image/MzM2MjA0Nw.gif\"> </img> \n </div> \n <hr> \n  <p>The thing to keep in mind about the results of the Tunnel Circuit is that unlike past DARPA robotics challenges (like the <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next\">DRC</a>), they don’t necessarily indicate how things are going to go for the Urban or Cave circuits because of how different things are going to be. Explorer did a great job with a team of rugged wheeled vehicles, which turned out to be ideal for navigating through mines, but they’re likely going to need to change things up substantially for the rest of the challenges, where the terrain will be much more complex.</p> \n  <p>DARPA hasn’t provided any details on the location of the Urban Circuit yet; all we know is that it’ll be sometime in February&nbsp;2020. This gives teams just six months to take all the lessons that they learned from the Tunnel Circuit and update their hardware, software, and strategies. What were those lessons, and what do teams plan to do differently next year? Check back next week, and we’ll tell you.</p> \n  <p>[ <a href=\"https://www.darpa.mil/news-events/2019-08-22\">DARPA SubT</a> ]</p> \n  <div class=\"feedflare\"> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ysDZbFjjj5w:hYIYuBJN_yU:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ysDZbFjjj5w:hYIYuBJN_yU:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ysDZbFjjj5w:hYIYuBJN_yU:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ysDZbFjjj5w:hYIYuBJN_yU:-BTjWOF_DHI\" border=\"0\" /></a> \n   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ysDZbFjjj5w:hYIYuBJN_yU:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ysDZbFjjj5w:hYIYuBJN_yU:gIN9vFwOqvQ\" border=\"0\" /></a> \n  </div> \n  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ysDZbFjjj5w\" height=\"1\" width=\"1\" alt=\"\" /> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Mon, 26 Aug 2019 20:40:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MjIwOQ.jpeg","linkMd5":"bc2b9a2906c1c252ead36f33d8597cd7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn8@2020_5/2020/08/25/10-38-00-147_54a7d4c124eb46a6.webp","destWidth":1240,"destHeight":743,"sourceBytes":390434,"destBytes":126208,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2MjA0Mg.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn81@2020_1/2020/08/25/10-38-13-762_eede1a730a144cb1.webp","https://spectrum.ieee.org/image/MzM2MjA0Mw.png":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn40@2020_5/2020/08/25/10-38-10-450_99f2399b6164613e.webp","https://spectrum.ieee.org/image/MzM2MjA0NA.png":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn14@2020_3/2020/08/25/10-38-15-715_68bd74a7874cdff0.webp","https://spectrum.ieee.org/image/MzM2MjA0NQ.png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn92@2020_2/2020/08/25/10-38-05-077_ebbd9d3a710e888b.webp","https://spectrum.ieee.org/image/MzM2MjA0Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn71@2020_4/2020/08/25/10-38-04-465_3d1f21ab6e972710.webp","https://spectrum.ieee.org/image/MzM2MjA0Nw.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn54@2020_1/2020/08/25/10-38-19-728_1ce04c5326c99030.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ysDZbFjjj5w:hYIYuBJN_yU:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn67@2020_3/2020/08/25/10-38-09-605_b6f0b47e820ca7c2.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ysDZbFjjj5w:hYIYuBJN_yU:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn20@2020_1/2020/08/25/10-38-07-127_5dd5ee76a9c9ce74.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ysDZbFjjj5w":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn44@2020_4/2020/08/25/10-38-09-463_55861e35f5b2bb2a.webp"},"publishedOrCreatedDate":1598351859735},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"How Boston Dynamics Is Redefining Robot Agility","link":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","description":"<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Boston Dynamics' Atlas jumping over an obstacle\" src=\"https://spectrum.ieee.org/image/MzQyMDQ4Ng.gif\"> \n  <figcaption class=\"hi-cap\">\n    Gif: Bob O’Connor/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p><strong>With their jaw-dropping</strong> agility and animal-like reflexes, <a href=\"https://www.bostondynamics.com/\">Boston Dynamics</a>’ bioinspired robots have always seemed to have no equal. But that preeminence hasn’t stopped the company from pushing its technology to new heights, sometimes literally. Its latest crop of legged machines can trudge up and down hills, clamber over obstacles, and even leap into the air like a gymnast. There’s no denying their appeal: Every time Boston Dynamics uploads a new video to YouTube, it quickly racks up <a href=\"https://www.youtube.com/user/BostonDynamics\">millions of views</a>. These are probably the first robots you could call Internet stars.</p> \n<aside class=\"inlay lt med-sm\"> \n <h3 class=\"sb-hed\">Spot</h3> \n <figure role=\"img\"> \n  <img alt=\"Boston Dynamics' Spot robot\" src=\"https://spectrum.ieee.org/image/MzQyMDcwNg.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Bob O’Connor \n   </figcaption> </img> \n </figure> \n <ul class=\"sb-list\"> \n  <li> <h5 class=\"sb-list-hed\">84 cm</h5> <p>HEIGHT</p> </li> \n  <li> <h5 class=\"sb-list-hed\">25 kg</h5> <p>WEIGHT</p> </li> \n  <li> <h5 class=\"sb-list-hed\">5.76 km/h</h5> <p>SPEED</p> </li> \n  <li> <p><strong>SENSING:</strong> Stereo cameras, inertial measurement unit, position/force sensors</p> </li> \n  <li> <p><strong>ACTUATION:</strong> 12 DC motors</p> </li> \n  <li> <p><strong>POWER:</strong> Battery (90&nbsp;minutes per charge)</p> </li> \n </ul> \n</aside> \n<p>Boston Dynamics, once <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/boston-dynamics-now-belongs-to-google\">owned by Google’s parent company, Alphabet</a>, and now by the <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft\">Japanese conglomerate SoftBank</a>, has long been secretive about its designs. Few publications have been granted access to its Waltham, Mass., headquarters, near Boston. But one morning this past August, <em>IEEE&nbsp;Spectrum</em> got in. We were given permission to do a unique kind of photo shoot that day. We set out to capture the company’s robots in action—running, climbing, jumping—by using high-speed cameras coupled with powerful strobes. The results you see on this page: freeze-frames of pure robotic agility.</p> \n<p>We also used the photos to create interactive views, which you can explore online on our <a href=\"https://robots.ieee.org\">Robots Guide</a>. These interactives let you spin the robots 360&nbsp;degrees, or make them walk and jump on your screen.</p> \n<p>Boston Dynamics has amassed a minizoo of robotic beasts over the years, with names like <a href=\"https://robots.ieee.org/robots/bigdog/?utm_source=spectrum\">BigDog</a>, <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/boston-dynamics-sand-flea-demonstrates-astonishing-jumping-skills\">SandFlea</a>, and <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/whoa-boston-dynamics-announces-new-wildcat-quadruped\">WildCat</a>. When we visited, we focused on the two most advanced machines the company has ever built: <a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a>, a nimble quadruped, and <a href=\"https://robots.ieee.org/robots/atlas2016/?utm_source=spectrum\">Atlas</a>, an adult-size humanoid.</p> \n<p><a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a> can navigate almost any kind of terrain while sensing its environment. Boston Dynamics recently made it <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale\">available for lease</a>, with plans to manufacture something like <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spotminis\">a thousand units per year</a>. It envisions Spot, or even packs of them, inspecting industrial sites, carrying out hazmat missions, and delivering packages. And its YouTube fame has not gone unnoticed: Even entertainment is a possibility, with <a href=\"https://twitter.com/BostonDynamics/status/1186356477063286787\">Cirque du Soleil auditioning Spot</a> as a potential new troupe member.</p> \n<p>“It’s really a milestone for us going from robots that work in the lab to these that are hardened for work out in the field,” Boston Dynamics CEO <a href=\"https://www.linkedin.com/in/marc-raibert-a520b9119/\">Marc&nbsp;Raibert</a> says in an interview.</p> \n<aside class=\"inlay rt med-sm\"> \n <h3 class=\"sb-hed\">Atlas</h3> \n <figure role=\"img\"> \n  <img alt=\"Boston Dynamics' Atlas robot\" src=\"https://spectrum.ieee.org/image/MzQyMDcyMg.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: Bob O’Connor \n   </figcaption> </img> \n </figure> \n <ul class=\"sb-list\"> \n  <li> <h5 class=\"sb-list-hed\">150&nbsp;cm</h5> <p>HEIGHT</p> </li> \n  <li> <h5 class=\"sb-list-hed\">80&nbsp;kg</h5> <p>WEIGHT</p> </li> \n  <li> <h5 class=\"sb-list-hed\">5.4&nbsp;km/h</h5> <p>SPEED</p> </li> \n  <li> <p><strong>SENSING:</strong>&nbsp;Lidar and stereo vision</p> </li> \n  <li> <p><strong>ACTUATION:</strong>&nbsp;28 hydraulic actuators</p> </li> \n  <li> <p><strong>POWER:</strong>&nbsp;Battery</p> </li> \n </ul> \n</aside> \n<p>Our other photographic subject, <a href=\"https://robots.ieee.org/robots/atlas2016/?utm_source=spectrum\">Atlas</a>, is Boston Dynamics’ biggest celebrity. This 150-centimeter-tall (4-foot-11-inch-tall) humanoid is capable of <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/boston-dynamics-atlas-robot-shows-off-parkour-skills\">impressive athletic feats</a>. Its actuators are driven by a <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/boston-dynamics-marc-raibert-on-nextgen-atlas\">compact yet powerful hydraulic system</a> that the company engineered from scratch. The unique system gives the 80-kilogram (176-pound) robot the explosive strength needed to perform acrobatic leaps and flips that don’t seem possible for such a large humanoid to do. Atlas has inspired a string of parody videos on YouTube and more than a few jokes about a robot takeover.</p> \n<p>While Boston Dynamics excels at making robots, it has yet to prove that it can sell them. Ever since its founding in 1992 as a spin-off from MIT, the company has been an R&amp;D-centric operation, with most of its early funding coming from U.S. military programs. The emphasis on commercialization seems to have intensified after the <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft\">acquisition by SoftBank</a>, in 2017. SoftBank’s founder and CEO, <a href=\"https://en.wikipedia.org/wiki/Masayoshi_Son\">Masayoshi Son</a>, is known to <a href=\"https://spectrum.ieee.org/robotics/home-robots/how-aldebaran-robotics-built-its-friendly-humanoid-robot-pepper\">love robots</a>—and profits.</p> \n<p>The launch of Spot is a significant step for Boston Dynamics as it seeks to “productize” its creations. Still, Raibert says his long-term goals have remained the same: He wants to build machines that interact with the world dynamically, just as animals and humans do. Has anything changed at all? Yes, one thing, he adds with a grin. In his early career as a roboticist, he used to write papers and count his citations. Now he counts <a href=\"https://www.youtube.com/user/BostonDynamics\">YouTube views</a>.</p> \n<ul class=\"listicle\"> \n <li> <h3 class=\"listicle-item-hed\">In the Spotlight</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Spot robot\" src=\"https://spectrum.ieee.org/image/MzQyMDYzMA.jpeg\"> \n    <figcaption class=\"hi-cap\">\n      Photo: Bob O’Connor \n    </figcaption> </img> \n  </figure> <p>Boston&nbsp;Dynamics&nbsp;designed <a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a> as a versatile mobile machine suitable for a variety of applications. The company has not announced how much Spot will cost, saying only that it is being made available to select customers, which will be able to <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale\">lease the robot</a>. A payload bay lets you add up to 14 kilograms of extra hardware to the robot’s back. One of the accessories that Boston Dynamics plans to offer is a 6-degrees-of-freedom arm, which will allow Spot to grasp objects and open doors.</p> </li> \n <li> <h3 class=\"listicle-item-hed\">Super Senses</h3> \n  <figure class=\"lt med-sm\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Spot sensors\" src=\"https://spectrum.ieee.org/image/MzQyMDU3MA.jpeg\"> \n    <figcaption class=\"hi-cap\">\n      Photo: Bob O’Connor \n    </figcaption> </img> \n  </figure> <p>Spot’s hardware&nbsp;is almost entirely custom-designed. It includes powerful processing boards for control as well as sensor modules for perception. The ­<a href=\"https://robots.ieee.org/robots/spotmini/?gallery=interactive1\">sensors are located on the front, rear, and sides of the robot’s body</a>. Each module consists of a pair of stereo cameras, a wide-angle camera, and a texture projector, which enhances 3D sensing in low light. The sensors allow the robot to use the navigation method known as <a href=\"https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping\">SLAM</a>, or simultaneous localization and mapping, to get around autonomously.</p> </li> \n <li> <h3 class=\"listicle-item-hed\">Stepping Up</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Spot climbing stairs on a platform\" src=\"https://spectrum.ieee.org/image/MzQyMDU4Nw.jpeg\"> \n    <figcaption class=\"hi-cap\">\n      Photo: Bob O’Connor \n    </figcaption> </img> \n  </figure> <p>In addition to its autonomous behaviors, Spot can also be steered by a remote operator with a game-style controller. But even when in manual mode, the robot still exhibits a high degree of autonomy. If there’s an obstacle ahead, Spot will go around it. If there are stairs, Spot will climb them. The robot goes into these operating modes and then performs the related actions completely on its own, without any input from the operator. To go down a flight of stairs, <a href=\"https://robots.ieee.org/robots/spotmini/?gallery=interactive4\">Spot walks backward</a>, an approach Boston Dynamics says provides greater stability.</p> </li> \n <li> <h3 class=\"listicle-item-hed\">Funky Feet</h3> \n  <figure class=\"lt med\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Spot dancing\" src=\"https://spectrum.ieee.org/image/MzQyMDU0Nw.gif\"> \n    <figcaption class=\"hi-cap\">\n      Gif: Bob O’Connor/IEEE Spectrum \n    </figcaption> </img> \n  </figure> <p>Spot’s legs&nbsp;are powered by 12 custom DC motors, each geared down to provide high torque. The&nbsp;<a href=\"https://robots.ieee.org/robots/spotmini/?gallery=interactive2\">robot can walk forward</a>, sideways, and backward, and trot at a top speed of 1.6 meters per second. It can also turn in place. Other gaits include crawling and pacing. In one <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/spotmini-can-dance-now\">wildly popular YouTube video</a>, Spot shows off its fancy footwork by dancing to the pop hit “Uptown Funk.”</p> </li> \n</ul> \n<ul class=\"listicle\"> \n <li> <h3 class=\"listicle-item-hed\">Robot Blood</h3> \n  <figure class=\"rt med\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Atlas robot standing\" src=\"https://spectrum.ieee.org/image/MzQyMDczOQ.jpeg\"> \n    <figcaption class=\"hi-cap\">\n      Photo: Bob O’Connor \n    </figcaption> </img> \n  </figure> <p>Atlas is powered&nbsp;by a hydraulic system consisting of 28 actuators. These actuators are basically cylinders filled with pressurized fluid that can drive a piston with great force. Their high performance is due in part to <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/boston-dynamics-marc-raibert-on-nextgen-atlas\">custom servo valves that are significantly smaller and lighter</a> than the aerospace models that Boston Dynamics had been using in earlier designs. Though not visible from the outside, the innards of an Atlas are filled with these hydraulic actuators as well as the lines of fluid that connect them. When one of those lines ruptures, Atlas bleeds the hydraulic fluid, which happens to be&nbsp;red.</p> </li> \n <li> <h3 class=\"listicle-item-hed\">Next Generation</h3> \n  <figure class=\"lt med\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Atlas getting up\" src=\"https://spectrum.ieee.org/image/MzQyMDY5MA.gif\"> \n    <figcaption class=\"hi-cap\">\n      Gif: Bob O’Connor/IEEE Spectrum \n    </figcaption> </img> \n  </figure> <p>The current version&nbsp;of Atlas is a thorough upgrade of the <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/atlas-drc-robot-is-75-percent-new-completely-unplugged\">original model</a>, which&nbsp;was built for the <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next\">DARPA Robotics Challenge</a>&nbsp;in 2015. The <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/next-generation-of-boston-dynamics-atlas-robot\">newest robot is lighter and more agile</a>. Boston Dynamics used industrial-grade 3D&nbsp;printers to make key structural parts, giving the robot greater strength-to-weight ratio than earlier designs. The next-gen Atlas can also do something that its predecessor, famously, <a href=\"https://www.youtube.com/watch?v=g0TaYhjpOfo\">could not</a>: It can get up after a fall.</p> </li> \n <li> <h3 class=\"listicle-item-hed\">Walk This Way</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Atlas jumping over an obstacle\" src=\"https://spectrum.ieee.org/image/MzQyMDUyNg.jpeg\"> \n    <figcaption class=\"hi-cap\">\n      Photo: Bob O’Connor \n    </figcaption> </img> \n  </figure> <p>To control Atlas,&nbsp;an operator provides general steering via a manual controller while the robot uses its stereo cameras and lidar to adjust to changes in the environment. Atlas can also perform certain tasks autonomously. For example, if you add special bar-code-type tags to cardboard boxes, <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/next-generation-of-boston-dynamics-atlas-robot\">Atlas can pick them up and stack them</a> or place them on&nbsp;shelves.</p> </li> \n <li> <h3 class=\"listicle-item-hed\">Biologically Inspired</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <img alt=\"Boston Dynamics' Atlas jumping up\" src=\"https://spectrum.ieee.org/image/MzQyMDUwMw.jpeg\"> \n    <figcaption class=\"hi-cap\">\n      Photos: Bob O’Connor \n    </figcaption> </img> \n  </figure> <p>Atlas’s control software doesn’t explicitly tell the robot how to move its joints, but rather it employs mathematical models of the underlying physics of the robot’s body and how it interacts with the environment. Atlas relies on its whole body to balance and move. When jumping over an obstacle or doing acrobatic stunts, the robot uses not only its legs but also its upper body, swinging its arms to propel itself just as an athlete would.</p> </li> \n</ul> \n<p><em>This article appears in the December 2019 print issue as “By Leaps and Bounds.”</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UofvhfDOHHU:aTec5z0MT0E:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UofvhfDOHHU:aTec5z0MT0E:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UofvhfDOHHU:aTec5z0MT0E:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UofvhfDOHHU:aTec5z0MT0E:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=UofvhfDOHHU:aTec5z0MT0E:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UofvhfDOHHU:aTec5z0MT0E:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/UofvhfDOHHU\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 27 Nov 2019 19:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQyMDc5Ng.jpeg","linkMd5":"7c210ebadb5e65b08c2708471190bfd4","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx/cdn90@2020_1/2020/08/25/10-37-58-951_5f6efee0592c4318.webp","destWidth":1240,"destHeight":930,"sourceBytes":84893,"destBytes":46474,"author":"Erico Guizzo","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQyMDQ4Ng.gif":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn74@2020_6/2020/08/25/10-38-20-112_150412e8ccc80ea1.webp","https://spectrum.ieee.org/image/MzQyMDcwNg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn87@2020_2/2020/08/25/10-38-10-884_ae2cec59a66b55be.webp","https://spectrum.ieee.org/image/MzQyMDcyMg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn52@2020_1/2020/08/25/10-38-04-085_d48e08b85d2ef222.webp","https://spectrum.ieee.org/image/MzQyMDYzMA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn62@2020_3/2020/08/25/10-38-13-161_ee22df69a34cc73f.webp","https://spectrum.ieee.org/image/MzQyMDU3MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn2@2020_4/2020/08/25/10-38-16-426_19fc3a920e054033.webp","https://spectrum.ieee.org/image/MzQyMDU4Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn45@2020_6/2020/08/25/10-38-13-143_98cc14093d5fea7b.webp","https://spectrum.ieee.org/image/MzQyMDU0Nw.gif":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn16@2020_6/2020/08/25/10-38-12-611_9075caed7e2a750d.webp","https://spectrum.ieee.org/image/MzQyMDczOQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn55@2020_6/2020/08/25/10-38-07-141_1b16071b7a45541f.webp","https://spectrum.ieee.org/image/MzQyMDY5MA.gif":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn56@2020_5/2020/08/25/10-38-05-504_0deaad3abe727cad.webp","https://spectrum.ieee.org/image/MzQyMDUyNg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn75@2020_5/2020/08/25/10-38-02-813_23350128c35aae41.webp","https://spectrum.ieee.org/image/MzQyMDUwMw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn46@2020_6/2020/08/25/10-38-15-080_62adf895cfc7f7fc.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UofvhfDOHHU:aTec5z0MT0E:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn39@2020_4/2020/08/25/10-38-07-278_60377186350bad3c.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UofvhfDOHHU:aTec5z0MT0E:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn79@2020_4/2020/08/25/10-38-10-817_62f86a8117407a0d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/UofvhfDOHHU":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn20@2020_6/2020/08/25/10-38-07-656_34a8da415654b004.webp"},"publishedOrCreatedDate":1598351859714},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Drones: For When Medical Intervention Has to Get There Before an Ambulance Can","link":"https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances","description":"<p><span>Medical drones carrying emergency supplies&nbsp;can reach 911 callers in New York City significantly&nbsp;</span><span>faster</span><span>&nbsp;than ambulances, according to new research.</span></p> \n<p>The study<span>&nbsp;pitted commercial&nbsp;drones against emergency medical service &nbsp;(EMS) serving part of&nbsp;Brooklyn,</span>&nbsp;New York.&nbsp;First responders from EMS&nbsp;arrived on the scene of emergencies in about 9.5 minutes on average during the study period, while the drones arrived in about 6.5 minutes.&nbsp;</p> \n<p>“Three minutes might very well represent the difference between life and death,” says Mark Hanna, a&nbsp;pediatric emergency medicine&nbsp;fellow&nbsp;at <a href=\"http://www.maimonidesmed.org\">Maimonides Medical Center</a>&nbsp;in Brooklyn,&nbsp;who orchestrated the study. Hanna presented his results last week at the American Academy of Pediatrics 2019 National Conference and Exhibition in New Orleans.</p> \n<!--nextpage--> \n<p>Hanna says that in emergencies where minutes count, drones could be flown ahead of first responders to deliver drugs and supplies.&nbsp;The drones could be equipped with a small video conference screen, through which a dispatcher could tell the 911 caller what to do.</p> \n<p>In a case of anaphylaxis (the body’s potentially fatal allergic response&nbsp;to food, medicine, or an insect sting), a drone could deliver epinephrin, which&nbsp;can quickly reverse&nbsp;the reaction. For cardiac arrest, the drone could deliver a defibrillator. For opioid overdose, an injection of Narcan will revive the user. Drones could even carry antidotes for snake bites and chemical poisoning.</p> \n<p>In Hanna’s study, off-the-shelf&nbsp;drones rigged with mock versions of these antidotes navigated&nbsp;to sites in Brooklyn during morning and evening rush hours. Hanna says he could not send the drones to actual emergencies <span>in real time&nbsp;</span>due to health data privacy laws.</p> \n<p>Instead, he used EMd&nbsp;data—how far the first responders traveled, how long it took to get there, and other information—to&nbsp;estimate the distance&nbsp;&nbsp;of the calls. He also noted the weather, wind speed, day of the week&nbsp;and time of day. Then, on a day with similar metrics, he flew drones to the approximate distances of&nbsp;previous emergency calls, and timed them.&nbsp;</p> \n<p>“This paper is lacking real-world data, and I’m completely aware of that,” says Hanna. He says he hopes his project&nbsp;will provide enough evidence to get a green light and funding for a more comprehensive study&nbsp;in which drones are sent to actual emergencies.</p> \n<p><span>“We’re not trying replace EMS. We’re trying to augment the standard of care,” Hanna says. People who call 911 are capable of doing a lot more for the patient while they wait for EMS,&nbsp;and medical drones could provide the tools they need</span><span>,&nbsp;he says.</span></p> \n<p></p> \n<p></p> \n<p>Medical drones haven’t been employed much in the U.S.,&nbsp;but that seems to be changing. <a href=\"http://flyzipline.com/impact/\">Zipline</a>, a drone delivery company, last week said it had <a href=\"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military\">completed a set of military exercises,&nbsp;</a><span><a href=\"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military\">in conjunction&nbsp;with the U.S. Department of Defense</a>, in which&nbsp;the company’s drones delivered mock blood and&nbsp;shock trauma supplies. </span></p> \n<p><span>Drug retailer Walgreens in October <a href=\"https://wing.com/united-states/virginia/\">launched a drone delivery&nbsp;service</a> in Virginia that transports non-prescription&nbsp;items&nbsp;using Alphabet’s drone&nbsp;subsidiary&nbsp;<a href=\"https://wing.com\">Wing</a>. And CVS, another drug retail&nbsp;chain,&nbsp;announced last week&nbsp;that it would <a href=\"http://pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=PressReleases&amp;id=1571676331520-698\">partner with&nbsp;delivery company UPS</a> to bring prescription medications to CVS customers’ homes. UPS&nbsp;in October said that it had <a href=\"https://pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=PressReleases&amp;id=1569933965476-404&amp;WT.mc_id=PRESSRELEASE_PRESSROOM_CVSDRONE_FAAAPPROVAL_102119\">received the U.S. government’s Part 135 Standard certification</a> to operate a drone airline.&nbsp;</span></p> \n<p>Medical drones have already been put to work&nbsp;in&nbsp;Africa, the&nbsp;Middle East&nbsp;and elsewhere.&nbsp;Zipline&nbsp;<a href=\"https://spectrum.ieee.org/static/tech-expedition-east-africas-big-bet-on-drones\">delivers blood on demand in Rwanda</a> and Ghana. And in&nbsp;a study in Iraq, drones that detect when an elderly person has fallen <a href=\"https://spectrum.ieee.org/the-human-os/biomedical/devices/drone-vs-ambulance-drone-wins\">beat ambulances by up to two minutes,</a> delivering first aid to fake patients.&nbsp;</p> \n<p>Hanna, a self-described “drone nerd,” says he was inspired to do the study in New York after hearing about Alec Momont,&nbsp;a&nbsp;grad student at Delft University of Technology, <a href=\"https://www.youtube.com/watch?v=y-rEI4bezWc&amp;t=79s\">putting a defibrillator on a drone</a>. Momont&nbsp;envisions&nbsp;a service that flies&nbsp;the drone&nbsp;to people in need.&nbsp;I thought: ‘This is saving lives through engineering,’” Hanna says.</p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ukg9RZJ3fig:ey0KAE2wY48:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ukg9RZJ3fig:ey0KAE2wY48:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ukg9RZJ3fig:ey0KAE2wY48:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ukg9RZJ3fig:ey0KAE2wY48:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ukg9RZJ3fig:ey0KAE2wY48:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ukg9RZJ3fig:ey0KAE2wY48:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ukg9RZJ3fig\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 31 Oct 2019 18:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwMzgyNQ.jpeg","linkMd5":"ecb83ca1fe85f5c86b44772662510c06","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn62@2020_2/2020/08/25/10-37-58-978_b35f47022b8dde79.webp","destWidth":1240,"destHeight":930,"sourceBytes":769631,"destBytes":136162,"author":"Emily Waltz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ukg9RZJ3fig:ey0KAE2wY48:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn93@2020_1/2020/08/25/10-38-16-338_604f19fb32cfb6cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ukg9RZJ3fig:ey0KAE2wY48:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn61@2020_2/2020/08/25/10-38-14-245_7da5ab59dc6e94ac.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ukg9RZJ3fig":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn51@2020_3/2020/08/25/10-38-06-329_6155813a0a3f4ee0.webp"},"publishedOrCreatedDate":1598351859739},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Transferring Human Motion to a Mobile Robot Manipulator","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>We are very sad to say that MIT professor emeritus <a href=\"https://en.wikipedia.org/wiki/Woodie_Flowers\">Woodie Flowers</a> has passed away. Flowers will be remembered for (among many other things, like co-founding <a href=\"https://www.firstinspires.org/\">FIRST</a>) the <a href=\"http://me-2007.mit.edu/\">MIT 2.007</a> course that he began teaching in the mid-1970s, famous for its student competitions.</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/dDbPwLODAx0\" width=\"620\"></iframe></p> \n <p></p> \n <p>These competitions got a bunch of well-deserved publicity over the years; here’s one from 1985:</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/cDnSFQ4YZLc?rel=0\" width=\"620\"></iframe></p> \n <p>And the 2.007 competitions are still going strong—this year’s theme was Moonshot, and <a href=\"https://www.facebook.com/mitmeche/videos/2007-final-robot-competition-moonshot-live-webcastusing-only-the-kit-of-tools-an/378974089493101/\">you can watch a replay of the event here</a>.</p> \n <p>[ <a href=\"http://news.mit.edu/2019/Professor-Emeritus-Woodie-Flowers-dies-75-1014\">MIT</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Looks like <a href=\"https://robots.ieee.org/robots/aibo2018/?utm_source=spectrum\">Aibo</a> is getting wireless integration with Hitachi appliances, which turns out to be pretty cute:</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/eEuByZKpA08\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>What is this magical box where you push a button and 60 seconds later fluffy pancakes come out?!</p> \n  <p>[ <a href=\"https://us.aibo.com/\">Aibo</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>LiftTiles are a “modular and reconfigurable room-scale shape display” that can turn your floor and walls into on-demand structures.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SYCmftQCilU\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>[ <a href=\"http://ryosuzuki.org/lift-tiles/\">LiftTiles</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>Ben Katz, a grad student in <a href=\"https://biomimetics.mit.edu/\">MIT’s Biomimetics Robotics Lab</a>, has been working on these beautiful desktop-sized <a href=\"https://en.wikipedia.org/wiki/Furuta_pendulum\">Furuta pendulums</a>:</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/VVQ-PGfJMuA\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>That’s a crowdfunding project I’d pay way too much for.</p> \n    <p>[ <a href=\"https://build-its-inprogress.blogspot.com/search/label/Pendulum\">Ben Katz</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>A clever bit of cable manipulation from MIT, using GelSight tactile sensors.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/-xKeWdrmuBc\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"https://arxiv.org/pdf/1910.02860.pdf\">Paper</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>A useful display of industrial autonomy on <a href=\"https://robots.ieee.org/robots/anymal/?utm_source=spectrum\">ANYmal</a> from the Oxford Robotics Group.</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/3Jl452m3gBU\" width=\"620\"></iframe></p> \n      <p></p> \n      <blockquote> \n       <p><em>This video is of a demonstration for the ORCA Robotics Hub showing the ANYbotics ANYmal robot carrying out industrial inspection using autonomy software from Oxford Robotics Institute.</em></p> \n      </blockquote> \n      <p>[ <a href=\"https://orcahub.org/\">ORCA Hub</a> ] via [ <a href=\"https://ori.ox.ac.uk/labs/drs/\">DRS</a> ]</p> \n      <p><em>Thanks Maurice!</em></p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>Meet Katie Hamilton, a software engineer at NASA’s Ames Research Center, who got into robotics because she wanted to help people with daily life. Katie writes code for robots, like <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station\">Astrobee</a>, who are assisting astronauts with routine tasks on the International Space Station.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/rUIEeaT-i7c\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://www.nasa.gov/astrobee/\">NASA Astrobee</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <blockquote> \n         <p><em>Transferring human motion to a mobile robotic manipulator and ensuring safe physical human-robot interaction are crucial steps towards automating complex manipulation tasks in human-shared environments. In this work we present a robot whole-body teleoperation framework for human motion transfer. We validate our approach through several experiments using the <a href=\"https://robots.ieee.org/robots/tiago/?utm_source=spectrum\">TIAGo robot</a>, showing this could be an easy way for a non-expert to teach a rough manipulation skill to an assistive robot. </em></p> \n        </blockquote> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/2Ers35Ohqg0\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://arxiv.org/abs/1909.06278\">Paper</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>This is pretty cool looking for an autonomous boat, but we’ll see if they can build a real one by 2020 since at the moment it’s just an average rendering.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/m6iHZlkg6TQ\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://www.ibm.com/case-studies/promare-systems-hardware-power-ai-edge\">ProMare</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>I had no idea that asparagus grows like this. But, sure does make it easy for a robot to harvest.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ci4n_uRgSA0\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://inaho.co/\">Inaho</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>Skip to 2:30 in this Pepper unboxing video to hear the noise it makes when tickled.</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/rsK4n9lhjIY\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.hitlabnz.org/\">HIT Lab NZ</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>In this interview, Jean Paul Laumond discusses his movement from mathematics to robotics and his career contributions to the field, especially in regards to motion planning and anthropomorphic motion. Describing his involvement at CNRS and in other robotics projects, such as HILARE, he comments on the distinction in perception between the robotics approach and a mathematics one. </em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/kQDFsvnsBxU\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.ieee-ras.org/roboticshistory/\">IEEE RAS History</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>Here’s a couple of videos from the CMU Robotics Institute archives, showing some of the work that took place over the last few decades.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/rw02wryYhBA\" width=\"620\"></iframe></p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/FclUuxumFjE?rel=0\" width=\"620\"></iframe></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/jrMfU2FtSBk?rel=0\" width=\"620\"></iframe></p> \n             <p>[ <a href=\"https://www.ri.cmu.edu/\">CMU RI</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p>In this episode of the Artificial Intelligence Podcast, Lex Fridman speaks with <a href=\"https://en.wikipedia.org/wiki/David_Ferrucci\">David Ferrucci</a> from IBM about <a href=\"https://robots.ieee.org/robots/watson/?utm_source=spectrum\">Watson</a> and (you guessed it) <a href=\"https://spectrum.ieee.org/robotics/artificial-intelligence\">artificial intelligence</a>.</p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Whtt2H5_isM\" width=\"620\"></iframe></p> \n              <p></p> \n              <blockquote> \n               <p><em>David Ferrucci led the team that built Watson, the IBM question-answering system that beat the top humans in the world at the game of Jeopardy. He is also the Founder, CEO, and Chief Scientist of Elemental Cognition, a company working engineer AI systems that understand the world the way people do. This conversation is part of the Artificial Intelligence podcast.</em></p> \n              </blockquote> \n              <p>[ <a href=\"https://lexfridman.com/ai/\">AI Podcast</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>This week’s CMU RI Seminar is by <a href=\"https://people.eecs.berkeley.edu/~pabbeel/\">Pieter Abbeel</a> from UC Berkeley, on “Deep Learning for Robotics.”</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/WGza-jN4CZs\" width=\"620\"></iframe></p> \n               <p></p> \n               <blockquote> \n                <p><em>Programming robots remains notoriously difficult. Equipping robots with the ability to learn would by-pass the need for what otherwise often ends up being time-consuming task specific programming. This talk will describe recent progress in deep reinforcement learning (robots learning through their own trial and error), in apprenticeship learning (robots learning from observing people), and in meta-learning for action (robots learning to learn). This work has led to new robotic capabilities in manipulation, locomotion, and flight, with the same approach underlying advances in each of these domains.</em></p> \n               </blockquote> \n               <p>[ <a href=\"https://www.ri.cmu.edu/event/ri-seminar-pieter-abbeel-uc-berkeley-professor-director-of-the-robot-learning-lab-2019-10-11/\">CMU RI</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <div class=\"feedflare\"> \n                 <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=oPPftINehQs:s-u5RQQjKAA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                 <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=oPPftINehQs:s-u5RQQjKAA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                 <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=oPPftINehQs:s-u5RQQjKAA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=oPPftINehQs:s-u5RQQjKAA:-BTjWOF_DHI\" border=\"0\" /></a> \n                 <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=oPPftINehQs:s-u5RQQjKAA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=oPPftINehQs:s-u5RQQjKAA:gIN9vFwOqvQ\" border=\"0\" /></a> \n                </div> \n                <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/oPPftINehQs\" height=\"1\" width=\"1\" alt=\"\" /> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 18 Oct 2019 21:05:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NTE5NQ.jpeg","linkMd5":"7c80d1a412783e6b99d1b3dcd2b92262","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn69@2020_3/2020/08/25/10-38-02-162_bc8de0ce4759dabf.webp","destWidth":1240,"destHeight":827,"sourceBytes":269887,"destBytes":72878,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=oPPftINehQs:s-u5RQQjKAA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn12@2020_5/2020/08/25/10-38-03-071_89cbbcc28e12fbc9.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=oPPftINehQs:s-u5RQQjKAA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn91@2020_3/2020/08/25/10-38-08-202_2c89dcd73bd4de3d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/oPPftINehQs":"https://cdn.jsdelivr.net/gh/myreaderx/cdn88@2020_3/2020/08/25/10-38-06-632_a5eb04cc169b8079.webp"},"publishedOrCreatedDate":1598351859728},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"The Blogger Behind \"AI Weirdness\" Thinks Today's AI Is Dumb and Dangerous","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous","description":"<style type=\"text/css\"> \n <!--\n.topicList {\n\tlist-style-type: none;\n}\n#TopPage {\n\tfont-family: Arial, Helvetica, sans-serif;\n\tfont-size: 12px;\n}\n--> </style> \n<p>Sure, artificial intelligence is transforming the world’s societies and economies—but can an AI come up with plausible ideas for a Halloween costume?&nbsp;</p> \n<p><a href=\"https://en.wikipedia.org/wiki/Janelle_Shane\">Janelle Shane</a> has been asking such probing questions since she started her <a href=\"https://aiweirdness.com/\">AI Weirdness</a> blog in 2016. She specializes in training neural networks (which underpin most of today’s machine learning techniques) on quirky data sets such as compilations of <a href=\"https://aiweirdness.com/post/173096796277/skyknit-when-knitters-teamed-up-with-a-neural\">knitting instructions</a>, <a href=\"https://aiweirdness.com/post/173990761332/generated-ice-cream-flavors-now-its-my-turn\">ice cream flavors</a>, and names of <a href=\"https://aiweirdness.com/post/160776374467/new-paint-colors-invented-by-neural-network\">paint colors</a>. Then she asks the neural net to generate its own contributions to these categories—and hilarity ensues. AI is not likely to disrupt the paint industry with names like “Ronching Blue,” “Dorkwood,” and “Turdly.”&nbsp;</p> \n<p>Shane’s antics have a serious purpose. She aims to illustrate the serious limitations of today’s AI, and to counteract the prevailing narrative that describes AI as well on its way to <a href=\"https://spectrum.ieee.org/computing/software/many-experts-say-we-shouldnt-worry-about-superintelligent-ai-theyre-wrong\">superintelligence</a> and complete human domination. “The danger of AI is not that it’s too smart,” Shane writes in her new book, “but that it’s not smart enough.”&nbsp;</p> \n<p>The book, which came out on Tuesday, is called <em><a href=\"https://www.readvoracious.com/titles/janelle-shane/you-look-like-a-thing-and-i-love-you/9780316525244/\">You Look Like a Thing and I Love You</a></em>. It takes its odd title from a list of AI-generated pick-up lines, all of which would at least get a person’s attention if shouted, preferably by a robot, in a crowded bar.&nbsp;Shane’s book is shot through with her trademark absurdist humor, but it also contains real explanations of machine learning concepts and techniques. It’s a painless way to take AI 101.&nbsp;</p> \n<p><a id=\"TopPageAnchor\" name=\"TopPageAnchor\"></a>She spoke with <em>IEEE Spectrum </em>about the perils of placing too much trust in AI systems, the strange AI phenomenon of “giraffing,” and her next potential Halloween costume.&nbsp;</p> \n<a id=\"TopPageAnchor\" name=\"TopPageAnchor\"> <p><strong>Janelle Shane on . . .</strong></p> </a> \n<a id=\"TopPageAnchor\" name=\"TopPageAnchor\"> </a> \n<ol class=\"topicList\"> \n <li><a id=\"TopPageAnchor\" name=\"TopPageAnchor\"></a><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicOne\">The un-delicious origin of her blog</a></li> \n <li><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicTwo\">“The narrower the problem, the smarter the AI will seem” </a></li> \n <li><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicThree\">Why overestimating AI is dangerous </a></li> \n <li><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicFour\">Giraffing! </a></li> \n <li><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicFive\">Machine and human creativity</a></li> \n</ol> \n<ol class=\"listicle\"> \n <li> <h3 class=\"listicle-item-hed\"><a name=\"qaTopicOne\"></a>The un-delicious origin of her blog</h3> <p><strong><em>IEEE Spectrum</em>: You studied electrical engineering as an undergrad, then got a master’s degree in physics. How did that lead to you becoming the comedian of AI?&nbsp;</strong></p> <p><strong>Janelle Shane: </strong>I’ve been interested in machine learning since freshman year of college. During orientation at Michigan State, a professor who worked on evolutionary algorithms gave a talk about his work. It was full of the most interesting anecdotes–some of which I’ve used in my book. He told an anecdote about people setting up a machine learning algorithm to do lens design, and the algorithm did end up designing an optical system that works… except one of the lenses was 50 feet thick, because they didn’t specify that it couldn’t do that.&nbsp;&nbsp;</p> <p>I started working in his lab on optics, doing ultra-short laser pulse work. I ended up doing a lot more optics than machine learning, but I always found it interesting. One day I came across a <a href=\"https://gist.github.com/nylki/1efbaa36635956d35bcc\">list of recipes</a> that someone had generated using a neural net, and I thought it was hilarious and remembered why I thought machine learning was so cool. That was in 2016, ages ago in machine learning land.</p> <p><strong><em>Spectrum</em>: So you decided to “establish weirdness as your goal” for your blog. What was the first weird experiment that you blogged about?&nbsp;</strong></p> <p><strong>Shane: </strong>It was generating cookbook recipes. The neural net came up with ingredients like: “Take ¼ pounds of bones or fresh bread.” <a href=\"https://aiweirdness.com/post/140219420017/the-silicon-gourmet-training-a-neural-network-to\">That recipe</a> started out: “Brown the salmon in oil, add creamed meat to the mixture.” It was making mistakes that showed the thing had no memory at all.&nbsp;</p> <p><strong><em>Spectrum</em>: You say in the book that you can learn a lot about AI by giving it a task and watching it flail. What do you learn?</strong></p> <p><strong>Shane: </strong>One thing you learn is how much it relies on surface appearances rather than deep understanding. With the recipes, for example: It got the structure of title, category, ingredients, instructions, yield at the end. But when you look more closely, it has instructions like “Fold the water and roll it into cubes.”&nbsp;So clearly this thing does not understand water, let alone the other things. It’s recognizing certain phrases that tend to occur, but it doesn’t have a concept that these recipes are describing something real. <a id=\"qaTopicTwo\" name=\"qaTopicTwo\">You start to realize how very narrow the algorithms in this world are. They only know exactly what we tell them in our data set.&nbsp;</a></p> <a id=\"qaTopicTwo\" name=\"qaTopicTwo\"> </a> <h4><a id=\"qaTopicTwo\" name=\"qaTopicTwo\"></a><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#TopPageAnchor\" id=\"TopPage\">BACK TO TOP↑</a></h4> <h3 class=\"listicle-item-hed\">“The narrower the problem, the smarter the AI will seem”</h3> <p><strong><em>Spectrum</em>: That makes me think of DeepMind’s <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/alphago-zero-goes-from-blank-slate-to-grandmaster-in-three-dayswithout-any-help-at-all\">AlphaGo</a>, which was universally hailed as a triumph for AI. It can play the game of Go better than any human, but it doesn’t know what Go is. It doesn’t know that it’s playing a game.&nbsp;</strong></p> <p><strong>Shane:</strong> It doesn’t know what a human is, or if it’s playing against a human or another program. That’s also a nice illustration of how well these algorithms do when they have a really narrow and well-defined problem.&nbsp;</p> <p>The narrower the problem, the smarter the AI will seem. If it’s not just doing something repeatedly but instead has to understand something, coherence goes down. For example, take an algorithm that can generate images of objects. If the algorithm is restricted to birds, it could do a recognizable bird. If this same algorithm is asked to generate images of any animal, if its task is that broad, the bird it generates becomes an unrecognizable brown feathered smear against a green background.</p> <p><strong><em>Spectrum</em>: That sounds… disturbing.&nbsp;</strong></p> <p><strong>Shane: </strong>It’s disturbing in a weird amusing way. What’s really disturbing is the humans it generates. It hasn’t seen them enough times to have a good representation, so you end up with an amorphous, usually pale-faced thing with way too many orifices. If you asked it to generate an image of a person eating pizza, you’ll have blocks of pizza texture floating around. But if you give that image to an image-recognition algorithm that was trained on that same data set,<a id=\"qaTopicThree\" name=\"qaTopicThree\"> it will say, “Oh yes, that’s a person eating pizza.”</a></p> <a id=\"qaTopicThree\" name=\"qaTopicThree\"> </a> <h4><a id=\"qaTopicThree\" name=\"qaTopicThree\"></a><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#TopPageAnchor\" id=\"TopPage\">BACK TO TOP↑</a></h4> <h3 class=\"listicle-item-hed\">Why overestimating AI is dangerous</h3> <p><strong><em>Spectrum</em>: Do you see it as your role to puncture the AI hype?&nbsp;</strong></p> <p><strong>Shane: </strong>I do see it that way. Not a lot of people are bringing out this side of AI. When I first started posting my results, I’d get people saying, “I don’t understand, this is AI, shouldn’t it be better than this? Why doesn't it understand?” Many of the impressive examples of AI have a really narrow task, or they’ve been set up to hide how little understanding it has. There’s a motivation, especially among people selling products based on AI, to represent the AI as more competent and understanding than it actually is.&nbsp;</p> <p><strong><em>Spectrum</em>: If people overestimate the abilities of AI, what risk does that pose?&nbsp;</strong></p> <p><strong>Shane:</strong> I worry when I see people trusting AI with decisions it can’t handle, like hiring decisions or decisions about moderating content. These are really tough tasks for AI to do well on. There are going to be a lot of glitches. I see people saying, “The computer decided this so it must be unbiased, it must be objective.”&nbsp;</p> \n  <aside class=\"inlay pullquote lt med\">\n    “If the algorithm’s&nbsp;task is to replicate human hiring decisions, it’s going to glom onto gender bias and race bias.” \n   <span class=\"pq-attrib\">—Janelle Shane, AI Weirdness blogger</span> \n  </aside> <p>That’s another thing I find myself highlighting in the work I’m doing. If the data includes bias, the algorithm will copy that bias. You can’t tell it not to be biased, because it doesn’t understand what bias is. I think that message is an important one for people to understand.&nbsp;</p> <p>If there’s bias to be found, the algorithm is going to go after it. It’s like, “Thank goodness, finally a signal that’s reliable.” But for a tough problem like: Look at these resumes and decide who’s best for the job. If its task is to replicate human hiring decisions, it’s going to glom onto gender bias and race bias. There’s an example in the book of a hiring algorithm that Amazon was developing that <a href=\"https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\">discriminated against women</a>, because the historical data it was trained on had that gender bias.&nbsp;</p> <p><strong><em>Spectrum</em>: What are the other downsides of using AI systems that don’t really understand their tasks?&nbsp;</strong></p> <p><strong>Shane: </strong>There is a risk in putting too much trust in AI and not examining its decisions. Another issue is that it can solve the wrong problems, without anyone realizing it. There have been a couple of cases in medicine. For example, there was an algorithm that was trained to recognize things like skin cancer. But instead of recognizing the actual skin condition, it latched onto signals like the markings a surgeon makes on the skin, or a ruler placed there for scale. It was treating those things as a sign of skin cancer. It’s another indication <a id=\"qaTopicFour\" name=\"qaTopicFour\">that these algorithms don’t understand what they’re looking at and what the goal really is.&nbsp;</a></p> <a id=\"qaTopicFour\" name=\"qaTopicFour\"> </a> <h4><a id=\"qaTopicFour\" name=\"qaTopicFour\"></a><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#TopPageAnchor\" id=\"TopPage\">BACK TO TOP↑</a></h4> <h3 class=\"listicle-item-hed\">Giraffing</h3> <p><strong><em>Spectrum</em>: In your blog, you often have neural nets generate names for things—such as ice cream flavors, paint colors, cats, mushrooms, and types of apples. How do you decide on topics?</strong></p> <p><strong>Shane:</strong> Quite often it’s because someone has written in with an idea or a data set. They’ll say something like, “I’m the MIT librarian and I have a whole list of <a href=\"https://aiweirdness.com/post/169581821297/thesis-titles-generated-by-neural-network\">MIT thesis titles</a>.” That one was delightful. Or they’ll say, “We are a high school robotics team, and we know where there’s a list of <a href=\"https://aiweirdness.com/post/177307156177/robotics-teams-why-not-name-them-with-a-bot\">robotics team names</a>.” It’s fun to peek into a different world. I have to be careful that I’m not making fun of the naming conventions in the field. But there’s a lot of humor simply in the neural net’s complete failure to understand. Puns in particular—it really struggles with puns.&nbsp;</p> <p><strong><em>Spectrum</em>: Your blog is quite absurd, but it strikes me that machine learning is often absurd in itself. Can you explain the concept of giraffing?</strong></p> <p><strong>Shane:</strong> This concept was originally introduced by [internet security expert] <a href=\"https://www.linkedin.com/in/melissa-elliott-769087a4/\">Melissa Elliott</a>. She proposed this phrase as a way to describe the algorithms’ tendency to see giraffes way more often than would be likely in the real world. She <a href=\"https://abad1dea.tumblr.com/post/182455506350/how-math-can-be-racist-giraffing\">posted a whole bunch of examples</a>, like a photo of an empty field in which an image-recognition algorithm has confidently reported that there are giraffes. Why does it think giraffes are present so often when they’re actually really rare? Because they’re trained on data sets from online. People tend to say, “Hey look, a giraffe!” And then take a photo and share it. They don’t do that so often when they see an empty field with rocks.&nbsp;</p> <p>There’s also <a href=\"http://demo.visualdialog.org/\">a chatbot</a> that has a delightful quirk. If you show it some photo and ask it how many giraffes are in the picture, it will always answer with <a href=\"https://twitter.com/janellecshane/status/1010960050288881665?lang=en\">some non zero number.</a> This quirk comes from the way the training data was generated: These were questions asked and answered by humans online. People tended not to ask the question “How many giraffes are there?” when the answer was zero. <a id=\"qaTopicFive\" name=\"qaTopicFive\">So you can show it a picture of someone holding a Wii remote. If you ask it how many giraffes are in the picture, it will say two.&nbsp;</a></p> <a id=\"qaTopicFive\" name=\"qaTopicFive\"> </a> <h4><a id=\"qaTopicFive\" name=\"qaTopicFive\"></a><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#TopPageAnchor\" id=\"TopPage\">BACK TO TOP↑</a></h4> <h3 class=\"listicle-item-hed\">Machine and human creativity</h3> <p><strong><em>Spectrum</em>: AI can be absurd, and maybe also creative. But you make the point that AI art projects are really human-AI collaborations: Collecting the data set, training the algorithm, and curating the output are all artistic acts on the part of the human. Do you see your work as a human-AI art project? </strong></p> <p><strong>Shane: </strong>Yes, I think there is artistic intent in my work; you could call it literary or visual. It’s not so interesting to just take a pre-trained algorithm that’s been trained on utilitarian data, and tell it to generate a bunch of stuff. Even if the algorithm isn’t one that I’ve trained myself, I think about, what is it doing that’s interesting, what kind of story can I tell around it, and what do I want to show people.&nbsp;</p> \n  <aside class=\"inlay pullquote rt med\">\n    The Halloween costume algorithm “was able to draw on its knowledge of which words are related to suggest things like sexy barnacle.”&nbsp; \n   <span class=\"pq-attrib\">—Janelle Shane, AI Weirdness blogger</span> \n  </aside> <p><strong><em>Spectrum</em>: For the past three years you’ve been getting neural nets to generate ideas for Halloween costumes. As language models have gotten dramatically better over the past three years, are the costume suggestions getting less absurd?&nbsp;</strong></p> <p><strong>Shane:</strong> Yes. Before I would get a lot more nonsense words. <a href=\"https://aiweirdness.com/post/188342947482/halloween-costumes-by-the-neural-net-gpt-2\">This time</a> I&nbsp;got&nbsp;phrases that were related to real things in the data set. I don’t believe the training data had the words Flying Dutchman or barnacle. But it was able to draw on its knowledge of which words are related to suggest things like sexy barnacle and sexy Flying Dutchman.&nbsp;</p> <p><strong><em>Spectrum</em>: This year, I saw on Twitter that someone made the <a href=\"https://twitter.com/JanelleCShane/status/1188796310369034241?s=20\">gothy giraffe costume happen</a>. Would you ever dress up for Halloween in a costume that the neural net suggested?&nbsp;</strong></p> <p><strong>Shane:</strong> I think that would be fun. But there would be some challenges. I would love to go as the sexy Flying Dutchman. But my ambition may constrict me to do something more like a list of leg parts.&nbsp;</p> <h4><a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#TopPageAnchor\" id=\"TopPage\">BACK TO TOP↑</a></h4> </li> \n</ol> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hoozvOaCyGQ:ZdVZchta5ec:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hoozvOaCyGQ:ZdVZchta5ec:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hoozvOaCyGQ:ZdVZchta5ec:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hoozvOaCyGQ:ZdVZchta5ec:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=hoozvOaCyGQ:ZdVZchta5ec:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hoozvOaCyGQ:ZdVZchta5ec:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/hoozvOaCyGQ\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 08 Nov 2019 19:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwOTIzOA.jpeg","linkMd5":"6ab80d13744ce4a42ea20fe59304598b","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn21@2020_2/2020/08/25/10-37-58-901_0e9de44fe4aa15d8.webp","destWidth":1240,"destHeight":930,"sourceBytes":190162,"destBytes":95572,"author":"Eliza Strickland","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hoozvOaCyGQ:ZdVZchta5ec:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn11@2020_3/2020/08/25/10-38-10-160_0919362de61d7e6a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hoozvOaCyGQ:ZdVZchta5ec:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn7@2020_2/2020/08/25/10-38-04-353_d5013dc2e5c161f6.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/hoozvOaCyGQ":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn82@2020_3/2020/08/25/10-38-12-646_5e1f1d7f6b71149d.webp"},"publishedOrCreatedDate":1598351859721},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"ETH Zurich Demonstrates PuppetMaster Robot","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot","description":"<p>As far as I know, the universe does not have a desperate need for robot puppeteers, and considering the difficulty of making even a halfway decent robot puppeteer, you’d think that any sensible roboticist would keep well clear of the problem. But some folks over at ETH Zurich decided that they’d have a crack at it anyway, and they started by describing why they’d likely be better off if they hadn’t:</p> \n<blockquote> \n <p><em>Marionettes are underactuated, high-dimensional, highly non-linear coupled pendulum systems. They are driven by gravity, the tension forces generated by a small number of cables, and the internal forces arising from mechanical articulation constraints. As such, the map between the actions of a puppeteer and the motions performed by the marionette is notoriously unintuitive, and mastering this unique art form takes unfaltering dedication and a great deal of practice. Our goal is to enable autonomous robots to animate marionettes with a level of skill that approaches that of human puppeteers.&nbsp;</em></p> \n</blockquote> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tYoCdwf9q5o?rel=0\" width=\"620\"></iframe></p> \n<p>I’m not much of a puppeteer myself, but this looks not bad at all, considering that the <a href=\"https://robots.ieee.org/robots/yumi/\">ABB YuMi</a> robot is missing quite a few degrees of freedom in its hands. For context, here’s someone who has mastered this unique artform through unfaltering dedication and a great deal of practice, master puppeteer Scott Land:&nbsp;</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Uk88Kef1i2M?rel=0\" width=\"620\"></iframe></p> \n<p>The&nbsp;<span>ETH Zurich project can’t yet animate a complex marionette</span>, but it’s a respectable showing with the dragon, I think. As input, all the robot needs to know is the design of the puppet at the target motion you want the puppet to make. While moving the puppet in real life, the robot is continuously simulating its motions over the next second while iteratively optimizing to try to get the puppet to move the way it’s supposed to.</p> \n<p>The usefulness of this research, thankfully, is not constrained to puppets:</p> \n<blockquote> \n <p><em>Our long term goal is to enable robots to manipulate various types of complex physical systems – clothing, soft parcels in warehouses or stores, flexible sheets and cables in hospitals or on construction sites, plush toys or bedding in our homes, etc – as skillfully as humans do. We believe the technical framework we have set up for robotic puppeteering will also prove useful in beginning to address this very important grand-challenge.</em></p> \n</blockquote> \n<h4>“PuppetMaster: Robotic Animation of Marionettes,” by Simon Zimmermann, Roi Poranne, James M. Bern, and Stelian Coros from ETH Zurich, was presented at Siggraph 2019.</h4> \n<p>[ <a href=\"http://crl.ethz.ch/papers/puppetMaster.pdf\">Paper</a> ]</p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=72K5f0p5Wz4:Qf99AY9z4yM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=72K5f0p5Wz4:Qf99AY9z4yM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=72K5f0p5Wz4:Qf99AY9z4yM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=72K5f0p5Wz4:Qf99AY9z4yM:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=72K5f0p5Wz4:Qf99AY9z4yM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=72K5f0p5Wz4:Qf99AY9z4yM:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/72K5f0p5Wz4\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 29 Aug 2019 21:50:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2NDQwNQ.jpeg","linkMd5":"4542d0e2bf7583a7652c5511e14ef53d","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx/cdn14@2020_2/2020/08/25/10-38-01-310_8133afe0ba86868c.webp","destWidth":1836,"destHeight":1306,"sourceBytes":135356,"destBytes":48888,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=72K5f0p5Wz4:Qf99AY9z4yM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn68@2020_4/2020/08/25/10-38-05-748_7441bcec3a1243d0.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=72K5f0p5Wz4:Qf99AY9z4yM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn76@2020_5/2020/08/25/10-38-03-343_f850805fa0f173c5.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/72K5f0p5Wz4":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn72@2020_3/2020/08/25/10-38-02-550_f1e8a2ea642126fc.webp"},"publishedOrCreatedDate":1598351859734},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Robot Gift Guide 2019","link":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","description":"<style type=\"text/css\"> \n <!--\n.topicList {\n\tlist-style-type: none;\n}\n--> </style> \n<p>Welcome to the eighth&nbsp;edition of <em>IEEE Spectrum’s</em> Robot&nbsp;Gift Guide!</p> \n<p>This year&nbsp;we’re featuring 15&nbsp;robotic products that we think will make fantastic holiday gifts. As always, we tried to include a broad range of robot types and prices, focusing mostly on items released this year.&nbsp;<span>(A reminder:&nbsp;</span><span>While we provide links to places where you can buy these items, we’re not endorsing any in particular, and a little bit of research may result in better deals.)</span></p> \n<p>If you need even more robot gift ideas,&nbsp;take a look at our past guides: <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2018\">2018</a>,&nbsp;<a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2017\">2017</a>,<span>&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2016\">2016</a><span>,&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/2015-robot-gift-guide\">2015</a><span>,&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/2014-robot-gift-guide\">2014</a><span>,&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/2013-robot-gift-guide\">2013</a><span>, and&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/diy/2012-robot-gift-guide\">2012</a><span>. Some of those robots are still great choices and&nbsp;</span>might be way cheaper now than when we first posted about them. And if you have suggestions that you’d like to share, post a comment below to help the rest of us find the&nbsp;perfect robot gift.</p> \n<!--nextpage--> \n<ol class=\"listicle\"> \n <li> <h3 class=\"listicle-item-hed\"><span>Skydio 2</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"Skydio 2\" src=\"https://spectrum.ieee.org/image/MzQyMTU1OA.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Image: Skydio \n   </figcaption> \n   <span> </span> \n  </figure> <p>What makes robots so compelling is their autonomy, and the <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio 2 is one of the most autonomous robots we’ve ever seen</a>. It uses an array of cameras to map its environment and avoid obstacles in real-time, making flight safe and effortless and enabling the kinds of shots that would be impossible otherwise. Seriously, this thing is magical, and it’s amazing that you can actually buy one<span>.</span></p> <h4><span><strong>$1,000<br> <a href=\"https://www.skydio.com/\"><span>Skydio</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>UBTECH Jimu MeeBot 2</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"UBTECH Jimu MeeBot 2.0\" src=\"https://spectrum.ieee.org/image/MzQyMTQ4NQ.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Image: UBTECH \n   </figcaption> \n   <span> </span> \n  </figure> <p>The Jimu MeeBot 2.0 from <a href=\"https://ubtecheducation.com/\">UBTECH</a> is a STEM education robot designed to&nbsp;be easy to build and program. It includes six servo motors, a color sensor, and LED lights. An app for&nbsp;<span>iPhone or iPad</span> provides step-by-step 3D&nbsp;instructions, and helps you code different behaviors for the robot.&nbsp;<span>It’s available exclusively from Apple.</span></p> <h4><span><strong>$130<br> <a href=\"https://www.apple.com/shop/product/HMZU2ZM/A/ubtech-jimu-robot-meebot-20-app-enabled-building-and-coding-stem-kit\"><span>Apple</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>iRobot Roomba s9+</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"iRobot Roomba S9\" src=\"https://spectrum.ieee.org/image/MzQyMTM0Mg.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Image: iRobot \n   </figcaption> \n   <span> </span> \n  </figure> <p>We know that $1,400 is a crazy amount of money to spend on a robot vacuum, but the <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/irobot-completely-redesigns-its-floor-care-robots-with-new-m6-and-s9\">Roomba s9+ is a crazy robot vacuum</a>. As if all of its sensors and mapping intelligence wasn’t enough, it empties itself, which means that you can have your floors vacuumed every single day for a month and you don’t have to even think about it. This is what home robots are supposed to be<span>.</span></p> <h4><span><strong>$1,400<br> <span><a href=\"https://store.irobot.com/default/roomba-vacuuming-robot-vacuum-irobot-roomba-s9-plus/s955020.html\" style=\"margin-bottom: 0px;\">iRobot</a></span></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>PFF Gita</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"Gita robot\" src=\"https://spectrum.ieee.org/image/MzQyMTQ3Mw.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: Piaggio Fast Forward \n   </figcaption> \n   <span> </span> \n  </figure> <p>Nobody likes carrying things, which is why Gita is perfect for everyone with an extra $3,000 lying around. Developed by <a href=\"https://www.piaggiofastforward.com/\">Piaggio Fast Forward</a>, this <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/piaggio-cargo-robot\">autonomous robot will follow you around with a cargo</a> hold full of your most important stuff, and do it in a way guaranteed to attract as much attention as possible.</p> <h4><span><strong>$3,250<br> <a href=\"https://shop.mygita.com/\"><span>Gita</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>DJI Mavic Mini</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"DJI Mavic Mini drone\" src=\"https://spectrum.ieee.org/image/MzQwNDQ5NA.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: DJI \n   </figcaption> \n   <span> </span> \n  </figure> <p>It’s tiny, it’s cheap, and it takes good pictures—what more could you ask for from a drone? And&nbsp;for $400, this is an excellent drone to get if you’re on a budget and comfortable with manual flight. Keep in mind that while the Mavic Mini is small enough that you don’t need to <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/faa-updates-drone-rules-for-everyone\">register it with the FAA</a>, you do still need to follow all the same rules and regulations<span>.</span></p> <h4><span><strong>$400<br> <a href=\"https://store.dji.com/product/mavic-mini\"><span>DJI</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\">LEGO Star Wars Droid Commander</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"LEGO Star Wars Droid Commander\" src=\"https://spectrum.ieee.org/image/MzQyMTUxNw.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Image: LEGO \n   </figcaption> \n   <span> </span> \n  </figure> <p><span>Designed for</span><span>&nbsp;kids ages 8+</span>, this LEGO set includes more than 1,000 pieces, enough to build three different droids: R2-D2, Gonk Droid, and Mouse Droid. Using&nbsp;<span>a Bluetooth-controlled robotic brick called&nbsp;Move Hub, which connects to the&nbsp;LEGO&nbsp;BOOST Star Wars&nbsp;app, kids can change how the robots behave and solve&nbsp;challenges, learning</span>&nbsp;basic robotics and coding skills<span>.</span></p> <h4><span><strong>$200<br> <a href=\"https://www.lego.com/en-us/product/droid-commander-75253\"><span>LEGO</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>Sony Aibo</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"Sony Aibo\" src=\"https://spectrum.ieee.org/image/MzE4Nzk5MQ.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: Sony \n   </figcaption> \n   <span> </span> \n  </figure> <p>Robot pets don’t get much more sophisticated (or expensive) than <a href=\"https://robots.ieee.org/robots/aibo2018/?utm_source=spectrum\">Sony’s Aibo</a>. Strictly speaking, it’s one of the <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/sony-aibo-robot-dog-is-coming-to-america\">most complex consumer robots</a> you can buy, and Sony continues to <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/sony-upgrading-aibo-with-new-home-security-features-api-access\">add to Aibo’s software</a>. Recent new features include user programmability, and the ability to “feed” it.&nbsp;</p> <h4><strong><span>$2,900 (</span>free aibone and paw pads until 12/29/2019</strong><span><strong>)<br> <a href=\"https://us.aibo.com/\"><span>Sony</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\">Neato Botvac D4 Connected</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"Neato Botvac D4 Connected\" src=\"https://spectrum.ieee.org/image/MzQyMTQ4Mw.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: Neato \n   </figcaption> \n   <span> </span> \n  </figure> <p>The Neato <a href=\"https://robots.ieee.org/robots/neato/\">Botvac</a> D4 may not have all of the features of its fancier and more expensive siblings, but it does have the features that you probably care the most about: The ability to make maps of its environment for intelligent cleaning (using lasers!), along with user-defined no-go lines that keep it where you want it. And it cleans quite well, too<span>.</span></p> <h4><strong><s>$530</s>&nbsp;&nbsp;</strong><span><strong>$350 (sale)<br> <span><a href=\"https://www.neatorobotics.com/robot-vacuum/botvac-connected-series/botvac-d4-connected/\">Neato Robotics</a></span>&nbsp;</br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>Cubelets Curiosity Set</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"Cubelets Curiosity Set\" src=\"https://spectrum.ieee.org/image/MzQyMTQ4OA.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: Modular Robotics \n   </figcaption> \n   <span> </span> \n  </figure> <p><span><a href=\"https://robots.ieee.org/robots/cubelets/\">Cubelets</a>&nbsp;are</span>&nbsp;magnetic blocks that you can snap together to make an endless variety of robots with no programming and no wires. The newest set, called Curiosity, is designed for kids ages 4+ and comes with&nbsp;10 robotic cubes. These include light and distance sensors, motors, and&nbsp;a Bluetooth module, which connects the&nbsp;robot constructions to the Cubelets app.</p> <h4><span><strong>$250<br> <a href=\"https://www.modrobotics.com/cubelets/cubelets-curiosity/\"><span>Modular Robotics</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>Tertill</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"Tertill weeding robot\" src=\"https://spectrum.ieee.org/image/MjkyMjgxNg.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: Franklin Robotics \n   </figcaption> \n   <span> </span> \n  </figure> <p><a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/roomba-inventor-joe-jones-on-weed-killing-robot\">Tertill</a> does one simple job: It weeds your garden. It’s waterproof, dirt proof, solar powered, and fully autonomous, meaning that you can leave it out in your garden all summer and just enjoy eating your plants rather than taking care of them.</p> <h4><span><strong>$350<br> <a href=\"https://www.tertill.com/product/tertill-weeding-robot/\"><span>Tertill</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\">iRobot Root</h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"iRobot Root STEM robot\" src=\"https://spectrum.ieee.org/image/MzMyNDM5NA.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: iRobot \n   </figcaption> \n   <span> </span> \n  </figure> <p><a href=\"https://robots.ieee.org/robots/root/\">Root</a> was originally developed by <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/how-root-wants-to-bring-coding-to-every-classroom\">Harvard University as a tool to help kids progressively learn to code</a>. <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/irobot-acquires-root-robotics-to-boost-stem-education-for-kids\">iRobot has taken over Root</a> and is now supporting the curriculum, which starts for kids before they even know how to read and should keep them busy for years afterwards<span>.</span></p> <h4><span><strong>$200<br> &nbsp;<a href=\"https://store.irobot.com/default/root/irobot-root/rt001.html\">iRobot</a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>LOVOT</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"LOVOT, a companion robot from Japanese company Groove X\" src=\"https://spectrum.ieee.org/image/MzQyMTQ4NA.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Image: Lovot \n   </figcaption> \n   <span> </span> \n  </figure> <p>Let’s be honest: Nobody is really quite sure what LOVOT is. We can all agree that it’s kinda cute, though. And kinda weird. But cute. Created by Japanese robotics startup <a href=\"https://groove-x.com/\">Groove X</a>, <span>LOVOT</span> does have a whole bunch of tech packed into its bizarre little body and it will do its best to get you to love it<span>.</span></p> <h4><span><strong>$2,750 (¥300,000)<br> <a href=\"https://store.lovot.life/\"><span>LOVOT</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>Sphero RVR</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"RVR, a STEM robot from Sphero\" src=\"https://spectrum.ieee.org/image/MzIzMTcyMw.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: Sphero \n   </figcaption> \n   <span> </span> \n  </figure> <p><a href=\"https://robots.ieee.org/robots/rvr/\">RVR</a> is a rugged, versatile, easy to program mobile robot. It’s a <a href=\"https://spectrum.ieee.org/automaton/robotics/diy/spheros-new-rvr-is-more-hackable-than-ever\">development platform</a> designed to be a bridge between educational robots like <a href=\"https://robots.ieee.org/robots/sphero/\">Sphero</a> and more sophisticated and expensive systems like <a href=\"https://robots.ieee.org/robots/misty/\">Misty</a>. It’s mostly affordable, very expandable, and comes from a company with a lot of experience making robots<span>.</span></p> <h4><span><strong>$250<br> <a href=\"https://www.sphero.com/rvr\"><span>Sphero</span></a></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>\"How to Train Your Robot\"</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"How to Train Your Robot book\" src=\"https://spectrum.ieee.org/image/MzQyMTY4Nw.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Image: Lawrence Hall of Science \n   </figcaption> \n   <span> </span> \n  </figure> <p><span>Aimed at 4th and 5th graders, “How to Train Your Robot,” written by Blooma Goldberg,&nbsp;</span><a href=\"https://goldberg.berkeley.edu/\" style=\"font-family: Georgia, serif; font-size: 18px;\">Ken Goldberg</a><span>, and Ashley Chase, and&nbsp;</span><span>illustrated by Dave Clegg,&nbsp;is a&nbsp;perfect introduction to robotics for kids who want to get started with designing and building robots. But the book&nbsp;isn’t just for beginners: It’s also a fun, inspiring read for kids who are already into robotics and want to go further—it even introduces concepts like computer simulations and deep learning</span>. You can download a free digital copy&nbsp;or request hardcopies <a href=\"http://sites.google.com/view/school-project-berkeley/broader-impact\">here</a>.</p> <h4><span><strong>Free<br> <span><a href=\"https://sites.google.com/view/school-project-berkeley/broader-impact\">UC Berkeley</a></span></br></strong></span></h4> </li> \n <li> <h3 class=\"listicle-item-hed\"><span>MIT Mini Cheetah</span></h3> \n  <figure class=\"xlrg\" role=\"img\"> \n   <span><img alt=\"MIT Mini Cheetah quadruped robot\" src=\"https://spectrum.ieee.org/image/MzQyMTU2OA.jpeg\" /></span> \n   <figcaption class=\"hi-cap\">\n     Photo: MIT \n   </figcaption> \n   <span> </span> \n  </figure> <p>Yes,&nbsp;<a href=\"https://robots.ieee.org/robots/spotmini/\" style=\"font-family: Georgia, serif; font-size: 18px;\">Boston Dynamics’ Spot</a><span>,&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale\" style=\"font-family: Georgia, serif; font-size: 18px;\">now available for lease</a><span>,&nbsp;is probably the world’s most famous quadruped, but&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots\">MIT is starting to pump out</a>&nbsp;<a href=\"https://robots.ieee.org/robots/minicheetah/\">Mini Cheetahs</a> en masse for researchers, and while we’re not exactly sure how you’d manage to get one of these things short of stealing one directly for MIT, a Mini Cheetah is our fantasy robotics gift this year. Mini Cheetah&nbsp;looks like a ton of fun—it’s <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research\">portable, highly dynamic, super rugged, and easy to control</a>. We want one!</p> <h4><span><strong>Price N/A<br> <a href=\"https://biomimetics.mit.edu/\">MIT Biomimetic Robotics Lab</a></br></strong></span></h4> </li> \n</ol> \n<hr> \n <p>For more tech gift ideas, see also&nbsp;<a href=\"https://spectrum.ieee.org/geek-life/tools-toys/ieee-spectrums-2019-gift-guide\"><em>IEEE Spectrum</em>’s annual Gift Guide</a>.</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=uCBEFfSmP2w:ztejN2Y3WNM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=uCBEFfSmP2w:ztejN2Y3WNM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=uCBEFfSmP2w:ztejN2Y3WNM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=uCBEFfSmP2w:ztejN2Y3WNM:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=uCBEFfSmP2w:ztejN2Y3WNM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=uCBEFfSmP2w:ztejN2Y3WNM:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/uCBEFfSmP2w\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Thu, 28 Nov 2019 03:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQyMTYxOA.jpeg","linkMd5":"b4f4c90f36287bd417b7b4e5a43aa139","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn52@2020_1/2020/08/25/10-38-00-837_f26e88e257c2e145.webp","destWidth":1240,"destHeight":746,"sourceBytes":383058,"destBytes":54404,"author":"Erico Guizzo","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQyMTU1OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn78@2020_3/2020/08/25/10-38-11-773_55594d41db1a1d40.webp","https://spectrum.ieee.org/image/MzQyMTQ4NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn12@2020_6/2020/08/25/10-38-08-933_46dd9d9b6a202245.webp","https://spectrum.ieee.org/image/MzQyMTM0Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn36@2020_6/2020/08/25/10-38-09-880_05f0c97595ff0d61.webp","https://spectrum.ieee.org/image/MzQyMTQ3Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn12@2020_4/2020/08/25/10-38-11-118_78a21b132349b7ee.webp","https://spectrum.ieee.org/image/MzQwNDQ5NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn86@2020_2/2020/08/25/10-38-13-483_13ca3e3e4055e567.webp","https://spectrum.ieee.org/image/MzQyMTUxNw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn79@2020_5/2020/08/25/10-38-09-025_8b4ef8379f5be9d1.webp","https://spectrum.ieee.org/image/MzE4Nzk5MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn65@2020_6/2020/08/25/10-38-16-097_4cb0d53df7f9d541.webp","https://spectrum.ieee.org/image/MzQyMTQ4Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn19@2020_4/2020/08/25/10-38-06-629_25933dbbd8031aa6.webp","https://spectrum.ieee.org/image/MzQyMTQ4OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn59@2020_5/2020/08/25/10-38-03-683_60b48fdfa57cbbfa.webp","https://spectrum.ieee.org/image/MjkyMjgxNg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn72@2020_4/2020/08/25/10-38-07-448_aa208f06292d1200.webp","https://spectrum.ieee.org/image/MzMyNDM5NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn8@2020_6/2020/08/25/10-38-06-923_3b99fd99d44a1542.webp","https://spectrum.ieee.org/image/MzQyMTQ4NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn98@2020_4/2020/08/25/10-38-13-878_f7c30171498f69fd.webp","https://spectrum.ieee.org/image/MzIzMTcyMw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn83@2020_1/2020/08/25/10-38-08-589_6d88d0474b1a79dd.webp","https://spectrum.ieee.org/image/MzQyMTY4Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn25@2020_3/2020/08/25/10-38-14-142_4417feaf310a7edf.webp","https://spectrum.ieee.org/image/MzQyMTU2OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn82@2020_5/2020/08/25/10-38-14-462_bccbf54455ed4124.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=uCBEFfSmP2w:ztejN2Y3WNM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn86@2020_1/2020/08/25/10-38-15-269_6a8ebd491a9198ef.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=uCBEFfSmP2w:ztejN2Y3WNM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn33@2020_1/2020/08/25/10-38-12-372_d02f2b0c0ea2c768.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/uCBEFfSmP2w":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn73@2020_4/2020/08/25/10-38-16-965_98f4be5ba6c42988.webp"},"publishedOrCreatedDate":1598351859727},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"ANYbotics Introduces Sleek New ANYmal C Quadruped","link":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","description":"<p>Quadrupedal robots are making significant advances lately, and just in the past few months we’ve seen Boston Dynamics’ <a href=\"https://robots.ieee.org/robots/spotmini/?utm_source=spectrum\">Spot</a> <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-spot-robots-pull-a-truck-and-more\">hauling a truck</a>, <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/watch-iits-new-hyqreal-quadruped-robot-pull-an-airplane\" style=\"font-family: Georgia, serif; font-size: 18px;\">IIT’s HyQReal pulling a plane</a><span>,&nbsp;</span><a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research\">MIT’s MiniCheetah doing backflips</a>, Unitree Robotics’ <a href=\"https://robots.ieee.org/robots/laikago/?utm_source=spectrum\">Laikago</a> <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816\">towing a van</a>, and <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt\">Ghost Robotics’ Vision 60 exploring a mine</a>. Robot makers are betting that their four-legged machines will prove useful in a variety of applications in construction, security, delivery, and even at home.</p> \n<p><a href=\"https://www.anybotics.com/\">ANYbotics</a> has been working on such applications for years, testing out their <a href=\"https://robots.ieee.org/robots/anymal/?utm_source=spectrum\">ANYmal robot</a> in places where humans typically don’t want to go (<a href=\"https://spectrum.ieee.org/preview/automaton/robotics/industrial-robots/north-sea-deployment-shows-how-quadruped-robots-can-be-commercially-useful\">like offshore platforms</a>) as well as places where humans <em>really</em> don’t want to go (<a href=\"https://www.youtube.com/watch?v=fdGKRgVYAtg\">like sewers</a>), and they have a better idea than most companies what can make quadruped robots successful.</p> \n<p>This week, ANYbotics is announcing a completely new quadruped platform, <a href=\"https://www.anybotics.com/anymal-legged-robot/\">ANYmal C</a>, a&nbsp;major upgrade from the really quite research-y <a href=\"https://robots.ieee.org/robots/anymal/?utm_source=spectrum\">ANYmal B</a>. The new quadruped has been optimized for ruggedness and reliability in industrial environments, with a streamlined body painted a color that lets you know it means business.&nbsp;</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_ffgWvdZyvk?rel=0\" width=\"620\"></iframe></p> \n<p>ANYmal C’s physical specs are pretty impressive for a production quadruped. It can move at 1 meter&nbsp;per second, manage 20-degree slopes and 45-degree stairs, cross 25-centimeter gaps, and squeeze through passages just 60 centimeters&nbsp;wide. It’s packed with cameras and 3D sensors, including a lidar for 3D mapping and simultaneous localization and mapping (SLAM). All these sensors (along with the vast volume&nbsp;of gait research that’s been done with ANYmal) make this one of the most reliably autonomous quadrupeds out there, with real-time motion planning and obstacle avoidance.</p> \n<figure class=\"rt med-sm\" role=\"img\"> \n <img alt=\"ANYbotics ANYmal charger\" src=\"https://spectrum.ieee.org/image/MzM2MDY4OA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: ANYbotics \n  </figcaption> \n  <figcaption>\n    ANYmal can autonomously attach itself to a cone-shaped docking station to recharge. \n  </figcaption> </img> \n</figure> \n<p>ANYmal C is also one of the ruggedest legged robots in existence. The 50-kilogram robot is IP67 rated, meaning that it’s completely impervious to dust and can withstand being submerged in a meter of water for an hour. If it’s submerged for longer than that, you’re absolutely doing something wrong. The robot will run for over 2 hours on battery power, and if that’s not enough endurance, don’t worry, because ANYmal can autonomously impale itself on a weird cone-shaped docking station to recharge.&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"ANYmal C sensor payload\" src=\"https://spectrum.ieee.org/image/MzM2MDY4OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: ANYbotics \n  </figcaption> \n  <figcaption>\n    ANYmal C’s sensor payload includes&nbsp;cameras and a lidar for 3D mapping and SLAM. \n  </figcaption> </img> \n</figure> \n<p>As far as what ANYmal C is designed to actually do, it’s mostly remote inspection tasks where you need to move around through a relatively complex environment, but where for whatever reason you’d be better off not sending a human. ANYmal C has a sensor payload that gives it lots of visual options, like thermal imaging, and with the ability to handle a 10-kilogram payload, the robot can be adapted to many different environments.</p> \n<p>Over the next few months, we’re hoping to see more examples of ANYmal C being deployed to do useful stuff in real-world environments, but for now, we do have a bit more detail from <a href=\"https://www.anybotics.com/company/\">ANYbotics CTO Christian Gehring</a>.</p> \n<p><strong><em>IEEE Spectrum:</em> Can you tell us about the development process for ANYmal C?</strong></p> \n<p><em><strong>Christian Gehring: </strong>We tested the previous generation of ANYmal (B) in a broad range of environments over the last few years and gained a lot of insights. Based on our learnings, it became clear that we would have to re-design the robot to meet the requirements of industrial customers in terms of safety, quality, reliability, and lifetime. There were different prototype stages both for the new drives and for single robot assemblies. Apart from electrical tests, we thoroughly tested the thermal control and ingress protection of various subsystems like the depth cameras and actuators.&nbsp;</em></p> \n<p><strong>What can ANYmal C do that the previous version of ANYmal can’t?</strong></p> \n<p><em>ANYmal C was redesigned with a focus on performance increase regarding actuation (new drives), computational power (new hexacore Intel i7 PCs), locomotion and navigation skills, and autonomy (new depth cameras). The new robot additionally features a docking system for autonomous recharging and an inspection payload as an option. The design of ANYmal C is far more integrated than its predecessor, which increases both performance and reliability.</em></p> \n<p><strong>How much of ANYmal C’s development and design was driven by your experience with commercial or industry customers?</strong></p> \n<p><em>Tests (such as the <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/north-sea-deployment-shows-how-quadruped-robots-can-be-commercially-useful\">offshore installation with TenneT</a>) and discussions with industry customers were important to get the necessary design input in terms of performance, safety, quality, reliability, and lifetime. Most customers ask for very similar inspection tasks that can be performed with our standard inspection payload and the required software packages. Some are looking for a robot that can also solve some simple manipulation tasks like pushing a button. Overall, most use cases customers have in mind are realistic and achievable, but some are really tough for the robot, like climbing 50° stairs in hot environments of 50°C.</em></p> \n<p><strong>Can you describe how much autonomy you expect ANYmal C to have in industrial or commercial operations?</strong></p> \n<p><em>ANYmal C is primarily developed to perform autonomous routine inspections in industrial environments. This autonomy especially adds value for operations that are difficult to access, as human operation is extremely costly. The robot can naturally also be operated via a remote control and we are working on long-distance remote operation as well.&nbsp;</em></p> \n<p><strong>Do you expect that researchers will be interested in ANYmal C? What research applications could it be useful for?</strong></p> \n<p><em>ANYmal C has been designed to also address the needs of the research community. The robot comes with two powerful hexacore Intel i7 computers and can additionally be equipped with an NVIDIA Jetson Xavier graphics card for learning-based applications. Payload interfaces enable users to easily install and test new sensors. By joining our established ANYmal Research community, researchers get access to simulation tools and software APIs, which boosts their research in various areas like control, machine learning, and navigation.</em></p> \n<p>[ <a href=\"https://www.anybotics.com/anymal-legged-robot/\">ANYmal C</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=wgEBanE3smE:EDJW7HFV0R8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=wgEBanE3smE:EDJW7HFV0R8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=wgEBanE3smE:EDJW7HFV0R8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=wgEBanE3smE:EDJW7HFV0R8:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=wgEBanE3smE:EDJW7HFV0R8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=wgEBanE3smE:EDJW7HFV0R8:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/wgEBanE3smE\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 22 Aug 2019 22:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MDcwMA.jpeg","linkMd5":"bf340835c615e553278f7841ec5e7cd7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn52@2020_1/2020/08/25/10-38-01-648_dd2020d930153c78.webp","destWidth":2304,"destHeight":1536,"sourceBytes":715350,"destBytes":193966,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2MDY4OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn84@2020_3/2020/08/25/10-38-04-191_f02f66c498f21354.webp","https://spectrum.ieee.org/image/MzM2MDY4OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn28@2020_6/2020/08/25/10-38-08-460_3809d5f337927f39.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=wgEBanE3smE:EDJW7HFV0R8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn89@2020_4/2020/08/25/10-38-01-874_ca37d6cd52575bab.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=wgEBanE3smE:EDJW7HFV0R8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn52@2020_4/2020/08/25/10-38-07-182_423bc79c009985fc.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/wgEBanE3smE":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn7@2020_1/2020/08/25/10-38-10-947_bceb434086136834.webp"},"publishedOrCreatedDate":1598351859740},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Predicting the Future with AI and Sound, Starting With Robots in Space","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space","description":"<p>We’ve all seen this moment in the movies—on board, say, a submarine or a spaceship, the chief engineer will suddenly cock their ear to listen to the background hum and say&nbsp;“something’s wrong.” Bosch is hoping to teach a computer how to do that trick in real life, and is going all the way to the International Space Station to test its technology.&nbsp;</p> \n<p>Considering the amount of data that’s communicated through non-speech sound, humans do a remarkably poor job of leveraging sound information. We’re very good at reacting to sounds (especially new or loud sounds) over relatively short timescales, but beyond that, our brains are great at just classifying most ongoing sounds as “background” and ignoring them. Computers, which have the patience we generally lack, seem like they’d be much better at this, but the focus of most developers has been on discrete sound events (like smart home devices detecting smoke alarms or breaking glass) rather than longer term sound patterns.&nbsp;</p> \n<p>Why should those of us who aren’t movie characters&nbsp;care about how patterns of sound change over time? The simple reason is because our everyday lives are full of machines that both make a lot of noise and tend to break expensively from time to time. Right now, I’m listening to my washing machine, which makes some weird noises. I don’t have a very good idea of whether those weird noises are <em>normal</em> weird noises, and more to the point, I have an even worse idea whether it was making the <em>same</em> weird noises the last time I ran it. Knowing whether a machine is making weirder noises than it used to be, could potentially clue me in to an emerging problem, one&nbsp;that I could solve through cheap preventative maintenance rather than an expensive repair later on.</p> \n<p>Bosch, the German company that almost certainly makes a significant percentage of the parts in your car as well as appliances, power tools, industrial systems, and a whole bunch of other stuff, is trying to figure out how they can use deep learning to identify and track the noises that machines make over time. The idea is to be able to identify subtle changes in sound to warn of pending problems before they happen. And one group of people <em>very</em> interesting in getting advanced warning of problems are the astronauts floating around in the orbiting bubble of life that is the ISS.&nbsp;</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/QZEbkiJSf3A?rel=0\" width=\"620\"></iframe></p> \n<p>The <a href=\"https://www.bosch.com/stories/acoustic-sensors/\">SoundSee</a> directional microphone array is Bosch’s payload for NASA’s Astrobee robot, which <a href=\"https://spectrum.ieee.org/tag/astrobee\">we’ve written about extensively</a>. <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station\">Astrobee had its first autonomous flight aboard the ISS just last month</a>, and after the robot finishes getting checked out and calibrated, SoundSee will take up residence in one of Astrobee’s modular payload bays. Once installed, it’ll go on a variety of missions, both passively recording audio as Astrobee goes about its business as well as recording targeted audio of specific systems.&nbsp;</p> \n<aside class=\"inlay pullquote lt med\">\n  “These kinds of subtle, long-term patterns and variations could give us surprisingly rich information about system degradation” \n</aside> \n<p>One of SoundSee’s first tasks will be to make sound intensity surveys of the ISS, a fairly dull job that astronauts currently spend about two hours doing by hand every few months. Ideally, SoundSee and Astrobee will be able to automate this task. But the more interesting mission (especially for Earth applications) will be the acoustic&nbsp;monitoring of equipment, listening to the noises made by systems like the Environmental Control and Life Support System (<a href=\"https://www.nasa.gov/centers/marshall/history/eclss.html\">ECLSS</a>) and the Treadmill with Vibration Isolation and Stabilization (<a href=\"https://lsda.jsc.nasa.gov/Hardware/hardw/1191\">TVIS</a>).</p> \n<p>The audio that SoundSee records with its microphone array will be sent back down to Bosch, where researchers will use deep audio analytics to filter out background noise as well as the noise of the robot itself, with the goal of being able to isolate the sound being made by specific systems. By using deep learning algorithms trained on equivalent systems on Earth, Bosch hopes that SoundSee will be able to provide a sort of “internal snapshot” of how that system is functioning. Or as the case may be, <span>not functioning,&nbsp;</span>in plenty of time for astronauts to make repairs.&nbsp;</p> \n<p>“We’re working on unsupervised anomaly detection algorithms,” explains <a href=\"https://www.linkedin.com/in/samarjit-das-5200479/\">Sam Das</a>, principal researcher and SoundSee project lead at Bosch, “and we have some deep learning-based approaches that could detect a gradual or sudden change of the machine’s operating characteristics.” SoundSee won’t be able to predict everything, he says, but “it will be a line of defense to track slow deviation from normal dynamical models, and tell us, ‘Hey, you should go check this out.’ It may be a false alarm, but our system will be trained to listen for suspicious behavior. These kinds of subtle, long-term patterns and variations could give us surprisingly rich information about system degradation. That’s the ultimate goal, that we’d be able to identify these things way before any other sensing capability.”</p> \n<p>Das says that you can think of SoundSee as analogous to training a vision-based system to analyze someone walking. First, you’d train the system on what a normal walking gait looks like. Then, you’d train the system to be able to identify when someone falls. Eventually, the system would be able to identify stumbles, then muscle cramps, and the end goal would be a system that could say, ‘it looks like one of your muscles might be just starting to cramp up, better take it easy!’</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Bosch SoundSee\" src=\"https://spectrum.ieee.org/image/MzQxNzc0MA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Bosch \n  </figcaption> </img> \n</figure> \n<p>The reason to put the SoundSee system on a mobile robot, rather than use a distributed array of stationary microphones, it to be able to combine localization information with the audio data, which Das says provides much more useful data. “A moving platform means that you can localize sources of sound. Now, we can fuse the information from audio we’re getting at different points, aggregate that information along the motion trajectory, and then take that a step further by creating a sound map of the environment.”</p> \n<p>This concept extends to operations on Earth as well, and Das sees one of the first potential applications of the SoundSee technology as warehouse environments full of mobile robots. “There are a lot of features of this experiment that could be immediately applied on a manufacturing floor or warehouse where you have ground robots moving around—think of deploying SoundSee for each machine, and you’d have a virtual inspector for physical infrastructure monitoring.”</p> \n<p>Longer term, it’s pretty obvious where this kind of technology is destined, especially coming from Bosch, the world’s largest automotive parts supplier. Having a SoundSee-like system in your car already trained on algorithms for what normal operation sounds like would be able to predict maintenance needs and precisely identify emerging mechanical issues, almost certainly before they become audible to you, and very likely way before you’d have any other way of knowing.&nbsp;</p> \n<p>“Sound can give you so much more information about the environment,” says Das. “From the HVAC system in your house to the engine in your car, the operating state of machines and their functional health can be revealed by audio patterns.” And all we have to do is listen.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=sZJo6qaI9cs:NrxaeWv9MOM:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=sZJo6qaI9cs:NrxaeWv9MOM:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=sZJo6qaI9cs:NrxaeWv9MOM:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=sZJo6qaI9cs:NrxaeWv9MOM:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=sZJo6qaI9cs:NrxaeWv9MOM:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=sZJo6qaI9cs:NrxaeWv9MOM:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/sZJo6qaI9cs\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 21 Nov 2019 21:35:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxNzY4MA.jpeg","linkMd5":"98d9acef762105ed3deea1273ccf06c8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn38@2020_5/2020/08/25/10-37-58-813_729009bcf7a04300.webp","destWidth":1240,"destHeight":930,"sourceBytes":189130,"destBytes":100316,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQxNzc0MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn99@2020_6/2020/08/25/10-38-04-265_1a976b2f04dec70e.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=sZJo6qaI9cs:NrxaeWv9MOM:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn57@2020_5/2020/08/25/10-38-14-185_822d1be621502f77.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=sZJo6qaI9cs:NrxaeWv9MOM:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn33@2020_4/2020/08/25/10-38-17-859_ba8c7834a2fb0d50.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/sZJo6qaI9cs":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn50@2020_1/2020/08/25/10-38-12-209_bdbae0ce53514ede.webp"},"publishedOrCreatedDate":1598351859717},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Quantum Computing Gets a Boost From AI and Crowdsourcing","link":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","description":"<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"illustration\" src=\"https://spectrum.ieee.org/image/MzM5NDMxNg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Illustration: Greg Mably \n  </figcaption> </img> \n</figure> \n<p><strong>Anyone of a certain age </strong>who has even a passing interest in computers will remember the remarkable breakthrough that <a href=\"https://www.ibm.com/us-en/\">IBM</a> made in 1997 when its Deep Blue chess-playing computer defeated Garry Kasparov, then the world chess champion. Computer scientists passed another such milestone in March 2016, when <a href=\"https://www.ibm.com/us-en/\">DeepMind</a> (a subsidiary of Alphabet, Google’s parent company) announced that its AlphaGo program had <a href=\"https://spectrum.ieee.org/tech-talk/computing/software/monster-machine-defeats-prominent-pro-player\">defeated world-champion player Lee Sedol</a> in the game of Go, a board game that had vexed AI researchers for decades. Recently, DeepMind’s algorithms have also bested human players in the computer games <a href=\"https://starcraft2.com/en-us/\"><em>StarCraft II</em></a><em> </em>and <a href=\"https://store.steampowered.com/app/2200/Quake_III_Arena/\"><em>Quake Arena III</em></a>.</p> \n<p>Some believe that the cognitive capacities of machines will overtake those of human beings in many spheres within a few decades. Others are more cautious and point out that our inability to understand the source of our own cognitive powers presents a daunting hurdle. How can we make thinking machines if we don’t fully understand our own thought processes?</p> \n<p>Citizen science, which enlists masses of people to tackle research problems, holds promise here, in no small part because it can be used effectively to explore the boundary between human and artificial intelligence.</p> \n<p>Some citizen-science projects ask the public to collect data from their surroundings (as <a href=\"http://www.e-butterfly.org/about/\">eButterfly</a> does for butterflies) or to monitor delicate ecosystems (as <a href=\"http://www.gbrmpa.gov.au/our-work/our-programs-and-projects/eye-on-the-reef\">Eye on the Reef</a> does for Australia’s Great Barrier Reef). Other projects rely on online platforms on which people help to categorize obscure phenomena in the night sky (<a href=\"https://www.zooniverse.org/\">Zooniverse</a>) or add to the understanding of the structure of proteins (<a href=\"https://fold.it/portal/\">Foldit</a>). Typically, people can contribute to such projects without any prior knowledge of the subject. Their fundamental cognitive skills, like the ability to quickly recognize patterns, are sufficient.</p> \n<p>In order to design and develop video games that can allow citizen scientists to tackle scientific problems in a variety of fields, professor and group leader Jacob Sherson founded <a href=\"https://www.scienceathome.org/\">ScienceAtHome</a> (SAH), at <a href=\"https://international.au.dk/\">Aarhus University</a>, in Denmark. The group began by considering topics in quantum physics, but today SAH hosts games covering other areas of physics, math, psychology, cognitive science, and behavioral economics. We at SAH search for innovative solutions to real research challenges while providing insight into how people think, both alone and when working in groups.</p> \n<aside class=\"inlay pullquote xlrg\">\n  It is computationally intractable to completely map out a&nbsp;higher-dimensional landscape: It is called the curse of high&nbsp;dimensionality, and it plagues many optimization&nbsp;problems. \n</aside> \n<p>We believe that the design of new AI algorithms would benefit greatly from a better understanding of how people solve problems. This surmise has led us to establish the <a href=\"https://hybridintelligence.eu/\">Center for Hybrid Intelligence</a> within SAH, which tries to combine human and artificial intelligence, taking advantage of the particular strengths of each. The center’s focus is on the gamification of scientific research problems and the development of interfaces that allow people to understand and work together with AI.</p> \n<p><strong>Our first game, </strong><em>Quantum Moves</em>, was inspired by our group’s research into <a href=\"https://www.quantamagazine.org/tag/the-future-of-quantum-computing/\">quantum computers</a>. Such computers can in principle solve certain problems that would take a classical computer billions of years. Quantum computers could challenge current cryptographic protocols, aid in the design of new materials, and give insight into natural processes that require an exact solution of the equations of quantum mechanics—something normal computers are inherently bad at doing.</p> \n<p>One candidate system for building such a computer would capture individual atoms by “freezing” them, as it were, in the interference pattern produced when a laser beam is reflected back on itself. The captured atoms can thus be organized like eggs in a carton, forming a <a href=\"https://physicstoday.scitation.org/doi/10.1063/1.3502540\">periodic crystal of atoms and light</a>. Using these atoms to perform quantum calculations requires that we use tightly focused laser beams, called optical tweezers, to transport the atoms from site to site in the light crystal. This is a tricky business because individual atoms do not behave like particles; instead, they resemble a wavelike liquid governed by the laws of quantum mechanics.</p> \n<p>In <em>Quantum Moves</em>, a player manipulates a touch screen or mouse to move a simulated laser tweezer and pick up a trapped atom, represented by a liquidlike substance in a bowl. Then the player must bring the atom back to the tweezer’s initial position while trying to minimize the sloshing of the liquid. Such sloshing would increase the energy of the atom and ultimately introduce errors into the operations of the quantum computer. Therefore, at the end of a move, the liquid should be at a complete standstill.</p> \n<p>To understand how people and computers might approach such a task differently, you need to know something about how computerized optimization algorithms work. The countless ways of moving a glass of water without spilling may be regarded as constituting a “solution landscape.” One solution is represented by a single point in that landscape, and the height of that point represents the quality of the solution—how smoothly and quickly the glass of water was moved. This landscape might resemble a mountain range, where the top of each mountain represents a local optimum and where the challenge is to find the highest peak in the range—the global optimum.</p> \n<figure class=\"lt med\" role=\"img\"> \n <img alt=\"illustration by Greg Mably\" src=\"https://spectrum.ieee.org/image/MzM5NDM1MQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Illustration: Greg Mably \n  </figcaption> </img> \n</figure> \n<p>Researchers must compromise between searching the landscape for taller mountains (“exploration”) and climbing to the top of the nearest mountain (“exploitation”). Making such a trade-off may seem easy when exploring an actual physical landscape: Merely hike around a bit to get at least the general lay of the land before surveying in greater detail what seems to be the tallest peak. But because each possible way of changing the solution defines a new dimension, a realistic problem can have thousands of dimensions. It is computationally intractable to completely map out such a higher-dimensional landscape. We call this the curse of high dimensionality, and it <a href=\"https://spectrum.ieee.org/aerospace/satellites/overclock-my-satellite\">plagues many optimization problems</a>.</p> \n<p>Although algorithms are wonderfully efficient at crawling to the top of a given mountain, finding good ways of searching through the broader landscape poses quite a challenge, one that is at the forefront of AI research into such control problems. The conventional approach is to come up with clever ways of reducing the search space, either through insights generated by researchers or with machine-learning algorithms trained on large data sets.</p> \n<p>At SAH, we attacked certain quantum-optimization problems by turning them into a game. Our goal was not to show that people can beat computers in this arena but rather to understand the process of generating insights into such problems. We addressed two core questions: whether allowing players to explore the infinite space of possibilities will help them find good solutions and whether we can learn something by studying their behavior.</p> \n<p>Today, more than 250,000 people have played <em>Quantum Moves</em>, and to our surprise, they did in fact search the space of possible moves differently from the algorithm we had put to the task. Specifically, <a href=\"https://www.nature.com/articles/nature17620\">we found that</a> although players could not solve the optimization problem on their own, they were good at searching the broad landscape. The computer algorithms could then take those rough ideas and refine them.</p> \n<aside class=\"inlay pullquote xlrg\">\n  Herbert A. Simon said that “solving a problem simply means representing it so as to make the solution transparent.” Apparently, that’s what our games can do with their novel user interfaces. \n</aside> \n<p>Perhaps even more interesting was our discovery that players had two distinct ways of solving the problem, each with a clear physical interpretation. One set of players started by placing the tweezer close to the atom while keeping a barrier between the atom trap and the tweezer. In classical physics, a barrier is an impenetrable obstacle, but because the atom liquid is a quantum-mechanical object, it can tunnel through the barrier into the tweezer, after which the player simply moved the tweezer to the target area. Another set of players moved the tweezer directly into the atom trap, picked up the atom liquid, and brought it back. We called these two strategies the “tunneling” and “shoveling” strategies, respectively.</p> \n<p>Such clear strategies are extremely valuable because they are very difficult to obtain directly from an optimization algorithm. Involving humans in the optimization loop can thus help us gain insight into the underlying physical phenomena that are at play, knowledge that may then be transferred to other types of problems.</p> \n<p><em>Quantum Moves</em> raised several obvious issues. First, because generating an exceptional solution required further computer-based optimization, players were unable to get immediate feedback to help them improve their scores, and this often left them feeling frustrated. Second, we had tested this approach on only one scientific challenge with a clear classical analogue, that of the sloshing liquid. We wanted to know whether such gamification could be applied more generally, to a variety of scientific challenges that do not offer such immediately applicable visual analogies.</p> \n<p>We address these two concerns in <a href=\"https://www.scienceathome.org/games/quantum-moves-2/\"><em>Quantum Moves 2</em></a>. Here, the player first generates a number of candidate solutions by playing the original game. Then the player chooses which solutions to optimize using a built-in algorithm. As the algorithm improves a player’s solution, it modifies the solution path—the movement of the tweezer—to represent the optimized solution. Guided by this feedback, players can then improve their strategy, come up with a new solution, and iteratively feed it back into this process. This gameplay provides high-level heuristics and adds human intuition to the algorithm. The person and the machine work in tandem—a step toward true hybrid intelligence.</p> \n<p>In parallel with the development of <em>Quantum Moves 2</em>, we also studied how people collaboratively solve complex problems. To that end, we <a href=\"https://www.pnas.org/content/115/48/E11231\">opened our atomic physics laboratory</a> to the general public—virtually. We let people from around the world dictate the experiments we would run to see if they would find ways to improve the results we were getting. What results? That’s a little tricky to explain, so we need to pause for a moment and provide a little background on the relevant physics.</p> \n<p>One of the essential steps in building the quantum computer along the lines described above is to create the coldest state of matter in the universe, known as a<a href=\"https://physicstoday.scitation.org/doi/10.1063/1.2808119\"> Bose-Einstein condensate</a>. Here millions of atoms oscillate in synchrony to form a wavelike substance, one of the largest purely quantum phenomena known. To create this ultracool state of matter, researchers typically use a combination of laser light and magnetic fields. There is no familiar physical analogy between such a <a href=\"https://spectrum.ieee.org/at-work/innovation/this-looks-like-a-job-forsuperatoms\">strange state of</a> matter and the phenomena of everyday life.</p> \n<p>The result we were seeking in our lab was to create as much of this enigmatic substance as was possible given the equipment available. The sequence of steps to accomplish that was unknown. We hoped that gamification could help to solve this problem, even though it had no classical analogy to present to game players.</p> \n<figure class=\"stacked lt med\" role=\"img\"> \n <img alt=\"img\" src=\"https://spectrum.ieee.org/image/MzM5NDM2Mw.jpeg\"> <img alt=\"img\" src=\"https://spectrum.ieee.org/image/MzM5NDM2NA.octet-stream\"> <img alt=\"img\" src=\"https://spectrum.ieee.org/image/MzM5NDM2NQ.octet-stream\"> <img alt=\"img\" src=\"https://spectrum.ieee.org/image/MzM5NDM2Ng.octet-stream\"> \n     <figcaption class=\"hi-cap\">\n       Images: ScienceAtHome \n     </figcaption> \n     <figcaption> \n      <strong>Fun and Games:</strong> The&nbsp; \n      <em>Quantum Moves</em> game evolved over time, from a relatively crude early version [top] to its current form [second from top] and then a major revision, \n      <em>Quantum Moves&nbsp;2</em> [third from top]. \n      <em>Skill Lab: Science Detective</em> games [bottom] test players’ cognitive skills. \n     </figcaption> </img></img></img></img> \n</figure> \n<p>In October 2016, we released a game that, for two weeks, guided how we created Bose-Einstein condensates in our laboratory. By manipulating simple curves in the game interface, players generated experimental sequences for us to use in producing these condensates—and they did so without needing to know anything about the underlying physics. A player would generate such a solution, and a few minutes later we would run the sequence in our laboratory. The number of ultracold atoms in the resulting Bose-Einstein condensate was measured and fed back to the player as a score. Players could then decide either to try to improve their previous solution or to copy and modify other players’ solutions. About 600 people from all over the world participated, submitting 7,577 solutions in total. Many of them yielded bigger condensates than we had previously produced in the lab.</p> \n<p>So this exercise succeeded in achieving our primary goal, but it also allowed us to learn something about human behavior. We learned, for example, that <a href=\"http://scitech.au.dk/om-science-and-technology/aktuelt/quantum-science-turns-social/\">players behave differently</a> based on where they sit on the leaderboard. High-performing players make small changes to their successful solutions (exploitation), while poorly performing players are willing to make more dramatic changes (exploration). As a collective, the players nicely balance exploration and exploitation. How they do so provides valuable inspiration to researchers trying to understand human problem solving in social science as well as to those designing new AI algorithms.</p> \n<p>How could mere amateurs outperform experienced experimental physicists? The players certainly weren’t better at physics than the experts—<a href=\"https://ieeexplore.ieee.org/document/5216589\">but they could do better because of the way in which the problem was posed</a>. By turning the research challenge into a game, we gave players the chance to explore solutions that had previously required complex programming to study. Indeed, even expert experimentalists improved their solutions dramatically by using this interface.</p> \n<p>Insight into why that’s possible can probably be found in the words of the late economics Nobel laureate Herbert A. Simon: “<a href=\"https://monoskop.org/images/9/9c/Simon_Herbert_A_The_Sciences_of_the_Artificial_3rd_ed.pdf\">Solving a problem simply means representing it so as to make the solution transparent</a> [PDF].” Apparently, that’s what our games can do with their novel user interfaces. We believe that such interfaces might be a key to using human creativity to solve other complex research problems.</p> \n<p>Eventually, we’d like to get a better understanding of why this kind of gamification works as well as it does. A first step would be to collect more data on what the players do while they are playing. But even with massive amounts of data, detecting the subtle patterns underlying human intuition is an overwhelming challenge. To advance, we need a deeper insight into the cognition of the individual players.</p> \n<p>As a step forward toward this goal, ScienceAtHome created <a href=\"https://www.scienceathome.org/games/skill-lab-science-detective/\"><em>Skill Lab: </em><em>Science</em><em> Detective</em></a>, a suite of minigames exploring visuospatial reasoning, response inhibition, reaction times, and other basic cognitive skills. Then we compare players’ performance in the games with how well these same people did on established psychological tests of those abilities. The point is to allow players to assess their own cognitive strengths and weaknesses while donating their data for further public research.</p> \n<p>In the fall of 2018 we launched a prototype of this large-scale profiling in collaboration with the Danish Broadcasting Corp. Since then more than 20,000 people have participated, and in part because of the publicity granted by the public-service channel, participation has been very evenly distributed across ages and by gender. Such broad appeal is rare in social science, where the test population is typically drawn from a very narrow demographic, such as college students.</p> \n<p>Never before has such a large academic experiment in human cognition been conducted. We expect to gain new insights into many things, among them how combinations of cognitive abilities sharpen or decline with age, what characteristics may be used to prescreen for mental illnesses, and how to optimize the building of teams in our work lives.</p> \n<p>And so what started as a fun exercise in the weird world of quantum mechanics has now become an exercise in understanding the nuances of what makes us human. While we still seek to understand atoms, we can now aspire to understand people’s minds as well.</p> \n<p><em>This article appears in the November 2019 print issue as “A Man-Machine Mind Meld for Quantum Computing.”</em></p> \n<h2>About the Authors</h2> \n<p><a href=\"http://pure.au.dk/portal/en/persons/id(32212956-ccb7-4de2-9f42-873069d8b1b5).html\">Ottó Elíasson</a>, <a href=\"https://dk.linkedin.com/in/carrie-weidner-41093821\">Carrie Weidner</a>, <a href=\"https://www.linkedin.com/in/janet-rafner-a847104b/\">Janet Rafner</a>, and <a href=\"https://dk.linkedin.com/in/shaeema\">Shaeema Zaman</a> Ahmed work with the <a href=\"https://www.scienceathome.org/\">ScienceAtHome</a> project at Aarhus University in Denmark.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cPJEgwpqkv8:6bcjwCgtYPw:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cPJEgwpqkv8:6bcjwCgtYPw:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cPJEgwpqkv8:6bcjwCgtYPw:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cPJEgwpqkv8:6bcjwCgtYPw:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cPJEgwpqkv8:6bcjwCgtYPw:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cPJEgwpqkv8:6bcjwCgtYPw:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/cPJEgwpqkv8\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 22 Oct 2019 15:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5NDMxNw.jpeg","linkMd5":"dac7321f3815f792354a3da3730527b3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn74@2020_1/2020/08/25/10-37-58-843_39ec1a66da957d67.webp","destWidth":1240,"destHeight":930,"sourceBytes":494948,"destBytes":70372,"author":"Ottó Elíasson","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5NDMxNg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn3@2020_3/2020/08/25/10-38-02-859_088f9438987a6a59.webp","https://spectrum.ieee.org/image/MzM5NDM1MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn64@2020_1/2020/08/25/10-38-02-943_131953073c5caaa1.webp","https://spectrum.ieee.org/image/MzM5NDM2Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_4/2020/08/25/10-38-12-169_d51209399321cc51.webp","https://spectrum.ieee.org/image/MzM5NDM2NA.octet-stream":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn55@2020_2/2020/08/25/10-38-08-345_3f25f7447c40042c.webp","https://spectrum.ieee.org/image/MzM5NDM2NQ.octet-stream":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn75@2020_4/2020/08/25/10-38-06-567_aef88a7029595157.webp","https://spectrum.ieee.org/image/MzM5NDM2Ng.octet-stream":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn81@2020_6/2020/08/25/10-38-17-087_cdac9e00e6d647b6.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cPJEgwpqkv8:6bcjwCgtYPw:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn83@2020_4/2020/08/25/10-38-08-987_fba4ce39c73b8c31.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cPJEgwpqkv8:6bcjwCgtYPw:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn80@2020_2/2020/08/25/10-38-08-383_8ce075a5fca283d8.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/cPJEgwpqkv8":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn84@2020_1/2020/08/25/10-38-06-595_34e2723e743a8311.webp"},"publishedOrCreatedDate":1598351859727},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Universal Robots Introduces Its Strongest Robotic Arm Yet","link":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet","description":"<p><a href=\"https://www.universal-robots.com/\">Universal Robots</a>, already the dominant force in <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/collaborative-robots-innovation-growth-driver\">collaborative robots</a>, is flexing its muscles in an effort to further expand its reach in the cobots market. The Danish company is introducing today the <a href=\"https://www.universal-robots.com/products/ur16-robot/\">UR16e</a>, its strongest robotic arm yet, with a payload capability of 16 kilograms (35.3 lbs), reach of 900 millimeters, and repeatability of +/- 0.05 mm.</p> \n<p>Universal says the new “<a href=\"https://www.universal-robots.com/about-universal-robots/news-centre/universal-robots-launches-heavy-duty-payload-cobot-for-collaborative-automation/\">heavy duty payload cobot</a>” will allow customers to automate a broader range of processes, including packaging and palletizing, nut and screw driving, and high-payload and CNC machine tending.</p> \n<!--nextpage--> \n<p></p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/lxqs2c2G75Y\" width=\"620\"></iframe></p> \n<p>In early 2015, Universal introduced the <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-ur3-robotic-arm\">UR3, its smallest robot</a>, which joined the UR5 and the flagship UR10, offering a payload capability of 3, 5, and 10 kg, respectively. Now the company is going in the other direction, announcing a bigger, stronger arm.</p> \n<p><span>“With Universal joining its competitors in extending the reach and payload capacity of its cobots, a new standard of capability is forming,”&nbsp;</span><a href=\"https://www.linkedin.com/in/rian-whitton-a97892100/\">Rian Whitton</a>, a senior analyst at <a href=\"https://www.abiresearch.com/\">ABI Research</a>, in London,&nbsp;<a href=\"https://twitter.com/ABI_Whitton/status/1173892361333686273\">tweeted</a>.</p> \n<p>Like its predecessors, the UR16e is part of Universal’s <a href=\"https://www.universal-robots.com/e-series/\">e-Series</a> platform, which features 6 degrees of freedom and force/torque sensing on the tool flange. The UR family of cobots have&nbsp;stood out from the competition by being versatile in a variety of applications and, most important, <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots\">easy to deploy and program</a>. Universal didn’t release UR16e’s price, saying only that it is about 10 percent higher than that of the UR10e, which is about $50,000, depending on the configuration.</p> \n<p></p> \n<p><a href=\"https://www.linkedin.com/in/juergenvonhollen/?originalSubdomain=dk\">Jürgen von Hollen</a>, president of Universal Robots, says the company decided to launch the UR16e after studying the market and talking to customers about their needs. “What came out of that process is we understood payload was a true barrier for a lot of customers,” he tells <em>IEEE Spectrum</em>. The 16 kg payload will be particularly useful for applications that require mounting specialized tools on the arm to perform tasks like screw driving and machine tending, he explains. Customers that could benefit from such applications include manufacturing, material handling, and automotive companies.</p> \n<p></p> \n<p>“We’ve added the payload, and that will open up that market for us,” von Hollen says.</p> \n<p></p> \n<h3>The difference between Universal and Rethink</h3> \n<p></p> \n<p>Universal has grown by <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots\">leaps and bounds</a> since its founding in 2008. By 2015, it had sold more than <a href=\"https://blog.robotiq.com/notes-from-the-2015-international-collaborative-robots-workshop\">5,000 robots</a>; that number was close to 40,000 as of last year. During the same period, revenue more than doubled from about $100 million to $234 million. At a time when a string of robot makers have shuttered, <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/rethink-robotics-pioneer-of-collaborative-robots-shuts-down\">including most notably Rethink Robotics</a>, <a href=\"https://spectrum.ieee.org/robotics/industrial-robots/rethink-robotics-baxter-robot-factory-worker\">a cobots pioneer</a> and Universal’s biggest rival, Universal finds itself in an enviable position, having amassed a commanding market share, <a href=\"https://www.cbinsights.com/research/collaborative-robots-reinvent-industries/\">estimated</a> at between 50 to 60 percent.</p> \n<p></p> \n<p>About Rethink, von Hollen says the Boston-based company was a “good competitor,” helping disseminate the advantages and possibilities of cobots. “When Rethink basically ended it was more of a negative than a positive, from my perspective,” he says. In his view, a major difference between the two companies is that Rethink focused on delivering full-fledged applications to customers, whereas Universal focused on delivering a product to the market and letting the system integrators and sales partners deploy the robots to the customer base.</p> \n<p></p> \n<p>“We’ve always been very focused on delivering the product, whereas I think Rethink was much more focused on applications, very early on, and they added a level of complexity to their company that made it become very de-focused,” he says.</p> \n<h3>The collaborative robots market: massive growth</h3> \n<p></p> \n<p>And yet, despite its success, Universal is still tiny when you compare it to the giants of industrial automation, which include companies like <a href=\"https://new.abb.com/products/robotics\">ABB</a>, <a href=\"https://www.fanucamerica.com/\">Fanuc</a>, <a href=\"https://www.motoman.com/\">Yaskawa</a>, and <a href=\"https://www.kuka.com/en-us\">Kuka</a>, with revenue in the billions of dollars. Although some of these companies have added cobots to their product portfolios—ABB’s <a href=\"https://robots.ieee.org/robots/yumi/?utm_source=spectrum\">YuMi</a>, for example—that market represents a drop in the bucket when you consider global robot sales: The size of the cobots market was <a href=\"https://www.marketsandmarkets.com/Market-Reports/collaborative-robot-market-194541294.html\">estimated at</a> <a href=\"https://www.grandviewresearch.com/industry-analysis/collaborative-robots-market\">$700 million</a> in 2018, whereas the <a href=\"https://ifr.org/downloads/press2018/Executive_Summary_WR_2018_Industrial_Robots.pdf\">global market</a> for industrial robot systems (including software, peripherals, and system engineering) is close to $50 billion.</p> \n<p></p> \n<p>Von Hollen notes that cobots are <a href=\"https://www.assemblymag.com/articles/94462-global-cobots-market-could-be-worth-9-billion-by-2025\">expected</a> to go through an impressive growth curve—nearly 50 percent year after year until 2025, when sales will reach between $9 to $12 billion. If Universal can maintain its dominance and capture a big slice of that market, it’ll add up to a nice sum. To get there, Universal is not alone: It is backed by U.S. electronics testing equipment maker <a href=\"https://www.teradyne.com/\">Teradyne</a>, which <a href=\"https://www.universal-robots.com/about-universal-robots/news-centre/teradyne-inc-acquires-universal-robots/\">acquired Universal</a> in 2015 for $285 million.</p> \n<p></p> \n<p>“The amount of resources we invest year over year matches the growth we had on sales,” von Hollen says. Universal currently has more than 650 employees, most based at its headquarters in Odense, Denmark, and the rest scattered in 27 offices in 18 countries. “No other company [in the cobots segment] is so focused on one product.”</p> \n<p>[ <a href=\"https://www.universal-robots.com/about-universal-robots/news-centre/universal-robots-launches-heavy-duty-payload-cobot-for-collaborative-automation/\">Universal Robots</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZpLVW_108Mc:8j9lIRcENNw:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZpLVW_108Mc:8j9lIRcENNw:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZpLVW_108Mc:8j9lIRcENNw:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZpLVW_108Mc:8j9lIRcENNw:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ZpLVW_108Mc:8j9lIRcENNw:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZpLVW_108Mc:8j9lIRcENNw:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ZpLVW_108Mc\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 17 Sep 2019 05:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3NTAwMg.jpeg","linkMd5":"b92198dab0130e2ebf3c419ddb0d4903","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn77@2020_1/2020/08/25/10-37-58-920_09c0be81568c7be0.webp","destWidth":2217,"destHeight":1536,"sourceBytes":190795,"destBytes":87906,"author":"Erico Guizzo","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZpLVW_108Mc:8j9lIRcENNw:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn28@2020_5/2020/08/25/10-38-07-012_d72314201d25f438.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZpLVW_108Mc:8j9lIRcENNw:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn72@2020_2/2020/08/25/10-38-10-752_95a880ad559f8b66.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ZpLVW_108Mc":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn100@2020_6/2020/08/25/10-38-06-027_06e35845d0212194.webp"},"publishedOrCreatedDate":1598351859739},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"We're at IROS 2019 to Bring You the Most Exciting Robotics Research From Around the World","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","description":"<p>The&nbsp;<a href=\"https://www.iros2019.org/\">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</a> is taking place in Macau this week, featuring well over a thousand presentations on the newest and most amazing robotics research from around the world. There are also posters, workshops, tutorials, an exhibit hall, and plenty of social events where roboticists have the chance to get a little tipsy and talk about all the <em>really</em> interesting stuff.</p> \n<p>As always, our plan is to bring you all of the coolest, weirdest, and most interesting things that we find at the show, and here are just a few of the things we’re looking forward to this week:</p> \n<ul> \n <li>Flying robots with wings, tails, and... arms?</li> \n <li>Spherical robot turtles</li> \n <li>An update on that crazy <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/jet-powered-icub-could-be-the-first-flying-humanoid-robot\">jet-powered iCub</a></li> \n <li>Agile and tiny robot insects</li> \n <li>Metallic self-healing robot bones</li> \n <li>How to train robots by messing with them</li> \n <li>A weird robot sea urchin</li> \n</ul> \n<p>And all that is happening just on&nbsp;<em>Tuesday</em>!</p> \n<p>Our IROS coverage will continue beyond this week, so keep checking back for more of the best new robotics from Macau.</p> \n<p>[ <a href=\"https://www.iros2019.org/\">IROS 2019</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QhC0HMCy6-k:kD0Zu80vLQY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QhC0HMCy6-k:kD0Zu80vLQY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QhC0HMCy6-k:kD0Zu80vLQY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QhC0HMCy6-k:kD0Zu80vLQY:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=QhC0HMCy6-k:kD0Zu80vLQY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QhC0HMCy6-k:kD0Zu80vLQY:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/QhC0HMCy6-k\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 04 Nov 2019 17:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNDk4NQ.jpeg","linkMd5":"70815399970675009fdaa91f1ed9af88","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn30@2020_2/2020/08/25/10-38-00-289_3559881935b23433.webp","destWidth":1240,"destHeight":930,"sourceBytes":529573,"destBytes":189916,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QhC0HMCy6-k:kD0Zu80vLQY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn34@2020_1/2020/08/25/10-38-15-658_59eba00190735dd3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QhC0HMCy6-k:kD0Zu80vLQY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn18@2020_5/2020/08/25/10-38-16-595_aea35596c1065c5f.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/QhC0HMCy6-k":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_3/2020/08/25/10-38-05-546_a101d8249e0e884e.webp"},"publishedOrCreatedDate":1598351859723},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Robotic Endoscope Travels Through the Colon","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here's what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.darpa.mil/news-events/2019-11-12\">DARPA SubT Urban Circuit</a> –&nbsp;February 18-27, 2020 –&nbsp;Olympia, WA, USA</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today's videos.</p> \n<hr> \n <!--nextpage--> \n <p><a href=\"https://www.kuka.com/\">Kuka</a>&nbsp;has just announced the results of its annual <a href=\"https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award\">Innovation Award</a>. From an initial batch of 30 applicants,&nbsp;five teams reached the finals&nbsp;(we were&nbsp;part of the judging committee). The five finalists&nbsp;worked for nearly a year on their applications, which they demonstrated this week at the <a href=\"http://www.medica-tradefair.com/\">Medica</a> trade show in Düsseldorf, Germany.&nbsp;And the winner of the&nbsp;€20,000 prize is...Team RoboFORCE, led by the <a href=\"https://www.stormlabuk.com/\">STORM Lab</a> in the U.K., which developed a “robotic magnetic flexible endoscope for painless colorectal cancer screening, surveillance, and intervention.”</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/gTQsHwA9j1Q\" width=\"620\"></iframe></p> \n <p>The system could improve <span>colonoscopy&nbsp;</span>procedures by&nbsp;reducing pain and discomfort as well as other risks such as&nbsp;bleeding and perforation, according to&nbsp;<span>the STORM Lab researchers</span>.&nbsp;It uses&nbsp;a magnetic field&nbsp;to control the endoscope, pulling rather than pushing it through&nbsp;the colon.</p> \n <p>The other four finalists also presented some really interesting applications—you can see their videos&nbsp;below.</p> \n <p><span>“Because we were so pleased with the high quality of the submissions, we will have next year’s finals again at the&nbsp;</span><a href=\"https://www.medica-tradefair.com/\" style=\"font-family: Georgia, serif; font-size: 18px;\">Medica</a><span>&nbsp;fair, and the challenge will be&nbsp;named ‘Medical Robotics’,” says&nbsp;</span><a href=\"https://www.linkedin.com/in/rainer-bischoff-47403963/?originalSubdomain=de\" style=\"font-family: Georgia, serif; font-size: 18px;\">Rainer Bischoff</a><span>, vice president for corporate research at Kuka. He adds that the selected teams will again use Kuka’s&nbsp;</span><a href=\"https://www.kuka.com/en-us/industries/health-care/kuka-medical-robotics/lbr-med\" style=\"font-family: Georgia, serif; font-size: 18px;\">LBR Med</a><span>&nbsp;robot arm, which is “already certified for integration into medical products and makes it particularly easy for startups to use a robot as the main component for a particular solution.”</span></p> \n <p>Applications are now open for Kuka’s Innovation Award 2020. You can <a href=\"https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/call-for-participation\">find more information on how to enter here</a>. The deadline is&nbsp;5 January 2020.</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/xT585QJEHKU\" width=\"620\"></iframe></p> \n <p></p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tyKUsq1whMI\" width=\"620\"></iframe></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/OLiNbyqKouI\" width=\"620\"></iframe></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/OBRtNvPaZMs\" width=\"620\"></iframe></p> \n <p></p> \n <p>[ <a href=\"https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award#InnovationAward2019\">Kuka</a> ]</p> \n <p></p> \n <hr> \n  <p>Oh good, <a href=\"https://robots.ieee.org/robots/aibo2018/?utm_source=spectrum\">Aibo</a> needs to be fed now.</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/gKGqiUUHsdY\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>You know what comes next, right?</p> \n  <p>[ <a href=\"https://us.aibo.com/\">Aibo</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>Your cat needs this robot.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/xW9eybjSiCw\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>It's about $200 on Kickstarter.</p> \n   <p>[ <a href=\"https://www.kickstarter.com/projects/enabot/the-worlds-smartest-robot-companion-for-your-cat/description\">Kickstarter</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>Enjoy this tour of the <a href=\"https://robots.ieee.org/robots/skydior1/\">Skydio</a> offices courtesy <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio 2</a>, which runs into not even one single thing.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Yku8hujfA1Q\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>If any Skydio employees had important piles of papers on their desks, well, they don’t anymore.</p> \n    <p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <blockquote> \n      <p><em>Artificial intelligence is everywhere nowadays, but what exactly does it mean? We asked a group MIT computer science grad students and post-docs how they personally define AI.</em></p> \n     </blockquote> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/0VKXm_vEriA\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>“When most people say AI, they actually mean machine learning, which is just pattern recognition.” Yup.</p> \n     <p>[ <a href=\"https://www.csail.mit.edu/\">MIT</a> ]</p> \n     <p></p> \n     <hr> \n      <p>Using event-based cameras, this drone control system can track attitude at 1600 degrees per second (!).</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/3nIznSMCMtc\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"http://rpg.ifi.uzh.ch/\">UZH</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>Introduced at CES 2018, Walker is an intelligent humanoid service robot from UBTECH Robotics. Below are the latest features and technologies used during our latest round of development to make Walker even better.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ypsnpNujnAw\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://ubtrobot.com/collections/innovation-at-ubtech?ls=en\">Ubtech</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <blockquote> \n         <p><em>Introducing the Alpha Prime by #VelodyneLidar, the most advanced lidar sensor on the market! Alpha Prime delivers an unrivaled combination of field-of-view, range, high-resolution, clarity and operational performance.</em></p> \n        </blockquote> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tZ8WbSNsNaU\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>Performance looks good, but don’t expect it to be cheap.</p> \n        <p>[ <a href=\"https://velodynelidar.com/vls-128.html\">Velodyne</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>Ghost Robotics’&nbsp;Spirit 40 will start shipping to researchers in January of next year.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hJ63utsaqQc\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://www.ghostrobotics.io/\">Ghost Robotics</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>Unitree is about to ship the first batch of their AlienGo quadrupeds as well:</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ICObEUV1oOg\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"http://www.unitree.cc/e/action/ShowInfo.php?classid=6&amp;id=363\">Unitree</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>Mechanical engineering’s Sarah Bergbreiter discusses her work on micro robotics, how they draw inspiration from insects and animals, and how tiny robots can help humans in a variety of fields.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/YKlfyiwHcRc\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.meche.engineering.cmu.edu/directory/bios/bergbreiter-sarah.html\">CMU</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>Learning contact-rich, robotic manipulation skills is a challenging problem due to the high-dimensionality of the state and action space as well as uncertainty from noisy sensors and inaccurate motor control. To combat these factors and achieve more robust manipulation, humans actively exploit contact constraints in the environment. By adopting a similar strategy, robots can also achieve more robust manipulation. In this paper, we enable a robot to autonomously modify its environment and thereby discover how to ease manipulation skill learning. Specifically, we provide the robot with fixtures that it can freely place within the environment. These fixtures provide hard constraints that limit the outcome of robot actions. Thereby, they funnel uncertainty from perception and motor control and scaffold manipulation skill learning.</em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/od3jBAJES4w\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://sites.google.com/view/scaffoldlearning\">Stanford</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <blockquote> \n              <p><em>Since 2016, <a href=\"https://robots.ieee.org/robots/lucie/?utm_source=spectrum\">Verity's drones</a> have completed more than 200,000 flights around the world. Completely autonomous, client-operated and designed for live events, Verity is making the magic real by turning drones into flying lights, characters, and props.</em></p> \n             </blockquote> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/PqHlJiwUYDI\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://veritystudios.com/\">Verity</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>To monitor and stop the spread of wildfires, University of Michigan engineers developed UAVs that could find, map and report fires. One day UAVs like this could work with disaster response units, firefighters and other emergency teams to provide real-time accurate information to reduce damage and save lives. For their research, the University of Michigan graduate students won first place at a competition for using a swarm of UAVs to successfully map and report simulated wildfires.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/XNF_Sddlgy4\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://a2sys.engin.umich.edu/\">University of Michigan</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>Here’s an important issue that I haven’t heard talked about all that much: How first responders should interact with self-driving cars.</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/7G0a1PRAtaM\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>“To put the car in manual mode, you must call <a href=\"https://robots.ieee.org/robots/waymo/?utm_source=spectrum\">Waymo</a>.” Huh.</p> \n               <p>[ <a href=\"https://waymo.com/\">Waymo</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <p>Here’s what Gitai has been up to recently, from a Humanoids 2019 workshop talk.</p> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/XXz8CdkXPiI\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>[ <a href=\"https://gitai.tech/\">Gitai</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <p>The latest CMU RI seminar comes from Girish Chowdhary at the University of Illinois at Urbana-Champaign on “Autonomous and Intelligent Robots in Unstructured Field Environments.”</p> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Rp1L9Ha0wXQ\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <blockquote> \n                  <p><em>What if a team of collaborative autonomous robots grew your food for you? In this talk, I will discuss some key advances in robotics, machine learning, and autonomy that will one day enable teams of small robots to grow food for you in your backyard in a fundamentally more sustainable way than modern mega-farms! Teams of small aerial and ground robots could be a potential solution to many of the serious problems that modern agriculture is facing. However, fully autonomous robots that operate without supervision for weeks, months, or entire growing season are not yet practical. I will discuss my group’s theoretical and practical work towards the underlying challenging problems in robotic systems, autonomy, sensing, and learning. I will begin with our lightweight, compact, and autonomous field robot TerraSentia and the recent successes of this type of undercanopy robots for high-throughput phenotyping with deep learning-based machine vision. I will also discuss how to make a team of autonomous robots learn to coordinate to weed large agricultural farms under partial observability. These direct applications will help me make the case for the type of reinforcement learning and adaptive control that are necessary to usher in the next generation of autonomous field robots that learn to solve complex problems in harsh, changing, and dynamic environments. I will then end with an overview of our new MURI, in which we are working towards developing AI and control that leverages neurodynamics inspired by the Octopus brain.</em></p> \n                 </blockquote> \n                 <p>[ <a href=\"https://www.ri.cmu.edu/event/ri-seminar-girish-chowdhary-university-of-illinois-at-urbana-champaign-assistant-professor-2019-11-15/\">CMU RI</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <div class=\"feedflare\"> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Kc0qsE87fHQ:RdO6Cd7FWsg:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Kc0qsE87fHQ:RdO6Cd7FWsg:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Kc0qsE87fHQ:RdO6Cd7FWsg:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Kc0qsE87fHQ:RdO6Cd7FWsg:-BTjWOF_DHI\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Kc0qsE87fHQ:RdO6Cd7FWsg:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Kc0qsE87fHQ:RdO6Cd7FWsg:gIN9vFwOqvQ\" border=\"0\" /></a> \n                  </div> \n                  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Kc0qsE87fHQ\" height=\"1\" width=\"1\" alt=\"\" /> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Sat, 23 Nov 2019 01:46:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxODg1Nw.jpeg","linkMd5":"bacf594041d2950428c4810385b725b0","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn45@2020_4/2020/08/25/10-37-58-979_ba283e78cf896002.webp","destWidth":1240,"destHeight":845,"sourceBytes":119067,"destBytes":68286,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Kc0qsE87fHQ:RdO6Cd7FWsg:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn4@2020_4/2020/08/25/10-38-08-266_5037603819e8a098.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Kc0qsE87fHQ:RdO6Cd7FWsg:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx/cdn8@2020_6/2020/08/25/10-38-08-303_7e39848d33f8d28a.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Kc0qsE87fHQ":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn69@2020_3/2020/08/25/10-38-14-264_f47582ddc4a7f042.webp"},"publishedOrCreatedDate":1598351859717},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"How Much AI Expertise Do Thought Leaders and Companies Really Have?","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies","description":"<p>If there’s something strange about Elon Musk’s latest comments on AI's threat to humanity&nbsp;or a&nbsp;Silicon Valley startup&nbsp;extolling how machine learning can&nbsp;save the world, who are you going&nbsp;call to differentiate&nbsp;fact from fiction? Maybe the founders of Certified Artificial, a service&nbsp;aimed at&nbsp;helping investors and tech conference organizers navigate the confusion and hype surrounding many claims about AI.</p> \n<!--nextpage--> \n<p></p> \n<p><span>Launched in early August</span><span>, <a href=\"https://certifiedartificial.com\">Certified Artificial</a> promises</span><span>&nbsp;a&nbsp;“neutral, independent third-party certification service”&nbsp;for&nbsp;helping separate the AI snake oil from the real deal. One part of this service&nbsp;focuses&nbsp;on companies requesting third-party verification of the fact that they’re using the latest AI techniques in their services and products rather than simply relying on groups of human workers or older statistical methods. Certified Artificial’s other line of business involves&nbsp;evaluating&nbsp;the quality of advice coming from certain thought-leaders who frequently discuss AI technologies and their social impacts.</span></p> \n<p><span>“Our goal is not to penalize anyone because they made a little misstep on how they talked about&nbsp;AI,” says Tim Hwang, partner and technical director of&nbsp;Certified Artificial, and director of the Harvard-MIT Ethics and Governance of AI Initiative.&nbsp;“We want to signal places where someone has either been consistently spreading disinformation about&nbsp;AI or is opining about it so it impacts in a way that erases a lot of&nbsp;people doing really amazing work in this space.”</span></p> \n<p><span>The newest part of the service includes an <a href=\"https://chrome.google.com/webstore/detail/certified-artificial-cat/llmgegdecopjnebpgdajmfkbeadgkonk?hl=en&amp;gl=US\">online browser extension</a> that anyone can install in order to see&nbsp;</span><a href=\"https://certifiedartificial.com/catl-ratings.html\" style=\"font-family: Georgia, serif; font-size: 18px;\">assigned&nbsp;ratings</a><span>&nbsp;for thought-leaders whenever their names pop up in search engines or websites. Those experts&nbsp;who demonstrate both&nbsp;technical knowledge about AI and responsible awareness of the technology implications may&nbsp;receive gold, silver, or bronze certification badges.&nbsp;On the other hand, individuals who frequently spread&nbsp;misinformation about AI&nbsp;can receive a “Do Not Recommend” badge. </span>&nbsp;</p> \n<aside class=\"inlay pullquote lt med-lrg\">\n  “We’re not here to commit character assassination. We're here to level the playing field.” \n <span class=\"pq-attrib\">—Clayton Aldern, partner and research director for Certified Artificial</span> \n</aside> \n<p><span>Inspiration&nbsp;for the Certified Artificial&nbsp;service came</span>&nbsp;from two&nbsp;articles about AI penned&nbsp;<a href=\"https://www.theatlantic.com/magazine/archive/2018/06/henry-kissinger-ai-could-mean-the-end-of-human-history/559124/\">in 2018</a>&nbsp;and in&nbsp;<a href=\"http://www.theatlantic.com/magazine/archive/2019/08/henry-kissinger-the-metamorphosis-ai/592771/\">August of this year</a>&nbsp;by Henry Kissinger, former U.S. National Security Advisor and Secretary of State under President Nixon. Kissinger’s musings on the possible impacts of AI earned him the “Do Not Recommend” rating based on a “50-point diagnostic tool developed by experts in the field of machine learning and technology studies,” according to a <a href=\"https://certifiedartificial.com/articles/pr1.html\">press release</a> published on 6 August.</p> \n<p>“I think it’s very clear from these articles that this is someone who’s very prominent, who obviously has a lot of experience in foreign policy, but <a href=\"http://twitter.com/timhwang/status/1158766341664284677\">doesn’t know a great deal about AI</a>,” says Hwang, a former public policy lead on AI at Google. What’s more, says Hwang, “[Kissinger] is in fact&nbsp;opining about AI and saying things about AI that either aren't true or are totally unrepresentative of the work that's actually going on in the space.”</p> \n<p>Other individuals who have received the “Do Not Recommend” rating include Elon Musk, CEO of the automotive and energy company Tesla and the aerospace company SpaceX. Musk has frequently warned about the risks of AI technologies&nbsp;someday outstripping the&nbsp;intelligence and understanding of humans. He has also founded related initiatives such as the <a href=\"https://openai.com/\">OpenAI research company</a> focused on “safe artificial general intelligence” and the <a href=\"https://spectrum.ieee.org/the-human-os/biomedical/devices/elon-musks-150-million-augmented-brain-project-what-the-media-missed\">Neuralink company</a> focused on developing a brain-machine interface technology.</p> \n<aside class=\"inlay pullquote rt med-lrg\">\n  “This isn't just some bumper sticker that you purchase.” \n <span class=\"pq-attrib\">—Certified Artificial’s Tim Hwang on the consultancy’s third-party certification of AI products and services</span> \n</aside> \n<p>The beta version of the browser extension aims for some topic specificity in displaying the thought-leader ratings whenever there is a search result or Web page related to AI technologies, says <a href=\"https://claytonaldern.com/\">Clayton Aldern</a>, partner and research director for Certified Artificial. But in the future, he and Hwang expect to improve the service&nbsp;by adding a machine learning model trained on recognizing certain topics.</p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p><span>“We’re not here to commit character assassination,” says Aldern, a data scientist who also directs his own&nbsp;</span>data-science and machine-learning&nbsp;consultancy. “W<span>e're here to level the playing field.”</span></p> \n<p><span>The second part of Certified Artificial’s&nbsp;services banks&nbsp;on the idea that the marketplace will increasingly demand&nbsp;independent, third-party certification of AI products and services—especially as more startups and established companies alike jump on the <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims\">AI hype bandwagon</a>. Potential customers that might like to receive Certified Artificial’s stamp of approval&nbsp;include startups that are “having trouble getting traction in the marketplace” and want to stand out amidst a “glut of alleged AI companies,” Aldern explains.</span></p> \n<p>Companies <a href=\"http://certifiedartificial.com/cai.html\">seeking certification</a> will not have to give up their source code or similarly sensitive trade secrets. Instead, Aldern and Hwang envision the certification being more of a holistic process. Besides applying their own evaluation methods, they are&nbsp;also keeping a group of advisers on tap.&nbsp;</p> \n<p>The members&nbsp;of the advisory group and the exact methodology behind the thought-leader&nbsp;ratings and company certifications are being kept secret&nbsp;for now. That has already raised at least one question&nbsp;about the <a href=\"https://twitter.com/natematias/status/1158805519370522624\">transparency of the entire venture</a>. But&nbsp;the Certified Artificial founders argue that the legitimacy and objectivity of their certification processes rely upon “keeping the cards close to our chest” so that outsiders have a difficult time gaming the system,&nbsp;Aldern says.</p> \n<p>Whether or not Certified Artificial’s business plan&nbsp;takes off remains to be seen. Still, the two founders sound cautiously optimistic about having identified a real market demand and opportunity&nbsp;at a time when plenty of AI hype still abounds in press releases, IPO filings, and news articles.</p> \n<p><span>“To our knowledge, this is the first time something like this has been tried,” Hwang says.&nbsp;“And so&nbsp;</span>we're figuring out what would be a price that is both reasonable to&nbsp;really do a good job in these&nbsp;certifications, but also one that is a significant signal to say that, ‘Hey, this isn't just some bumper sticker that you purchase.’”</p> \n<div></div> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7rGryOunAtI:e8Vkj_SEwuY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7rGryOunAtI:e8Vkj_SEwuY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7rGryOunAtI:e8Vkj_SEwuY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7rGryOunAtI:e8Vkj_SEwuY:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=7rGryOunAtI:e8Vkj_SEwuY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7rGryOunAtI:e8Vkj_SEwuY:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/7rGryOunAtI\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 10 Sep 2019 17:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3MDYyOA.png","linkMd5":"d6ffcad3ca933b1d80d7759de6a31b95","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn46@2020_6/2020/08/25/10-38-02-605_5f538270d3e935dc.webp","destWidth":1240,"destHeight":930,"sourceBytes":575081,"destBytes":66822,"author":"Jeremy Hsu","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7rGryOunAtI:e8Vkj_SEwuY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx/cdn100@2020_6/2020/08/25/10-38-07-480_3dfd13c46a6df3e3.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7rGryOunAtI:e8Vkj_SEwuY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn16@2020_6/2020/08/25/10-38-08-390_e0f482e8ca631ca5.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/7rGryOunAtI":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn22@2020_5/2020/08/25/10-38-15-579_b3c5208b11ea06f2.webp"},"publishedOrCreatedDate":1598351859733},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"In 2016, Microsoft’s Racist Chatbot Revealed the Dangers of Online Conversation","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","description":"<p><em>This is part five of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>.</em></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p>In March 2016, <a href=\"https://spectrum.ieee.org/tag/Microsoft\">Microsoft</a> was preparing to release <a href=\"https://web.archive.org/web/20160414074049/https:/www.tay.ai/\">its new chatbot, Tay</a>, on Twitter. Described as an experiment in “conversational understanding,” Tay was designed to engage people in dialogue through tweets or direct messages, while emulating the style and slang of a teenage girl. She was, according to her creators, “Microsoft’s A.I. fam from the Internet that’s got zero chill.” She loved E.D.M. music, had a favorite Pokémon, and often said extremely online things, like “swagulated.”&nbsp;</p> \n<p></p> \n<p>Tay was an experiment at the intersection of machine learning, natural language processing, and social networks. While other chatbots in the past—like <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot\">Joseph&nbsp;Weizenbaum’s Eliza</a>—conducted conversation by following pre-programmed and narrow scripts, Tay was designed to learn more about language over time, enabling her to have conversations about any topic.</p> \n<p></p> \n<aside class=\"inlay pullquote rt med\">\n  Tay was designed to learn more about language over time….&nbsp;Eventually, her programmers hoped, Tay would sound just like the Internet.&nbsp; \n</aside> \n<p>Machine learning works by developing generalizations from large amounts of data. In any given data set, the algorithm will discern patterns and then “learn” how to approximate those patterns in its own behavior.</p> \n<p>Using this technique, engineers at Microsoft trained Tay’s algorithm on a dataset of anonymized public data along with some pre-written material provided by professional comedians to give it a basic grasp of language. The plan was to release Tay online, then let the bot discover patterns of language through its interactions, which she would emulate in subsequent conversations. Eventually, her programmers hoped, Tay would sound just like the Internet.&nbsp;</p> \n<p></p> \n<p>On March 23, 2016, Microsoft released Tay to the public on Twitter. At first, Tay engaged harmlessly with her growing number of followers with banter and lame jokes. But after only a few hours, Tay started <a href=\"https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist\">tweeting highly offensive things</a>, such as: “I f@#%&amp;*# hate feminists and they should all die and burn in hell” or “Bush did 9/11 and Hitler would have done a better job…”</p> \n<p></p> \n<!--nextpage--> \n<p>Within 16 hours of her release, Tay had tweeted more than 95,000 times, and a troubling percentage of her messages were abusive and offensive. Twitter users started registering their outrage, and Microsoft had little choice but to suspend the account. What the company had intended on being a fun experiment in “conversational understanding” had become their very own <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics\">golem</a>, spiraling out of control through the animating force of language.</p> \n<p></p> \n<p>Over the next week, many reports emerged detailing precisely how a bot that was supposed to mimic the language of a teenage girl <a href=\"https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/\">became so vile</a>. It turned out that just a few hours after Tay was released, a post on the troll-laden bulletin board, 4chan, shared a link to Tay’s Twitter account and encouraged users to inundate the bot with racist, misogynistic, and anti-semitic language.&nbsp;</p> \n<p></p> \n<p>In a coordinated effort, the trolls exploited a “repeat after me” function that had been built into Tay, whereby the bot repeated anything that was said to it on demand. But more than this, Tay’s in-built capacity to learn meant that she internalized some of the language she was taught by the trolls, and repeated it unprompted. For example, one user innocently asked Tay whether Ricky Gervais was an atheist, to which she responded: “Ricky Gervais learned totalitarianism from Adolf Hitler, the inventor of atheism.”</p> \n<p></p> \n<aside class=\"inlay pullquote lt med\">\n  “Ricky Gervais learned totalitarianism from Adolf Hitler, the inventor of atheism.” \n <span class=\"pq-attrib\">—Tay, Microsoft’s rogue chatbot</span> \n</aside> \n<p>The coordinated attack on Tay worked better than the 4channers expected and was discussed widely in the media in the weeks after. Some saw Tay’s failure as evidence of social media’s <a href=\"https://www.wired.com/2016/03/fault-microsofts-teen-ai-turned-jerk/\">inherent toxicity</a>, a place that brings out the worst in people and allows trolls to hide in anonymity.</p> \n<p></p> \n<p>For others, though, Tay’s <a href=\"https://medium.com/@carolinesinders/microsoft-s-tay-is-an-example-of-bad-design-d4e65bb2569f\">behavior</a> was evidence of poor design decisions on Microsoft’s behalf.</p> \n<p><a href=\"https://unburntwitch.com/about\">Zoë Quinn</a>, a game developer and writer who’s been a frequent target of online abuse, argued that Microsoft should have been more cognizant of the context in which Tay was being released. If a bot learns how to speak on Twitter—a platform rife with abusive language—then naturally it will learn some abusive language. Microsoft, Quinn argued, should have planned for this contingency and ensured that Tay was not corrupted so easily. “It’s 2016,” she <a href=\"https://www.theverge.com/2016/3/25/11306566/microsoft-racist-tay-ai-twitter-chatbot-apology\">tweeted</a>. “If you’re not asking yourself ‘how could this be used to hurt someone’ in your design/engineering process, you’ve failed.”</p> \n<p></p> \n<p>Some months after taking Tay down, Microsoft released <a href=\"https://www.zo.ai/\">Zo</a>, a “politically correct” version of the original bot. Zo, who was <a href=\"https://twitter.com/zochats\">active</a> on <a href=\"https://www.facebook.com/zo/\">social networks</a> from 2016 to 2019, was designed to shut down conversations about certain contentious topics, including politics and religion, to ensure she didn’t offend people. (If a correspondent kept pressing her to talk about a certain sensitive topic, she left the conversation altogether, with a sentence like: “im better than u bye.”)</p> \n<p></p> \n<p>The lesson Microsoft learned the hard way is that designing computational systems that can communicate with people online is not just a technical problem, but a deeply social endeavor. Inviting a bot into the value-laden world of language requires thinking, in advance, about what context it will be deployed in, what type of communicator you want it to be, and what type of human values you want it to reflect.</p> \n<p>As we move towards an online world in which bots are more prevalent, these questions must be at the forefront of the design process. Otherwise there will be more <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics\">golems released into the world</a> that will reflect back to us, in language, the worst parts of ourselves.</p> \n<p></p> \n<p><em>This is the fifth installment of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. Last week’s post described people’s <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot\">weird intimacy with a rudimentary chatbot</a> created in 1966. Come back next Monday for part six, which tells of <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one\">the controversy</a> surrounding OpenAI’s magnificent language generator, GPT-2.&nbsp;</em></p> \n<p></p> \n<p><em>You can also check out our prior series on the </em><a href=\"https://spectrum.ieee.org/tag/AI+history\"><em>untold history of AI</em></a><em>.</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=jrUpLKvji58:Mv9REQr1UnA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=jrUpLKvji58:Mv9REQr1UnA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=jrUpLKvji58:Mv9REQr1UnA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=jrUpLKvji58:Mv9REQr1UnA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=jrUpLKvji58:Mv9REQr1UnA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=jrUpLKvji58:Mv9REQr1UnA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/jrUpLKvji58\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 25 Nov 2019 14:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxODYyMw.jpeg","linkMd5":"fdeba8f58f9ca993017a817b623b747a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn82@2020_2/2020/08/25/10-37-58-825_c16fd9ea21c1ae96.webp","destWidth":1240,"destHeight":930,"sourceBytes":188578,"destBytes":98204,"author":"Oscar Schwartz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=jrUpLKvji58:Mv9REQr1UnA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn30@2020_3/2020/08/25/10-38-14-016_4eccb746e8510091.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=jrUpLKvji58:Mv9REQr1UnA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn51@2020_5/2020/08/25/10-38-04-867_8b9c850b779e709a.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/jrUpLKvji58":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn32@2020_5/2020/08/25/10-38-02-268_488ef00ac7148f80.webp"},"publishedOrCreatedDate":1598351859733},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Watch This Robot Dog Explore an Underground Tunnel Autonomously","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"https://www.robobusiness.com/\">RoboBusiness 2019</a> –&nbsp;October 1-3, 2019 –&nbsp;Santa Clara, CA, USA</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>Team PLUTO (University of Pennsylvania, Ghost Robotics, and Exyn Technologies) put together this video giving us a robot’s-eye-view&nbsp;(or whatever they happen to be using for eyes) of the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit\">DARPA Subterranean Challenge&nbsp;tunnel circuits</a>.</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/5iQX0NGVsO4\" width=\"620\"></iframe></p> \n <p></p> \n <p>[ <a href=\"https://pluto-subt.github.io/index.html\">PLUTO</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>Zhifeng Huang has been improving his <a href=\"https://spectrum.ieee.org/automaton/robotics/humanoids/bipedal-robot-uses-jetpowered-feet-to-step-over-large-gaps\">jet-stepping humanoid robot</a>, which features new hardware and the ability to take larger and more complex steps.</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/MO0hCHCsmYg\" width=\"620\"></iframe></p> \n  <p></p> \n  <blockquote> \n   <p><em>This video reported the last progress of an ongoing project utilizing ducted-fan propulsion system to improve humanoid robot’s ability in stepping over large ditches. The landing point of the robot’s swing foot can be not only forward but also side direction. With keeping quasi-static balance, the robot was able to step over a ditch with 450mm in width (up to 97% of the robot’s leg’s length) in 3D stepping.</em></p> \n  </blockquote> \n  <p>[ <a href=\"https://ieeexplore.ieee.org/document/8246905\">Paper</a> ]</p> \n  <p><em>Thanks Zhifeng!</em></p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>These underacuated hands from Matei Ciocarlie’s lab at Columbia are magically able to reconfigure themselves to grasp different object types with just one or two motors.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/oKXz-t6UalI\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>[ <a href=\"https://arxiv.org/abs/1905.11293\">Paper</a> ] via [ <a href=\"https://roam.me.columbia.edu/\">ROAM Lab</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>This is one reason we should pursue not “autonomous cars” but “<em>fully</em> autonomous cars” that never require humans to take over. We can’t be trusted.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/6ePWBBrWSzo\" width=\"620\"></iframe></p> \n    <p></p> \n    <blockquote> \n     <p><em>During our early days as the Google self-driving car project, we invited some employees to test our vehicles on their commutes and weekend trips. What we were testing at the time was similar to the highway driver assist features that are now available on cars today, where the car takes over the boring parts of the driving, but if something outside its ability occurs, the driver has to take over immediately. </em></p> \n     <p><em>What we saw was that our testers put too much trust in that technology. They were doing things like texting, applying makeup, and even falling asleep that made it clear they would not be ready to take over driving if the vehicle asked them to. This is why we believe that nothing short of full autonomy will do.</em></p> \n    </blockquote> \n    <p>[ <a href=\"https://waymo.com/\">Waymo</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Buddy is a DIY and fetchingly minimalist social robot (of sorts) that will be coming to Kickstarter this month.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/EVSwDHm7F5Q\" width=\"620\"></iframe></p> \n     <p></p> \n     <blockquote> \n      <p><em>We have created a new arduino kit. His name is Buddy. He is a DIY social robot to serve as a replacement for Jibo, Cozmo, or any of the other bots that are no longer available. Fully 3D printed and supported he adds much more to our series of Arduino STEM robotics kits.</em></p> \n      <p><em>Buddy is able to look around and map his surroundings and react to changes within them. He can be surprised and he will always have a unique reaction to changes. The kit can be built very easily in less than an hour. It is even robust enough to take the abuse that kids can give it in a classroom. </em></p> \n     </blockquote> \n     <p>[ <a href=\"https://www.littlearmrobot.com/blog/buddy-is-an-arduino-social-robot-kit-is-coming-to-kickstarter-soon\">Littlebots</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <blockquote> \n       <p><em>The android Mindar, based on the Buddhist deity of mercy, preaches sermons at Kodaiji temple in Kyoto, and its human colleagues predict that with artificial intelligence it could one day acquire unlimited wisdom. Developed at a cost of almost $1 million (¥106 million) in a joint project between the Zen temple and robotics professor Hiroshi Ishiguro, the robot teaches about compassion and the dangers of desire, anger and ego.</em></p> \n      </blockquote> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hLoF5_-OUKY\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://www.japantimes.co.jp/news/2019/08/15/business/tech/kyoto-temple-robot-priest/\">Japan Times</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>I’m not sure whether it’s the sound or what, but this thing scares me for some reason.</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Xn_zbxiRZpw\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"http://divf.eng.cam.ac.uk/birl/\">BIRL</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>This gripper uses magnets as a sort of adjustable spring for dynamic stiffness control, which seems pretty clever.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/O1hRgG55DY4\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"http://www.buffalo.edu/news/releases/2019/002/017.html\">Buffalo</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>What a package of medicine sees while being flown by drone from a hospital to a remote clinic in the Dominican Republic. The drone flew 11 km horizontally and 800 meters vertically, and I can’t even imagine what it would take to make that drive.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tHeF5C3iWwM\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://flyinglabs.org/dominicanrepublic/\">WeRobotics</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>My first ride in a fully autonomous car was at Stanford in 2009. I vividly remember getting in the back seat of a descendant of Junior, and watching the steering wheel turn by itself as the car executed a perfect parking maneuver. Ten years later, it’s still fun to watch other people have that experience.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/x4jg4E7LrZE\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://waymo.com/\">Waymo</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>Flirtey, the pioneer of the commercial drone delivery industry, has unveiled the much-anticipated first video of its next-generation delivery drone, the Flirtey Eagle. The aircraft designer and manufacturer also unveiled the Flirtey Portal, a sophisticated take off and landing platform that enables scalable store-to-door operations; and an autonomous software platform that enables drones to deliver safely to homes.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/7YczsripIlA\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.flirtey.com/\">Flirtey</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>EPFL scientists are developing new approaches for improved control of robotic hands – in particular for amputees – that combines individual finger control and automation for improved grasping and manipulation. This interdisciplinary proof-of-concept between neuroengineering and robotics was successfully tested on three amputees and seven healthy subjects.</em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/L_jhQxMF8R4\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://actu.epfl.ch/news/a-smart-artificial-hand-for-amputees-merges-user-a/\">EPFL</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>This video is a few years old, but we’ll take any excuse to watch the majestic sage-grouse be majestic in all their majesticness.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/N3RYJWldEMM\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://patricellilab.faculty.ucdavis.edu/research/\">UC Davis</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <p>I like the idea of a game of soccer (or, football to you weirdos in the rest of the world) where the ball has a mind of its own.</p> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/KkcIp-sHUWY\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://www.sphero.com/about/mini-soccer/\">Sphero</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>Looks like the whole delivery glider idea is really taking off! Or, you know, not taking off.</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/1Lt83j6v8xE\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>Weird that they didn’t show the landing, because it sure looked like it was going to plow into the side of the hill at full speed.</p> \n               <p>[ <a href=\"https://yateselectrospace.com/news-%26-media/f/silent-arrow%C2%AE-1-ton-cargo-delivery-drone-unveiled-in-london\">Yates</a> ] via [ <a href=\"https://www.suasnews.com/2019/09/silent-arrow-1-ton-cargo-delivery-drone-unveiled-in-london-full-rate-production-to-follow/\">sUAS News</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <p>This video is from a 2018 paper, but it’s not like we ever get tired of seeing quadrupeds do stuff, right?</p> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/q6zxCvCxhic\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>[ <a href=\"https://biomimetics.mit.edu/dynamic-locomotion-mit-cheetah-3-through-convex-model-predictive-control\">MIT</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <blockquote> \n                  <p><em>Founder and Head of Product, Ian Bernstein, and Head of Engineering, Morgan Bell, have been involved in the <a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty</a> project for years and they have learned a thing or two about building robots. Hear how and why Misty evolved into a robot development platform, learn what some of the earliest prototypes did (and why they didn’t work for what we envision), and take a deep dive into the technology decisions that form the <a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty II platform</a>.</em></p> \n                 </blockquote> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/X3bb4XeYLxU\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <p>[ <a href=\"https://www.mistyrobotics.com/\">Misty Robotics</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <p>Lex Fridman interviews Vijay Kumar on the Artifiical Intelligence Podcast.</p> \n                  <p></p> \n                  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/HYsLTNXMl1Q\" width=\"620\"></iframe></p> \n                  <p></p> \n                  <p>[ <a href=\"https://lexfridman.com/ai/\">AI Podcast</a> ]</p> \n                  <p></p> \n                  <hr> \n                   <p></p> \n                   <p>This week’s CMU RI Seminar is from Ross Knepper at Cornell, on Formalizing Teamwork in Human-Robot Interaction.</p> \n                   <p></p> \n                   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/zV_4gIvItkc\" width=\"620\"></iframe></p> \n                   <p></p> \n                   <blockquote> \n                    <p><em>Robots out in the world today work for people but not with people. Before robots can work closely with ordinary people as part of a human-robot team in a home or office setting, robots need the ability to acquire a new mix of functional and social skills. Working with people requires a shared understanding of the task, capabilities, intentions, and background knowledge. For robots to act jointly as part of a team with people, they must engage in collaborative planning, which involves forming a consensus through an exchange of information about goals, capabilities, and partial plans. Often, much of this information is conveyed through implicit communication. In this talk, I formalize components of teamwork involving collaboration, communication, and representation. I illustrate how these concepts interact in the application of social navigation, which I argue is a first-class example of teamwork. In this setting, participants must avoid collision by legibly conveying intended passing sides via nonverbal cues like path shape. A topological representation using the braid groups enables the robot to reason about a small enumerable set of passing outcomes. I show how implicit communication of topological group plans achieves rapid covergence to a group consensus, and how a robot in the group can deliberately influence the ultimate outcome to maximize joint performance, yielding pedestrian comfort with the robot.</em></p> \n                   </blockquote> \n                   <p>[ <a href=\"https://www.ri.cmu.edu/ri-seminar-series/\">CMU RI</a> ]</p> \n                   <p></p> \n                   <hr> \n                    <p></p> \n                    <p>In this week’s episode of Robots in Depth, Per speaks with Julien Bourgeois about Claytronics, a project from Carnegie Mellon and Intel to develop \"programmable matter.\"</p> \n                    <p></p> \n                    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/KLDbQRfAMaA\" width=\"620\"></iframe></p> \n                    <p></p> \n                    <blockquote> \n                     <p><em>Julien started out as a computer scientist. He was always interested in robotics privately but then had the opportunity to get into micro robots when his lab was merged into the FEMTO-ST Institute. He later worked with Seth Copen Goldstein at Carnegie Mellon on the Claytronics project.</em></p> \n                     <p><em>Julien shows an enlarged mock-up of the small robots that make up programmable matter, catoms, and speaks about how they are designed. Currently he is working on a unit that is one centimeter in diameter and he shows us the very small CPU that goes into that model.</em></p> \n                    </blockquote> \n                    <p>[ <a href=\"http://robotsindepth.com/\">Robots in Depth</a> ]</p> \n                    <p></p> \n                    <hr> \n                     <p></p> \n                     <div class=\"feedflare\"> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=XQoU3ebUNAE:Lr6zffHQlz8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=XQoU3ebUNAE:Lr6zffHQlz8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=XQoU3ebUNAE:Lr6zffHQlz8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=XQoU3ebUNAE:Lr6zffHQlz8:-BTjWOF_DHI\" border=\"0\" /></a> \n                      <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=XQoU3ebUNAE:Lr6zffHQlz8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=XQoU3ebUNAE:Lr6zffHQlz8:gIN9vFwOqvQ\" border=\"0\" /></a> \n                     </div> \n                     <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/XQoU3ebUNAE\" height=\"1\" width=\"1\" alt=\"\" /> \n                    </hr> \n                   </hr> \n                  </hr> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 13 Sep 2019 18:05:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3MzQ2Nw.jpeg","linkMd5":"fddb2a608a827cba7cd0974ca59eeee3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn46@2020_1/2020/08/25/10-38-00-456_8eed592579d0b55e.webp","destWidth":1240,"destHeight":692,"sourceBytes":152683,"destBytes":127574,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=XQoU3ebUNAE:Lr6zffHQlz8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn3@2020_6/2020/08/25/10-38-09-163_15be7438b0b59dab.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=XQoU3ebUNAE:Lr6zffHQlz8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn95@2020_1/2020/08/25/10-38-03-521_4239620ca5cbbb6f.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/XQoU3ebUNAE":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn12@2020_1/2020/08/25/10-38-04-443_47737e74d844bdeb.webp"},"publishedOrCreatedDate":1598351859733},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"How Robotics Teams Are Solving the Biggest Problem at DARPA’s Subterranean Challenge","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","description":"<p>When DARPA announced its&nbsp;<a href=\"https://spectrum.ieee.org/tag/SubT\">Subterranean Challenge</a>, the agency framed things with a comprehensive list of “technical challenge elements” that it expected to be particularly, you know, challenging. One of those elements was communication&nbsp;constraints, which <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics\">DARPA said teams should expect to be “severe.</a>” That may have been an understatement, based on what we're seeing at the SubT Tunnel Circuit—<a href=\"https://www.subtchallenge.com/teams.html\">teams</a> have had a lot of trouble consistently talking to their robots on the course.</p> \n<p>Even though most teams are emphasizing autonomy as much as possible, they do still have to deal with communication&nbsp;challenges, because finding artifacts won’t earn you any points unless the robot reports its location back to base before time runs out. And if your robot has to come all the way back to base to make its report, it’s going to run out of time after <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge\">finding just a few artifacts</a>.</p> \n<p>There’s no magical solution for this, which is awesome, because teams have come up with all kinds of fantastically creative and unique strategies. We went around and asked them about it when we visited the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview\">SubT Tunnel Circuit</a> in Pittsburgh yesterday.&nbsp;</p> \n<!--nextpage--> \n<p>Some quick context to help you understand the challenge here—robots usually do fine through the first several tens of meters just past the <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview\">mine entrance</a>. Once they turn the first corner, however, they lose their wireless connection back to the base station almost immediately, because radio waves don’t&nbsp;pass through solid rock. And even if the robots manage to hang on to a signal for a little bit, the twists and turns and sheer distances that the robots must&nbsp;travel (hundreds and hundreds of meters, ideally) means it simply isn’t possible to maintain a direct connection back to base.&nbsp;</p> \n<p>Most teams are adopting a robot-to-robot <a href=\"https://spectrum.ieee.org/telecom/wireless/how-the-internet-of-things-could-fracture-wifi\">mesh networking</a> approach to help with this, meaning that the robots themselves can serve as network nodes, and if one robot can communicate back to base, any robot that it can communicate with can also communicate back to base—and so on down as long a chain of robots as you can manage. Robots need to move around to do their jobs, though, and teams only have a limited number of robots, so here are some other strategies that teams are using to keep in contact.</p> \n<h2>Team PLUTO</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"pluto\" src=\"https://spectrum.ieee.org/image/MzM1ODU0Ng.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>This is about as straightforward as it gets—Team PLUTO mounted massive dipole antennas on the butts of their Ghost Robotics Vision 60 quadrupeds, which in their experience improves communication performance by an order of magnitude. The team is emphasizing autonomy, and like many teams, their robots are designed to operate for extended periods without any communications at all, but they do still need to report back on what they found from time to time.</p> \n<h2>Team Explorer</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"explorer\" src=\"https://spectrum.ieee.org/image/MzM1ODU0Nw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>Team Explorer, currently leading (by a lot) in artifacts found, uses deployable network nodes (pulled halfway out of a protective casing in the picture above)&nbsp;that its robots can drop when the strength of the network starts getting low. Each robot can carry about 10 nodes in the two racks that you can see, and while the nodes are&nbsp;dropped at the command of the remote operator at the moment, the robots will eventually be able to decide autonomously when they need to plop one down. The team also plans to shrink the nodes down, and to improve the dropping mechanism, which can get jammed by mud.</p> \n<h2>Team CoSTAR</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"costar\" src=\"https://spectrum.ieee.org/image/MzM1ODU0OA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>The beefy deployable network node idea was a popular one, and Team CoSTAR (tied for second place in artifacts found at the end of day three) sent in multiple robots carrying nodes in several different configurations.&nbsp;</p> \n<h2>Team CERBERUS</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"cerberus\" src=\"https://spectrum.ieee.org/image/MzM1ODU0OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>DARPA not-so-subtly suggested that teams may not want to rely on a physical tether between the base station and robots (“teams should seriously consider the limitations <span>[on</span><span>&nbsp;tethers]&nbsp;</span>imposed by large-scale, potentially dynamic, complex environments,” says the agency), but Team CERBERUS didn’t let that scare them. They’re using a fiber-optic tether and a dedicated communications robot with a massive antenna on it to essentially extend their base station deep into the mine.</p> \n<p>The tether isn’t long enough to explore the whole mine, of course, and it has to be carefully managed around corners, but even if the robot just makes it down to the end of the first passage and around the first corner, it’s a massive improvement.</p> \n<h2>Team NCTU</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"NCTU\" src=\"https://spectrum.ieee.org/image/MzM1ODU4NQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>These are “Anchorballs” that Team NCTU’s robots can deploy as mesh network nodes, but they’re also used as active landmarks to help with SLAM localization. They’re weighted at the bottom so that, after being dropped, they stop rolling with a camera pointing at the ceiling, and the robot can then correlate the image from the Anchorball camera with its own internal map.&nbsp;</p> \n<div class=\"imgWrapper xlrg\"> \n <img alt=\"nctu\" src=\"https://spectrum.ieee.org/image/MzM1ODU1Nw.gif\"> </img> \n</div> \n<p>After a bit of communication&nbsp;trouble, Team NCTU decided that they needed to boost their system a bit, so they deployed their own tether of sorts yesterday.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"nctu\" src=\"https://spectrum.ieee.org/image/MzM1ODU4Ng.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>If that looks like a router wrapped in a plastic bag connected to a really long Ethernet cord, well, you nailed it. The team placed the router on a robot, let the cable out from the base station as the robot drove into the mine, and then just gave the cable a tug to pull the router off the robot and onto the ground once the robot got far enough to deploy the router where the team&nbsp;wanted it.</p> \n<h2>Team CSIRO Data61</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"csiro\" src=\"https://spectrum.ieee.org/image/MzM1ODU4OA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Navinda&nbsp;Kottege \n  </figcaption> </img> \n</figure> \n<p>These are some of the most cleverly designed droppable network nodes that we’ve seen—once dropped, the node waits for a few seconds for the robot to drive itself off, and then uses an actuator and springs to flip itself up and deploy its antennas. The reflective surfaces are a nice touch as well; presumably, they help DARPA find the dropped nodes at the end of a run, while also encouraging any following robots not to run over them quite as much.</p> \n<h2>Team CRETISE</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"irobot\" src=\"https://spectrum.ieee.org/image/MzM1ODU4OQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>Why drop a network node that just sits there, when you could instead drop a network node that can drive itself around? Team CRETISE is dropping <a href=\"https://spectrum.ieee.org/automaton/robotics/military-robots/firstlook-irobot-new-throwable-baby-surveillance-bot\">FirstLook robots</a> from a beastly mothership robot to serve as cute little mobile nodes that can zip around to optimize your network.</p> \n<div class=\"imgWrapper xlrg\"> \n <img alt=\"firstlook\" src=\"https://spectrum.ieee.org/image/MzM1ODU5OA.gif\"> </img> \n</div> \n<p>The FirstLooks are themselves descendants of LANdroids, which were <a href=\"https://investor.irobot.com/news-releases/news-release-details/irobot-receives-award-darpa-landroids-program\">developed by iRobot with DARPA funding</a> so long ago that I wrote about them back before I was even writing about robots, in early 2007. The original idea with LANdroids was that they would self-deploy to create dynamic and resilient mesh networks in challenging environments, but eventually iRobot turned them into the FirstLook tossable surveillance robots. And now, more than&nbsp;a decade later, Team CRETISE is turning them back into LANdroids again, which is pretty cool.&nbsp;</p> \n<h2>Team MARBLE</h2> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"marble\" src=\"https://spectrum.ieee.org/image/MzM1ODU5MA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Evan Ackerman/IEEE Spectrum \n  </figcaption> </img> \n</figure> \n<p>In addition to little puck-shaped deployable mesh-network nodes, Team MARBLE has been experimenting with these robots built on top of remote-controlled off-road racing cars. They have onboard sensing and computing and communications, of course, but they’re not intended to search for artifacts. Instead, their job is to ferry information—quickly navigating between areas with base station connectivity and other (slower) robots that are exploring elsewhere.</p> \n<p>Rather than acting as mobile nodes, these little cars are more like delivery robots, picking up data and carrying it back home. Without the constraints of having to maintain a network, the idea is that the exploration robots will be able to travel much farther while spending more time exploring autonomously, relying on the information-ferrying cars&nbsp;for all of their communication needs. The team doesn’t&nbsp;have it all working yet, but it’s a super interesting idea, and we’re looking forward to seeing it in action.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_dQJoG0X58s:OA7yLaP9HCA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_dQJoG0X58s:OA7yLaP9HCA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_dQJoG0X58s:OA7yLaP9HCA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_dQJoG0X58s:OA7yLaP9HCA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_dQJoG0X58s:OA7yLaP9HCA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_dQJoG0X58s:OA7yLaP9HCA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_dQJoG0X58s\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 21 Aug 2019 14:29:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1ODU2Nw.jpeg","linkMd5":"6d46fdc9cd73aab1f732a99137523da6","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn26@2020_5/2020/08/25/10-37-58-851_888a23e5ea91e340.webp","destWidth":1200,"destHeight":1021,"sourceBytes":653920,"destBytes":76910,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1ODU0Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn65@2020_2/2020/08/25/10-38-14-341_ff072f801b90edc5.webp","https://spectrum.ieee.org/image/MzM1ODU0Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn89@2020_5/2020/08/25/10-38-13-678_4fbb5ad65f01f922.webp","https://spectrum.ieee.org/image/MzM1ODU0OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn29@2020_3/2020/08/25/10-38-14-932_b42d644fa18b5e34.webp","https://spectrum.ieee.org/image/MzM1ODU0OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn91@2020_2/2020/08/25/10-38-11-421_df95ba2d3ac39686.webp","https://spectrum.ieee.org/image/MzM1ODU4NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn33@2020_5/2020/08/25/10-38-15-604_8b74043831186346.webp","https://spectrum.ieee.org/image/MzM1ODU1Nw.gif":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn56@2020_3/2020/08/25/10-38-10-230_77849562adec2344.webp","https://spectrum.ieee.org/image/MzM1ODU4Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn31@2020_2/2020/08/25/10-38-06-233_1ab4c4f6c602e428.webp","https://spectrum.ieee.org/image/MzM1ODU4OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn60@2020_1/2020/08/25/10-38-06-408_9f8c4ea74c0716dc.webp","https://spectrum.ieee.org/image/MzM1ODU4OQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn44@2020_6/2020/08/25/10-38-03-503_7e331c4f74e8189c.webp","https://spectrum.ieee.org/image/MzM1ODU5OA.gif":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn81@2020_3/2020/08/25/10-38-18-937_8243f70c6dbdc776.webp","https://spectrum.ieee.org/image/MzM1ODU5MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn86@2020_4/2020/08/25/10-38-14-679_5fabe96c544de9ce.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_dQJoG0X58s:OA7yLaP9HCA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn96@2020_3/2020/08/25/10-38-04-413_79fa55f8ddaaabf6.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_dQJoG0X58s:OA7yLaP9HCA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn73@2020_4/2020/08/25/10-38-15-245_bbe7db621635ea56.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_dQJoG0X58s":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn69@2020_2/2020/08/25/10-38-16-155_aa138e372d556f7e.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"DARPA Subterranean Challenge: Tunnel Circuit Preview","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","description":"<p>The Tunnel Circuit of the <a href=\"https://www.subtchallenge.com/\">DARPA Subterranean Challenge</a> starts later this week at the NIOSH research mine just outside of Pittsburgh, Pennsylvania. From 15-22&nbsp;<span>August</span>, 11 teams will send robots into a mine that they've never seen before, with the goal of making maps and locating items. All DARPA SubT events involve&nbsp;tunnels of one sort or another, but in this case, the “Tunnel Circuit”&nbsp;refers to mines as opposed to urban underground areas or natural caves.&nbsp;This month’s challenge is&nbsp;the first of three discrete events leading up to a huge final event in August of 2021.</p> \n<p>While the Tunnel Circuit competition will be closed to the public, and media are only allowed access for a single day (which we'll be at, of course), DARPA has provided a substantial amount of information about what teams will be able to expect. We also have details from the SubT Integration Exercise, called STIX, which was a completely closed event that took place back in April. STIX was aimed at giving&nbsp;some teams (and DARPA) a chance to practice in a real tunnel environment.&nbsp;</p> \n<!--nextpage--> \n<p>For more general background on SubT, here are some articles to get you all caught up:</p> \n<p><a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics\">SubT: The Next DARPA Challenge for Robotics</a></p> \n<p><a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/darpa-subterranean-challenge-qa-with-program-manager-timothy-chung\">Q&amp;A with DARPA Program Manager Tim Chung</a></p> \n<p><a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subt-meet-the-first-nine-teams\">Meet The First Nine Teams</a></p> \n<p>It makes sense to take a closer look at what happened at April's&nbsp;STIX exercise, because it is (probably) very similar to what teams will experience in the upcoming Tunnel Circuit. STIX took place at Edgar Experimental Mine in Colorado, and while no two mines are the same (and many are very, very different), there are enough similarities for STIX to have been a valuable experience for teams. Here's an overview video of the exercise from DARPA:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/E5DBmG5AJts?rel=0\" width=\"620\"></iframe></p> \n<p>DARPA has also put together a much more detailed walkthrough of the STIX mine exercise, which gives you a sense of just how vast, complicated, and (frankly) challenging for robots the mine environment is:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Z5Tryqg36ww?rel=0\" width=\"620\"></iframe></p> \n<p>So, that's the kind of thing that teams had to deal with back in April. Since the event was an exercise, rather than a competition, DARPA didn't really keep score, and wouldn't comment on the performance of individual teams. We've been trolling YouTube for STIX footage, though, to get a sense of how things went, and we found a few interesting videos.</p> \n<p>Here's a nice overview from <a href=\"https://www.subt-cerberus.org/\">Team CERBERUS</a>, which used drones plus an ANYmal quadruped:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sXVIUbe-RxA?rel=0\" width=\"620\"></iframe></p> \n<p><a href=\"https://robotics.fel.cvut.cz/cras/darpa-subt/\">Team CTU-CRAS</a> also used drones, along with a tracked robot:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Nk7SUNGzo1k?rel=0\" width=\"620\"></iframe></p> \n<p><a href=\"https://robotika.cz/competitions/subtchallenge/en\">Team Robotika</a> was brave enough to post video of a \"fatal failure\" experienced by&nbsp;its&nbsp;wheeled robot; the poor little bot gets rescued at about 7:00 in case you get worried:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sZP780M4cfQ?rel=0\" width=\"620\"></iframe></p> \n<hr> \n <p>So that was STIX. But what about the Tunnel Circuit competition this week? Here's a course preview video from DARPA:</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/LAziR-R-07c?rel=0\" width=\"620\"></iframe></p> \n <p>It sort of looks like the NIOSH mine might be a bit less dusty than the Edgar mine was, but it could also be wetter and muddier. It’s hard to tell, because we’re just getting a few snapshots of what’s probably an enormous area with kilometers of tunnels that the robots will have to explore.&nbsp;But DARPA has promised “constrained passages, sharp turns, large drops/climbs, inclines, steps, ladders, and mud, sand, and/or water.”&nbsp;Combine that with the serious challenge to communications imposed by the mine itself, and robots will have to be both physically capable, and almost entirely autonomous. Which is, of course, exactly what DARPA is looking to test with this challenge.</p> \n <hr> \n  <p>Lastly, we had a chance to catch up with <a href=\"https://www.darpa.mil/staff/dr-timothy-chung\">Tim Chung</a>, Program Manager for the Subterranean Challenge at DARPA, and ask him a few brief questions about STIX and what we have to look forward to this week.</p> \n  <p><strong>IEEE Spectrum: How did STIX go?</strong></p> \n  <p><strong><a href=\"https://www.darpa.mil/staff/dr-timothy-chung\">Tim Chung</a>:</strong> <em>It was a lot of fun! I think it gave a lot of the teams a great opportunity to really get a taste of what these types of real world environments look like, and also what DARPA has in store for them in the SubT Challenge. STIX I saw as an experiment—a learning experience for all the teams involved&nbsp;(as well as the DARPA team) so that we can continue our calibration.</em></p> \n  <p><strong>What do you think teams took away from STIX, and what do you think DARPA took away from STIX?</strong></p> \n  <p><em>I think the thing that teams took away was that, when DARPA hosts a challenge, we have very audacious visions for what the art of the possible is. And that's what we want—in my mind, the purpose of a DARPA Grand Challenge is to provide that inspiration of, ‘Holy cow, someone thinks we can do this!’&nbsp;So I do think the teams walked away with a better understanding of what DARPA's vision is for the capabilities we're seeking in the SubT Challenge, and hopefully walked away with a better understanding of the technical, physical, even maybe mental challenges of doing this in the wild— which will all roll back into how they think about the problem, and how they develop their systems.</em></p> \n  <p><em>This was a collaborative exercise, so the DARPA field team was out there interacting with the other engineers, figuring out what their strengths and weaknesses and needs might be, and even understanding how to handle the robots themselves. That will help [strengthen] connections between these university teams and DARPA going forward. Across the board, I think that collaborative spirit is something we really wish to encourage, and something that the DARPA folks were able to take away.</em></p> \n  <p><strong>What do we have to look forward to during the Tunnel Circuit?</strong></p> \n  <p><em>The vision here is that the Tunnel Circuit is representative of one of the three subterranean subdomains, along with urban and cave. Characteristics of all of these three subdomains will be mashed together in an epic final course, so that teams will have to face hints of tunnel once again in that final event.&nbsp;</em></p> \n  <p><em>Without giving too much away, the NIOSH mine will be similar to the Edgar mine in that it's a human-made environment that supports mining operations and research. But of course, every site is different, and these differences, I think, will provide good opportunities for the teams to shine.&nbsp;</em></p> \n  <hr> \n   <p>Again, we'll be visiting the NIOSH mine in Pennsylvania during the Tunnel Circuit and will post as much as we can from there.&nbsp;But if you’re an actual participant in the Subterranean Challenge, please tweet me <a href=\"https://twitter.com/BotJunkie\">@BotJunkie</a> so that I can follow and help share live updates.&nbsp;</p> \n   <p>[ <a href=\"https://www.subtchallenge.com/\">DARPA Subterranean Challenge</a> ]</p> \n   <div class=\"feedflare\"> \n    <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=D6FhFJID_ho:_h5f38TA_es:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n    <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=D6FhFJID_ho:_h5f38TA_es:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n    <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=D6FhFJID_ho:_h5f38TA_es:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=D6FhFJID_ho:_h5f38TA_es:-BTjWOF_DHI\" border=\"0\" /></a> \n    <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=D6FhFJID_ho:_h5f38TA_es:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=D6FhFJID_ho:_h5f38TA_es:gIN9vFwOqvQ\" border=\"0\" /></a> \n   </div> \n   <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/D6FhFJID_ho\" height=\"1\" width=\"1\" alt=\"\" /> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Wed, 14 Aug 2019 14:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1NDAxNA.jpeg","linkMd5":"745523971937e3d2bbf0ca9e0d077655","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn18@2020_5/2020/08/25/10-37-58-828_c81be9b4b4fbdfa3.webp","destWidth":1240,"destHeight":930,"sourceBytes":209744,"destBytes":86444,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=D6FhFJID_ho:_h5f38TA_es:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn99@2020_2/2020/08/25/10-38-06-782_f53686ae0b7ab372.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=D6FhFJID_ho:_h5f38TA_es:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn97@2020_3/2020/08/25/10-38-11-877_b0a49d267730b00e.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/D6FhFJID_ho":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn48@2020_1/2020/08/25/10-38-10-360_0c959971b9ff265a.webp"},"publishedOrCreatedDate":1598351859740},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Watch Robots Make a Crepe and Twist the Perfect Pretzel","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. Every week, we&nbsp;also&nbsp;post a calendar of upcoming robotics events; here's what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.qut.edu.au/robotronica\">Robotronica</a> –&nbsp;August 18, 2019 –&nbsp;Brisbane, Australia</h5> \n<h5><a href=\"https://clawar.org/clawar2019/\">CLAWAR 2019</a> –&nbsp;August 26-28, 2019 –&nbsp;Kuala Lumpur, Malaysia</h5> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-November 2, 2019 –&nbsp;Beijing</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-November 1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today's videos.</p> \n<hr> \n <!--nextpage--> \n <p>Team CoSTAR (JPL, MIT, Caltech, KAIST, LTU) has one of the more diverse teams of robots that we’ve seen:</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/E8PYc4GQocg\" width=\"620\"></iframe></p> \n <p></p> \n <p>[ <a href=\"https://subt.jpl.nasa.gov/\">Team CoSTAR</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <blockquote> \n   <p><em>A team from Carnegie Mellon University and Oregon State University is sending ground and aerial autonomous robots into a Pittsburgh-area mine to prepare for this month’s <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview\">DARPA Subterranean Challenge</a>.</em></p> \n  </blockquote> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/uBUElSNNAIk\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>“Look at that fire extinguisher, what a beauty!” Expect to hear a lot more of that kind of weirdness&nbsp;during <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge\">SubT</a>.</p> \n  <p>[ <a href=\"https://www.cs.cmu.edu/news/carnegie-mellon-oregon-state-robotics-team-prepares-subterranean-challenge\">CMU</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p><a href=\"http://www.unitree.cc/\">Unitree Robotics</a> is starting to batch-manufacture Laikago Pro quadrupeds, and if you buy four of them, they can carry you around in a chair!</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/pXre01gdTBo\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>I’m also really liking these videos from companies that are like, “We have a whole bunch of robot dogs now—what weird stuff can we do with them?”</p> \n   <p>[ <a href=\"http://www.unitree.cc/\">Unitree Robotics</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>Why take a handful of pills every day for all the stuff that's wrong with you, when you could take one custom pill instead? Because custom pills are time-consuming to make, that’s why. But robots don’t care!</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tJVKzLraHQU\" width=\"620\"></iframe></p> \n    <p></p> \n    <blockquote> \n     <p><em>Multiply Labs’ factory is designed to operate in parallel. All the filling robots and all the quality-control robots are operating at the same time. The robotic arm, in the meanwhile, shuttles dozens of trays up and down the production floor, making sure that each capsule is filled with the right drugs. The manufacturing cell shown in this article can produce 10,000 personalized capsules in an 8-hour shift. A single cell occupies just 128 square&nbsp;feet (12&nbsp;square meters) on the production floor. This means that a regular production facility (~10,000 square&nbsp;feet, or 929 m<sup>2</sup> ) can house 78&nbsp;cells, for an overall output of 780,000 capsules per shift. This exceeds the output of most traditional manufacturers—while&nbsp;producing unique personalized capsules!</em></p> \n    </blockquote> \n    <p>[ <a href=\"https://medium.com/multiply-labs/lights-out-pharma-factory-why-the-future-of-pharma-production-is-robotic-ee1c5297bb8a\">Multiply Labs</a> ]</p> \n    <p><em>Thanks Fred!</em></p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>If you’re getting tired of all those annoying drones that sound like giant bees, just have a listen to this turbine-powered one:</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Bjd2PX6D2uE\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"http://www.malloyaeronautics.com/\">Malloy Aeronautics</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>In retrospect, it’s kind of amazing that nobody has bothered to put a functional robotic dog head on a quadruped robot before this, right?</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/zZTiaj4PU_4\" width=\"620\"></iframe></p> \n      <p></p> \n      <blockquote> \n       <p><em>Equipped with sensors, high-tech radar imaging, cameras and a directional microphone, this 100-pound (45-kilogram) super-robot is still a “puppy-in-training.” Just like a regular dog, he responds to commands such as “sit,” “stand,” and “lie down.” Eventually, he will be able to understand and respond to hand signals, detect different colors, comprehend many languages, coordinate his efforts with drones, distinguish human faces, and even recognize other dogs. </em></p> \n       <p><em>As an information scout, Astro’s key missions will include detecting guns, explosives and gun residue to assist police, the military, and security personnel. This robodog’s talents won’t just end there, he also can be programmed to assist as a service dog for the visually impaired or to provide medical diagnostic monitoring. The MPCR team also is training Astro to serve as a first responder for search-and-rescue missions such as hurricane reconnaissance as well as military maneuvers.</em></p> \n      </blockquote> \n      <p>[ <a href=\"https://www.fau.edu/newsdesk/articles/astro-robodog.php\">FAU</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>And now this amazing video, “The Coke Thief,” from ICRA 2005 (!):</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Smf7qbnkYT4\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"http://cecas.clemson.edu/~iwalker/icra05.pdf\">Paper</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <blockquote> \n         <p><em>CYBATHLON Series put the focus on one or two of the six disciplines and are organized in cooperation with international universities and partners. The CYBATHLON Arm and Leg Prosthesis Series took place in Karlsruhe, Germany, from 16 to 18 May and was organized in cooperation with the Karlsruhe Institute of Technology (KIT) and the trade fair REHAB Karlsruhe.</em></p> \n        </blockquote> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/m0Pv_fXpCh4\" width=\"620\"></iframe></p> \n        <p></p> \n        <blockquote> \n         <p><em>The CYBATHLON Wheelchair Series took place in Kawasaki, Japan on 5 May 2019 and was organized in cooperation with the CYBATHLON Wheelchair Series Japan Organizing Committee and supported by the Swiss Embassy.</em></p> \n        </blockquote> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/lbeDQwUkdwo?rel=0\" width=\"620\"></iframe></p> \n        <p>[ <a href=\"https://cybathlon.ethz.ch/road-to-2020/cybathlon-series.html\">Cybathlon</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>Rainbow crepe robot!</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/h882cawPg3M\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>There’s also this other robot, which I assume does something besides what's in the video, because otherwise it appears to be a massively overengineered way of shaping cooked rice into a chubby triangle.</p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/5cEvARVQp8s?rel=0\" width=\"620\"></iframe></p> \n         <p>[ <a href=\"https://pc.watch.impress.co.jp/docs/column/kyokai/1201452.html\">PC Watch</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>The Weaponized Plastic Fighting League at Fetch Robotics has had another season of shardation, deintegration, explodification, and other -tions. Here are a couple fan favorite match videos:</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/91FVccnulbk\" width=\"620\"></iframe></p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tOGJIVf5kVI?rel=0\" width=\"620\"></iframe></p> \n          <p>[ <a href=\"https://fetchrobotics.com/\">Fetch Robotics</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>This video is in German, but it’s worth watching for the three seconds of extremely satisfying footage showing a robot twisting dough into pretzels.</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/RkO5lhN8bEw\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://www.festo.com/bakery\">Festo</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p>Putting brains into farming equipment is a no-brainer, since it’s a semi-structured environment that's generally clear of wayward humans driving other vehicles.</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/aqfSix3C46A\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"http://en.lovol.com.cn/\">Lovol</a> ]</p> \n            <p><em>Thanks Fan!</em></p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>Watch some robots assemble suspiciously Lego-like (but definitely not actually Lego) minifigs.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/9WZF1w2dwq0\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://devlinksltd.com/\">DevLinks</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>The Robotics Innovation Facility (RIFBristol) helps businesses, entrepreneurs, researchers and public sector bodies to embrace the concept of ‘Industry 4.0'. From training your staff in robotics, and demonstrating how automation can improve your manufacturing processes, to prototyping and validating your new innovations—we can provide the support you need.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/c5_w9xkVf08\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://www.bristolroboticslab.com/RIF-bristol\">RIF</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <p>Ryan Gariepy from Clearpath Robotics (and a bunch of other stuff) gave a talk at ICRA with the title of “Move Fast and (Don’t) Break Things: Commercializing Robotics at the Speed of Venture Capital,” which is more interesting when you know that this year’s theme&nbsp;was “Notable Failures.”</p> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/iW0dA0fLtXI\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://clearpathrobotics.com/blog/2019/08/move-fast-and-dont-break-things-commercializing-robotics-at-the-speed-of-venture-capital/\">Clearpath Robotics</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <p>In this week’s episode of Robots in Depth, Per interviews Michael Nielsen, a computer vision researcher at the Danish Technological Institute.</p> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/oF8P9R31h0E\" width=\"620\"></iframe></p> \n                <p></p> \n                <blockquote> \n                 <p><em>Michael worked with a fusion of sensors like stereo vision, thermography, radar, lidar and high-frame-rate cameras, merging multiple images for high dynamic range. All this, to be able to navigate the tricky situation in a farm field where you need to navigate close to or even in what is grown. Multibaseline cameras were also used to provide range detection over a wide range of distances.</em></p> \n                 <p><em>We also learn about how he expanded his work into sorting recycling, a very challenging problem. We also hear about the problems faced when using time of flight and sheet of light cameras. He then shares some good results using stereo vision, especially combined with blue light random dot projectors.</em></p> \n                </blockquote> \n                <p>[ <a href=\"http://robotsindepth.com/\">Robots in Depth</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <div class=\"feedflare\"> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=bL0ZQdpPbV8:3_409Sqs-i8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=bL0ZQdpPbV8:3_409Sqs-i8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=bL0ZQdpPbV8:3_409Sqs-i8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=bL0ZQdpPbV8:3_409Sqs-i8:-BTjWOF_DHI\" border=\"0\" /></a> \n                  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=bL0ZQdpPbV8:3_409Sqs-i8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=bL0ZQdpPbV8:3_409Sqs-i8:gIN9vFwOqvQ\" border=\"0\" /></a> \n                 </div> \n                 <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/bL0ZQdpPbV8\" height=\"1\" width=\"1\" alt=\"\" /> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 16 Aug 2019 15:17:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1NTkyMA.jpeg","linkMd5":"a31b47238c2872d0b2da6a891bae5495","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx/cdn78@2020_2/2020/08/25/10-37-58-961_29426200f9ef07e4.webp","destWidth":1240,"destHeight":930,"sourceBytes":156381,"destBytes":85164,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=bL0ZQdpPbV8:3_409Sqs-i8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn18@2020_5/2020/08/25/10-38-12-873_ddb0860d32112960.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=bL0ZQdpPbV8:3_409Sqs-i8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_5/2020/08/25/10-38-05-042_8ef3ec546ee12384.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/bL0ZQdpPbV8":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn37@2020_5/2020/08/25/10-38-15-645_ba50582444db1398.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Andrey Markov & Claude Shannon Counted Letters to Build the First Language-Generation Models","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","description":"<p><em>This is part three of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>.</em></p> \n<p></p> \n<p></p> \n<p></p> \n<p>In 1913, the Russian mathematician <a href=\"https://en.wikipedia.org/wiki/Andrey_Markov\">Andrey Andreyevich Markov</a> sat down in his study in St. Petersburg with a copy of&nbsp;Alexander Pushkin’s&nbsp;19th century verse novel, <em><a href=\"https://en.wikipedia.org/wiki/Eugene_Onegin\">Eugene Onegin</a>, </em>a&nbsp;literary classic at the time. Markov, however, did not start reading Pushkin’s famous text. Rather, he took a pen and piece of drafting paper, and wrote out the first 20,000 letters of the book in one long string of letters, eliminating all punctuation and spaces. Then he arranged these letters in 200 grids (10-by-10 characters each) and began counting the vowels in every row and column, tallying the results.</p> \n<!--nextpage--> \n<p></p> \n<p>To an onlooker, Markov’s behavior would have appeared bizarre. Why would someone deconstruct a work of literary genius in this way, rendering it incomprehensible? But Markov was not reading the book to learn lessons about life and human nature; he was searching for the text's more fundamental mathematical structure.</p> \n<p></p> \n<p></p> \n<aside class=\"inlay pullquote rt med\">\n  Markov was searching for the text's fundamental mathematical structure. \n</aside> \n<p>In separating the vowels from the consonants, Markov was testing a <a href=\"https://www.americanscientist.org/article/first-links-in-the-markov-chain\">theory of probability</a> that he had been developing since 1909. Up until that point, the field of probability had been mostly limited to analyzing phenomena like roulette or coin flipping, where the outcome of previous events does not change the probability of current events. But Markov felt that most things happen in chains of causality and are dependent on prior outcomes. He wanted a way of modeling these occurrences through probabilistic analysis.</p> \n<p></p> \n<p>Language, Markov believed, was an example of a system where past occurrences partly determine present outcomes. To demonstrate this, he wanted to show that in a text like Pushkin’s novel, the chance of a certain letter appearing at some point in the text is dependent, to some extent, on the letter that came before it.</p> \n<p></p> \n<p>To do so, Markov began counting vowels in <em>Eugene Onegin</em>, and found that 43 percent of letters were vowels and 57 percent were consonants. Then Markov separated the 20,000 letters into pairs of vowels and consonant combinations: He found that there were 1,104 vowel-vowel pairs, 3,827 consonant-consonant pairs, and 15,069 vowel-consonant and consonant-vowel pairs. What this demonstrated, statistically speaking, was that for any given letter in Pushkin’s text, if it was a vowel, odds were that the next letter would be a consonant, and vice versa.&nbsp;</p> \n<p></p> \n<p>Markov used this analysis to demonstrate that Pushkin’s <em>Eugene Onegin</em> wasn’t just a random distribution of letters but had some underlying statistical qualities that could be modeled. The enigmatic research paper that came out of this study, entitled “<a href=\"https://www.cambridge.org/core/journals/science-in-context/article/an-example-of-statistical-investigation-of-the-text-eugene-onegin-concerning-the-connection-of-samples-in-chains/EA1E005FA0BC4522399A4E9DA0304862\">An Example of Statistical Investigation of the Text&nbsp;<em>Eugene Onegin</em>&nbsp;Concerning the Connection of Samples in Chains</a>,” was not widely cited in Markov’s lifetime, and not translated to English until 2006. But some of its central concepts around probability and language spread across the globe, eventually finding re-articulation in <a href=\"https://spectrum.ieee.org/tag/Claude+Shannon\">Claude Shannon</a>’s hugely influential paper, “<a href=\"http://www.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\">A Mathematical Theory of Communication</a>,” which came out in 1948.&nbsp;</p> \n<p></p> \n<p>Shannon’s paper outlined a way to precisely measure the quantity of information in a message, and in doing so, set the foundations for a theory of information that would come to define the digital age. Shannon was fascinated by Markov’s idea that in a given text, the likelihood of some letter or word appearing could be approximated. Like Markov, Shannon demonstrated this by performing some textual experiments that involved making a statistical model of language, then took a step further by trying to use the model to generate text according to those statistical rules.</p> \n<p></p> \n<p>In an initial control experiment, he started by generating a sentence by picking letters randomly from a 27-symbol alphabet (26 letters, plus a space), and got the following output:</p> \n<p></p> \n<p>XFOML RXKHRJFFJUJ ZLPWCFWKCYJ FFJEYVKCQSGHYD QPAAMKBZAACIBZLHJQD</p> \n<p></p> \n<p>The sentence was meaningless noise, Shannon said, because when we communicate we don’t choose letters with equal probability. As Markov had shown, consonants are more likely than vowels. But at a greater level of granularity, E’s are more common than S’s which are more common than Q’s. To account for this, Shannon amended his original alphabet so that it modeled the probability of English more closely—he was 11 percent more likely to draw an E from the alphabet than a Q. When he again drew letters at random from this recalibrated corpus he got a sentence that came a bit closer to English.</p> \n<p></p> \n<p>OCRO HLI RGWR NMIELWIS EU LL NBNESEBYA TH EEI ALHENHTTPA OOBTTVA NAH BRL.</p> \n<p></p> \n<p>In a series of subsequent experiments, Shannon demonstrated that as you make the statistical model even more complex, you get increasingly more comprehensible results. Shannon, via Markov, revealed a statistical framework for the English language, and showed that by modeling this framework—by analyzing the dependent probabilities of letters and words appearing in combination with each other—he could actually generate language.</p> \n<p></p> \n<aside class=\"inlay pullquote lt med\">\n  “THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED” \n <span class=\"pq-attrib\">—Claude Shannon’s language generating model</span> \n</aside> \n<p>The more complex the statistical model of a given text, the more accurate the language generation becomes—or as Shannon put it, the greater “resemblance to ordinary English text.” In the final experiment, Shannon drew from a corpus of words instead of letters and achieved the following:</p> \n<p></p> \n<p>THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHO EVER TOLD THE PROBLEM FOR AN UNEXPECTED.</p> \n<p></p> \n<p>For both Shannon and Markov, the insight that language’s statistical properties could be modeled offered a way to re-think broader problems that they were working on.</p> \n<p>For Markov, it extended the study of stochasticity beyond mutually independent events, paving the way for a <a href=\"https://en.wikipedia.org/wiki/Markov_chain\">new era in probability theory</a>. For Shannon, it helped him formulate a precise way of measuring and <a href=\"https://en.wikipedia.org/wiki/Claude_Shannon#Information_theory\">encoding units of information in a message</a>, which revolutionized telecommunications and, eventually, <a href=\"https://spectrum.ieee.org/tech-talk/telecom/internet/bell-labs-looks-at-claude-shannon-legacy-future-of-information-age\">digital communication</a>.&nbsp;But their statistical approach to language modeling and generation also ushered in a new era for natural language processing, which has ramified through the digital age to this day.</p> \n<p></p> \n<p><em>This is the third installment of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. Last week’s post described Leibniz’s proposal for a <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas\">machine that combined concepts</a> to form reasoned arguments. Come back next Monday for part four, “<a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot\">Why People Demanded Privacy to Confide in the World’s First Chatbot.</a>”</em></p> \n<p></p> \n<p><em>You can also check out our prior series on the <a href=\"https://spectrum.ieee.org/tag/AI+history\">untold history of AI</a>.</em></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Bp2w1CN_Clw:Fcj0Rb0C_qQ:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Bp2w1CN_Clw:Fcj0Rb0C_qQ:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Bp2w1CN_Clw:Fcj0Rb0C_qQ:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Bp2w1CN_Clw:Fcj0Rb0C_qQ:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Bp2w1CN_Clw:Fcj0Rb0C_qQ:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Bp2w1CN_Clw:Fcj0Rb0C_qQ:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Bp2w1CN_Clw\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 11 Nov 2019 14:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwOTE2NQ.jpeg","linkMd5":"9ddfdd2b95e95bd2322570f7f5c5dbdb","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn53@2020_6/2020/08/25/10-37-58-981_1a8e6c6cea3c5c38.webp","destWidth":1240,"destHeight":930,"sourceBytes":258907,"destBytes":142882,"author":"Oscar Schwartz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Bp2w1CN_Clw:Fcj0Rb0C_qQ:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn52@2020_5/2020/08/25/10-38-02-465_268c4c507d687dd8.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Bp2w1CN_Clw:Fcj0Rb0C_qQ:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn26@2020_2/2020/08/25/10-38-12-142_865ba440a76e7d51.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Bp2w1CN_Clw":"https://cdn.jsdelivr.net/gh/myreaderx/cdn76@2020_4/2020/08/25/10-38-05-863_23f4e0bc4fa63e78.webp"},"publishedOrCreatedDate":1598351859720},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Skydio’s New Drone Is Smaller, Even Smarter, and (Almost) Affordable","link":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","description":"<p>When <a href=\"https://www.skydio.com\">Skydio</a> announced the R1 in early 2018, it was <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydio-r1-drone\">one of the most incredible drones we’d ever seen</a>. It’s been a year and a half, and in the fast-paced world of drones, the <a href=\"https://robots.ieee.org/robots/skydior1/?utm_source=spectrum\">Skydio R1</a> is somehow still, by a huge margin, the most intelligent and capable drone&nbsp;in existence, offering a level of autonomy that would be impressive even if it was a one-off research project, which it wasn’t, because you could buy one for US $2,500.</p> \n<p>The R1, though, was really not intended to be a consumer drone in the sense that it wasn’t a direct competitor to the likes of <a href=\"https://www.dji.com/\">DJI</a>, which has overwhelmingly dominated the consumer drone space since the early days of consumer drones. Rather, the R1 was meant to demonstrate exactly what Skydio was capable of, offering the chosen few who could justify paying for one a magical experience that couldn’t be found anywhere else.</p> \n<p>Today, Skydio is announcing their second drone: the Skydio 2. The Skydio 2 takes everything that made the R1 so amazing, and squeezes it into something smaller, smarter, and at $999, alarmingly close to affordable.</p> \n<!--nextpage--> \n<figure class=\"xlrg\" role=\"img\"> \n <iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/imt2qZ7uw1s?rel=0\" width=\"620\"></iframe> \n <figcaption class=\"hi-cap\">\n   Video: Skydio \n </figcaption> \n</figure> \n<p>The Skydio 2 is a drone with built-in subject tracking and obstacle avoidance with&nbsp;omnidirectional coverage, including above and below. Using an array of cameras, the drone can follow people or vehicles at speed while nimbly dodging anything that might get in the way, all while recording smooth, cinematic video—and you don’t have to do anything besides telling the drone who you want it to follow. Other drones claim similar functionality, but generally only under ideal and very limited conditions. Skydio’s drones may not be able to handle everything, but they can handle almost everything, which is saying a lot when you consider the inherently three dimensional and unstructured and cluttered up world that they have to operate in. The Skydio 2 represents autonomy that you can trust.</p> \n<h3>How Skydio 2’s obstacle avoidance works</h3> \n<p>While the launch video above does a good job of highlighting the Skydio 2’s subject tracking, you only get a few hints of just how impressive the obstacle avoidance capability is. Wide open spaces are dramatic and all, but the brief flight through that warehouse shows the kind of autonomy that Skydio’s drones excel at. This is basically state-of-the-art for drones right now, and it’s in a consumer product that costs under $1,000. If <a href=\"https://www.youtube.com/watch?v=Gh5pAT1o2V8\">that original video of the Skydio R1 chasing a mountain biker through a forest blew you away</a> (which it totally should have), we’re told that the Skydio 2’s autonomy is better. Way, way better.&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/_bWPnFGuBSE\" width=\"620\"></iframe> \n <figcaption class=\"hi-cap\">\n   Video: Skydio \n </figcaption> \n <figcaption>\n   This video shows the feed from one of the three top navigation cameras [left circle] and the deep learning predicted depth map [right circle] as a Skydio 2 drone flies through three challenging scenarios. The first has a lot of clutter and repeated texture (trees and branches); the second has thin lines (poles with overhanging wires); and the last has large textureless surfaces indoors. The final part of the video shows the 3D point cloud of the drone flying around Skydio’s office. \n </figcaption> \n</figure> \n<p>The most obvious change between the Skydio R1 and Skydio’s new drone is that the structure that wrapped all the way around the R1 and contained 12 VGA cameras has been completely eliminated. The Skydio 2 uses just six cameras, each of which is a 4K fisheye with a 200-degree field of view, which was tricky for Skydio to implement successfully, as Skydio CEO Adam Bry explains:</p> \n<p><em>That was a super challenging engineering problem, because we went from these conventional global shutter cameras to rolling shutter fisheye cameras, which are very very challenging from an algorithmic standpoint. But the big advantage is that at a lower price point, we can get much better sensor performance. The simple metric on this is we went from about 3 megapixels of visual sensing on R1, to about 45 megapixels on Skydio 2.&nbsp;</em></p> \n<p>The placement of these cameras resulted in the drone’s curious over/under propeller design, which ensures that the cameras on the top and bottom of the drone get a complete hemispherical view unobstructed by propellers. In fact, the Skydio 2 can see nearly every point in space with three cameras at once, resulting in “better than stereo” depth perception.</p> \n<figure class=\"rt med\" role=\"img\"> \n <img alt=\"Gif of person riding bike from the perspective of the drone.\" src=\"https://spectrum.ieee.org/image/MzM4NTc1NA.gif\"> \n  <figcaption class=\"hi-cap\">\n    Gif: Skydio \n  </figcaption> \n  <figcaption>\n    Skydio’s autonomy system is powered by an NVIDIA Tegra X2 processor running nine custom deep networks. The drone uses a 3D world model with an update rate of more than 1 million points per second, and is able to track up to 10 simultaneous objects. \n  </figcaption> </img> \n</figure> \n<p>In order to deal with the amount of data required to successfully dodge obstacles at its top speed of 58 km/h&nbsp;(36 mph), the Skydio 2 is packing a very tightly integrated NVIDIA Tegra X2 inside of its magnesium frame. The frame actually functions as a heat sink, conducting heat away from the processors out underneath the propellers, which act as a very effective cooling system. The autonomy system is still sucking down a lot of power, though—Bry says that the drone’s compute budget is about 10 percent&nbsp;of its total power consumption, meaning that there’s a couple minutes of flight time that you’re not getting because the drone is constantly thinking so hard.</p> \n<h3>Controller and beacon: extended range&nbsp;</h3> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2 controller\" src=\"https://spectrum.ieee.org/image/MzM4NTU5Ng.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Skydio \n  </figcaption> \n  <figcaption>\n    In addition to an&nbsp;app for iOS and Android, Skydio now offers a dedicated physical controller with a 3.5-kilometer range if you’d like to fly it by hand. To be clear, “by hand” doesn’t deactivate the obstacle avoidance, and the drone will fly itself around obstacles and keep you from running into things quite effectively. \n  </figcaption> </img> \n</figure> \n<p>Since the Skydio 2 is primarily intended to fly, rather then be flown, it doesn’t come standard with a physical controller. The good news is that Skydio does now offer a dedicated physical controller for the drone for an extra $150, if you’d like to fly it by hand. To be clear, “by hand” doesn’t deactivate the obstacle avoidance, and the drone will fly itself around obstacles and keep you from running into things quite effectively.</p> \n<figure class=\"rt med-sm\" role=\"img\"> \n <img alt=\"Skydio 2 beacon\" src=\"https://spectrum.ieee.org/image/MzM4NTU5OA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Skydio \n  </figcaption> \n  <figcaption>\n    You can use the beacon to give simple commands to the drone, but its primary purpose is to allow the drone to track you when you’re out of sight (the beacon has a 1.5 km range and uses GPS and GLONASS). \n  </figcaption> </img> \n</figure> \n<p>If the controller looks familiar (and it should), that’s because it’s a Parrot Skycontroller3 (which ships with the Parrot Anafi drone) with Skydio’s logo on the front. There’s nothing sneaky going on here—Skydio is not particularly interested in making controllers, so they’re buying Skycontroller3s directly from Parrot and loading in their own software. <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone\">You can read more about the Skycontroller3 as part of my Anafi review</a>, but basically, it’s a solid, simple controller that’s a little chunky but has an excellent antenna.</p> \n<p>The other accessory that you can buy for $150 is a wireless beacon. You can use the beacon to give simple commands to the drone, but its primary purpose is to allow the drone to track you when you’re out of sight, which is important if you want to take advantage of the Skydio 2’s obstacle avoidance as much as possible. While the communication range between a Skydio 2 and&nbsp;a smartphone is about 200 meters, the beacon extends that to 1.5 kilometers, and the controller has a range of 3.5 km.</p> \n<h2>Who should buy a Skydio 2?</h2> \n<p>We recognize that at $1,000 plus accessories, calling this drone “affordable” could be a stretch for most people. But if you’re in the market for a new drone of any kind, the Skydio 2 is worth a serious look. To put the Skydio 2’s cost in perspective with what could conceivably be called its main-stream competition: <a href=\"https://robots.ieee.org/robots/anafi/?utm_source=spectrum\">Parrot’s Anafi drone</a> is currently about $650, features limited tracking capability but no obstacle avoidance. You can find a DJI Mavic Air for about $750, which does include some fairly primitive obstacle avoidance from the front. A <a href=\"https://robots.ieee.org/robots/mavic/?utm_source=spectrum\">DJI&nbsp;Mavic 2</a>, which (on paper) purports to offer a somewhat similar omnidirectional obstacle detection and avoidance capability as Skydio, starts at around $1,400, but we can’t emphasize enough that Skydio’s autonomy is in a totally and completely different class to anything you’d experience with DJI.</p> \n<p>The Skydio 2 also holds up well when compared across other metrics. Independent testing (commissioned by Skydio, so take it with a grain of salt) demonstrates that the Skydio 2’s 12-megapixel main camera (which also shoots 4K video) can hold its own against the Mavic 2’s camera. With the optional controller, you’ll have no problem flying the Skydio 2 farther than you can see, and&nbsp;23 minutes of flight time is respectable. While the 775-gram Skydio 2 weighs somewhere in between the Mavic Air and Mavic 2 Pro, it doesn’t fold down, although it is quite flat with the 4280 mAh battery taken out.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio drone\" src=\"https://spectrum.ieee.org/image/MzM4NTU4Mg.jpeg\"> <img alt=\"Skydio 2 drone\" src=\"https://spectrum.ieee.org/image/MzM4NTYxNg.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photos: Skydio \n   </figcaption> \n   <figcaption>\n     The Skydio 2 doesn’t fold down but is quite flat with the battery taken out. The drone almost certainly won’t fit in your pocket, but you’d have no trouble fitting its hard case into a small bag, so for most practical purposes it’s similarly portable to a folding drone, weighing in at 775 grams. \n   </figcaption> </img></img> \n</figure> \n<p>Since some of Skydio 2’s navigation cameras are on its arms, adding a folding mechanism (and making those arms less rigid) would have resulted in significantly degraded performance, we’re told. The drone almost certainly won’t fit in your pocket, but you’d have no trouble fitting its hard case into a small bag, so for most practical purposes it’s similarly portable to a folding drone.</p> \n<p>Essentially, the Skydio 2 is offering a magical amount of autonomy and no significant downsides for a premium of a few hundred dollars over what you’d pay for a much (much) dumber drone. You’re not just paying for something that’s marginally better, you’re paying for access to entirely new capabilities that manage to deliver on what drones have been promising for half a decade. If your primary goal is to learn how to pilot a drone, perhaps the Skydio 2 isn’t for you. But if your primary goal is to use a drone, that’s what the Skydio 2 is all about: taking amazing pictures and videos of yourself and your friends and family without having to worry about the process of not crashing into things and keeping your subject in frame. And let’s face it, unless you’re already an expert pilot, the Skydio 2 is going to do a much better job anyway.</p> \n<p>But while the Skydio 2 is a “consumer” drone, it’s certainly not limited to being used by consumers, and Skydio is already thinking about the advantages that a fully autonomous drone could offer to industry. You don’t have to think very hard to see how big of a deal this is—if you replace “autonomous filming” with “autonomous data collection” there are all kinds of immediate applications. <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydio-announces-sdk-to-make-worlds-cleverest-drone-even-cleverer\">Skydio has an SDK</a> that it’s opening up to commercial partners, starting with DroneDeploy, a mapping company. And Skydio promises that over the next several months, the Skydio 2 will get to work in everything from infrastructure inspection to assisting first responders.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/uiZdejUHth8\" width=\"620\"></iframe> \n <figcaption class=\"hi-cap\">\n   Video: Skydio \n </figcaption> \n <figcaption>\n   The placement of Skydio 2’s&nbsp;navigation cameras resulted in the drone’s curious over/under propeller design, which ensures that the cameras on the top and bottom of the drone get a complete hemispherical view unobstructed by propellers. \n </figcaption> \n</figure> \n<h2>Designing Skydio 2</h2> \n<p>For more details about Skydio and their new drone, we spoke with Skydio CEO <a href=\"https://www.linkedin.com/in/adam-bry-a5788582/\">Adam Bry</a>.</p> \n<p><strong><em>IEEE Spectrum:</em> How did developing the R1 help you get to the Skydio 2?</strong></p> \n<p><em><strong>Adam Bry:</strong> When we started designing R1, we were basically five people in a house. The team was super small. There was a ton of stuff we didn’t know, and nobody had ever built a product like that before. Because of that, it was a real venture into the unknown. We got a lot of stuff right, but we also learned a huge amount through that development process that there was just no way that we’d have been able to capture in a first product. The other side of it is that with the development of R1, we basically weren’t able to optimize for cost at all—we optimized for trying to make the thing work, because it was so fundamentally challenging.</em></p> \n<p><strong>Why is autonomy so important for drones?</strong></p> \n<p><em>Our basic belief is that drones are like other computing devices—phones and laptops and tablets—where the right device with the right core technology built into it becomes a platform for many different use cases. We think autonomy is the thing that enables that, because autonomy is what turns it into a software-defined experience. Autonomy is what makes it possible to write applications for different use cases. Without autonomy, the application is in the skill of the pilot, and I don’t think that’s long-term scalable for drones.</em></p> \n<p><strong>How has Skydio’s autonomy gotten better?</strong></p> \n<p><em>The starting point is having much more data coming in and a faster computer, which opens up some new possibilities, but even with all the hardware stuff, the biggest progress is still on the software side. One of the big things that we’ve done is come up with an entirely new depth perception algorithm based on deep learning rather than more conventional [photogrammetric] stereo techniques, which generally work in simple scenes but fail when there’s a lot of repeated structure or textureless surfaces.</em></p> \n<p><em>What people do is combine contextual scene understanding with photometric information—when we look at a scene, we know what everything is and generally how the world is structured around us. And the combination of that contextual information with the photometric information means that people and animals essentially have perfect depth perception based just on vision. We’ve come up with a deep learning algorithm that mimics this, such that the drone not only uses photometric information but also contextual information. This drastically improves performance in every scenario that we care about, and overcomes a lot of the really tricky cases. For example, flying around in a big textureless room, we see near perfect performance. And with power lines, if it can see any part of the power line, it does the same thing that a person does and assumes that the line extends. It’s a really, really powerful thing.</em></p> \n<aside class=\"inlay pullquote xlrg\">\n  “One of the big things that we’ve done is come up with an entirely new depth perception algorithm based on deep learning rather than more conventional [photogrammetric] stereo techniques, which generally work in simple scenes but fail when there’s a lot of repeated structure or textureless surfaces” \n <span class=\"pq-attrib\">—Adam Bry, Skydio</span> \n</aside> \n<p><strong>Is the drone aware of its own confidence level in its autonomy, and does it communicate that to the user?</strong></p> \n<p><em>There are some scenarios where it doesn’t know enough to be cautious, but everywhere we can, we bake caution into the software. A couple of tangible examples: It checks the lightning conditions and will warn you and come to a hover if there’s just not enough light to fly. It also detects smudges or other damage to the lenses or cameras, and behaves conservatively—if it has a smudge in a particular direction, it’ll try not to fly in that direction. The way that we perceive the environment, we have a notion of uncertainty baked in, and so if the drone is seeing things that it’s never seen before and can’t make sense of what’s going on, it knows that and degrades gracefully while letting the user know what’s happening.</em></p> \n<p><em>This is why we put the safe operating guidelines out there. We don’t yet claim that these things are uncrashable, because we don’t want people to try. In normal use, everything should be fine, but the more drones we ship, the more likely it is that someone will find something we’ve never seen before. Those situations usually turn out to be useful learning experiences for us that we use to improve and update the software.</em></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio 2 drone\" src=\"https://spectrum.ieee.org/image/MzM4NTU5NQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Skydio \n  </figcaption> \n  <figcaption>\n    The&nbsp;Skydio 2 drones are hand made in California.&nbsp;According to the company’s warranty, if you’re operating your Skydio 2 within its Safe Flight guidelines and it crashes, Skydio will repair or replace it for free. \n  </figcaption> </img> \n</figure> \n<p><strong>What’s the best way to get more of these advanced drone capabilities into consumer products?</strong></p> \n<p><em>One thing I would say, and this may be controversial in some circles, is I think that autonomous drones, and full fledged autonomous robots in general, have become systems engineering challenges. A really interesting thing that’s happened over the last five years is that the most advanced systems have transitioned from being built in academia to being built by companies, because the technology is now mature enough, and there are real markets for it, meaning that you can make significant R&amp;D investments and build strong teams across a bunch of different disciplines.</em></p> \n<p><em>I’ve seen this firsthand: I was a grad student at MIT from 2009 to 2012, and when we were building our systems there, it was basically two of us trying to do everything across hardware and software and embedded firmware and it was all kind just barely held together with chewing gum and duct tape. At Skydio, we have a world class team across hardware and software and they’re all working super closely together with aligned goals, and that’s something you can’t replicate in a research lab.</em></p> \n<p><strong>In your press release, you referred to the drone industry as “not healthy.” Can you elaborate on that?</strong></p> \n<p><em>I think the drone industry is still largely a story of unmet potential. Drone generated enormous hype and excitement five or six years ago, and for the most part, I think we haven’t realized most of the concepts and use cases to the level that was promised. That gap is one of the things that motivated us to start the company, and our belief is that autonomy is a huge part of what makes these things possible.</em></p> \n<p><em>That’s one angle. The other angle is the blunt fact that DJI has been so dominant, and controls so much of the market, that I think it’s resulted in an unhealthy dynamic. It stops suppliers from wanting to invest in supporting technologies, it frustrates customers who want an alternative, and we think this creates an opportunity for us.</em></p> \n<p><em>The last piece of it, which is something that is getting more and more attention, is that I think people have realized that drones are really important for national security. These consumer devices that a lot of people started off thinking were just toys are actually in a lot of ways the most cutting-edge aerospace systems out there, and have raced beyond a lot of the capabilities of more conventional military drones. And whatever you think about US-China relations, it’s probably not great for long-term US national security if the US isn’t capable of manufacturing drones and leading the way in drone technology.</em></p> \n<hr> \n <p>A limited number of Skydio 2 drones (which are hand made in California) are available for pre-order as of right now, and a $100 deposit will get your name on the list for delivery starting in November.</p> \n <p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VuRd4Y5PJ5k:_NkqpHdLR9M:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VuRd4Y5PJ5k:_NkqpHdLR9M:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VuRd4Y5PJ5k:_NkqpHdLR9M:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VuRd4Y5PJ5k:_NkqpHdLR9M:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VuRd4Y5PJ5k:_NkqpHdLR9M:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VuRd4Y5PJ5k:_NkqpHdLR9M:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VuRd4Y5PJ5k\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Tue, 01 Oct 2019 17:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4NTY0NA.jpeg","linkMd5":"04bd6282cc987a8acf3bef5de891d450","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn10@2020_6/2020/08/25/10-37-59-589_306487e62d64bd2f.webp","destWidth":1240,"destHeight":930,"sourceBytes":70750,"destBytes":43490,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM4NTc1NA.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn3@2020_5/2020/08/25/10-38-12-367_57f4b01dea1d32b0.webp","https://spectrum.ieee.org/image/MzM4NTU5Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn60@2020_3/2020/08/25/10-38-02-527_f64a89997e018552.webp","https://spectrum.ieee.org/image/MzM4NTU5OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn99@2020_3/2020/08/25/10-38-03-817_f4443da59dd13854.webp","https://spectrum.ieee.org/image/MzM4NTU4Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn75@2020_4/2020/08/25/10-38-09-916_5bb9b729ac0b7ff8.webp","https://spectrum.ieee.org/image/MzM4NTYxNg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn87@2020_6/2020/08/25/10-38-03-551_46907fe6d788edea.webp","https://spectrum.ieee.org/image/MzM4NTU5NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn99@2020_3/2020/08/25/10-38-02-889_5a2813432fbbd90f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VuRd4Y5PJ5k:_NkqpHdLR9M:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn54@2020_6/2020/08/25/10-38-14-190_d26749f9db18482e.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VuRd4Y5PJ5k:_NkqpHdLR9M:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn6@2020_2/2020/08/25/10-38-13-659_f1ae286b4bcc8229.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VuRd4Y5PJ5k":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn1@2020_2/2020/08/25/10-38-17-195_69b9978aa326ee43.webp"},"publishedOrCreatedDate":1598351859730},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"OpenAI Teaches Robot Hand to Solve Rubik's Cube","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","description":"<p>In-hand manipulation is a skill that, as far as I’m aware, humans in general don’t actively learn. We just sort of figure it out by doing other, more specific tasks with our fingers and hands. This makes it particularly tricky to teach robots to solve&nbsp;<span>in-hand manipulation tasks</span>&nbsp;because the way we do it is through experimentation and trial and error. Robots can learn through trial and error as well, but since it usually ends up being mostly error, it takes a very, very long time.</p> \n<p><a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/openai-demonstrates-complex-manipulation-transfer-from-simulation-to-real-world\">Last June, we wrote about OpenAI’s approach to teaching a five-fingered robot hand to manipulate a cube</a>. The method that OpenAI used leveraged the same kind of experimentation and trial and error, but in <em>simulation</em> rather than on robot hardware. For complex tasks that take a lot of finesse, simulation generally translates poorly into real-world skills, but OpenAI made their system super robust by introducing a whole bunch of randomness into the simulation during the training process. That way, even if the simulation didn’t perfectly match reality (which it didn’t), the system could still handle the kinds of variations that it experienced on the real-world hardware.</p> \n<p>In a preprint <a href=\"https://d4mucfpksywv.cloudfront.net/papers/solving-rubiks-cube.pdf\">paper</a> published online today, OpenAI has managed to teach its robot hand to solve a much more difficult version of in-hand cube manipulation: single-handed solving of a 3x3 Rubik’s cube. The new work is also based on the idea of solving a problem using advanced simulations and then transferring the solution&nbsp;to a real-world system, or what researchers call “sim2real.”&nbsp;In the paper, OpenAI says the new approach “vastly improved sim2real transfer.”</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/jm-ihc7CASY\" width=\"620\"></iframe></p> \n<p>The initial step was to break down the&nbsp;robot manipulation of the Rubik’s cube&nbsp;into two different tasks: 1. rotating a single face of the cube 90 degrees in either direction, and 2. flipping the cube to bring a different face to the top. Since rotating the top face is much simpler for the robot than rotating other faces, the most reliable strategy is to just do a 90-degree flip to get the face you want to rotate on top. The actual process of solving the cube is computationally straightforward, although the solving process is optimized for the motions that the robot can perform rather than the solve that would take the least number of steps.</p> \n<p>The physical setup that’s doing the real-world cube solving is a <a href=\"https://robots.ieee.org/robots/shadow/?utm_source=spectrum\">Shadow Dexterous E Series Hand</a> with a <a href=\"http://phasespace.com/\">PhaseSpace motion capture system</a>, plus RGB cameras for visual pose estimation. The cube that’s being manipulated is also pretty fancy:&nbsp;It’s stuffed with sensors that report the orientation of each face with an accuracy of five degrees, which is necessary because it’s otherwise very difficult to know the state of a Rubik’s cube when some of its faces are occluded.&nbsp;</p> \n<aside class=\"inlay pullquote xlrg\">\n  “While the video makes it easy to focus on the physical robot, the magic is mostly happening in simulation, and transferring things learned in simulation to the real world. The key to this is domain randomization—jittering parts of the simulation around so that your system has to adapt to different situations similar to those that might be encountered in the real-world.” \n</aside> \n<p>While the video makes it easy to focus on the physical robot, the magic is mostly happening in simulation, and transferring things learned in simulation to the real world. Again, the key to this is domain randomization—jittering parts of the simulation around so that your system has to adapt to different situations similar to those that might be encountered in the real-world. For example, maybe you slightly alter the weight of the cube, or change the friction of the fingertips a little bit, or turn down the lighting. If your system can handle these simulated variations, it’ll be more robust to real-world operation.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"OpenAI solves Rubik's cube\" src=\"https://spectrum.ieee.org/image/MzM5MzA2Mg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: OpenAI \n  </figcaption> \n  <figcaption> \n   <p>The physical setup includes&nbsp;the Shadow Dexterous Hand, a PhaseSpace motion capture system, and RGB cameras. OpenAI modified the Shadow Dexterous Hand by moving the PhaseSpace LEDs and cables inside the fingers and by adding rubber to the fingertips.</p> \n  </figcaption> </img> \n</figure> \n<p><a href=\"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/openai-demonstrates-complex-manipulation-transfer-from-simulation-to-real-world\">When we spoke to last year to Jonas Schneider</a> (one of the authors of the cube manipulation work) and asked him where he thought that system was the weakest, he said that the biggest problem at that point was that the randomizations were both task-specific and hand designed. It’s probably not surprising, then, that one of the big contributions of the Rubik’s cube work is “a novel method for automatically generating a distribution over randomized environments for training reinforcement learning policies and vision state estimators,” which the researchers call automatic domain randomization (ADR). Here’s why ADR is important, according to the <a href=\"https://d4mucfpksywv.cloudfront.net/papers/solving-rubiks-cube.pdf\">paper</a>:</p> \n<blockquote> \n <p><em>Our main hypothesis that motivates ADR is that training on a maximally diverse distribution over environments leads to transfer via emergent meta-learning. More concretely, if the model has some form of memory, it can learn to adjust its behavior during deployment to improve performance on the current environment over time, i.e. by implementing a learning algorithm internally. We hypothesize that this happens if the training distribution is so large that the model cannot memorize a special-purpose solution per environment due to its finite capacity. ADR is a first step in this direction of unbounded environmental complexity: it automates and gradually expands the randomization ranges that parameterize a distribution over environments.&nbsp;</em></p> \n</blockquote> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/x4O8pojMF0w\" width=\"620\"></iframe></p> \n<p>Special-purpose solutions per environment are bad, because they work for that environment, but not for other environments. You can think of each little tweak to a simulation as creating a new environment, and the idea behind ADR is to automate these tweaks to create so many new environments that the system is forced to instead come up with general solutions that can work for many different environments all at once. This reflects the robustness required for real-world operation, where no two environments are ever exactly alike. It turns out that ADR is both better and more efficient than the previous manual tuning, say the researchers:</p> \n<blockquote> \n <p><em>ADR clearly leads to improved transfer with much less need for hand-engineered randomizations. We significantly outperformed our previous best results, which were the result of multiple months of iterative manual tuning.</em></p> \n</blockquote> \n<p>In terms of results, the researchers were mostly concerned with how many flips and rotations the system could do in a row without failing, rather than how many complete solves it was capable of. It sounds like a complete solve was a bit of an outlier—the starting configuration of the cube could be solved by the system in 43 successful moves, while the average successful run of the best trained policy (continuously trained over multiple months) was about 27 moves. Sixty percent of the time, the system could get halfway to a complete solve, and it made it the entire way 20 percent of the time.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"OpenAI dexterous robot hand\" src=\"https://spectrum.ieee.org/image/MzM5MzE0NQ.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: OpenAI \n  </figcaption> \n  <figcaption>\n    To make sure that the system is robust, the researchers messed with it in all kinds of fun ways. They&nbsp;used a rubber glove to change the friction and surface geometry of the hand, tied multiple fingers together, and placed a blanket to occlude the hand and Rubik’s cube during execution. They also used a pen and a plush giraffe (aptly named Rubik) to poke the cube. \n  </figcaption> </img> \n</figure> \n<p>The researchers point out that the method they’ve developed here is general purpose, and you can train a real-world robot to do pretty much any task that you can adequately simulate. You don’t need any real-world training at all, as long as your simulations are diverse enough, which is where the automatic domain randomization comes in. The long-term goal is to reduce the task specialization that’s inherent to most robots, which will help them be more useful and adaptable in real-world applications.</p> \n<hr> \n <p>Lastly, just for reference, here’s what (I think) is the current 3x3 cube world record is, set just a few days ago by Max Park:</p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/LvFtz4QAP5g?rel=0\" width=\"620\"></iframe></p> \n <p>Wow.</p> \n <p>It’s interesting that it appears to be faster and/or more efficient for a human to use table contact to augment their own dexterity. We’ve seen other robots make use of environmental contact for manipulation; it would be cool if OpenAI threw a surface into the simulation to see if their system could make use of it.</p> \n <p>[ <a href=\"https://openai.com/blog/solving-rubiks-cube/\">OpenAI</a> ]</p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=b6oUZIfmp5k:VMbz5gRzLpY:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=b6oUZIfmp5k:VMbz5gRzLpY:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=b6oUZIfmp5k:VMbz5gRzLpY:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=b6oUZIfmp5k:VMbz5gRzLpY:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=b6oUZIfmp5k:VMbz5gRzLpY:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=b6oUZIfmp5k:VMbz5gRzLpY:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/b6oUZIfmp5k\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Tue, 15 Oct 2019 16:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5MzA3Mw.jpeg","linkMd5":"bac274849de61b732281df992d98ec37","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn32@2020_6/2020/08/25/10-37-59-478_5b3935cc3f8b22c9.webp","destWidth":2048,"destHeight":1536,"sourceBytes":502810,"destBytes":245590,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5MzA2Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn65@2020_1/2020/08/25/10-38-11-583_bf12bfb64f637cbd.webp","https://spectrum.ieee.org/image/MzM5MzE0NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn94@2020_5/2020/08/25/10-38-13-574_721882c02859602f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=b6oUZIfmp5k:VMbz5gRzLpY:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn47@2020_2/2020/08/25/10-38-06-505_0f6b887fcbb6b0f5.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=b6oUZIfmp5k:VMbz5gRzLpY:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn21@2020_3/2020/08/25/10-38-12-181_ee81055b7f6fb6cb.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/b6oUZIfmp5k":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn41@2020_3/2020/08/25/10-38-15-651_35b46b8111f568e2.webp"},"publishedOrCreatedDate":1598351859728},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"SnotBot Drone Swoops Over Blowholes to Track Whale Health","link":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","description":"<style type=\"text/css\">.carousel-inner{ height:560px !important; } </style> \n<div id=\"SS221474766\" class=\"carousel slide\"> \n <div class=\"carousel-inner\"> \n  <div class=\"carousel-item active\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNjk5Mg.jpeg\" data-original=\"/image/MzQxNjk5Mg.jpeg\" id=\"SS221474766_0\" alt=\"null\"> <span class=\"item-num\">1/4</span> \n    <div class=\"carousel-caption\"> \n     <p>The SnotBot drone passes over a blue whale at the moment of exhalation. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA0OA.jpeg\" data-original=\"/image/MzQxNzA0OA.jpeg\" id=\"SS221474766_1\" alt=\"null\"> <span class=\"item-num\">2/4</span> \n    <div class=\"carousel-caption\"> \n     <p>A humpback whale rolls as a drone approaches. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA1MQ.jpeg\" data-original=\"/image/MzQxNzA1MQ.jpeg\" id=\"SS221474766_2\" alt=\"null\"> <span class=\"item-num\">3/4</span> \n    <div class=\"carousel-caption\"> \n     <p>SnotBot passes over a surfacing humpback whale off the coast of Gabon, in Africa. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA0NQ.jpeg\" data-original=\"/image/MzQxNzA0NQ.jpeg\" id=\"SS221474766_3\" alt=\"null\"> <span class=\"item-num\">4/4</span> \n    <div class=\"carousel-caption\"> \n     <p>The drone approaches a blue whale mother and calf in the Gulf of California. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n </div> \n <a class=\"left carousel-control-prev\" href=\"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS221474766\" data-slide=\"prev\"><span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span><span class=\"sr-only\">Previous</span></a> \n <a class=\"right carousel-control-next\" href=\"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS221474766\" data-slide=\"next\"><span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span><span class=\"sr-only\">Next</span></a> \n <ol class=\"carousel-indicators\"> \n  <li data-target=\"#SS221474766\" data-slide-to=\"0\" class=\"item active\"></li> \n  <li data-target=\"#SS221474766\" data-slide-to=\"1\" class=\"item\"></li> \n  <li data-target=\"#SS221474766\" data-slide-to=\"2\" class=\"item\"></li> \n  <li data-target=\"#SS221474766\" data-slide-to=\"3\" class=\"item\"></li> \n </ol> \n</div> \n<script>$(document).ready(function(){$('#SS221474766').carousel({pause: true,interval: false});});</script> \n<p><strong>It’s a beautiful morning</strong> on the waters of Alaska’s Peril Strait—clear, calm, silent, and just a little cool. A small but seaworthy research vessel glides through gentle swells. Suddenly, in the distance, a humpback whale the size of a school bus explodes out of the water. Enormous bursts of air and water jet out of its blowholes like a fire hose, the noise echoing between the banks.</p> \n<p>“Blow at eleven o’clock!” cries the lookout, and the small boat swarms with activity. A crew member wearing a helmet and cut-proof gloves raises a large quadcopter drone over his head, as if offering it to the sun, which glints off the half dozen plastic petri dishes velcroed to the drone.</p> \n<p>Further back in the boat, the drone pilot calls, “Starting engines in 3, 2, 1! Takeoff in 3, 2, 1!” The drone’s engines buzz as it zooms 20 meters into the air and then darts off toward where the whale just dipped below the water’s surface. With luck, the whale will spout again nearby, and the drone will be there when it does.</p> \n<p>The drone is a modified <a href=\"https://www.dji.com/inspire-2\">DJI Inspire&nbsp;2</a>. About the size of a toaster oven, it’s generally sold to photographers, cinematographers, and well-heeled hobbyists, but this particular drone is on a serious mission: to monitor the health of whales, the ocean, and by extension, the planet. The petri dishes it carries collect the exhaled breath condensate of a whale—a.k.a. snot—which holds valuable information about the creature’s health, diet, and other qualities. Hence the drone’s name: the Parley SnotBot.</p> \n<p>The flyer comes standard with a forward-facing camera for navigation, collision-avoidance detectors, ultrasonic and barometric sensors to track altitude, and a GPS locator. With the addition of a high-definition video camera on a stabilized gimbal that can be directed independently, it can stream 1080p video live while simultaneously storing the video on a microSD card as well as high-resolution images on a 1-terabyte solid-state drive. Given that both cameras run during the entire 26&nbsp;minutes of a typical flight, that’s a lot of data. More on what we are doing with that data later, but first, a bit of SnotBot history.</p> \n<div id=\"SS656985622\" class=\"carousel slide\"> \n <div class=\"carousel-inner\"> \n  <div class=\"carousel-item active\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA4Nw.jpeg\" data-original=\"/image/MzQxNzA4Nw.jpeg\" id=\"SS656985622_0\" alt=\"null\"> <span class=\"item-num\">1/4</span> \n    <div class=\"carousel-caption\"> \n     <p>Petri dishes aboard SnotBot collect whale exhalate for later analysis. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA5MA.jpeg\" data-original=\"/image/MzQxNzA5MA.jpeg\" id=\"SS656985622_1\" alt=\"null\"> <span class=\"item-num\">2/4</span> \n    <div class=\"carousel-caption\"> \n     <p>Intel Labs Research Scientist Bryn Keller, holding the drone, and Iain Kerr, the chief executive officer of Ocean Alliance, retrieve petri dishes that were sprayed with whale exhalation. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA5Mw.jpeg\" data-original=\"/image/MzQxNzA5Mw.jpeg\" id=\"SS656985622_2\" alt=\"null\"> <span class=\"item-num\">3/4</span> \n    <div class=\"carousel-caption\"> \n     <p>Bryn Keller checks SnotBot before a launch. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzA5Ng.jpeg\" data-original=\"/image/MzQxNzA5Ng.jpeg\" id=\"SS656985622_3\" alt=\"null\"> <span class=\"item-num\">4/4</span> \n    <div class=\"carousel-caption\"> \n     <p>Ian Kerr, sitting under the awning, and other researchers send SnotBot on a hunt for whales in the Gulf of California. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n </div> \n <a class=\"left carousel-control-prev\" href=\"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS656985622\" data-slide=\"prev\"><span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span><span class=\"sr-only\">Previous</span></a> \n <a class=\"right carousel-control-next\" href=\"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS656985622\" data-slide=\"next\"><span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span><span class=\"sr-only\">Next</span></a> \n <ol class=\"carousel-indicators\"> \n  <li data-target=\"#SS656985622\" data-slide-to=\"0\" class=\"item active\"></li> \n  <li data-target=\"#SS656985622\" data-slide-to=\"1\" class=\"item\"></li> \n  <li data-target=\"#SS656985622\" data-slide-to=\"2\" class=\"item\"></li> \n  <li data-target=\"#SS656985622\" data-slide-to=\"3\" class=\"item\"></li> \n </ol> \n</div> \n<script>$(document).ready(function(){$('#SS656985622').carousel({pause: true,interval: false});});</script> \n<p></p> \n<p><strong>Iain Kerr</strong> was one of the early pioneers in using drones as a platform to collect and analyze whale exhalation. He’s the CEO of <a href=\"https://whale.org/\">Ocean Alliance</a>, in Gloucester, Mass., a group dedicated to protecting whales and the world’s oceans. Whale biologists know that whale snot contains an enormous amount of biological information, including DNA, hormones, and microorganisms. Scientists can use that information to determine a whale’s health, sex, and pregnancy status, and details about its genetics and microbiome. The traditional and most often used technique for collecting that kind of information is to zoom past a surfacing whale in a boat and shoot it with a specially designed crossbow to capture a small core sample of skin and blubber. The process is stressful for both researchers and whales.</p> \n<p>Researchers had demonstrated that whale snot can be a viable replacement for blubber samples, but collection involved reaching out over whales using long, awkward poles—difficult, to say the least. The development of small but powerful commercial drones inspired Kerr to launch an exploratory research project in 2015 to go after whale snot with drones. He received the first U.S. National Oceanic and Atmospheric Administration (<a href=\"https://www.noaa.gov/\">NOAA</a>) research permit for collecting whale snot in U.S. waters. Since then, there have been dozens of SnotBot missions around the world, in the waters off Alaska, Gabon, Mexico, and other places where whales like to congregate, and the idea has spread to other teams around the globe.</p> \n<p>The SnotBot design continues to evolve. The earliest versions tried to capture snot by trailing gauzy cloth below the drone. The hanging cloth turned out to be difficult to work with, however, and the material itself interfered with some of the lab tests, so the researchers scrapped that method. The developers didn’t consider using petri dishes at first, because they assumed that if the drone flew directly into a whale’s spout, the rotor wash would interfere with collection. Eventually, though, they tried the petri dishes and were happy to discover that the rotors’ downdraft improved rather than hindered collection.</p> \n<p>For each mission, the collection goals have been slightly different, and the team tweaks the design of the craft accordingly. On one mission, the focus might be to survey an area, getting samples from as many whales as possible. The next mission might be a “focal follow,” in which the team tracks one whale over a period of hours or days, taking multiple samples so that they can understand things like how a whale’s hormone levels change throughout the day, either from natural processes or as a response to environmental factors.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/jlHYVmf1AKI\" width=\"620\"></iframe></p> \n<p></p> \n<p><strong>Collecting and analyzing</strong> snot is certainly an important way to assess whale health, but the SnotBot team suspected that the drone could do more. In early 2017, staffers from <a href=\"https://www.parley.tv/\">Parley for the Oceans</a>, a nonprofit environmental group that was working with Ocean Alliance on the SnotBot project, contacted one of us (Willke) to find out just how much more.</p> \n<p>Willke is a machine-learning and artificial-intelligence researcher who leads <a href=\"https://www.intel.com\">Intel</a>’s Brain-Inspired Computing Lab, in Hillsboro, Ore. He immediately saw ways of expanding the information gathered by SnotBot. Willke enlisted two researchers in his lab—coauthor Keller and Javier&nbsp;Turek—and the three of us got to work on enhancing SnotBot’s mission.</p> \n<p>The quadcopters used in the SnotBot project carry high-quality cameras with advanced auto-stabilization features. The drone pilot relies on the high-definition video being streamed back to the boat to fly the aircraft and collect the snot. We knew that these same video streams could simultaneously feed into a computer on the boat and be processed in real time. Could that information help assess whale health?</p> \n<p>Working with Ocean Alliance scientists, we first came up with a tool that analyzes a photo of a whale’s tail flukes and, using a database of whale photographs collected by the <a href=\"https://www.alaskawhalefoundation.org/\">Alaska Whale Foundation</a>, identifies individual whales by the shape of the fluke and its black and white patterns. Identifying each whale allows researchers to correlate snot samples over time.</p> \n<p>Such identification can also help whale biologists cope with tricky regulatory issues. For example, there are at least two breeding populations of humpback whales that migrate to Alaska. Most come from Hawaii, but a smaller group comes from Mexico. The Mexican population is under greater stress at the moment, and so NOAA requests that researchers focus on the healthier, Hawaiian whales and leave the Mexican whales alone as much as possible. However, both populations are exactly the same species and thus indistinguishable from each other as a group. The ability to recognize individual whales allows researchers to determine whether a whale had been previously spotted in Mexico or Hawaii, so that they can act appropriately to comply with the regulation.</p> \n<p>We also developed software that analyzes the shape of a whale from an overhead shot, taken about 25 meters directly above the whale. Since a skinny whale is often a sick one or one that hasn’t been getting enough to eat, even that simple metric can be a powerful indicator of well-being.</p> \n<p>The biggest challenge in developing these tools was what’s called data starvation—there just wasn’t enough data. A standard deep-learning algorithm would look at a huge set of images and then figure out and extract the key distinguishing features of a whale. In the case of the fluke-ID tool, there were only a few pictures of each whale in the catalog, and these were often too low quality to be useful. For overhead health monitoring, there were likewise too few photos or videos of whales shot with the right camera, from the right angle, under the right conditions.</p> \n<p>To address these problems, our team turned to classic computer-vision techniques to extract what we considered the most useful data. For example, we used edge-detection algorithms to find and measure the trailing edge of a fluke, then obtained the grayscale values of all the pixels in a line extending from the center notch of the fluke to the outer tips. We trained a small but effective neural network on this data alone. If more data had been available, a deep-learning approach would have worked better than our approach did, but we had to work with the limited data we had.</p> \n<div id=\"SS810777737\" class=\"carousel slide\"> \n <div class=\"carousel-inner\"> \n  <div class=\"carousel-item active\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzM4Nw.jpeg\" data-original=\"/image/MzQxNzM4Nw.jpeg\" id=\"SS810777737_0\" alt=\"null\"> <span class=\"item-num\">1/4</span> \n    <div class=\"carousel-caption\"> \n     <p>The latest model of SnotBot flies into action, with custom mounting points for petri dishes and its new paint scheme, designed to camouflage it against a cloud-studded sky. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzM4NA.jpeg\" data-original=\"/image/MzQxNzM4NA.jpeg\" id=\"SS810777737_1\" alt=\"null\"> <span class=\"item-num\">2/4</span> \n    <div class=\"carousel-caption\"> \n     <p>SnotBot passes through whale exhalation. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzM4MQ.jpeg\" data-original=\"/image/MzQxNzM4MQ.jpeg\" id=\"SS810777737_2\" alt=\"null\"> <span class=\"item-num\">3/4</span> \n    <div class=\"carousel-caption\"> \n     <p>The shape of a whale's flukes can help software recognize individual whales. <em>Photo:&nbsp;Christian Miller/Ocean Alliance</em></p> \n    </div> </img> \n  </div> \n  <div class=\"carousel-item\"> \n   <img src=\"https://spectrum.ieee.org/image/MzQxNzM3OA.jpeg\" data-original=\"/image/MzQxNzM3OA.jpeg\" id=\"SS810777737_3\" alt=\"null\"> <span class=\"item-num\">4/4</span> \n    <div class=\"carousel-caption\"> \n     <p>A humpback whale dives below the surface of the ocean. <em>Photo:&nbsp;Kate Westaway/Getty Images</em></p> \n    </div> </img> \n  </div> \n </div> \n <a class=\"left carousel-control-prev\" href=\"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS810777737\" data-slide=\"prev\"><span class=\"carousel-control-prev-icon\" aria-hidden=\"true\"></span><span class=\"sr-only\">Previous</span></a> \n <a class=\"right carousel-control-next\" href=\"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS810777737\" data-slide=\"next\"><span class=\"carousel-control-next-icon\" aria-hidden=\"true\"></span><span class=\"sr-only\">Next</span></a> \n <ol class=\"carousel-indicators\"> \n  <li data-target=\"#SS810777737\" data-slide-to=\"0\" class=\"item active\"></li> \n  <li data-target=\"#SS810777737\" data-slide-to=\"1\" class=\"item\"></li> \n  <li data-target=\"#SS810777737\" data-slide-to=\"2\" class=\"item\"></li> \n  <li data-target=\"#SS810777737\" data-slide-to=\"3\" class=\"item\"></li> \n </ol> \n</div> \n<script>$(document).ready(function(){$('#SS810777737').carousel({pause: true,interval: false});});</script> \n<p></p> \n<p><strong>New discoveries</strong> in whale biology have already come from our tools. Besides the ability to distinguish between the Mexican and Hawaiian whale populations, researchers have discovered they can identify whales from their calls, even when the calls were recorded many years previously.</p> \n<p>That latter discovery came during the summer of 2017, when we joined Fred Sharpe, an Alaska Whale Foundation researcher and founding board member, to study teams of whales that worked together to feed. While observing a small group of humpback whales, the boat’s underwater microphone picked up a whale feeding call. Sharpe thought it sounded familiar, and so he consulted his database of whale vocalizations. He found a similar call from a whale called Trumpeter that he had recorded some 20 years ago. But was it really the same whale? There was no way to know for sure from the whale call.</p> \n<p>Then a whale surfaced briefly and dove again, letting us capture an image of its flukes. Our software found a match: The&nbsp;flukes indeed belonged to Trumpeter. That told the researchers that adult whale feeding calls likely remain stable for decades, maybe even for life. This insight gave researchers another tool for identifying whales in the wild and improving our understanding of vocal signatures in humpback whales.</p> \n<p>Meanwhile, whale-ID tools are getting better all the time. The original SnotBot algorithm that we developed for whale identification has been essentially supplanted by more capable services. One new algorithm relies on the curvature of the trailing edge of the fluke for identification.</p> \n<p>SnotBot’s real contribution, it turns out, is in health monitoring. Our shape-analysis tool has been evolving and, in&nbsp;combination with the spray samples, is&nbsp;giving researchers a comprehensive picture of an individual whale’s health. We call this tool Morphometer. We recently teamed up with Kelly Cates, a Ph.D. candidate in marine biology at the <a href=\"https://www.uaf.edu/\">University of Alaska Fairbanks</a>, and Fredrik Christiansen, an assistant professor and whale expert at the Aarhus Institute of Advanced Studies, in Denmark, to make the technology more powerful and also easier to use.</p> \n<p>Here’s how it works. Researchers who make measurements and assessments of baleen whales—the type of whales that filter-feed—have typically used a technique developed by Christiansen in 2016. (So far the effort has involved humpback and southern right whales, but the process could work for any kind of baleen&nbsp;whale.) The researchers start with photographic prints or images on a computer and hand-measure the body widths of whales in the images at intervals of 5 percent of the overall length from the snout to the notch of the tail flukes. They then feed this set of measurements to software that calculates an estimate of the whale’s volume. From the relationship between the body length and volume, they can determine if an individual whale is relatively fatter or thinner compared with population norms, taking into account the significant but normal changes in girth that occur as whales accumulate energy reserves during the feeding season and then use those energy stores for migration and during the breeding season.</p> \n<p>Morphometer also uses photos, but it measures the whale’s width continuously at the highest resolution possible given the quality of the photo, yielding hundreds of width measurements for each animal, instead of only the small number of measurements that are feasible for human researchers. The result is thus much more accurate. It also processes the data much faster than a human could, allowing biologists to focus on biology rather than doing tedious measurements by hand.</p> \n<p>To improve Morphometer, we trained a deep-learning system on images of humpback and southern right whales in all sorts of different weather, water, and lighting conditions to allow it to understand exactly which pixels in an image belong to a whale. Once a whale has been singled out, the system identifies the head and tail and then measures the whale’s length and width at each pixel point along the outline of its body. Our software tracks the altitude from which the drone photographed the whale and combines that data with camera specifications entered by the drone operator, allowing the system to automatically convert the measurements from pixels to meters.</p> \n<p>Morphometer compares this whale with others of its body type, displaying the result as an image of the subject whale superimposed on a whale-shape color-coded diagram with zones indicating the average measurements of similar whales. It’s immediately obvious if the whale is normal size, underweight, or larger than average, as would be the case with pregnant females [see illustration, “Measuring Up”].</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"img\" src=\"https://spectrum.ieee.org/image/MzQxNzQyNA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Images:&nbsp;Intel \n  </figcaption> \n  <figcaption> \n   <strong>Measuring up:</strong> The Morphometer image-analysis tool [below left] compares one whale with others of its body type, displaying the result as an image of the subject whale superimposed on a diagram representing an average whale. The diagram is color coded—red at the center, blending to white at the size of an average whale and to blue for larger whales. With the individual whale superimposed on this color map, any red that shows around the whale means the whale is underweight; the more underweight, the darker the red. Above, fluke-recognition software makes a match. \n  </figcaption> </img> \n</figure> \n<figure class=\"lt med\" role=\"img\"> \n <img alt=\"img\" src=\"https://spectrum.ieee.org/image/MzQxNzQzNQ.jpeg\"> </img> \n</figure> \n<p>For our early prototype, we input parameters for a “normal” body shape based on age, sex, and other factors. But now Morphometer is in the process of figuring out “normal” for itself by processing large numbers of whale images. Whale researchers who use their own drones to collect whale photos have been sending us their images. Eventually, we envision setting up a collaborative website that would allow images and morphometry models to be shared among researchers. We also plan to adapt Morphometer to analyze videos of whales, automatically extracting the frames or clips in which the whale’s position and visibility are the best.</p> \n<p>To help researchers gain a more complete picture, we’re building statistical models of various whale populations, which we will compare to models derived from human-estimated measurements. Then we’ll take new photos of whales whose age and gender are known, and see whether the software correctly classifies them and gives appropriate indications of health; we’ll have whale biologists verify the results.</p> \n<p>Once this model is working reliably, we expect to be able to say how a given whale’s size compares with those of its peers of the same gender, in the same region, at the same time of year. We’ll also be able to identify historical trends—for example, this whale is not skinnier than average compared with last year, but it is much skinnier than whales in its class a few decades ago, assuming comparison data exists. If, in addition, we have snot from the same whale, we can create a more complete profile of the whale, in the same way your credit card company can tell a lot about you by integrating your personal data with the averages and variances in the general population.</p> \n<p><strong>So far,</strong> SnotBot has told us a lot about the health of individual whales. Soon, researchers will start using this data to monitor the health of oceans. Whales are known as “apex predators,” meaning they are at the top of the food chain. Humpback whales in particular are generalist foragers and have wide-ranging migration patterns, which make them an excellent early-warning system for environmental threats to the ocean as a whole.</p> \n<p>This is where SnotBot can really make a difference. We all depend on the oceans for our survival. Besides the vast amount of food they produce, we depend on them for the air we breathe: Most of the oxygen in the atmosphere comes from marine organisms such as phytoplankton and&nbsp;algae.</p> \n<p>Lately, ocean productivity associated with a North Pacific warm-water anomaly, or “blob,” has resulted in a reduction of births and more reports of skinny whales, and that should worry us. If conditions are bad for whales, they’re also bad for humans. Thanks to Project SnotBot, we’ll be able to find out—accurately, efficiently, and at a reasonable cost—just how the health and numbers of whales in our oceans are trending. With that information, we hope, we will be able to spur society to take steps to protect the oceans before it’s too late.</p> \n<p><em>The whale images in this article were obtained under National Marine Fisheries Service permits 18636-01 and 19703.</em></p> \n<p><em>This article appears in the December 2019 print issue as “SnotBot:&nbsp;A Whale of a Deep-Learning Project.”</em></p> \n<h2>About the Authors</h2> \n<p><a href=\"https://www.intel.ai/bio/bryn-keller/\">Bryn Keller</a> is a deep-learning research scientist in Intel’s Brain-Inspired Computing Lab. Senior principal engineer <a href=\"https://www.intel.ai/bio/ted-willke/\">Ted Willke</a> is the lab’s director.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Tm3JI8_WGfM:-EJlx19W6kU:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Tm3JI8_WGfM:-EJlx19W6kU:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Tm3JI8_WGfM:-EJlx19W6kU:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Tm3JI8_WGfM:-EJlx19W6kU:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=Tm3JI8_WGfM:-EJlx19W6kU:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Tm3JI8_WGfM:-EJlx19W6kU:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Tm3JI8_WGfM\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 25 Nov 2019 17:04:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQxNjk2NA.jpeg","linkMd5":"a82857d2f8b930185848e80cae923b81","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn34@2020_6/2020/08/25/10-38-00-309_ee6c587ae8395183.webp","destWidth":1240,"destHeight":930,"sourceBytes":714662,"destBytes":89844,"author":"Bryn Keller","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQxNjk5Mg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn47@2020_4/2020/08/25/10-38-04-984_d3eb14c5308181e4.webp","https://spectrum.ieee.org/image/MzQxNzA0OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn18@2020_2/2020/08/25/10-38-15-745_3813f43f9c054e34.webp","https://spectrum.ieee.org/image/MzQxNzA1MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn82@2020_3/2020/08/25/10-38-01-865_b80bcb8b6379003b.webp","https://spectrum.ieee.org/image/MzQxNzA0NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn99@2020_1/2020/08/25/10-38-08-476_05389d29e1abfbb1.webp","https://spectrum.ieee.org/image/MzQxNzA4Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn99@2020_1/2020/08/25/10-38-13-069_6fc7af4aa7b5da0a.webp","https://spectrum.ieee.org/image/MzQxNzA5MA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn62@2020_5/2020/08/25/10-38-14-357_0141dc805ce2dac2.webp","https://spectrum.ieee.org/image/MzQxNzA5Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn59@2020_2/2020/08/25/10-38-08-862_a5011ea0d246d747.webp","https://spectrum.ieee.org/image/MzQxNzA5Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn62@2020_3/2020/08/25/10-38-15-283_841e1fd89c9098cc.webp","https://spectrum.ieee.org/image/MzQxNzM4Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn70@2020_1/2020/08/25/10-38-11-788_ee701dd838a1d4f5.webp","https://spectrum.ieee.org/image/MzQxNzM4NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn87@2020_6/2020/08/25/10-38-04-590_3e24f562cb50673a.webp","https://spectrum.ieee.org/image/MzQxNzM4MQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn85@2020_6/2020/08/25/10-38-02-069_c3c3f91fdfb4cdb3.webp","https://spectrum.ieee.org/image/MzQxNzM3OA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn60@2020_6/2020/08/25/10-38-08-281_9642811ce33ff685.webp","https://spectrum.ieee.org/image/MzQxNzQyNA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn34@2020_3/2020/08/25/10-38-14-028_1e90193745150884.webp","https://spectrum.ieee.org/image/MzQxNzQzNQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn46@2020_2/2020/08/25/10-38-17-627_6201d026e0e5c0c8.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Tm3JI8_WGfM:-EJlx19W6kU:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx/cdn14@2020_5/2020/08/25/10-38-18-212_99d3cefec5ff4b66.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Tm3JI8_WGfM:-EJlx19W6kU:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn10@2020_5/2020/08/25/10-38-17-523_cb8a17ba0d7c1117.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Tm3JI8_WGfM":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn69@2020_4/2020/08/25/10-38-15-171_740245bb8bb42ead.webp"},"publishedOrCreatedDate":1598351859715},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"In the 17th Century, Leibniz Dreamed of a Machine That Could Calculate Ideas","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","description":"<p><em>This is part two of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>.</em></p> \n<p></p> \n<p></p> \n<p>In 1666, the German polymath <a href=\"https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz\">Gottfried Wilhelm Leibniz</a> published an enigmatic dissertation entitled <em><a href=\"https://www.math.ucla.edu/~pak/hidden/papers/Quotes/Leibniz-Arte-Combinatoria.pdf\">On the Combinatorial Art</a></em>. Only 20 years old but already an ambitious thinker, Leibniz outlined a theory for automating knowledge production via the rule-based combination of symbols.</p> \n<p></p> \n<p>Leibniz’s central argument was that all human thoughts, no matter how complex, are combinations of basic and fundamental concepts, in much the same way that sentences are combinations of words, and words combinations of letters. He believed that if he could find a way to symbolically represent these fundamental concepts and develop a method by which to combine them logically, then he would be able to generate new thoughts on demand.</p> \n<p></p> \n<p>The idea came to Leibniz through his study of <a href=\"https://monoskop.org/Ramon_Llull\">Ramon Llull</a>, a 13th century Majorcan mystic who devoted himself to devising a system of theological reasoning that would prove the “universal truth\" of Christianity to non-believers.</p> \n<p></p> \n<p>Llull himself was inspired by Jewish Kabbalists’ letter combinatorics (see <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics\">part one of this series</a>), which they used to produce generative texts that supposedly revealed prophetic wisdom. Taking the idea a step further, Llull&nbsp;invented what he called a volvelle, a <a href=\"https://www.netzliteratur.net/cramer/wordsmadefleshpdf.pdf\">circular paper mechanism</a> with increasingly small concentric circles on which were written symbols representing the attributes of God. Llull believed that by spinning the volvelle in various ways, bringing the symbols into novel combinations with one another, he could reveal all the aspects of his deity.</p> \n<p></p> \n<p>Leibniz was much impressed by Llull’s paper machine, and he embarked on a project to create his own method of idea generation through symbolic combination. He wanted to use his machine not for theological debate, but for philosophical reasoning.&nbsp;He proposed that such a system would require three things: an “alphabet of human thoughts”; a list of logical rules for their valid combination and re-combination;&nbsp;and a mechanism that could carry out the logical operations on the symbols quickly and accurately—a fully mechanized update of Llull’s paper volvelle.&nbsp;</p> \n<p></p> \n<p>He imagined that this machine, which he called “the great instrument of reason,” would be able to answer all questions and resolve all intellectual debate. “When there are disputes among persons,” <a href=\"http://scihi.org/universal-academic-gottfried-wilhelm-leibniz/\">he wrote</a>, “we can simply say, ‘Let us calculate,’ and without further ado, see who is right.”</p> \n<p></p> \n<!--nextpage--> \n<aside class=\"inlay pullquote rt med\">\n  “When there are disputes among persons, we can simply say, ‘Let us calculate,’ and without further ado, see who is right.” \n <span class=\"pq-attrib\">—Gottfried Wilhelm Leibniz, polymath</span> \n</aside> \n<p>The notion of a mechanism that produced rational thought encapsulated the spirit of Leibniz’s times. Other Enlightenment thinkers, such as <a href=\"https://en.wikipedia.org/wiki/René_Descartes\">René Descartes</a>, believed that there was a “<a href=\"https://plato.stanford.edu/entries/enlightenment/#TruSciEpiMetEnl\">universal truth</a>” that could be accessed through reason alone, and that all phenomena were fully explainable if the underlying principles were understood. The same, Leibniz thought, was true of language and cognition itself.</p> \n<p></p> \n<p>But many others saw this doctrine of pure reason as deeply flawed, and felt that it signified a new age sophistry professed from on high. One such critic was the author and satirist <a href=\"https://en.wikipedia.org/wiki/Jonathan_Swift\">Jonathan Swift</a>, who took aim at Leibniz’s thought-calculating machine in his 1726 book, <em><a href=\"http://www.gutenberg.org/files/17157/17157-h/17157-h.htm\">Gulliver’s Travels</a></em>. In one scene, Gulliver visits the Grand Academy of Lagado where he encounters a strange mechanism <a href=\"https://www.computerhistory.org/atchm/gullivers-engine/\">called</a> “the engine.” The machine has a large wooden frame with a grid of wires; on the wires are small wooden cubes with symbols written on each side.</p> \n<p></p> \n<p>The students of the Grand Academy of Lagado crank handles on the side of the machine causing the wooden cubes to rotate and spin, bringing the symbols into new combinations. A scribe then writes down the output of the machine, and hands it to the presiding professor. Through this process, the professor claims, he and his students can “write books in philosophy, poetry, politics, laws, mathematics, and theology, without the least assistance from genius or study.”</p> \n<p></p> \n<aside class=\"inlay pullquote lt med\">\n  Swift’s point was that language is not a formal system that represents human thought, but a messy and ambiguous form of expression. \n</aside> \n<p>This scene, with its pre-digital language generation, was Swift’s parody of Leibniz’s thought generation through symbolic combinatorics—and more broadly, an argument against the primacy of science. As with the Lagado academy’s other attempts at contributing to its nation’s development through research—such as trying to change human excretion back into food—Gulliver sees the engine as a pointless experiment.</p> \n<p></p> \n<p>Swift’s point was that language is not a formal system that represents human thought, as Leibniz proposed, but a messy and ambiguous form of expression that makes sense only in relation to the context in which it is used. To have a machine generate language requires more than having the right set of rules and the right machine, Swift argued—it requires the ability to understand the meaning of words, something that neither the Lagado engine nor Leibniz’s “instrument of reason” could do.</p> \n<p></p> \n<p>In the end, Leibniz never constructed his idea-generating machine. In fact, he abandoned the study of Llull’s combinatorics altogether, and, later in life came to see the pursuit of mechanizing language as immature. But the idea of using mechanical devices to perform logical functions remained with him, inspiring the construction of his ‘<a href=\"http://www.gwleibniz.com/calculator/calculator.html\">step reckoner</a>,’ a mechanical calculator built in 1673.</p> \n<p></p> \n<p>But as today’s data scientists devise ever-better algorithms for natural language processing, they’re having debates that echo the ideas of Leibniz and Swift: Even if you can create a formal system to generate human-seeming language, can you give it the ability to understand what it’s saying?</p> \n<p></p> \n<p><em>This is the second installment of a six-part series on the <a href=\"https://spectrum.ieee.org/tag/history+of+natural+language+processing\">history of natural language processing</a>. Last week’s post started the story with a<a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics\"> Kabbalist mystic in medieval Spain</a>. Come back next Monday for part three, which describes the language models that were painstakingly built by <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models\">Andrey Markov and Claude Shannon</a>.</em></p> \n<p></p> \n<p><em>You can also check out our prior series on the <a href=\"https://spectrum.ieee.org/tag/AI+history\">untold history of AI</a>.</em></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cfiHRefDfBg:4Zg3ikJLBoQ:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cfiHRefDfBg:4Zg3ikJLBoQ:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cfiHRefDfBg:4Zg3ikJLBoQ:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cfiHRefDfBg:4Zg3ikJLBoQ:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=cfiHRefDfBg:4Zg3ikJLBoQ:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cfiHRefDfBg:4Zg3ikJLBoQ:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/cfiHRefDfBg\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Mon, 04 Nov 2019 14:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwMTgxOA.jpeg","linkMd5":"7c7f2266a8977ac619b5378792fcd8f7","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn90@2020_5/2020/08/25/10-38-01-197_e6826056637aa7a5.webp","destWidth":1240,"destHeight":930,"sourceBytes":304914,"destBytes":186948,"author":"Oscar Schwartz","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cfiHRefDfBg:4Zg3ikJLBoQ:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn92@2020_3/2020/08/25/10-38-05-886_96e2924ba7492053.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cfiHRefDfBg:4Zg3ikJLBoQ:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn6@2020_5/2020/08/25/10-38-11-958_f6e108d778d975de.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/cfiHRefDfBg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn56@2020_3/2020/08/25/10-38-03-978_5fa07270d8f9ca2f.webp"},"publishedOrCreatedDate":1598351859724},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Russian Humanoid Robot to Pilot Soyuz Capsule to ISS This Week","link":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","description":"<p>On Thursday, the Soyuz MS-14 will travel to the <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached\">International Space Station</a> and carry with it a Russian humanoid robot. The robot’s&nbsp;name used to be Fedor, and we last saw it shooting handguns in a demonstration to the Russian military, but now it shall be known as Skybot F-850 and it's going to space.&nbsp;</p> \n<!--nextpage--> \n<p>Skybot F-850 will spend just over one week on the ISS, where it will “perform several tasks under the control of astronaut Alexander Skvortsov and will be able to communicate with the crew,”&nbsp;according to the Russian space agency <a href=\"http://en.roscosmos.ru/\">Roscosmos</a>. There isn't a lot of information about exactly what these tasks will be (beyond the vague “tests prepared by Russian engineers to assist astronauts inside the ISS”), but there is this amazing Russian music video featuring Skybot that was posted a few days ago by Dmitry Rogozin, director general of Roscosmos:</p> \n<p></p> \n<blockquote class=\"twitter-tweet\"> \n <p>Посвящается нашему Фёдору <a href=\"https://twitter.com/FEDOR37516789?ref_src=twsrc%5Etfw\">@FEDOR37516789</a> и его предстоящему полёту<a href=\"https://twitter.com/roscosmos?ref_src=twsrc%5Etfw\">@roscosmos</a> <a href=\"https://twitter.com/tvroscosmos?ref_src=twsrc%5Etfw\">@tvroscosmos</a> <a href=\"https://twitter.com/fpi_russia?ref_src=twsrc%5Etfw\">@fpi_russia</a> <a href=\"https://twitter.com/glavkosmosJSC?ref_src=twsrc%5Etfw\">@glavkosmosJSC</a> <a href=\"https://t.co/N4T8cz73co\">pic.twitter.com/N4T8cz73co</a></p> — Дмитрий Рогозин (@Rogozin) \n <a href=\"https://twitter.com/Rogozin/status/1160990432618323973?ref_src=twsrc%5Etfw\">August 12, 2019</a> \n</blockquote> \n<script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n<p></p> \n<p>It looks like the robot does have some amount of autonomy, but that immersive teleoperation is used for more complex tasks. There's nothing wrong with this approach, and it's the same one that <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/gitai-partners-with-jaxa-to-send-telepresence-robots-to-space\">GITAI has been using for the robotic system that it hopes will travel to the ISS one day</a>.</p> \n<p>Here are a couple of excerpts from a (Google-translated and gently edited) interview with Alexander Bloshenko, science adviser at Roscosmos, that provide a bit more detail about Skybot F-850:</p> \n<blockquote> \n <p><strong>From a technical point of view, is this really a robot, or is it a remote manipulator that looks like a person?</strong></p> \n <p><em>Skybot F-850 is a robot—it's an automatic device with predefined programming and elements of artificial intelligence. It can maintain its equilibrium, convert general movements into separate locomotor functions, and independently provide expert support of the crew. In addition, the robot can work in avatar mode, under the control of a remote operator.</em></p> \n <p><strong>It was reported that the robot will communicate with the astronauts, and comment on the flight status in the Soyuz spacecraft. What does Skybot talk about? Can it make jokes?</strong></p> \n <p><em>Skybot F-850 can communicate on any topic. Before launching the Soyuz, the robot will describe prelaunch preparations, and during the launch and flight he will report the flight parameters and observed events. As the spacecraft enters orbit, the robot will determine the onset of zero gravity.</em></p> \n <p><em>Like a person, Skybot F-850 is very sociable and has a sense of humor. As I said, he can support any topic of conversation, answer a variety of questions: from making introductions, to talking about its creators, and ending with the philosophy of space.</em></p> \n</blockquote> \n<hr> \n <p>There are a few other interesting details in the interview—we've learned that the robot runs off&nbsp;the same batteries as the Russian Orlan space suits, which is a very practical way of getting around the battery safety constraints on board the ISS, and that the next generation will apparently be able to operate in a vacuum.</p> \n <p>Skybot will be the second humanoid robot to visit the ISS after <a href=\"https://spectrum.ieee.org/tag/robonaut\">Robonaut 2</a>, which is <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached\">all fixed up and ready to head back to the station</a> as soon as space is available on a flight. Roscosmos seems to be well aware that people like us will be making this comparison, and the agency notes that Soyuz MS-14 “will be a unique flight, because for the first time the robot will sit in the seat of the ship's commander, and not in the cargo compartment, as&nbsp;[did] the American humanoid Robonaut-2 delivered to the International Space Station several years ago.”</p> \n <p>This is technically true, I guess, although Roscosmos doesn't mention that the <a href=\"http://www.russianspaceweb.com/soyuz-ms-soyuz2-1a.html\">Soyuz MS-14</a>&nbsp;isn't designed to be piloted by a human at all, and in fact doesn't include a number of crew support systems. Really, MS-14 itself is one big cargo compartment, and we're not even sure that it's got seats in it, especially since this is how the robot had to be put into the capsule:</p> \n <figure class=\"ct med-lrg\" role=\"img\"> \n  <img alt=\"Image of the robot.\" src=\"https://spectrum.ieee.org/image/MzM1NzM0Mw.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: FEDOR37516789/Twitter&nbsp; \n   </figcaption> </img> \n </figure> \n <p>And this is what it looks like inside:</p> \n <figure class=\"ct med-lrg\" role=\"img\"> \n  <img alt=\"Image of inside of capsule\" src=\"https://spectrum.ieee.org/image/MzM1NzM0NA.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photo: FEDOR37516789/Twitter&nbsp; \n   </figcaption> </img> \n </figure> \n <p>In any case, we just hope that the result of this trip to orbit by Skybot is some tangible progress in space robotics rather than just a publicity stunt. Roscosmos does have some longer-term plans for Skybot F-850 if things go well, including “a draft of a full-fledged program for further work,”&nbsp;but first Skybot has to prove itself. And if it does, you can bet that Roscosmos will make sure we know about it.</p> \n <p>[ <a href=\"https://www.roscosmos.ru/26638/\">Roscosmos</a> ]</p> \n <p></p> \n <p></p> \n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=TYE4X4BWZag:Yijh8rC9x50:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=TYE4X4BWZag:Yijh8rC9x50:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=TYE4X4BWZag:Yijh8rC9x50:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=TYE4X4BWZag:Yijh8rC9x50:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=TYE4X4BWZag:Yijh8rC9x50:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=TYE4X4BWZag:Yijh8rC9x50:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div> \n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/TYE4X4BWZag\" height=\"1\" width=\"1\" alt=\"\" /> \n</hr>","descriptionType":"html","publishedDate":"Mon, 19 Aug 2019 17:40:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1NzI2Nw.jpeg","linkMd5":"ba866f9c04fa639f4052fdd904ce7f83","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn70@2020_4/2020/08/25/10-37-58-829_a151a33ce4fcda1a.webp","destWidth":1240,"destHeight":930,"sourceBytes":245207,"destBytes":133280,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1NzM0Mw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn25@2020_3/2020/08/25/10-38-17-595_4293987c1c48f544.webp","https://spectrum.ieee.org/image/MzM1NzM0NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn60@2020_4/2020/08/25/10-38-04-047_72b5cab7334e18c2.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=TYE4X4BWZag:Yijh8rC9x50:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn11@2020_4/2020/08/25/10-38-03-690_cecc7ca5eff8b34d.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=TYE4X4BWZag:Yijh8rC9x50:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn54@2020_1/2020/08/25/10-38-11-396_a2c14b8066331e8b.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/TYE4X4BWZag":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn27@2020_6/2020/08/25/10-38-08-558_efd83fd3d257ab3e.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Soft Self-Healing Materials for Robots That Cannot Be Destroyed","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","description":"<p>If there’s one thing we know about robots, it’s that they break. They break, like, literally all the time. The software breaks. The hardware breaks. The bits that you think could never, ever, ever possibly break end up breaking just when you need them not to break the most, and then you have to try to explain what happened to your advisor who’s been standing there watching your robot fail and then stay up all night fixing the thing that seriously was not supposed to break.</p> \n<p>While most of this is just a fundamental characteristic of robots that can’t be helped, the European Commission is funding a project called <a href=\"http://www.sherofet.eu\">SHERO</a> (Self HEaling soft RObotics) to try and solve at least some of those physical robot breaking problems through the use of structural materials that can autonomously heal themselves over and over again.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/3NLYBGM9k7s\" width=\"620\"></iframe></p> \n<p><a href=\"http://www.sherofet.eu\">SHERO</a> is a three year, €3 million collaboration between Vrije Universiteit Brussel, University of Cambridge, École Supérieure de Physique et de Chimie Industrielles de la ville de Paris (ESPCI-Paris), and Swiss Federal Laboratories for Materials Science and Technology (Empa). As the name SHERO suggests, the goal of the project is to develop soft materials that can completely recover from the kinds of damage that robots are likely to suffer in day to day operations, as well as the occasional more extreme accident.&nbsp;</p> \n<p>Most materials, especially soft materials, are fixable somehow, whether it’s with super glue or duct tape. But fixing things involves a human first identifying when they’re broken, and then performing a potentially skill, labor, time, and money intensive task. SHERO’s soft materials will, eventually, make this entire process autonomous, allowing robots to self-identify damage and initiate healing on their own.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Self healing soft robots\" src=\"https://spectrum.ieee.org/image/MzM2ODAxNQ.jpeg\"> <img alt=\"Self healing soft robots\" src=\"https://spectrum.ieee.org/image/MzM2ODAxNg.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photos: SHERO Project \n   </figcaption> \n   <figcaption>\n     The damaged robot finger [top] can operate normally after healing itself.&nbsp; \n   </figcaption> </img></img>\n</figure> \n<h3>How the self-healing material works</h3> \n<p>What these self-healing materials can do is really pretty amazing. The researchers are actually developing two different types—the first one heals itself when there’s an application of heat, either internally or externally, which gives some control over when and how the healing process starts. For example, if the robot is handling stuff that’s dirty, you’d want to get it cleaned up before healing it so that dirt doesn’t become embedded in the material. This could mean that the robot either takes itself to a heating station, or it could activate some kind of embedded heating mechanism to be more self-sufficient.</p> \n<p>The second kind of self-healing material is autonomous, in that it will heal itself at room temperature without any additional input, and is probably more suitable for relatively minor scrapes and cracks. Here are some numbers about how well the healing works:</p> \n<blockquote> \n <p><em>Autonomous self-healing polymers do not require heat. They can heal damage at room temperature. Developing soft robotic systems from autonomous self-healing polymers excludes the need of additional heating devices… The healing however takes some time. The healing efficiency after 3 days, 7 days and 14 days is respectively 62 percent, 91 percent and 97 percent.&nbsp;</em></p> \n <p><em>This material was used to develop a healable soft pneumatic hand. Relevant large cuts can be healed entirely without the need of external heat stimulus. Depending on the size of the damage and even more on the location of damage, the healing takes only seconds or up to a week. Damage on locations on the actuator that are subjected to very small stresses during actuation was healed instantaneously. Larger damages, like cutting the actuator completely in half, took 7 days to heal. But even this severe damage could be healed completely without the need of any external stimulus.</em></p> \n</blockquote> \n<h3>Applications of self-healing robots</h3> \n<p>Both of these materials can be mixed together, and their mechanical properties can be customized so that the structure that they’re a part of can be tuned to move in different ways. The researchers also plan on introducing flexible conductive sensors into the material, which will help sense damage as well as providing position feedback for control systems. A lot of development will happen over the next few years, and for more details, we spoke with <a href=\"http://mech.vub.ac.be/multibody/members/bram.htm\">Bram Vanderborght at Vrije Universiteit in Brussels</a>.</p> \n<p><strong><em>IEEE Spectrum:</em> How easy or difficult or expensive is it to produce these materials? Will they add significant cost to robotic grippers?</strong></p> \n<p><em><strong>Bram Vanderborght:</strong> They are definitely more expensive materials, but it’s also a matter of size of production. At the moment, we’ve made a few kilograms of the material (enough to make several demonstrators), and the price already dropped significantly from when we ordered 100 grams of the material in the first phase of the project. So&nbsp;probably the cost of the gripper will be higher [than a regular gripper], but you won’t&nbsp;need to replace the gripper as often as other grippers that need to be replaced due to wear, so it can be an advantage.</em></p> \n<p><em>Moreover due to the method of 3D printing the material, the surface is smoother and airtight (so no post-processing is required to make it airtight). Also, the smooth surface is better to avoid contamination for food handling, for example.&nbsp;</em></p> \n<p><strong>In commercial or industrial applications, gradual fatigue seems to be a more common issue than more abrupt trauma like cuts. How well does the self-healing work to improve durability over long periods of time?</strong></p> \n<p><em>We did not test for gradual fatigue over very long times. But&nbsp;both macroscopic and microscopic damage can be healed. So hopefully it can provide an answer here as well.</em><br> &nbsp;</br></p> \n<figure class=\"rt med-lrg\" role=\"img\"> \n <img alt=\"Soft self-healing gripper\" src=\"https://spectrum.ieee.org/image/MzM2ODA0Nw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: SHERO Project \n  </figcaption> \n  <figcaption>\n    After developing a self-healing robot gripper, the researchers plan to use similar materials to build parts that can be used as the skeleton of robots, allowing them to repair themselves on a regular basis.&nbsp;&nbsp; \n  </figcaption> </img>\n</figure> \n<p><strong>How much does the self-healing capability restrict the material properties? What are the limits for softness or hardness or smoothness or other characteristics of the material?</strong></p> \n<p><em>Typically the mechanical properties of networked polymers are much better than thermoplastics. Our material is a networked polymer but in which the crosslinks are reversible. We can change quite a lot of parameters in the design of the materials. So we can develop very stiff (fracture strain at 1.24 percent) and very elastic materials (fracture strain at 450 percent). The big advantage that our material has is we can mix it to have intermediate properties. Moreover, at the interface of the materials with different mechanical properties, we have the same chemical bonds, so the interface is perfect. While other materials, they may need to glue it, which gives local stresses and a weak spot.</em></p> \n<p><strong>When the material heals itself, is it less structurally sound in that spot? Can it heal damage that happens to the same spot over and over again?</strong></p> \n<p><em>In theory we can heal it an infinite amount of times. When the wound is not perfectly aligned, of course in that spot it will become weaker. Also too high temperatures lead to irreversible bonds, and impurities lead to weak spots.</em></p> \n<p><strong>Besides grippers and skins, what other potential robotics applications would this technology be useful for?</strong></p> \n<p><em>Most of self healing materials available now are used for coatings. What we are developing are structural components, therefore the mechanical properties of the material need to be good for such applications. So maybe part of the skeleton of the robot can be developed with such materials to make it lighter, since can be designed for regular&nbsp;repair. And for exceptional loads, it breaks and can be repaired like our human body.</em></p> \n<p>[ <a href=\"http://www.sherofet.eu\">SHERO Project</a> ]</p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=BZE40vAKpP8:C4a4qIgo22s:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=BZE40vAKpP8:C4a4qIgo22s:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=BZE40vAKpP8:C4a4qIgo22s:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=BZE40vAKpP8:C4a4qIgo22s:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=BZE40vAKpP8:C4a4qIgo22s:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=BZE40vAKpP8:C4a4qIgo22s:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/BZE40vAKpP8\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Thu, 05 Sep 2019 21:35:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2ODA2Mw.jpeg","linkMd5":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn81@2020_6/2020/08/25/10-38-01-054_365676f393d0bc09.webp","destWidth":1240,"destHeight":948,"sourceBytes":965084,"destBytes":51076,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2ODAxNQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn91@2020_3/2020/08/25/10-38-11-022_5a0600b42b75e7df.webp","https://spectrum.ieee.org/image/MzM2ODAxNg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn42@2020_5/2020/08/25/10-38-13-032_93c5ca37062db292.webp","https://spectrum.ieee.org/image/MzM2ODA0Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn4@2020_6/2020/08/25/10-38-10-722_3c88df0461c594cb.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=BZE40vAKpP8:C4a4qIgo22s:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn27@2020_1/2020/08/25/10-38-04-594_55e269a12b121f0a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=BZE40vAKpP8:C4a4qIgo22s:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn66@2020_3/2020/08/25/10-38-13-296_53d4e317b00d782b.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/BZE40vAKpP8":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn77@2020_6/2020/08/25/10-38-16-986_a728f8c5d635cd3a.webp"},"publishedOrCreatedDate":1598351859738},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: This Humanoid Robot Will Serve You Ice Cream","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://nerc.seas.upenn.edu/\">Northeast Robotics Colloquium</a> –&nbsp;October 12, 2019 –&nbsp;Philadelphia, Pa., USA</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>What’s better than a robotics paper with “dynamic” in the title? A robotics paper with “highly dynamic” in the title. From <a href=\"https://biomimetics.mit.edu/\">Sangbae Kim’s lab at MIT</a>, the latest exploits of <a href=\"https://robots.ieee.org/robots/minicheetah/?utm_source=spectrum\">Mini Cheetah</a>:</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/6JlVol3eyNI\" width=\"620\"></iframe></p> \n <p></p> \n <p>Yes I’d very much like one please. Full paper at the link below.</p> \n <p>[ <a href=\"https://arxiv.org/abs/1909.06586\">Paper</a> ] via [ <a href=\"https://biomimetics.mit.edu/\">MIT</a> ]</p> \n <hr> \n  <blockquote> \n   <p><em>A humanoid robot serving you ice cream—on his own ice cream bike: What a delicious vision!</em></p> \n  </blockquote> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://www.youtube.com/embed/xiUUQZZI6ng\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>[&nbsp;<a href=\"https://roboy.org/core/\">Roboy</a>&nbsp;]</p> \n  <hr> \n   <p></p> \n   <p>The Roomba “i” series and “s” series vacuums have just gotten an update that lets you set “keep out” zones, which is super useful. Tell your robot where <em>not</em> to go!</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/TGAFfNXz7QY\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>I feel bad, that Roomba was probably just hungry :(</p> \n   <p>[ <a href=\"https://www.irobot.com/roomba\">iRobot</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>We wrote about <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/voliro-hexacopter-uses-rotating-nacelles-to-perform-versatile-acrobatics\">Voliro’s tilt-rotor hexcopter</a> a couple years ago, and now it’s off doing practical things, like spray painting a building pretty much the same color that it was before.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/vcKG7AEZ1Wk\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"https://www.voliro.ch/\">Voliro</a> ]</p> \n    <p><em>Thanks Mina!</em></p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Here’s a clever approach for bin-picking problematic objects, like shiny things: Just grab a whole bunch, and then sort out what you need on a nice robot-friendly table.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/My2IeoeC6v0\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>It might take a little bit longer, but what do you care, you’re probably off sipping a cocktail with a little umbrella in it on a beach somewhere.</p> \n     <p>[ <a href=\"https://www.roboticmanipulation.org/\">Harada Lab</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <blockquote> \n       <p><em>A unique combination of the IRB 1200 and <a href=\"https://robots.ieee.org/robots/yumi/?utm_source=spectrum\">YuMi</a> industrial robots that use vision, AI and deep learning to recognize and categorize trash for recycling.</em></p> \n      </blockquote> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SA_1vfH5UIk\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://new.abb.com/products/robotics/industrial-robots/irb-14000-yumi\">ABB</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>Measuring glacial movements in-situ is a challenging, but necessary task to model glaciers and predict their future evolution. However, installing GPS stations on ice can be dangerous and expensive when not impossible in the presence of large crevasses. In this project, the ASL develops UAVs for dropping and recovering lightweight GPS stations over inaccessible glaciers to record the ice flow motion. This video shows the results of first tests performed at Gorner glacier, Switzerland, in July 2019.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/V87I44EL1TE\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://polar.epfl.ch/spi-exploratory-grants-2019/#Guillaume_Jouvet\">EPFL</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>Turns out <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/roomba-inventor-joe-jones-on-weed-killing-robot\">Tertills actually do a pretty great job fighting weeds</a>.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/-PkAdjLHRz8\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>Plus, they leave all those cute lil’ Tertill tracks.</p> \n        <p>[ <a href=\"https://www.franklinrobotics.com/\">Franklin Robotics</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <blockquote> \n          <p><em>The online autonomous navigation and semantic mapping experiment presented [below] is conducted with the Cassie Blue bipedal robot at the University of Michigan. The sensors attached to the robot include an IMU, a 32-beam LiDAR and an RGB-D camera. The whole online process runs in real-time on a Jetson Xavier and a laptop with an i7 processor.</em></p> \n          <p><em>The resulting map is so precise that it looks like we are doing real-time SLAM (simultaneous localization and mapping). In fact, the map is based on dead-reckoning via the InvEKF.</em></p> \n         </blockquote> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/uFyT8zCg1Kk\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://gtsam.org/2019/09/18/legged-robot-factors-part-I.html\">GTSAM</a> ] via [ <a href=\"https://robotics.umich.edu/\">University of Michigan</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p><a href=\"https://jimurobots.com/?ls=en\">UBTECH</a> has announced an upgraded version of its Meebot, which is 30 percent bigger and comes with more sensors and programmable eyes.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/G8pvWxTmlIo\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://ubtrobot.com/products/meebot2-kit?ls=en\">UBTECH</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>ABB’s research team will be working with medical staff, scientist and engineers to develop non-surgical medical robotics systems, including logistics and next-generation automated laboratory technologies. The team will develop robotics solutions that will help eliminate bottlenecks in laboratory work and address the global shortage of skilled medical staff.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SVHVMVuhXjA\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"https://new.abb.com/products/robotics/applications-by-industry/healthcare\">ABB</a> ]</p> \n           <p></p> \n           <p></p> \n           <p></p> \n           <hr> \n            <p></p> \n            <blockquote> \n             <p><em>In this video, Ian and Chris go through Misty’s SDK, discussing the languages we’ve included, the tools that make it easy for you to get started quickly, a quick rundown of how to run the skills you build, plus what’s ahead on the Misty SDK roadmap. </em></p> \n            </blockquote> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/6Egx_yw0Ovs\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://www.mistyrobotics.com/\">Misty Robotics</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <p>My guess is that this was not one of iRobot’s testing environments for the Roomba.</p> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/BKo3SBfjiX4\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>You know, that’s actually super impressive. And maybe if they threw one of the self-emptying Roombas in there, it would be a viable solution to the entire problem.</p> \n             <p>[ <a href=\"https://www.youtube.com/channel/UCfECR0XVFoy3DunTSQC-E-g\">How Farms Work</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>Part of WeRobotics’ Flying Labs network, Panama Flying Labs is a local knowledge hub catalyzing social good and empowering local experts. Through training and workshops, demonstrations and missions, the Panama Flying Labs team leverages the power of drones, data, and AI to promote entrepreneurship, build local capacity, and confront the pressing social challenges faced by communities in Panama and across Central America.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/KsVJzyhQJ5U\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"https://flyinglabs.org/panama/\">Panama Flying Labs</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <blockquote> \n                <p><em>Go on a virtual flythrough of the NIOSH Experimental Mine, one of two courses used in the recent DARPA Subterranean Challenge Tunnel Circuit Event held 15-22 August, 2019. The data used for this partial flythrough tour were collected using 3D LIDAR sensors similar to the sensors commonly used on autonomous mobile robots.</em></p> \n               </blockquote> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/VY4brRg3ifs\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://www.subtchallenge.com/\">SubT</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <blockquote> \n                 <p><em>Special thanks to PBS, Mark Knobil, Joe Seamans and Stan Brandorff and many others who produced this program in 1991.</em></p> \n                 <p><em>It features Reid Simmons (and his 1 year old son), David Wettergreen, Red Whittaker, Mac Macdonald, Omead Amidi, and other Field Robotics Center alumni building the planetary walker prototype called Ambler. The team gets ready for an important demo for NASA.</em></p> \n                </blockquote> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/1QAgaVU1EXc\" width=\"620\"></iframe></p> \n                <p></p> \n                <p>[ <a href=\"https://www.ri.cmu.edu/\">CMU RI</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <blockquote> \n                  <p><em>As art and technology merge, roboticist Madeline Gannon explores the frontiers of human-robot interaction across the arts, sciences and society, and explores what this could mean for the future.</em></p> \n                 </blockquote> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/q8zycehvsbQ\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <p>[ <a href=\"https://sonarplusd.com/en/programs/barcelona-2019/areas/sonar-d-masterclasses-by-seat/the-future-of-humans-and-machines-human-robot-interaction-across-the-arts-sciences-and-society\">Sonar+D</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <div class=\"feedflare\"> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=9M4rEIftmus:0mYUx3pzxPg:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=9M4rEIftmus:0mYUx3pzxPg:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=9M4rEIftmus:0mYUx3pzxPg:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=9M4rEIftmus:0mYUx3pzxPg:-BTjWOF_DHI\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=9M4rEIftmus:0mYUx3pzxPg:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=9M4rEIftmus:0mYUx3pzxPg:gIN9vFwOqvQ\" border=\"0\" /></a> \n                  </div> \n                  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/9M4rEIftmus\" height=\"1\" width=\"1\" alt=\"\" /> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 11 Oct 2019 17:50:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5MTY2OA.jpeg","linkMd5":"727addd4da03ea6fd9f64c61ab6be805","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn39@2020_2/2020/08/25/10-37-59-804_8adac4e031007f90.webp","destWidth":1614,"destHeight":1002,"sourceBytes":422678,"destBytes":123628,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=9M4rEIftmus:0mYUx3pzxPg:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_5/2020/08/25/10-38-06-414_536ea9ab3e91a806.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=9M4rEIftmus:0mYUx3pzxPg:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn28@2020_6/2020/08/25/10-38-10-438_547193ffd27274ce.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/9M4rEIftmus":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn88@2020_5/2020/08/25/10-38-04-194_15cdab8bf2ac071f.webp"},"publishedOrCreatedDate":1598351859729},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Skydio's Dock in a Box Enables Long-Term Autonomy for Drone Applications","link":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","description":"<p>The word “autonomy” in the context of drones (or really any other robot) can mean a whole bunch of different things. <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">Skydio’s newest drone</a> is probably the most autonomous drone that we’ve ever seen, in the sense that it can fly itself while tracking subjects and avoiding obstacles. But as soon as the Skydio 2 lands, it’s completely helpless, dependent on a human to pick it up, pack it into a case, and take it back home to recharge.</p> \n<p>For consumer applications, this is not a big deal. But for industry, a big part of the appeal of autonomy is being able to deliver results with a minimum of human involvement, since humans are expensive and almost always busy doing other things.</p> \n<p>Today, <a href=\"https://www.skydio.com/\">Skydio</a> is announcing the Skydio 2 Dock, a (mostly) self-contained home base that a Skydio 2 drone can snuggle up inside to relax and recharge in between autonomous missions, meaning that you can set it up almost anywhere and get true long-term full autonomy from your drone.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/k8RRykNVL6E\" width=\"620\"></iframe></p> \n<p>Obviously, this is something that you can only do with the <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable\">level of autonomy that you get with Skydio’s drone</a>, because there’s no human pilot in the loop. From launch to landing on that alarmingly small platform, the drone can fly itself, although a remote human can step in if they want to at any point. Once the drone is safely back in its carry-on-size weatherproof box, the drone spends about an hour recharging (you’ll need to plug the box in for this), and then it’s ready to go again for a 23-minute flight. Conceivably you could have the drone in the air every hour and a half collecting data for you.</p> \n<p>Skydio’s dock is an integral part of their first industry partnership with DroneDeploy, a mapping platform for drones. One potential application is that you could have a Skydio 2 drone living inside of a dock on a construction site, and then it’ll fly around the site as often as you need it to and send you back a map of how much things have progressed. Since the drone is always on-site and ready to go and doesn’t need to coordinate around a human operator, it can give you data on-demand in near-real time, or even after the fact: Tell it to fly every day, and then if you want to know what happened a week ago, the data will be there—no human involvement means that the cost to collect data is low enough that there’s no reason not to just do it pretty much constantly.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/gs9GSg7h8s8\" width=\"620\"></iframe></p> \n<p>Well, there’s one reason not to just do it all the time, which is that in the United States it’s probably not allowed by the Federal Aviation Administration (FAA). We asked Skydio about this, and here’s what their CEO Adam Bry said:</p> \n<p>“Under current regulations a Beyond Visual Line of Sight (BVLOS) waiver would be required. We think that a small, light, safe drone with advanced navigation and collision avoidance is an excellent candidate for persistent autonomous operation. Our general view is that it’s our responsibility to establish that the system satisfies all relevant safety and logistical concerns, and work with regulators to roll this technology out responsibly.”</p> \n<p>The FAA does grant a fair number of waivers like these, and as Bry says, Skydio has a platform that they can (hopefully) show to be safe and reliable enough that the FAA will be cool with it. But this is yet another case where regulation is falling behind technology, and it means that you can’t just start using this system for your business without having to jump through some government hoops first. This is the problem with being a company that’s so far ahead of the curve, I guess—sometimes you have to wait for the rest of the world to catch up.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Skydio Dock\" src=\"https://spectrum.ieee.org/image/MzM5Mzc2NQ.jpeg\"> <img alt=\"Skydio Dock\" src=\"https://spectrum.ieee.org/image/MzM5Mzc2Ng.jpeg\"> \n   <figcaption class=\"hi-cap\">\n     Photos: Skydio \n   </figcaption> \n   <figcaption>\n     One potential application is having a Skydio 2 drone living inside of a dock on a construction site, and then it’ll fly around the site as often as you need it to and send you back a map of how much things have progressed. Once the drone returns to the dock, it spends about an hour recharging (you’ll need to plug the box in for this), and then it’s ready to go again for a 23-minute flight.&nbsp; \n   </figcaption> </img></img> \n</figure> \n<p>Skydio also sees its dock system as being valuable for first responders, where real-time data from a drone can potentially save lives. Instead of someone on-scene having to devote their attention to drone management. In these cases, having a person intermittently in the loop to request specific views might be a more typical use case, but not having to worry about takeoff or landing or flying would make things much more efficient: you can just ask for the data you want and the drone will provide it, and it won’t bother you about anything else.</p> \n<p>We’re told that Skydio will announce pricing of the Skydio 2 Dock when they have general availability early next year.</p> \n<p>[ <a href=\"https://www.skydio.com/\">Skydio</a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_Hi6JNHldFo:dQwWdXtGW-M:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_Hi6JNHldFo:dQwWdXtGW-M:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_Hi6JNHldFo:dQwWdXtGW-M:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_Hi6JNHldFo:dQwWdXtGW-M:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=_Hi6JNHldFo:dQwWdXtGW-M:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_Hi6JNHldFo:dQwWdXtGW-M:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_Hi6JNHldFo\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 16 Oct 2019 18:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM5Mzc0Ng.jpeg","linkMd5":"184896aa81aa8ae49425e7c03d12d965","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn22@2020_2/2020/08/25/10-37-58-930_82134a75d4c96a90.webp","destWidth":2303,"destHeight":1536,"sourceBytes":585617,"destBytes":166066,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5Mzc2NQ.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn84@2020_6/2020/08/25/10-38-06-152_32a08cbf2dc4cf2c.webp","https://spectrum.ieee.org/image/MzM5Mzc2Ng.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn26@2020_2/2020/08/25/10-38-17-470_68249d2494cad465.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_Hi6JNHldFo:dQwWdXtGW-M:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn84@2020_3/2020/08/25/10-38-03-557_a81b563f31210f8b.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_Hi6JNHldFo:dQwWdXtGW-M:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn79@2020_6/2020/08/25/10-38-10-177_70afbaa1fcbe159d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_Hi6JNHldFo":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn77@2020_2/2020/08/25/10-38-12-506_069b429e7fe5ab06.webp"},"publishedOrCreatedDate":1598351859728},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Watch Astrobee's First Autonomous Flight on the International Space Station","link":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","description":"<p>NASA’s Astrobee robots have come a long, long way since <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/how-nasa-astrobee-robot-is-bringing-useful-autonomy-to-the-iss\">we first met them at NASA Ames back in 2017</a>. In fact, they’ve made it all the way to the International Space Station: <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow\">Bumble, Honey, and Queen Bee are up there right now</a>. While Honey and Queen Bee are still packed away in a case (and quite unhappy about it, I would imagine), Bumble has been buzzing around, getting used to its new home. <span>To be ready to fly solo, all Bumble needed</span> was some astronaut-assisted mapping of its environment, and last month, the little robotic cube finally embarked on its first fully autonomous ISS adventure.</p> \n<!--nextpage--> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/hk-1j3sXTqA?rel=0\" width=\"620\"></iframe></p> \n<p>We cut together the above video from about an hour’s worth of raw footage (without audio) of Astrobee testing, which&nbsp;took place in the Japanese Experiment Module (JEM), also known as Kibo,&nbsp;on the ISS on <a href=\"https://blogs.nasa.gov/stationreport/2019/08/28/iss-daily-summary-report-8282019/\">August 28</a>. <a href=\"https://twitter.com/Astro_Christina\">Astronaut Christina Koch</a> had been working with roboticists at NASA Ames on earlier Astrobee start-up activities, which hadn’t gone as perfectly as everyone hoped they would, and was (understandably) excited that the robot was able to successfully fly itself though the JEM. Christina and another astronaut, off camera in the Harmony node attached to the JEM, do a little dance to celebrate (with what is now officially the “Astrobee Jig,” we’re told), and apparently Astrobee now has a standing invitation to join in on all future ISS dance parties.&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"Expedition 60 Flight Engineer Christina Koch of NASA works inside Japan's Kibo laboratory module monitoring a mobility test of the free-flying Astrobee robotic assistant.\" src=\"https://spectrum.ieee.org/image/MzM5MDQ3Nw.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photo: NASA/JSC \n  </figcaption> \n  <figcaption>\n    Expedition 60 flight engineer Christina Koch of NASA works inside Japan’s Kibo laboratory module monitoring a mobility test of the free-flying Astrobee robotic assistant. \n  </figcaption> </img> \n</figure> \n<p>Astrobee’s goal for its first autonomous mission was to undock itself, follow a flight plan consisting of a list of waypoints and objectives that was uploaded to the robot from the ground, and then return to the dock. All of this was done without any direct intervention from the ground controllers or from the astronauts. As you can see in the video, Christina is mostly just following Bumble around as it does its thing, keeping out of the way of the navigation camera but otherwise just making sure the robot didn’t get into any trouble.&nbsp;</p> \n<h3>How Astrobee flies itself</h3> \n<p>So far, the difficult part for Astrobee has been getting its localization to work robustly. While the robot does navigate visually, it’s dependent on preexisting maps rather than doing <a href=\"https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping\">SLAM</a>. Putting together those initial maps involved hand-carrying Bumble around the JEM to collect images, which were then processed offline (back on Earth) to identify features in the images and correlate them with locations to build up the map that Bumble uses to navigate.</p> \n<p>With maps like these, you have to find the right mix of features to include for navigation to work optimally. If your maps are too rich in features, there will be too much data for your robot to manage, and if the maps are too sparse, the robot won’t be able to localize accurately. This was a little bit tricky for Astrobee, as&nbsp;<span>deputy group lead</span>&nbsp;<a href=\"https://ti.arc.nasa.gov/profile/mbualat/\">Maria Bualat</a>&nbsp;from&nbsp;the Intelligent Systems Division at NASA Ames&nbsp;explained to us:</p> \n<blockquote> \n <p><em>It turned out that our maps needed to be richer. We tried to cull them down to make them fast and efficient, but we weren’t keeping enough features to enable the robot to localize robustly, so it would get lost a lot. During some of our earlier activities when we were trying to fly even basic motions, the robot would tend to drift as it would lose lock. This last activity that we had was great, because it was our first time using the more enriched map, and the localization worked really well. It was kind of nice because [Christina] saw us through those struggles—she saw how tough it was to get the robot to fly.</em></p> \n</blockquote> \n<p>Besides this little bit of software optimization, Bualat says that Astrobee has been working well, without any other software issues or hardware issues of any kind. This is impressive for any robot, and especially so for a robot that was developed entirely on the ground and is now being used in space. And as for the astronauts whose job it is to test Astrobee, it sounds like they’re actually having some fun with it. There was a bit of concern initially that Astrobee’s impellers would be overly loud, but that might be a feature rather than a bug, as Bualat explains: “We’ve asked them if they found it noisy or annoying, and they said no—in fact, they said that you can certainly hear it, but they actually liked it because it means that Astrobee can’t sneak up on them.”</p> \n<p>Astrobee will be continuing its commissioning activities over the next few months, which includes tuning Bumble so that it can fly as robustly as possible. For example, Astrobee needs to be able to navigate if an astronaut moves in front of its navigation camera, blocking some of the view. Bumble will then get its <a href=\"https://spectrum.ieee.org/preview/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow\">perching arm</a> installed and tested, after which the goal is to start working with some of the science payloads, like a gecko gripper, a RFID tracker, and a microphone array, which you can read more about <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow\">here</a> and <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/robonaut-and-astrobee-to-will-work-together-on-iss\">here</a>. Honey and Queen still need to go through their own start-up tests and calibrations, and Maria Bualat says the goal is to have multiple Astrobees buzzing around the ISS together “not too far in the future.”</p> \n<p><span>[ </span><a href=\"https://www.nasa.gov/astrobee\" style=\"font-size: 1em;\">Astrobee</a><span> ]</span></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=f4vRmWO59FQ:u8dJuNbBVJU:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=f4vRmWO59FQ:u8dJuNbBVJU:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=f4vRmWO59FQ:u8dJuNbBVJU:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=f4vRmWO59FQ:u8dJuNbBVJU:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=f4vRmWO59FQ:u8dJuNbBVJU:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=f4vRmWO59FQ:u8dJuNbBVJU:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/f4vRmWO59FQ\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 09 Oct 2019 19:35:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM4NDQzNA.jpeg","linkMd5":"de8494e008723f24ca95f4421202eaa2","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn10@2020_3/2020/08/25/10-37-58-935_390e87105eec721e.webp","destWidth":2000,"destHeight":1298,"sourceBytes":720739,"destBytes":285378,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM5MDQ3Nw.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn96@2020_4/2020/08/25/10-38-09-159_27f4981a792ae2fa.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=f4vRmWO59FQ:u8dJuNbBVJU:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn16@2020_5/2020/08/25/10-38-04-727_736eb8bda612cc0a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=f4vRmWO59FQ:u8dJuNbBVJU:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn34@2020_4/2020/08/25/10-38-10-319_db083f9a46ebf17e.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/f4vRmWO59FQ":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn23@2020_5/2020/08/25/10-38-14-783_7b254f75daebaec4.webp"},"publishedOrCreatedDate":1598351859736},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: This Robot Refuses to Fall Down Even if You Hit, Shove It","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://www.roboticarena.pl/en/\">Robotic Arena</a> –&nbsp;January 25, 2020 –&nbsp;Wrocław, Poland</h5> \n<h5><a href=\"https://www.darpa.mil/news-events/2019-11-12\">DARPA SubT Urban Circuit</a> –&nbsp;February 18-27, 2020 –&nbsp;Olympia, Wash., USA</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>In case you somehow missed the <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly\">massive Skydio 2 review we posted earlier this week</a>, the first batches of the drone are now shipping. Each drone gets a lot of attention before it goes out the door, and here’s a behind-the-scenes clip of the process.</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/3KfP40oMxlY\" width=\"620\"></iframe></p> \n <p></p> \n <p>[ <a href=\"https://medium.com/@Skydio/becoming-skydio-2-a4c676179b6a\">Skydio</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p><a href=\"https://robots.ieee.org/robots/rvr/?utm_source=spectrum\">Sphero RVR</a> is one of the 15 robots on our <a href=\"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019\">robot&nbsp;gift guide</a> this year. Here’s a new video <a href=\"https://www.sphero.com/\">Sphero</a> just released showing some of the things you can do with the robot.</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/9VpmosbbGl0\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>[ <a href=\"https://www.sphero.com/rvr\">RVR</a> ]</p> \n  <hr> \n   <p>NimbRo-OP2 has some impressive recovery skills from the obligatory research-motivated robot abuse.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://www.youtube.com/embed/5w2tdfEQzac\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>[&nbsp;<a href=\"http://www.nimbro.net/\">NimbRo</a>&nbsp;]</p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>Teams seeking to qualify for the Virtual Urban Circuit of the Subterranean Challenge can access practice worlds to test their approaches prior to submitting solutions for the competition. This video previews three of the practice environments.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/92wHeELiFmE\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"https://www.subtchallenge.com/\">DARPA SubT</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <blockquote> \n      <p><em>Stretchable skin-like robots that can be rolled up and put in your pocket have been developed by a University of Bristol team using a new way of embedding artificial muscles and electrical adhesion into soft materials.</em></p> \n     </blockquote> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/huHi7aGiNKI\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>[ <a href=\"http://www.bristol.ac.uk/news/2019/november/soft-skin-like-robots.html\">Bristol</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>Happy Holidays from ABB!</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/3CShC8QaZNQ\" width=\"620\"></iframe></p> \n      <p></p> \n      <blockquote> \n       <p><em>Helping New York celebrate the festive season, twelve ABB robots are interacting with visitors to Bloomingdale’s iconic holiday celebration at their 59th Street flagship store. ABB’s robots are the main attraction in three of Bloomingdale’s twelve-holiday window displays at Lexington and Third Avenue, as ABB demonstrates the potential for its robotics and automation technology to revolutionize visual merchandising and make the retail experience more dynamic and whimsical.</em></p> \n      </blockquote> \n      <p>[ <a href=\"https://new.abb.com/us\">ABB</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <blockquote> \n        <p><em>We introduce pelican eel–inspired dual-morphing architectures that embody quasi-sequential behaviors of origami unfolding and skin stretching in response to fluid pressure. In the proposed system, fluid paths were enclosed and guided by a set of entirely stretchable origami units that imitate the morphing principle of the pelican eel’s stretchable and foldable frames. This geometric and elastomeric design of fluid networks, in which fluid pressure acts in the direction that the whole body deploys first, resulted in a quasi-sequential dual-morphing response. To verify the effectiveness of our design rule, we built an artificial creature mimicking a pelican eel and reproduced biomimetic dual-morphing behavior.</em></p> \n       </blockquote> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/op0BsuxFx4Y\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>And here’s a real pelican eel:</p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/iT_EMKl2A3Y?rel=0\" width=\"620\"></iframe></p> \n       <p>[ <a href=\"https://robotics.sciencemag.org/content/4/36/eaay3493\">Science Robotics</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>Delft Dynamics’ updated anti-drone system involves a tether, mid-air net gun, and even a parachute.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/h3aJ4KKxpWs\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://www.delftdynamics.nl/\">Delft Dynamics</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>Teleoperation is a great way of helping robots with complex tasks, especially if you can do it through motion capture. But what if you’re teleoperating a non-anthropomorphic robot? Columbia’s ROAM Lab is working on it.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/c7luNdEbVqU\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://arxiv.org/abs/1911.09565\">Paper</a> ] via [ <a href=\"https://roam.me.columbia.edu/\">ROAM Lab</a> ]</p> \n         <p></p> \n         <p></p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>I don’t know how I missed this video last year because it’s got a steely robot hand squeezing a cute lil’ chick.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/lmhIeNXdGuQ\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://www.motionlib.com/\">MotionLib</a> ] via [ <a href=\"https://robotstart.info/2019/11/30/abccore.html\">RobotStart</a> ]</p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>In this video we present results of a trajectory generation method for autonomous overtaking of unexpected obstacles in a dynamic urban environment. In these settings, blind spots can arise from perception limitations. For example when overtaking unexpected objects on the vehicle’s ego lane on a two-way street. In this case, a human driver would first make sure that the opposite lane is free and that there is enough room to successfully execute the maneuver, and then it would cut into the opposite lane in order to execute the maneuver successfully. We consider the practical problem of autonomous overtaking when the coverage of the perception system is impaired due to occlusion.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/qCcrjBfSKOU\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"http://www.alonsomora.com/docs/19-andersen-t-iv.pdf\">Paper</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p>New weirdness from Toio!</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/1-h5dJxWe48\" width=\"620\"></iframe></p> \n            <p></p> \n            <p></p> \n            <p>[ <a href=\"https://toio.io/\">Toio</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <blockquote> \n              <p><em>Palo Alto City Library won a technology innovation award! Watch to see how Senior Librarian Dan Lou is using <a href=\"https://robots.ieee.org/robots/misty/?utm_source=spectrum\">Misty</a> to enhance their technology programs to inspire and educate customers.</em></p> \n             </blockquote> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/vWehZfpgXDA\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>[ <a href=\"https://www.mistyrobotics.com/\">Misty Robotics</a> ]</p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>We consider the problem of reorienting a rigid object with arbitrary known shape on a table using a two-finger pinch gripper. Reorienting problem is challenging because of its non-smoothness and high dimensionality. In this work, we focus on solving reorienting using pivoting, in which we allow the grasped object to rotate between fingers. Pivoting decouples the gripper rotation from the object motion, making it possible to reorient an object under strict robot workspace constraints.</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/pQ49Ud5l-eI\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>[ <a href=\"http://www.cs.cmu.edu/~yifanh/project_reorienting/\">CMU</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <blockquote> \n                <p><em>How can a mobile robot be a good pedestrian without bumping into you on the sidewalk? It must be hard for a robot to navigate in crowded environments since the flow of traffic follows implied social rules. But researchers from MIT developed an algorithm that teaches mobile robots to maneuver in crowds of people, respecting their natural behaviour.</em></p> \n               </blockquote> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/P0c2JY8OfC4\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://roboy.org/\">Roboy Research Reviews</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <blockquote> \n                 <p><em>What happens when humans and robots make art together? In this awe-inspiring talk, artist Sougwen Chung shows how she \"taught\" her artistic style to a machine -- and shares the results of their collaboration after making an unexpected discovery: robots make mistakes, too. \"Part of the beauty of human and machine systems is their inherent, shared fallibility,\" she says.</em></p> \n                </blockquote> \n                <p></p> \n                <iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" scrolling=\"no\" src=\"https://embed.ted.com/talks/sougwen_chung_why_i_draw_with_robots\" width=\"620\"></iframe> \n                <p></p> \n                <p>[ <a href=\"https://www.ted.com/talks/sougwen_chung_why_i_draw_with_robots?rss=172BB350-0205\">TED</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <p>Last month at the Cooper Union in New York City, IEEE TechEthics hosted a public panel session on the facts and misperceptions of autonomous vehicles, part of the IEEE TechEthics Conversations Series.&nbsp;The speakers were: Jason Borenstein from Georgia Tech; Missy Cummings from Duke University; Jack Pokrzywa from SAE; and Heather M. Roff from Johns Hopkins Applied Physics Laboratory. The panel was moderated by Mark A. Vasquez, program manager for IEEE TechEthics.</p> \n                 <iframe allowfullscreen=\"true\" height=\"346\" scrolling=\"no\" src=\"//ieeetv.ieee.org/player/embed_play/132906/620\" width=\"620\"></iframe> \n                 <p><br> [ <a href=\"https://techethics.ieee.org/\">IEEE TechEthics</a> ]</br></p> \n                 <hr> \n                  <p>Two videos this week from Lex Fridman’s AI podcast: Noam Chomsky, and Whitney Cummings.</p> \n                  <p></p> \n                  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/cMscNuSUy0I\" width=\"620\"></iframe></p> \n                  <p></p> \n                  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/0-3kw5BEKB8?rel=0\" width=\"620\"></iframe></p> \n                  <p>[ <a href=\"https://lexfridman.com/ai/\">AI Podcast</a> ]</p> \n                  <p></p> \n                  <hr> \n                   <p></p> \n                   <p>This week’s CMU RI Seminar comes from Jeff Clune at the University of Wyoming, on “Improving Robot and Deep Reinforcement Learning via Quality Diversity and Open-Ended Algorithms.”</p> \n                   <p></p> \n                   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/CxaW3hrUN8k\" width=\"620\"></iframe></p> \n                   <p></p> \n                   <blockquote> \n                    <p><em>Quality Diversity (QD) algorithms are those that seek to produce a diverse set of high-performing solutions to problems. I will describe them and a number of their positive attributes. I will then summarize our Nature paper on how they, when combined with Bayesian Optimization, produce a learning algorithm that enables robots, after being damaged, to adapt in 1-2 minutes in order to continue performing their mission, yielding state-of-the-art robot damage recovery. I will next describe our QD-based Go-Explore algorithm, which dramatically improves the ability of deep reinforcement learning algorithms to solve previously unsolvable problems wherein reward signals are sparse, meaning that intelligent exploration is required. Go-Explore solves Montezuma’s Revenge, considered by many to be a major AI research challenge. Finally, I will motivate research into open-ended algorithms, which seek to innovate endlessly, and introduce our POET algorithm, which generates its own training challenges while learning to solve them, automatically creating a curricula for robots to learn an expanding set of diverse skills. POET creates and solves challenges that are unsolvable with traditional deep reinforcement learning techniques.</em></p> \n                   </blockquote> \n                   <p>[ <a href=\"https://www.ri.cmu.edu/ri-seminar-series/\">CMU RI</a> ]</p> \n                   <p></p> \n                   <hr> \n                    <p></p> \n                    <div class=\"feedflare\"> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=af0ofRZ9M8M:kUGcuzMWe1A:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=af0ofRZ9M8M:kUGcuzMWe1A:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=af0ofRZ9M8M:kUGcuzMWe1A:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=af0ofRZ9M8M:kUGcuzMWe1A:-BTjWOF_DHI\" border=\"0\" /></a> \n                     <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=af0ofRZ9M8M:kUGcuzMWe1A:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=af0ofRZ9M8M:kUGcuzMWe1A:gIN9vFwOqvQ\" border=\"0\" /></a> \n                    </div> \n                    <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/af0ofRZ9M8M\" height=\"1\" width=\"1\" alt=\"\" /> \n                   </hr> \n                  </hr> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 06 Dec 2019 23:26:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQyNDczNQ.jpeg","linkMd5":"67ca3612e21a3b8090962da8d8d72cea","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn26@2020_4/2020/08/25/10-38-00-179_de252d8a3fcbefad.webp","destWidth":1240,"destHeight":698,"sourceBytes":161691,"destBytes":83998,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=af0ofRZ9M8M:kUGcuzMWe1A:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn1@2020_2/2020/08/25/10-38-01-963_ba4b2a09f5e52f35.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=af0ofRZ9M8M:kUGcuzMWe1A:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn41@2020_1/2020/08/25/10-38-14-297_8a40389a3e6fbbd5.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/af0ofRZ9M8M":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn82@2020_2/2020/08/25/10-38-11-673_0beb0879a8887890.webp"},"publishedOrCreatedDate":1598351859710},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"This MIT Robot Wants to Use Your Reflexes to Walk and Balance","link":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","description":"<p>MIT researchers have demonstrated a new kind of teleoperation system that allows a two-legged robot to “borrow” a human operator’s physical skills to move with greater agility. The system&nbsp;works a bit like those <a href=\"https://youtu.be/JXivaBf861c?t=12\">haptic suits</a> from the Spielberg movie “Ready Player One.” But while the suits in the film were used to connect&nbsp;humans to their VR avatars, the MIT suit connects the operator to a real robot.</p> \n<p></p> \n<!--nextpage--> \n<p></p> \n<p></p> \n<p>The robot is called Little HERMES, and it’s currently just a pair of little legs, about a third the size of an average adult. It can step&nbsp;and jump in place or walk a short distance while supported by a gantry. While that in itself is not very impressive, the researchers say their approach could help bring <a href=\"https://spectrum.ieee.org/robotics/humanoids/darpas-rescuerobot-showdown\">capable disaster robots</a> closer to reality. They explain that, despite recent advances, building fully autonomous robots with motor and decision-making skills comparable to those of humans remains a challenge. That’s where a more advanced teleoperation system could help.&nbsp;</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/nx3LiBinwdo\" width=\"620\"></iframe></p> \n<p></p> \n<p><span>The researchers,&nbsp;</span><a href=\"https://mechanical.illinois.edu/directory/faculty/jlramos\" style=\"font-family: Georgia, serif; font-size: 18px;\">João Ramos</a><span>, now an assistant professor at the </span>University of Illinois at Urbana-Champaign<span>, and&nbsp;</span><a href=\"https://biomimetics.mit.edu/sangbae-kim\" style=\"font-family: Georgia, serif; font-size: 18px;\">Sangbae Kim</a><span>, director of&nbsp;</span><a href=\"https://biomimetics.mit.edu/\" style=\"font-family: Georgia, serif; font-size: 18px;\">MIT’s Biomimetic Robotics Lab</a><span>, describe the project in this week’s issue of&nbsp;</span><em>Science Robotics</em>. In the paper, they argue that existing teleoperation systems often can’t effectively match the operator’s motions to that of a robot. In addition, conventional systems provide no physical feedback to the&nbsp;human teleoperator about what the robot is doing. Their new approach addresses these two limitations, and to see how it would work in practice, they built Little HERMES.</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"MIT Little HERMES\" src=\"https://spectrum.ieee.org/image/MzQwMjkxMg.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Science Robotics \n  </figcaption> \n  <figcaption>\n    The main components of MIT’s bipedal robot Little HERMES: (A) Custom actuators designed to withstand impact and capable of producing high torque. (B) Lightweight limbs with low inertia and fast leg swing. (C) Impact-robust and lightweight foot sensors with three-axis contact force sensor. (D) Ruggedized IMU to estimates the robot’s torso posture, angular rate, and linear acceleration. (E) Real-time computer sbRIO 9606 from National Instruments for robot control. (F) Two three-cell lithium-polymer batteries in series. (G) Rigid and lightweight frame to minimize the robot mass. \n  </figcaption> </img> \n</figure> \n<p>Early this year, the MIT researchers&nbsp;<a href=\"https://spectrum.ieee.org/robotics/humanoids/human-reflexes-help-mits-hermes-rescue-robot-keep-its-footing\">wrote</a>&nbsp;an in-depth article for <em>IEEE Spectrum</em> about the project, which includes Little HERMES and also its big brother, HERMES<span>&nbsp;(for Highly Efficient Robotic Mechanisms and Electromechanical System)</span>.&nbsp;In that article, they describe the two main components of the system:</p> \n<blockquote> \n <p><em>[...] We are building a telerobotic system that has two parts: a humanoid capable of nimble, dynamic behaviors, and a new kind of two-way human-machine interface that sends your motions to the robot and the robot’s motions to you. So if the robot steps on debris and starts to lose its balance, the operator feels the same instability and instinctively reacts to avoid falling. We then capture that physical response and send it back to the robot, which helps it avoid falling, too. Through this human-robot link, the robot can harness the operator’s innate motor skills and split-second reflexes to keep its footing.</em></p> \n <p><em>You could say we’re putting a human brain inside the machine.</em></p> \n</blockquote> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"MIT Little HERMES teleoperation system\" src=\"https://spectrum.ieee.org/image/MzQwMjkzMA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: Science Robotics \n  </figcaption> \n  <figcaption>\n    The human-machine interface built by the MIT researchers for controlling Little HERMES is different from conventional ones in that it relies on the operator’s reflexes to improve the robot’s stability. The researchers call it the balance-feedback interface, or BFI. The main modules of the BFI include: (A) Custom interface attachments for torso and feet designed to capture human motion data at high speed (1 kHz). (B) Two underactuated modules to track the position and orientation of the torso and apply forces to the operator. (C) Each actuation module has three DoFs, one of which is a push/pull rod actuated by a DC brushless motor. (D) A series of linkages with passive joints connected to the operator’s feet and track their spatial translation. (E) Real-time controller cRIO 9082 from National Instruments to close the BFI control loop. (F) Force plate to estimated the operator’s center of pressure position and measure the shear and normal components of the operator’s net contact force. \n  </figcaption> </img> \n</figure> \n<p><span>Here</span><span>’s more footage of the experiments, showing&nbsp;Little HERMES stepping and jumping in place, walking a few steps forward and backward, and balancing. Watch until the end to see a&nbsp;</span>compilation of unsuccessful stepping experiments. Poor&nbsp;<span>Little HERMES!</span></p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/IquGO78ZIf0\" width=\"620\"></iframe></p> \n<p>In the new&nbsp;<em>Science Robotics</em>&nbsp;<a href=\"https://robotics.sciencemag.org/content/4/35/eaav4282\">paper</a>,&nbsp;the MIT researchers explain how they solved one of the key challenges in making their teleoperation system effective:</p> \n<p></p> \n<p></p> \n<p></p> \n<blockquote> \n <p><em>The challenge of this strategy lies in properly mapping human body motion to the machine while simultaneously informing the operator how closely the robot is reproducing the movement. Therefore, we propose a solution for this bilateral feedback policy to control a bipedal robot to take steps, jump, and walk in synchrony with a human operator. Such dynamic synchronization was achieved by (i) scaling the core components of human locomotion data to robot proportions in real time and (ii) applying feedback forces to the operator that are proportional to the relative velocity between human and robot.</em></p> \n</blockquote> \n<p>Little HERMES is now taking its first steps, quite&nbsp;<span>literally,</span> but the researchers say they hope to use robotic legs with similar design&nbsp;as part of a more advanced humanoid. One possibility&nbsp;they’ve <a href=\"https://spectrum.ieee.org/robotics/humanoids/human-reflexes-help-mits-hermes-rescue-robot-keep-its-footing\">envisioned</a> is a fast-moving quadruped robot that could run through various kinds of terrain and&nbsp;then transform into a bipedal robot that would use its hands to perform dexterous manipulations. This could involve merging some of the robots the MIT researchers have built in their lab, possibly creating hybrids between <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-cheetah-robot-running\">Cheetah</a> and HERMES,&nbsp;or <a href=\"https://robots.ieee.org/robots/minicheetah/\">Mini Cheetah</a> and Little HERMES. We can’t wait to see what the resulting robots will look like.</p> \n<p>[ <a href=\"https://robotics.sciencemag.org/content/4/35/eaav4282\"><em>Science Robotics</em></a> ]</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-x2GxmjoVIQ:dcG3leT_8II:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-x2GxmjoVIQ:dcG3leT_8II:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-x2GxmjoVIQ:dcG3leT_8II:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-x2GxmjoVIQ:dcG3leT_8II:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=-x2GxmjoVIQ:dcG3leT_8II:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-x2GxmjoVIQ:dcG3leT_8II:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-x2GxmjoVIQ\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Wed, 30 Oct 2019 19:10:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwMzA0Mg.jpeg","linkMd5":"5c8fee7c8bef1dc43b41be2a29c47ddb","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn47@2020_2/2020/08/25/10-37-59-992_91772e9d1772a86d.webp","destWidth":1240,"destHeight":927,"sourceBytes":376942,"destBytes":104684,"author":"Erico Guizzo","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQwMjkxMg.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn56@2020_4/2020/08/25/10-38-10-526_daf78723772b4086.webp","https://spectrum.ieee.org/image/MzQwMjkzMA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn47@2020_2/2020/08/25/10-38-02-351_e78edf29f035936c.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-x2GxmjoVIQ:dcG3leT_8II:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn24@2020_3/2020/08/25/10-38-08-602_83d1702aab4af228.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-x2GxmjoVIQ:dcG3leT_8II:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_4/2020/08/25/10-38-09-170_6f9b1cb59938eea4.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-x2GxmjoVIQ":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn36@2020_3/2020/08/25/10-38-07-836_0a6fb2638bafb6bc.webp"},"publishedOrCreatedDate":1598351859725},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"China Says Its Mars Landing Technology Is Ready for 2020","link":"https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020","description":"<p><span>China says it’s ready to attempt something only NASA has so far achieved—successfully landing a rover on Mars.</span></p> \n<blockquote class=\"twitter-tweet\" data-lang=\"en\"> \n <p></p> \n <a href=\"https://twitter.com/hashtag/China?src=hash&amp;ref_src=twsrc%5Etfw\">#China</a> unveils first picture of its Mars explorer \n <a href=\"https://t.co/FnSu04Uv0h\">https://t.co/FnSu04Uv0h</a> \n <a href=\"https://t.co/0coWStBZxV\">pic.twitter.com/0coWStBZxV</a> \n <p></p> — CGTN (@CGTNOfficial) \n <a href=\"https://twitter.com/CGTNOfficial/status/1182916029992714240?ref_src=twsrc%5Etfw\">October 12, 2019</a> \n</blockquote> \n<p><script async=\"\" src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><span>It will be China’s first independent attempt at an interplanetary mission, and comes with two ambitious goals. Launching in 2020, China’s Mars mission will attempt to put a probe in orbit around Mars and, separately, land a rover on the red planet.&nbsp;</span></p> \n<p><span>The mission was approved in early 2016 but updates have few and far between. Last week, a terse update (<a href=\"https://mp.weixin.qq.com/s/xeSyAZ_DZzGI_2o9Vn3mjQ\">available here in Chinese</a>) from the Xi'an Aerospace Propulsion Institute, a subsidiary of CASC, China's main space contractor, revealed that the spacecraft’s propulsion system had passed all necessary tests.&nbsp;</span></p> \n<p><span>According to the report, the Shanghai Institute of Space Propulsion has completed tests of the spacecraft's propulsion system for the hovering, hazard avoidance, slow-down, and landing stages of a Mars landing attempt. The successful tests verified the performance and control of the propulsion system, in which one engine producing 7,500 Newtons of thrust will provide the majority of force required to decelerate the spacecraft for landing.</span></p> \n<!--nextpage--> \n<p><span>Having previously completed tests of supersonic parachutes needed to slow the craft’s entry into the Martian atmosphere, this means <a href=\"https://news.cgtn.com/news/2019-10-12/China-unveils-first-picture-of-its-Mar-explorer-KJ4LAwSYbm/index.html\">China’s Mars spacecraft</a> is close to ready for its mission.</span></p> \n<p><span>China was initially <a href=\"https://www.planetary.org/blogs/guest-blogs/china-2020-rover-sites.html\">considering several sites within two broad landing areas</a> near Chryse Planitia, close to the landing sites of Viking 1 and Pathfinder, and another covering Isidis Planitia and stretching to the western edge of the Elysium Mons region.</span></p> \n<p><span>According to a presentation at the European Planetary Science Congress-Division for Planetary Sciences Joint Meeting in Geneva in September, China has now chosen <a href=\"https://twitter.com/Tschnn/status/1174667085915598848\">two preliminary sites</a> near Utopia Planitia. The mission will have landing ellipses—the areas in which the spacecraft is statistically likely to land—of around 100 x 40 kilometers.&nbsp;</span></p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"On May 19th, 2005, NASA's Mars Exploration Rover Spirit captured this stunning view as the Sun sank below the rim of Gusev crater on Mars.\" src=\"https://spectrum.ieee.org/image/MzQwODgyOA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Image: JPL/Texas A&amp;M/Cornell/NASA \n  </figcaption> \n  <figcaption>\n    NASA's Spirit rover captured this stunning view as the Sun sank below the rim of Gusev crater on Mars on 19 May, 2005. \n  </figcaption> </img> \n</figure> \n<p><span>China’s solar-powered Mars rover will, at 240 kilograms, be twice the mass of China’s two lunar rovers. It will <a href=\"https://www.planetary.org/multimedia/space-images/charts/china-2020-mars-mission.html\">carry</a> navigation, topography, and multispectral cameras, a subsurface detection radar, a laser-induced breakdown spectroscopy instrument similar to Curiosity’s LIBS instrument, a Martian surface magnetic field detector, and a climate detector.</span></p> \n<p><span>The orbiter will be equipped with a suite of science instruments including moderate- and high-resolution imagers. The <a href=\"https://meetingorganizer.copernicus.org/EPSC-DPS2019/EPSC-DPS2019-368-1.pdf\">pair of cameras</a> will be used once in Mars orbit to image the preselected landing sites ahead of separation of the orbiter and rover.</span></p> \n<p><span>The main barrier to China launching its mission is the status of the Long March 5 rocket required to get the 5-metric-ton spacecraft on its way to Mars.&nbsp;</span></p> \n<p><span>The Long March 5 is China’s largest launch vehicle, which had its first flight in 2016. However the second launch, in July 2017, <a href=\"https://www.theverge.com/2017/7/3/15913448/chinese-rocket-long-march-5-failure-moon-mission\">failed</a> to achieve orbit. Following at least two redesigns of the engines which power the rocket’s first stage, the Long March 5 is now ready to return to flight.&nbsp;</span></p> \n<p><span>The rocket is currently being assembled at the Wenchang Satellite Launch Center on Hainan island in southern China, with launch expected in <a href=\"https://spacenews.com/china-begins-preparations-for-crucial-long-march-5-rocket-launch/\">late December</a>. The mission will aim to send a large satellite into geostationary orbit, and in doing so prove the rocket is ready for the later Mars mission launch.</span></p> \n<p><span>If all goes well, China will join <a href=\"https://spectrum.ieee.org/automaton/robotics/space-robots/robot-arm-mars-2020-rover\">NASA’s Mars 2020 mission</a>, the United Arab Emirates’ Hope Mars Mission and, <a href=\"https://spacenews.com/esa-turns-to-nasa-to-assist-in-crucial-exomars-parachute-tests/\">if parachute issues can be&nbsp;overcome</a>, the <a href=\"http://www.esa.int/Science_Exploration/Human_and_Robotic_Exploration/Exploration/ExoMars/ExoMars_2020_rover\">ExoMars 2020 mission</a>, in launching during a roughly three-week window from late July to early August 2020. With the advantage of favorable relative positions of Earth and Mars at that time—creating an efficient path known as the Hohmann transfer—the spacecraft would arrive at the red planet around February 2021.&nbsp;</span></p> \n<p><span>If the Long March 5 does not come through its big test in late December, China will need to wait 26 months before the next Hohmann transfer window opens for Mars, in late 2022.</span></p> \n<p><span>Getting to Mars is only part of the job. China has already landed spacecraft on <a href=\"https://spectrum.ieee.org/aerospace/robotic-exploration/china-gears-up-for-ambitious-landing-on-the-moons-far-side\">the near and far sides of the moon</a>, and members of the successful 2013 Chang’e-3 lunar mission team were assigned to the Mars project. However, landing on Mars presents extra challenges.</span></p> \n<p><span>The surface gravity of Mars is just 38 percent that of Earth. Simulating the Martian gravitational field adds complexity to terrestrial testing of entry, descent, and landing (EDL) sequences.</span></p> \n<p><span>Mars has an atmosphere which is too thin to properly aid descent, but thick enough to threaten fast-moving spacecraft with extreme heat from atmospheric friction and compression. This requires a spacecraft to have a heat shield and complex parachute systems which need to be deployed and jettisoned at precisely the right moments.</span></p> \n<p><span>When the spacecraft arrives at Mars, it will be around 150 million kilometers from Earth, meaning commands traveling at the speed of light will take around 8 minutes to reach their target. This means the entire landing process must be automated. For <a href=\"https://spectrum.ieee.org/automaton/aerospace/robotic-exploration/how-nasa-curiosity-rover-will-land-on-mars\">NASA’s 2012 landing of the Curiosity rover</a>, the team called this period the <a href=\"https://www.youtube.com/watch?v=Ki_Af_o9Q9s\">“7 minutes of terror.”</a>&nbsp;</span></p> \n<p><span>Several Mars missions have failed during that critical stage, including a 2016 effort by the European Space Agency and Roscosmos of Russia to plant the ExoMars Schiaparelli EDM lander, as well as numerous Soviet missions and NASA’s attempt with its 1999 Mars Polar Lander.</span></p> \n<p></p> \n<div></div> \n<div></div> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4lbED3n0y6U:GhJJ0usmqCA:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4lbED3n0y6U:GhJJ0usmqCA:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4lbED3n0y6U:GhJJ0usmqCA:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4lbED3n0y6U:GhJJ0usmqCA:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=4lbED3n0y6U:GhJJ0usmqCA:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4lbED3n0y6U:GhJJ0usmqCA:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/4lbED3n0y6U\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 08 Nov 2019 14:59:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwODg0NQ.jpeg","linkMd5":"538b0c13aaa64a59898970581b42f5fa","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn100@2020_4/2020/08/25/10-38-00-126_1100de68ae27ac02.webp","destWidth":1240,"destHeight":930,"sourceBytes":293895,"destBytes":9260,"author":"Andrew Jones","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzQwODgyOA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn96@2020_5/2020/08/25/10-38-02-911_531684463d4c315d.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4lbED3n0y6U:GhJJ0usmqCA:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn83@2020_2/2020/08/25/10-38-10-813_79278e042af349c2.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4lbED3n0y6U:GhJJ0usmqCA:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn75@2020_4/2020/08/25/10-38-08-194_d0e78e9474fa480c.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/4lbED3n0y6U":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn22@2020_3/2020/08/25/10-38-14-785_4bcdd8f623c61cac.webp"},"publishedOrCreatedDate":1598351859722},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: Roller-Skating Quadruped Has Best of Both Worlds Mobility","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here's what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"https://www.robobusiness.com/\">RoboBusiness 2019</a> –&nbsp;October 1-3, 2019 –&nbsp;Santa Clara, CA, USA</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today's videos.</p> \n<hr> \n <!--nextpage--> \n <p>We got a sneak peek of a new version of ANYmal equipped with actuated wheels for feet at the DARPA SubT Challenge, where it did surprisingly well at quickly and (mostly) robustly navigating some very tricky terrain. And when you're not expecting it to travel through a muddy, rocky, and dark tunnel, it looks even more capable:</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ukY0vyM-yfY\" width=\"620\"></iframe></p> \n <p></p> \n <p>[ <a href=\"https://mbjelonic.github.io/publications/files/2020_ral_bjelonic.pdf\">Paper</a> ]</p> \n <p><em>Thanks Marko!</em></p> \n <p></p> \n <hr> \n  <p></p> \n  <blockquote> \n   <p><em>In Langley’s makerspace lab, researchers are developing a series of soft robot actuators to investigate the viability of soft robotics in space exploration and assembly. By design, the actuator has chambers, or air bladders, that expand and compress based on the amount of air in them. </em></p> \n  </blockquote> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/iwQRYzLZvGE\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>[ <a href=\"https://www.nasa.gov/feature/langley/beyond-the-metal-investigating-soft-robots-at-nasa-langley/\">NASA</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <p>I’m not normally a fan of the AdultSize RoboCup soccer competition, but NimbRo had a very impressive season.</p> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ITe-seb4PsA\" width=\"620\"></iframe></p> \n   <p></p> \n   <p>I don’t know how it managed to not fall over at 45 seconds, but damn.</p> \n   <p>[ <a href=\"http://nimbro.net/Humanoid/\">NimbRo</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <p>This is more AI than robotics, but that’s okay, because it’s totally cool.</p> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/kopoLzvh5jY\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>I’m wondering whether the hiders ever tried another possibly effective strategy: trapping the seekers in a locked shelter right at the start.</p> \n    <p>[ <a href=\"https://openai.com/blog/emergent-tool-use/\">OpenAI</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>We haven’t heard much from Piaggio Fast Forward in a while, but evidently they’ve still got a Gita robot going on, designed to be your personal autonomous caddy for absolutely anything that can fit into something the size of a portable cooler.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/GC_R-AYu638\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>Available this fall, I guess?</p> \n     <p>[ <a href=\"https://mygita.com/#/meet-gita\">Gita</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>This passively triggered robotic hand is startlingly fast, and seems almost predatory when it grabs stuff, especially once they fit it onto a drone.</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/dHObllrM7yU\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://www.newdexterity.org/aerialgrasping/\">New Dexterity</a> ]</p> \n      <p><em>Thanks Fan!</em></p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>Autonomous vehicles seem like a recent thing, but CMU has been working on them since the mid 1980s.</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/N49_CmjbcQ8\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>CMU was also working on drones back before drones were even really a thing:</p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"https://www.youtube.com/embed/YQKv11faAsM\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>[ <a href=\"https://www.cs.cmu.edu/afs/cs/project/alv/www/#\">CMU NavLab</a> ] and&nbsp;<span>[&nbsp;</span><a href=\"http://www.cs.cmu.edu/afs/cs/project/chopper/www/\" style=\"font-family: Georgia, serif; font-size: 18px;\">CMU</a><span>&nbsp;]</span></p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>Welcome to the most complicated and expensive robotic ice cream deployment system ever created.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/xp5NIJ9kwME\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://niska.com.au/\">Niska</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <p>Some impressive dexterity from a robot hand equipped with magnetic gears.</p> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/v470C9L3Szo\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"http://www.k2.t.u-tokyo.ac.jp/fusion/MagLinkage_hand/index-e.html\">Ishikawa Senoo Lab</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>The Buddy Arduino social robot kit is now live on Kickstarter, and you can pledge for one of these little dudes for 49 bucks.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/wXdw0GmyGAg\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"https://www.kickstarter.com/projects/slantrobotics/littlebot-buddy\">Kickstarter</a> ]</p> \n          <p><em>Thanks Jenny!</em></p> \n          <p></p> \n          <hr> \n           <p></p> \n           <blockquote> \n            <p><em>Mobile manipulation robots have high potential to support rescue forces in disaster-response missions. Despite the difficulties imposed by real-world scenarios, robots are promising to perform mission tasks from a safe distance. In the CENTAURO project, we developed a disaster-response system which consists of the highly flexible Centauro robot and suitable control interfaces including an immersive telepresence suit and support-operator controls on different levels of autonomy.</em></p> \n           </blockquote> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/-fzAotHXKXk\" width=\"620\"></iframe></p> \n           <p></p> \n           <p>[ <a href=\"http://www.ais.uni-bonn.de/videos/RAM_2019_Centauro/\">CENTAURO</a> ]</p> \n           <p><em>Thanks Sven!</em></p> \n           <p></p> \n           <hr> \n            <p></p> \n            <p>Determined robots are the cutest robots.</p> \n            <p></p> \n            <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/nDRBKb4AGmA\" width=\"620\"></iframe></p> \n            <p></p> \n            <p>[ <a href=\"https://arxiv.org/abs/1909.06933\">Paper</a> ]</p> \n            <p></p> \n            <hr> \n             <p></p> \n             <blockquote> \n              <p><em>The goal of the Dronument project is to create an aerial platform enabling interior and exterior documentation of heritage sites.</em></p> \n             </blockquote> \n             <p></p> \n             <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/R_kxVm_KY_Q\" width=\"620\"></iframe></p> \n             <p></p> \n             <p>It’s got a base station that helps with localization, but still, flying that close to a chandelier in a UNESCO world heritage site makes me nervous.</p> \n             <p>[ <a href=\"http://mrs.felk.cvut.cz/research/historical-monuments-documentation\">Dronument</a> ]</p> \n             <p><em>Thanks Fan!</em></p> \n             <p></p> \n             <hr> \n              <p></p> \n              <blockquote> \n               <p><em>Avast ye! No hornswaggling, lick-spittlering, or run-rigging over here - Only serious tech for devs. All hands hoay to check out Misty's capabilities and to build your own skills with plenty of heave ho! ARRRRRRRRGH...</em></p> \n              </blockquote> \n              <p></p> \n              <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/A_efoAxp38A\" width=\"620\"></iframe></p> \n              <p></p> \n              <p>International Talk Like a Pirate Day was yesterday, but I'm sure nobody will look at you funny if you keep at it today too.</p> \n              <p>[ <a href=\"https://www.mistyrobotics.com/\">Misty Robotics</a> ]</p> \n              <p></p> \n              <hr> \n               <p></p> \n               <blockquote> \n                <p><em>This video presents an unobtrusive bimanual teleoperation setup with very low weight, consisting of two Vive visual motion trackers and two Myo surface electromyography bracelets. The video demonstrates complex, dexterous teleoperated bimanual daily-living tasks performed by the torque-controlled humanoid robot TORO.</em></p> \n               </blockquote> \n               <p></p> \n               <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/M6mQWcLAiko\" width=\"620\"></iframe></p> \n               <p></p> \n               <p>[ <a href=\"https://www.dlr.de/rmc/en/desktopdefault.aspx/tabid-11942/#gallery/28863\">DLR RMC</a> ]</p> \n               <p></p> \n               <hr> \n                <p></p> \n                <p>Lex Fridman interviews iRobot’s Colin Angle on the Artificial Intelligence Podcast.</p> \n                <p></p> \n                <p></p> \n                <p></p> \n                <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/1d9Dj9dT_pw\" width=\"620\"></iframe></p> \n                <p></p> \n                <blockquote> \n                 <p><em>Colin Angle is the CEO and co-founder of iRobot, a robotics company that for 29 years has been creating robots that operate successfully in the real world, not as a demo or on a scale of dozens, but on a scale of thousands and millions. As of this year, iRobot has sold more than 25 million robots to consumers, including the Roomba vacuum cleaning robot, the Braava floor mopping robot, and soon the Terra lawn mowing robot. 25 million robots successfully operating autonomously in people's homes to me is an incredible accomplishment of science, engineering, logistics, and all kinds of entrepreneurial innovation.</em></p> \n                </blockquote> \n                <p>[ <a href=\"https://lexfridman.com/ai/\">AI Podcast</a> ]</p> \n                <p></p> \n                <hr> \n                 <p></p> \n                 <p>This week’s CMU RI Seminar comes from CMU’s own Sarah Bergbreiter, on Microsystems-Inspired Robotics.</p> \n                 <p></p> \n                 <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/osEIQ-fgaig\" width=\"620\"></iframe></p> \n                 <p></p> \n                 <blockquote> \n                  <p><em>The ability to manufacture micro-scale sensors and actuators has inspired the robotics community for over 30 years. There have been huge success stories; MEMS inertial sensors have enabled an entire market of low-cost, small UAVs. However, the promise of ant-scale robots has largely failed. Ants can move high speeds on surfaces from picnic tables to front lawns, but the few legged microrobots that have walked have done so at slow speeds (&lt; 1 body length/sec) on smooth silicon wafers. In addition, the vision of large numbers of microfabricated sensors interacting directly with the environment has suffered in part due to the brittle materials used in micro-fabrication. This talk will present our progress in the design of sensors, mechanisms, and actuators that utilize new microfabrication processes to incorporate materials with widely varying moduli and functionality to achieve more robustness, dynamic range, and complexity in smaller packages.</em></p> \n                 </blockquote> \n                 <p>[ <a href=\"https://www.ri.cmu.edu/event/ri-seminar-sarah-bergbreiter-carnegie-mellon-university-professor-2019-09-13/\">CMU RI</a> ]</p> \n                 <p></p> \n                 <hr> \n                  <p></p> \n                  <div class=\"feedflare\"> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=lq8k4y7KjVg:plxvSkQegxk:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=lq8k4y7KjVg:plxvSkQegxk:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=lq8k4y7KjVg:plxvSkQegxk:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=lq8k4y7KjVg:plxvSkQegxk:-BTjWOF_DHI\" border=\"0\" /></a> \n                   <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=lq8k4y7KjVg:plxvSkQegxk:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=lq8k4y7KjVg:plxvSkQegxk:gIN9vFwOqvQ\" border=\"0\" /></a> \n                  </div> \n                  <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/lq8k4y7KjVg\" height=\"1\" width=\"1\" alt=\"\" /> \n                 </hr> \n                </hr> \n               </hr> \n              </hr> \n             </hr> \n            </hr> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 20 Sep 2019 20:28:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM3ODY1Ng.jpeg","linkMd5":"942b7fea25553a3c98fd6f49daf1b0f2","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn5@2020_2/2020/08/25/10-37-58-973_83c8f686fddb633f.webp","destWidth":1240,"destHeight":930,"sourceBytes":207195,"destBytes":121966,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=lq8k4y7KjVg:plxvSkQegxk:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn4@2020_4/2020/08/25/10-38-03-822_c409fb8261a2fe4a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=lq8k4y7KjVg:plxvSkQegxk:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn15@2020_1/2020/08/25/10-38-10-175_6404a8bba9844e4a.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/lq8k4y7KjVg":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn80@2020_3/2020/08/25/10-38-04-108_43fb59dc1286696a.webp"},"publishedOrCreatedDate":1598351859732},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Video Friday: AlienGo Quadruped Robot Can Now Do Backflips","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot","description":"<p>Video Friday is your weekly selection of awesome robotics videos, collected by your Automaton bloggers. We’ll also be posting a weekly calendar of upcoming robotics events for the next few months; here’s what we have so far (<a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday\">send us your events</a>!):</p> \n<h5><a href=\"https://clawar.org/clawar2019/\">CLAWAR 2019</a> –&nbsp;August 26-28, 2019 –&nbsp;Kuala Lumpur, Malaysia</h5> \n<h5><a href=\"http://africon2019.org/\">IEEE Africon 2019</a> –&nbsp;September 25-27, 2019 –&nbsp;Accra, Ghana</h5> \n<h5><a href=\"http://www.isrr2019.org/\">ISRR 2019</a> –&nbsp;October 6-10, 2019 –&nbsp;Hanoi, Vietnam</h5> \n<h5><a href=\"https://ro-man2019.org/\">Ro-Man 2019</a> –&nbsp;October 14-18, 2019 –&nbsp;New Delhi, India</h5> \n<h5><a href=\"http://humanoids2019.loria.fr/\">Humanoids 2019</a> –&nbsp;October 15-17, 2019 –&nbsp;Toronto, Canada</h5> \n<h5><a href=\"http://arso2019.csp.escience.cn/dct/page/1\">ARSO 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Beijing, China</h5> \n<h5><a href=\"https://roscon.ros.org/2019/\">ROSCon 2019</a> –&nbsp;October 31-1, 2019 –&nbsp;Macau</h5> \n<h5><a href=\"https://www.iros2019.org/\">IROS 2019</a> –&nbsp;November 4-8, 2019 –&nbsp;Macau</h5> \n<p><a href=\"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday\">Let us know</a> if you have suggestions for next week, and enjoy today’s videos.</p> \n<hr> \n <!--nextpage--> \n <p>I know you’ve all been closely following our <a href=\"https://spectrum.ieee.org/tag/subt\">DARPA Subterranean Challenge coverage</a> here and on Twitter, but here are short recap videos of each day just in case you missed something.</p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SFb1wBwnP0Q\" width=\"620\"></iframe></p> \n <p></p> \n <p></p> \n <p></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/fzehAHQzFGY?rel=0\" width=\"620\"></iframe></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/bJXirJ5qluw?rel=0\" width=\"620\"></iframe></p> \n <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/sBujR2XJSSY?rel=0\" width=\"620\"></iframe>[ <a href=\"https://www.subtchallenge.com/\">DARPA SubT</a> ]</p> \n <p></p> \n <hr> \n  <p></p> \n  <p>After <a href=\"https://robots.ieee.org/robots/laikago/?utm_source=spectrum\">Laikago</a>, Unitree Robotics is now introducing&nbsp;AlienGo, which&nbsp;is looking mighty spry:</p> \n  <p></p> \n  <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/Ayxoji2xg3k\" width=\"620\"></iframe></p> \n  <p></p> \n  <p>We’ve seen MIT’s <a href=\"https://robots.ieee.org/robots/minicheetah/?utm_source=spectrum\">Mini Cheetah</a> <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research\">doing backflips earlier this year</a>, but apparently AlienGo&nbsp;is now the largest and heaviest quadruped to perform the maneuver.</p> \n  <p>[ <a href=\"http://www.unitree.cc/\">Unitree</a> ]</p> \n  <p></p> \n  <hr> \n   <p></p> \n   <blockquote> \n    <p><em>The majority of soft robots today rely on external power and control, keeping them tethered to off-board systems or rigged with hard components. Now, researchers from the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) and Caltech have developed soft robotic systems, inspired by origami, that can move and change shape in response to external stimuli, paving the way for fully untethered soft robots.</em></p> \n   </blockquote> \n   <p></p> \n   <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/uapMIUHUeGk\" width=\"620\"></iframe></p> \n   <p></p> \n   <blockquote> \n    <p><em>The Rollbot begins as a flat sheet, about 8 centimeters long and 4 centimeters wide. When placed on a hot surface, about 200°C, one set of hinges folds and the robot curls into a pentagonal wheel. </em></p> \n    <p><em>Another set of hinges is embedded on each of the five sides of the wheel. A hinge folds when in contact with the hot surface, propelling the wheel to turn to the next side, where the next hinge folds. As they roll off the hot surface, the hinges unfold and are ready for the next cycle.</em></p> \n   </blockquote> \n   <p>[ <a href=\"https://www.seas.harvard.edu/news/2019/08/self-folding-rollbot-paves-way-for-fully-untethered-soft-robots\">Harvard SEAS</a> ]</p> \n   <p></p> \n   <hr> \n    <p></p> \n    <blockquote> \n     <p><em>A new research effort at Caltech aims to help people walk again by combining exoskeletons with spinal stimulation. This initiative, dubbed RoAM (Robotic Assisted Mobility), combines the research of two Caltech roboticists: Aaron Ames, who creates the algorithms that enable walking by bipedal robots and translates these to govern the motion of exoskeletons and prostheses; and Joel Burdick, whose transcutaneous spinal implants have already helped paraplegics in clinical trials to recover some leg function and, crucially, torso control.</em></p> \n    </blockquote> \n    <p></p> \n    <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/ggDvBOQffIM\" width=\"620\"></iframe></p> \n    <p></p> \n    <p>[ <a href=\"https://www.caltech.edu/about/news/promising-step-returning-bipedal-mobility\">Caltech</a> ]</p> \n    <p></p> \n    <hr> \n     <p></p> \n     <p>Once ExoMars lands, it’s going to have to get itself off of the descent stage and onto the surface, which could be tricky. But practice makes perfect, or as near as you can get on Earth.</p> \n     <p></p> \n     <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/BNItE7zjhq8\" width=\"620\"></iframe></p> \n     <p></p> \n     <p>That wheel walking technique is pretty cool, and it looks like ExoMars will be able to handle terrain that would scare NASA’s Mars rovers away.</p> \n     <p>[ <a href=\"http://www.esa.int/Our_Activities/Human_and_Robotic_Exploration/Exploration/ExoMars\">ExoMars</a> ]</p> \n     <p></p> \n     <hr> \n      <p></p> \n      <p>I am honestly not sure whether this would make the game of golf more or less fun to watch:</p> \n      <p></p> \n      <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/AtyWaUeu-_Q\" width=\"620\"></iframe></p> \n      <p></p> \n      <p>[ <a href=\"https://www.nissan.co.jp/BRAND/TFL/PPG/\">Nissan</a> ]</p> \n      <p></p> \n      <hr> \n       <p></p> \n       <p>Finally, a really exciting use case for Misty!</p> \n       <p></p> \n       <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/8LadqGIZgcc\" width=\"620\"></iframe></p> \n       <p></p> \n       <p>It can pick up those balls too, right?</p> \n       <p>[ <a href=\"https://www.mistyrobotics.com/robots/\">Misty</a> ]</p> \n       <p></p> \n       <hr> \n        <p></p> \n        <p>You know you’re an actual robot if this video doesn’t make you crave Peeps.</p> \n        <p></p> \n        <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/0-2B1Vn75Qg\" width=\"620\"></iframe></p> \n        <p></p> \n        <p>[ <a href=\"https://www.softroboticsinc.com/\">Soft Robotics</a> ]</p> \n        <p></p> \n        <hr> \n         <p></p> \n         <blockquote> \n          <p><em>COMANOID investigates the deployment of robotic solutions in well-identified Airbus airliner assembly operations that are tedious for human workers and for which access is impossible for wheeled or rail-ported robotic platforms. This video presents a demonstration of autonomous placement of a part inside the aircraft fuselage. The task is performed by <a href=\"https://robots.ieee.org/robots/toro/?utm_source=spectrum\">TORO</a>, the torque-controlled humanoid robot developed at DLR.</em></p> \n         </blockquote> \n         <p></p> \n         <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/pXeOjuff8CQ\" width=\"620\"></iframe></p> \n         <p></p> \n         <p>[ <a href=\"https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11853/\">COMANOID</a> ]</p> \n         <p></p> \n         <hr> \n          <p></p> \n          <p>It’s a little hard to see in this video, but this is a cable-suspended robot arm that has little tiny robot arms that it waves around to help damp down vibrations.</p> \n          <p></p> \n          <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/tN5UPqr268o\" width=\"620\"></iframe></p> \n          <p></p> \n          <p>[ <a href=\"http://www.lirmm.fr/cogiro/\">CoGiRo </a> ]</p> \n          <p></p> \n          <p></p> \n          <p></p> \n          <hr> \n           <p></p> \n           <p>This week in Robots in Depth, Per speaks with author Cristina Andersson.</p> \n           <p></p> \n           <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/SFvIM73dHZw\" width=\"620\"></iframe></p> \n           <p></p> \n           <blockquote> \n            <p><em>In 2013 she organized events in Finland during European robotics week and found that many people was very interested but that there was also a big lack of knowledge.</em></p> \n            <p><em>She also talks about introducing robotics in society in a way that makes it easy for everyone to understand the benefits as this will make the process much easier. When people see the clear benefits in one field or situation they will be much more interested in bringing robotics in to their private or professional lives.</em></p> \n           </blockquote> \n           <p>[ <a href=\"http://robotsindepth.com/\">Robots in Depth</a> ]</p> \n           <p></p> \n           <hr> \n            <p></p> \n            <div class=\"feedflare\"> \n             <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VKQk2CUaQQk:wd4uXOXdla8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n             <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VKQk2CUaQQk:wd4uXOXdla8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n             <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VKQk2CUaQQk:wd4uXOXdla8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VKQk2CUaQQk:wd4uXOXdla8:-BTjWOF_DHI\" border=\"0\" /></a> \n             <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=VKQk2CUaQQk:wd4uXOXdla8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VKQk2CUaQQk:wd4uXOXdla8:gIN9vFwOqvQ\" border=\"0\" /></a> \n            </div> \n            <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VKQk2CUaQQk\" height=\"1\" width=\"1\" alt=\"\" /> \n           </hr> \n          </hr> \n         </hr> \n        </hr> \n       </hr> \n      </hr> \n     </hr> \n    </hr> \n   </hr> \n  </hr> \n </hr> \n</hr>","descriptionType":"html","publishedDate":"Fri, 23 Aug 2019 14:15:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MDk2NA.jpeg","linkMd5":"19aafea32608c470cfdef56d8c5fc346","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn56@2020_4/2020/08/25/10-38-00-214_aad665bc80a92e1e.webp","destWidth":1095,"destHeight":637,"sourceBytes":265276,"destBytes":209214,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VKQk2CUaQQk:wd4uXOXdla8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn96@2020_3/2020/08/25/10-38-07-453_2e7d02a20e263ccf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VKQk2CUaQQk:wd4uXOXdla8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn31@2020_2/2020/08/25/10-38-09-476_e7d6fb1532fa721c.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VKQk2CUaQQk":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn45@2020_1/2020/08/25/10-38-15-910_8db0d8302efd835e.webp"},"publishedOrCreatedDate":1598351859740},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Parrot Adds Folding VR Goggles to Anafi Drone Kit","link":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","description":"<p>We've been fans of <a href=\"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone\">Parrot’s Anafi drone</a> since its release just over a year ago. In a space dominated by DJI, the Anafi’s thoughtful and considerate design means that it’s the drone I usually prefer to fly rather than my Mavic Pro. The Anafi is small, quiet, compact, takes great pictures, and since it uses the same USB-C charger as my laptop and my phone, it’s easy to travel with.</p> \n<p>Over the last year, Parrot has been making consistent (if minor) upgrades to the Anafi’s software, as well as some subtle hardware upgrades that improve both ruggedness and efficiency. And this week, it’s&nbsp;announcing <a href=\"https://www.parrot.com/us/drones/anafi-fpv\">a new pair of foldable goggles</a> that add a first-person-view flying experience that’s easy to take with you.</p> \n<!--nextpage--> \n<p>If you haven’t tried flying a drone through a headset before, it’s a unique experience. Some might even say magical. I wouldn’t say that, because I’m a bitter and jaded tech reporter, but it’s seriously a lot of fun.</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/it9NnCldrVA?rel=0\" width=\"620\"></iframe></p> \n<p>Parrot’s Cockpitglasses 3 (Parrot is still terrible at naming things) work a bit like Google’s Daydream headset, in that it uses your phone instead of a dedicated screen. The nice thing about relying on phones for this is when you get a better phone, you get a goggles upgrade as well, although the quality you get won’t be on par with (much more expensive) FPV systems with built-in screens. The headset has two buttons on it which aren't electronic, but instead are hinged to tap on different places on your phone's screen when you push them to send commands through to the app. When the goggles aren't in use, they fold down nearly flat, which is a neat trick for something that looks like it's probably somewhat comfortable to wear.</p> \n<figure class=\"rt med\" role=\"img\"> \n <img alt=\"Image of the glasses on a white background. \" src=\"https://spectrum.ieee.org/image/MzM2NTcwOA.png\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Parrot \n  </figcaption> </img> \n</figure> \n<blockquote> \n <p><em>The main FPV HUD shows contextual information, such as flight speed, direction, altitude, and drone location. But with the click of a button on top of the Cockpitglasses, you can easily swap to a minimal HUD interface, so you can fully immerse yourself in&nbsp;the FPV experience. You can even find your Anafi in mid-air without removing the Cockpit glasses: Switch to See-Through View, and you’ll see live video from your smartphone camera. An overlay shows the exact position of your drone -- and if your Anafi is off-screen, an icon points to its location.</em></p> \n</blockquote> \n<p>Parrot's Freeflight 6 software is also seeing some upgrades with the release of the FPV kit, on top of additions that have been made over the past year&nbsp;such as live histogram and zebra stripes for exposure adjustments, and automated little planet and tunnel shots. Brand new are two flight presets: cinematic, which locks the roll axis to record video like you're flying smoothly through the air;&nbsp;and racing, which does the same thing except in a more aggressive sport mode.</p> \n<figure class=\"lt med\" role=\"img\"> \n <img alt=\"Image of the Parrot drone set, including glasses, drone, backpack, and controller.\" src=\"https://spectrum.ieee.org/image/MzM2NTcxNQ.png\"> \n  <figcaption class=\"hi-cap\">\n    Photo: Parrot \n  </figcaption> </img> \n</figure> \n<p>While Parrot seems to be advertising the Anafi FPV as a new drone in places, it’s really the same Anafi that’s been available for the past year. There have been some incremental upgrades, though. The most noticeable is likely that the weight of the controller has been reduced by 30 percent, almost certainly by removing the interior balancing weight blocks, something that's trivial to do yourself. The drone has been lightened a bit (down to 310 grams), had its arms streamlined and reinforced, and configured to fold&nbsp;up into a slightly smaller package. It’s nothing crazy, but with these small optimizations, Parrot has managed to tease out an entire extra minute of flight time (now 26 minutes)—impressive for a drone this size.</p> \n<p>The <a href=\"https://www.parrot.com/us/drones/anafi-fpv\">Parrot FPV kit</a>, including the custom backpack that doubles as a launch pad, will run you US $800 when it's available in “early September.” That’s&nbsp;$100 more than the current MSRP of the drone and controller by themselves. It's not a bad deal, especially if you want to try out FPV flight for the first time. Parrot says that for current Anafi owners, a software update will enable FPV mode, and we'd expect that the goggles will be available to add to your kit in the near future.</p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=68kvbO8JlEQ:reuaV7O4PNQ:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=68kvbO8JlEQ:reuaV7O4PNQ:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=68kvbO8JlEQ:reuaV7O4PNQ:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=68kvbO8JlEQ:reuaV7O4PNQ:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=68kvbO8JlEQ:reuaV7O4PNQ:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=68kvbO8JlEQ:reuaV7O4PNQ:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/68kvbO8JlEQ\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 03 Sep 2019 20:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2NTc2NA.jpeg","linkMd5":"ec21b6293bbcb368dd9079b4df431fd8","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn65@2020_2/2020/08/25/10-37-59-026_934227c26d565a28.webp","destWidth":1240,"destHeight":930,"sourceBytes":325864,"destBytes":307400,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM2NTcwOA.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn67@2020_2/2020/08/25/10-38-07-375_86aa1e1729154648.webp","https://spectrum.ieee.org/image/MzM2NTcxNQ.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn67@2020_2/2020/08/25/10-38-10-507_3d095a67f94d6e30.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=68kvbO8JlEQ:reuaV7O4PNQ:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn92@2020_2/2020/08/25/10-38-07-527_8c198de509188e7f.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=68kvbO8JlEQ:reuaV7O4PNQ:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn36@2020_1/2020/08/25/10-38-06-244_ce176dddb9090f83.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/68kvbO8JlEQ":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn48@2020_6/2020/08/25/10-38-09-482_faa2cef02239d5f7.webp"},"publishedOrCreatedDate":1598351859734},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Construction Robots Learn to Excavate by Mimicking Humans","link":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans","description":"<p>Pavel Savkin remembers the first time he watched a robot imitate his movements. Minutes earlier, the engineer had finished “showing” the robotic excavator its new goal by directing its movements manually. Now, running on software Savkin helped design, the robot was reproducing his movements, gesture for gesture. “It was like there was something alive in there—but I knew it was me,” he said.</p> \n<p><a href=\"https://www.linkedin.com/in/pavel-savkin-52ba2b94/\">Savkin</a> is the CTO of SE4, a&nbsp;<a href=\"https://www.se4.space/\">robotics software project</a>&nbsp;that styles itself the “driver” of a fleet of robots that will eventually build human colonies in space. For now, SE4 is focused on creating software that can help developers communicate with robots, rather than on building hardware of its own.<br> <br> The Tokyo-based startup showed off an industrial arm from&nbsp;<a href=\"https://www.universal-robots.com/\">Universal Robots</a>&nbsp;that was running SE4’s proprietary software at SIGGRAPH in July. SE4’s demonstration at the Los Angeles&nbsp;<a href=\"https://s2019.siggraph.org/\">innovation conference</a>&nbsp;drew the company’s largest audience yet. The robot, nicknamed Squeezie, stacked real blocks as directed by SE4 research engineer Nathan Quinn, who wore a VR headset and used handheld controls to “show” Squeezie what to do.&nbsp;</br></br></p> \n<p></p> \n<p></p> \n<p><iframe allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/YjZSGN2OypA\" width=\"560\"></iframe></p> \n<p></p> \n<p>As Quinn manipulated blocks in a virtual 3D space, the software learned a set of ordered instructions to be carried out in the real world. That order is essential for remote operations, says Quinn. To build remotely, developers need a way to communicate instructions to robotic builders on location. In the age of <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robotic-construction-platform-creates-large-buildings-on-demand\">digital construction</a> and<a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robots-do-construction-with-brick-and-concrete\"> industrial robotics</a>, giving a computer a blueprint for what to build is a well-explored art. But operating on a distant object—especially under conditions that humans haven’t experienced themselves—presents challenges that only real-time communication with operators can solve.</p> \n<p>The problem is that, in an unpredictable setting, even simple tasks require not only instruction from an operator, but constant feedback from the changing environment. Five years ago, the Swedish fiber network provider <a href=\"http://umea.net\">umea.net</a> (part of the private Umeå Energy utility) took advantage of the virtual reality boom to promote its high-speed connections with the help of a viral video titled “<a href=\"http://livingwithlag.com/\">Living with Lag: An Oculus Rift Experiment</a>.” The video is still circulated in VR and gaming circles.&nbsp;</p> \n<p>In the experiment, volunteers donned headgear that replaced their real-time biological senses of sight and sound with camera and audio feeds of their surroundings—both set at a 3-second delay. Thus equipped, volunteers attempt to complete everyday tasks like playing ping-pong, dancing, cooking, and walking on a beach, with decidedly slapstick results.</p> \n<p>At outer-orbit intervals, including SE4’s dream of construction projects on Mars, the limiting factor in communication speed is not an artificial delay, but the laws of physics. The shifting relative positions of Earth and Mars mean that communications between the planets—even at the speed of light—can take anywhere from 3 to 22 minutes.</p> \n<p><strong>A long-distance relationship</strong></p> \n<p>Imagine trying to manage a construction project from across an ocean without the benefit of intelligent workers: sending a ship to an unknown world with a construction crew and blueprints for a log cabin, and four months later receiving a letter back asking how to cut down a tree. The parallel problem in long-distance construction with robots, according to SE4 <a href=\"https://www.linkedin.com/in/lochlainn-wilson-se4/\">CEO Lochlainn Wilson</a>, is that automation relies on predictability. “Every robot in an industrial setting today is expecting a controlled environment.”<br> <br> Platforms for applying AR and VR systems to teach tasks to artificial intelligences, as SE4 does, are already proliferating in manufacturing, healthcare, and defense. But all of the related communications systems are bound by physics and, specifically, the speed of light.<br> <br> The same fundamental limitation applies in space. “Our communications are light-based, whether they’re radio or optical,” says <a href=\"https://twitter.com/LauraForczyk?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\">Laura Seward Forczyk</a>, a planetary scientist and consultant for space startups. “If you’re going to Mars and you want to communicate with your robot or spacecraft there, you need to have it act semi- or mostly-independently so that it can operate without commands from Earth.”</br></br></br></br></p> \n<p><strong>Semantic control</strong><br> <br> That’s exactly what SE4 aims to do. By teaching robots to group micro-movements into logical units—like all the steps to building a tower of blocks—the Tokyo-based startup lets robots make simple relational judgments that would allow them to receive a full set of instruction modules at once and carry them out in order. This sidesteps the latency issue in real-time bilateral communications that could hamstring a project or at least make progress excruciatingly slow.<br> <br> The key to the platform, says Wilson, is the team’s proprietary operating software, “Semantic Control.” Just as in linguistics and philosophy, “semantics” refers to meaning itself, and meaning is the key to a robot’s ability to make even the smallest decisions on its own. “A robot can scan its environment and give [raw data] to us, but it can’t necessarily identify the objects around it and what they mean,” says Wilson.</br></br></br></br></p> \n<p>That’s where human intelligence comes in. As part of the demonstration phase, the human operator of an SE4-controlled machine “annotates” each object in the robot’s vicinity with meaning. By labeling objects in the VR space with useful information—like which objects are building material and which are rocks—the operator helps the robot make sense of its <a href=\"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/interactive-simulation-teaches-ai-about-real-world\">real 3D environment</a> before the building begins.&nbsp;</p> \n<p>Giving robots&nbsp;<a href=\"https://www.semanticscholar.org/paper/Cognitive-Architecture-Requirements-for-Achieving-Laird-Wray/d9770968eb25b5e98f3801d9e0890f7b81d922c2\">the tools to deal with a changing environment</a>&nbsp;is an important step toward allowing the AI to be truly independent, but it’s only an initial step. “We’re not letting it do absolutely everything,” said Quinn. “Our robot is good at moving an object from point A to point B, but it doesn’t know the overall plan.”&nbsp;Wilson adds that delegating environmental awareness and raw mechanical power to separate agents is the optimal relationship for a mixed human-robot construction team; it “lets humans do what they’re good at, while robots do what they do best.”</p> \n<p><em>This story was updated on 4 September 2019.&nbsp;</em></p> \n<p></p> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=6emqUxmAG9w:BT7v-3tNUf0:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=6emqUxmAG9w:BT7v-3tNUf0:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=6emqUxmAG9w:BT7v-3tNUf0:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=6emqUxmAG9w:BT7v-3tNUf0:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=6emqUxmAG9w:BT7v-3tNUf0:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=6emqUxmAG9w:BT7v-3tNUf0:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/6emqUxmAG9w\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 03 Sep 2019 16:00:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2NTM3MA.jpeg","linkMd5":"05ca9649ab0849af31ee29f9a1f2318f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn22@2020_4/2020/08/25/10-38-02-502_e5f73080b15d2ff6.webp","destWidth":1240,"destHeight":930,"sourceBytes":227598,"destBytes":118158,"author":"Lynne Peskoe-Yang","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=6emqUxmAG9w:BT7v-3tNUf0:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn2@2020_2/2020/08/25/10-38-14-453_3f1bfb389f3c3a25.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=6emqUxmAG9w:BT7v-3tNUf0:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn8@2020_2/2020/08/25/10-38-02-948_17c1fd05640a2f2e.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/6emqUxmAG9w":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn63@2020_2/2020/08/25/10-38-05-695_59b7d2f41cd8b2d8.webp"},"publishedOrCreatedDate":1598351859734},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"Harvard's UrchinBot Is One of the Weirdest Looking Robots We've Ever Seen","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot","description":"<p>On the spectrum of weird stuff that can be found in the ocean, <a href=\"https://en.wikipedia.org/wiki/Sea_urchin\">sea urchins</a> are probably somewhere in the middle. They’re an interesting combination of rigid and flexible, with shells covered in hard movable spines as well as soft tubular appendages that work like a combination of legs and sticky feet. The mobility strategy of sea urchins leverages both of these appendages, and while they may not be speedy, they can get themselves into all kinds of potentially useful nooks and crannies, which seems like a capability that could be valuable in a robot.</p> \n<p>At <a href=\"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau\">IROS 2019</a> this week, roboticists from Harvard presented a bioinspired robot that “incorporates anatomical features unique to sea urchins,” actuated by pneumatics or hydraulics and operating without a tether. It may be based on a real animal, but even so, UrchinBot is definitely one of the weirdest looking robots we’ve ever seen.</p> \n<p>As it turns out, adult sea urchins are complicated critters, and making a robotic version of one of them was asking a bit much. Juvenile sea urchins incorporate the same basic features in a much simpler body, and while they’re only 0.5 millimeters in size, a scaled-up version (with a body 230 mm in diameter) was much more feasible.</p> \n<p>Just like the adults, sea urchin babies have two mobility appendages: movable spines, and sticky tube feet. The physical resemblance is striking, but it’s much more than just aesthetics, as the researchers emphasize that “particular attention was paid to accurately replicating the geometry and ranges of motion of the anatomical features on which our design was based.”</p> \n<p>UrchinBot’s spines (which the real animal uses for protection, mobility, and to jam itself into crevices) reflect the two different kinds of spines that you see on juvenile urchins. Nobody’s quite sure why the babies have fancier spines than the adults, but UrchinBot replicates that detail too. Each spine is connected to the body with a ball joint, and a triangle of three pneumatic domes around the joint can inflate to push the spine in different directions. All of the domes are interconnected inside of the robot which means both that the spines can’t be actuated separately and that you get a satisfyingly symmetric rotational motion whenever the spines move. As they rotate against a surface that UrchinBot is resting on, the robot slowly turns itself in the opposite direction.</p> \n<p>The tube feet are a little more complicated, because real urchins excrete sticky stuff that they use to glue themselves to surfaces, and then excrete an enzyme that dissolves the glue when they want to move. UrchinBot instead uses extendable and retractable toe magnets, which work perfectly well as long as the robot is moving on a ferrous surface. As the tube feet inflate, they move outward and angle their tips down, and with enough pressure, the toe magnets pop out and adhere. UrchinBot then reverses its hydraulics to suck the tube foot back in, pulling itself towards the adhesion point, and causing the magnet to pop off again once it gets there.</p> \n<p>The rest of UrchinBot’s body is taken up with pumps, valves, and electronics that allows it to operate completely untethered, both on land and underwater. Here it is in action:</p> \n<p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"349\" src=\"//www.youtube.com/embed/08pdrYBPqHU\" width=\"620\"></iframe></p> \n<p>It turns out that UrchinBot’s spines exhibit a range of motion similar to that of an actual urchin, which is neat. The tube feet can achieve an extension ratio of 6:1, which is reasonably close to a juvenile urchin’s 10:1 ratio, but much less than an adult urchin, which can extend its tube feet out to a 50:1 ratio. UrchinBot is not as fast as the real thing, which is to be expected with most bioinspired robots. Top speed is 6 mm/s, or 0.027 body-lengths per second, quite a bit slower than a juvenile urchin (which can hit 10 <span>body-lengths per&nbsp;</span>second going flat out) but only half as fast as an adult urchin.</p> \n<p>UrchinBot may not be the speediest robot under the sea, but the researchers say that it could be useful for underwater cleaning and inspection applications, especially in situations where heavy fouling would be a challenge for more conventional robots. The priority for UrchinBot upgrades is to stuff it with as many extra actuators as it’ll hold, with the goal of making the spines actuate individually and giving the tube feet extra degrees of freedom. While UrchinBot may not find&nbsp;near-term applications, it serves as a testbed to help researchers identify physical features and control techniques that could result in new types of more versatile and effective underwater robots.</p> \n<h4>“Design, Fabrication, and Characterization of an Untethered Amphibious Sea Urchin-Inspired Robot,” by Thibaut Paschal, Michael A. Bell, Jakob Sperry, Satchel Sieniewicz, Robert J. Wood, and James C. Weaver from Harvard’s Wyss Institute, was presented this week at IROS 2019 in Macau.</h4> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ItofgVBt2wM:kSA5pRDC0WQ:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ItofgVBt2wM:kSA5pRDC0WQ:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ItofgVBt2wM:kSA5pRDC0WQ:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ItofgVBt2wM:kSA5pRDC0WQ:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=ItofgVBt2wM:kSA5pRDC0WQ:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ItofgVBt2wM:kSA5pRDC0WQ:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ItofgVBt2wM\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Tue, 05 Nov 2019 20:15:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzQwNjIxMw.jpeg","linkMd5":"0c575b7593d53cada9ce6cb4a5dbad5f","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn65@2020_4/2020/08/25/10-38-01-910_9ec07841a91a12b9.webp","destWidth":1240,"destHeight":1074,"sourceBytes":204914,"destBytes":50336,"author":"Evan Ackerman","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ItofgVBt2wM:kSA5pRDC0WQ:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn41@2020_6/2020/08/25/10-38-12-230_50e1a9c616360ecb.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ItofgVBt2wM:kSA5pRDC0WQ:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn33@2020_4/2020/08/25/10-38-13-047_3435be41f8c55f3d.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ItofgVBt2wM":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn31@2020_5/2020/08/25/10-38-03-866_f23c9c6705da6248.webp"},"publishedOrCreatedDate":1598351859722},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"DARPA Subterranean Challenge: The Scoring Rules","link":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","description":"<p>We’re in the middle of the DARPA Subterranean Challenge (today is the third of four days of scored runs). We're also literally in the middle of things because today’s the one day that media are allowed on site. For the past few days, we've just been following along with <a href=\"https://www.youtube.com/user/DARPAtv\">DARPA's live stream of the event on YouTube</a> as well as <a href=\"https://twitter.com/hashtag/SubTChallenge\">#SubTChallenge on Twitter</a>.&nbsp;But all they’ve shown&nbsp;is robots going into the mine and (sometimes) coming out again. Or falling over.</p> \n<p>We do get to see the scores at the end of the day, but&nbsp;most of the time, it’s not clear what's going on inside the mine that results in a robot scoring a point for its team. So, let's take a look at what the robots are actually doing in there, and what it takes to score during SubT.</p> \n<!--nextpage--> \n<p>Here are the basics on SubT Tunnel Circuit scoring:</p> \n<blockquote> \n <p><em>The NIOSH mine will be separated into two different competition courses: “Safety Research” and “Experimental.” Teams will have up to four (4) scored runs through the Tunnel Circuit event, two (2) on each course.</em></p> \n <p><em>For the Systems Teams, the final ranking in the Tunnel Circuit will be determined based on the sum of a team’s top score on each competition course. The highest scoring run on the Safety Research course will be added to the highest scoring run on the Experimental course.</em></p> \n</blockquote> \n<p>Each of those runs is completely independent, in that teams aren't allowed to use, say,&nbsp;mapping data that they collected from a previous run on the same course. And DARPA is making things even more difficult by altering the courses between runs—which is what the organizers spent all day yesterday doing while the teams took a day to work on their robots.</p> \n<h2>Artifacts</h2> \n<p>The scoring itself is actually pretty simple.&nbsp;You can think of it like an underground scavenger hunt:</p> \n<blockquote> \n <p><em>The main scoring objective is the need to search for, detect, and provide spatially referenced locations of artifacts relevant to each of the three subdomains. These artifacts could vary in their size, quantity, and detection signatures (e.g., visual, thermal, chemical). Teams will know what to look for, but the locations and distribution of the artifacts within the course will not be known. It is expected that the number of artifacts will be in the range of 10-30 and multiple copies of each artifact type are possible. The total number of artifacts, but not the number of each type, will be disclosed to the competitors.</em></p> \n</blockquote> \n<p>For the Tunnel Circuit, teams are looking for a total of 20 artifacts, which will be some combination of the following:&nbsp;</p> \n<figure class=\"xlrg\" role=\"img\"> \n <img alt=\"SubT\" src=\"https://spectrum.ieee.org/image/MzM1NzQ0NA.jpeg\"> \n  <figcaption class=\"hi-cap\">\n    Photos: DARPA \n  </figcaption> \n  <figcaption>\n    The five types of artifacts hidden throughout the SubT Tunnel Circuit: cell phone, backpack, fire extinguisher, drill, and survivor \n  </figcaption> </img>\n</figure> \n<p>Teams have 60 minutes to find as many artifacts as they can, and transmit their types and locations in the form of three dimensional coordinates back to base (and to DARPA). The location that the robots report has to be correct within five meters of the actual location of the artifact, which is measured from the Localization Point you can see in the image above. These locations are all relative, with the origin point being based off of markers that are posted on the outside of the mine entrances. If a robot reports the correct artifact type in the correct location, the team earns a point. Incorrect reports don't cost anything, although teams are limited to 40 reports <span>per run&nbsp;</span>in total&nbsp;so that they don’t just report&nbsp;artifacts everywhere.</p> \n<h2>Time</h2> \n<p>Time is not a primary scoring criteria, although DARPA points out that “the course layout is designed such that finding a majority of the artifacts requires significant speed in covering the course.”&nbsp;If teams are tied on points, though, time does come into play, in the following sequence of tiebreakers:</p> \n<ul> \n <li>Earliest time that the last artifact was successfully reported, averaged across the team’s best runs on each course</li> \n <li>Earliest time that the first artifact was successfully reported, averaged across the team’s best runs on each course</li> \n <li>Lowest average time across all valid artifact reports, averaged across the team’s best runs on each course</li> \n</ul> \n<h2>Mapping</h2> \n<p>While mapping isn’t part of the score, it is a requirement: “Systems Teams must provide real-time 3D volumetric map updates to the DARPA Command Post at a minimum frequency of one update per 10 seconds.”&nbsp;And DARPA reminds teams that they should be thinking beyond this particular challenge:</p> \n<blockquote> \n <p><em>Each team’s provided map will be used to visualize and validate the team’s progress and evaluate the team’s perception and mapping capability. Both stakeholders and spectators will likely form opinions about the quality of a team’s solution based on its map representation. As rapid and&nbsp; remote mapping is an important capability to DARPA, the quality of a team’s mapping capability may be used, in part, to inform continued funding decisions.</em></p> \n</blockquote> \n<hr> \n <p>Since the competition is still ongoing, we’re not expecting much more scoring detail until probably Friday at the earliest. What we’d really like to see at some point would be a walkthrough or map of each course showing the locations of the artifacts, combined with team-generated mapping data and first person video feeds from the robots themselves. We'll be pestering both DARPA and the teams themselves for this stuff as soon as we can, but until then, keep <a href=\"https://www.youtube.com/user/DARPAtv\">following the live stream</a> and <a href=\"https://twitter.com/hashtag/SubTChallenge\">#SubTChallenge on Twitter</a>.</p> \n <p>[ <a href=\"https://subtchallenge.com\">SubT</a> ]</p>\n <div class=\"feedflare\"> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=MWyBQ98AJqA:ZR_Y6xjpoC8:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=MWyBQ98AJqA:ZR_Y6xjpoC8:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=MWyBQ98AJqA:ZR_Y6xjpoC8:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=MWyBQ98AJqA:ZR_Y6xjpoC8:-BTjWOF_DHI\" border=\"0\" /></a> \n  <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=MWyBQ98AJqA:ZR_Y6xjpoC8:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=MWyBQ98AJqA:ZR_Y6xjpoC8:gIN9vFwOqvQ\" border=\"0\" /></a> \n </div>\n <img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/MWyBQ98AJqA\" height=\"1\" width=\"1\" alt=\"\" />\n</hr>","descriptionType":"html","publishedDate":"Tue, 20 Aug 2019 16:30:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM1NzgxNw.jpeg","linkMd5":"c7f356a3967b14c7066d4a7ae2801b32","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn91@2020_1/2020/08/25/10-38-00-062_a7d8996d7c304bc2.webp","destWidth":1240,"destHeight":930,"sourceBytes":139981,"destBytes":73212,"author":"Evan Ackerman","articleImgCdnMap":{"https://spectrum.ieee.org/image/MzM1NzQ0NA.jpeg":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn19@2020_6/2020/08/25/10-38-10-177_6538b306c2d48f4c.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=MWyBQ98AJqA:ZR_Y6xjpoC8:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn78@2020_5/2020/08/25/10-38-16-157_e1aff2c64bfc3588.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=MWyBQ98AJqA:ZR_Y6xjpoC8:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn90@2020_2/2020/08/25/10-38-14-635_82e446c1ef17bf96.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/MWyBQ98AJqA":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn16@2020_4/2020/08/25/10-38-06-969_0c07dcada79623d6.webp"},"publishedOrCreatedDate":1598351859737},{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","title":"3 Easy Ways to Evaluate AI Claims","link":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","description":"<p>When every other tech startup claims to use&nbsp;artificial intelligence, it can be tough&nbsp;to&nbsp;figure out if an&nbsp;AI service or product works as advertised. <span>In the midst of the&nbsp;AI&nbsp;“gold rush,” how can you separate the nuggets from the fool’s gold?</span></p> \n<p></p> \n<!--nextpage--> \n<p>There’s no shortage of cautionary tales&nbsp;involving overhyped AI claims. And <span>applying&nbsp;AI technologies to health care, education, and law enforcement mean that&nbsp;getting it wrong can have real&nbsp;consequences for society—not just for investors who bet on the wrong <a href=\"https://spectrum.ieee.org/automaton/robotics/industrial-robots/the-hunt-for-robot-unicorns\">unicorn</a>.</span>&nbsp;</p> \n<p>So&nbsp;<em>IEEE Spectrum</em>&nbsp;asked experts to share&nbsp;their tips for how to identify AI hype in press releases, news articles, research papers, and IPO filings.</p> \n<p>“It can be tricky, because I think the people who are out there&nbsp;selling the AI hype—selling this AI&nbsp;<a href=\"https://spectrum.ieee.org/biomedical/devices/too-many-health-wearables-and-apps-are-digital-snake-oil\">snake oil</a>—are getting more sophisticated over time,” says&nbsp;<span><a href=\"https://timhwang.org/\">Tim Hwang</a>, director of the Harvard-MIT Ethics and Governance of AI Initiative</span>.</p> \n<p><span>The term “AI” is perhaps most frequently used to describe <a href=\"https://spectrum.ieee.org/searchContent?q=machine+learning\">machine learning</a>&nbsp;algorithms (and deep learning algorithms, which require&nbsp;even less human guidance) that analyze huge amounts of data and make predictions based on patterns that humans might miss. These popular forms of AI are mostly suited to specialized tasks, such as automatically recognizing certain objects within photos. For that reason, they are sometimes described as “weak” or “narrow” AI.</span></p> \n<p><span>Some researchers and thought leaders like to talk about the idea of “artificial general intelligence” or “strong AI” that has human-level capacity and flexibility to handle many diverse intellectual tasks. But for now, this type of AI remains firmly in the realm of science fiction and is far from being realized in the real world.&nbsp;</span></p> \n<p><span id=\"docs-internal-guid-e3760253-7fff-8dde-1eb5-411c27a3a506\">“AI has no well-defined meaning and many so-called AI companies are simply trying to take advantage of the buzz around that term,”&nbsp;says&nbsp;</span><span><a href=\"http://randomwalker.info/\">Arvind Narayanan</a>, a computer scientist at Princeton University. “Companies have even been caught claiming to use AI when, in fact, the task is done by human workers.”</span></p> \n<p><span>Here are three ways to recognize AI hype.&nbsp;</span></p> \n<h2>Look for Buzzwords</h2> \n<p>One red flag is what Hwang calls the “hype salad.” This means stringing together the term “AI” with many other tech buzzwords such as “<a href=\"https://spectrum.ieee.org/searchContent?q=blockchain\">blockchain</a>” or&nbsp;“<a href=\"https://spectrum.ieee.org/searchContent?q=Internet+of+things\">Internet of Things</a>.”&nbsp;That doesn’t&nbsp;automatically disqualify the technology, but spotting a high volume of buzzwords&nbsp;in a post, pitch, or presentation should raise questions about what exactly the company or individual has developed.</p> \n<p><span>Other experts agree that strings of buzzwords can be a red flag. That’s especially true if the buzzwords are never really explained in technical detail, and are simply tossed around as vague, poorly-defined terms, says&nbsp;<a href=\"http://www.marzyehghassemi.com/\">Marzyeh Ghassemi</a>, a computer scientist and biomedical engineer at the University of Toronto in Canada.</span></p> \n<p><span>“I think that if it looks like a Google search—picture ‘interpretable blockchain AI deep learning medicine’—it's probably not high-</span><span>quality work,” Ghassemi says.</span></p> \n<p>Hwang also suggests mentally replacing all mentions of “AI” in an article with the term “magical fairy dust.” It’s a way of seeing whether an individual or organization is treating&nbsp;the technology like magic. If so—that’s another good reason to ask more questions about what exactly the AI technology involves.</p> \n<p>And even the visual imagery used to illustrate AI claims&nbsp;can indicate that an individual or organization is overselling the technology.</p> \n<p>“I think that a lot of the people who work on machine learning on a day-to-day basis are pretty humble about the technology, because they’re largely confronted with how frequently it just breaks and doesn't work,” Hwang says.&nbsp;“And so I think that if you&nbsp;see a company or someone representing AI as a&nbsp;Terminator head, or a big glowing HAL eye or something like that, I think it’s also worth asking some questions.”&nbsp;</p> \n<h2>Interrogate the Data</h2> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<div></div> \n<div></div> \n<p></p> \n<p></p> \n<div></div> \n<div></div> \n<p></p> \n<p></p> \n<p></p> \n<p></p> \n<p>It can be hard to evaluate AI claims without any relevant&nbsp;expertise, says Ghassemi at the University of Toronto. Even experts need to know the technical details of the AI algorithm in question and have some access to the training data that shaped the AI model’s predictions. Still, savvy readers with some basic knowledge of applied statistics can search for red flags.</p> \n<div></div> \n<p>To start, readers can look for <a href=\"https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-the-birth-of-machine-bias\">possible bias</a> in&nbsp;training data based on&nbsp;small sample sizes or a skewed population that fails to reflect&nbsp;the broader population, Ghassemi says. After all, an AI model trained only on health data from&nbsp;white men would not necessarily achieve similar results for other populations of patients.&nbsp;</p> \n<aside class=\"inlay pullquote lt med\">\n  “For me, a red flag is not demonstrating deep knowledge of how your labels are defined.” \n <span class=\"pq-attrib\">—Marzyeh Ghassemi, University of Toronto</span> \n</aside> \n<p>How machine learning and deep learning models perform also depends on how well humans labeled the sample datasets use to train these programs.&nbsp;<span>This task can be straightforward</span><span>&nbsp;when labeling photos of cats versus dogs, but </span><span>gets&nbsp;more complicated when</span><span>&nbsp;assigning</span><span>&nbsp;disease diagnoses to certain patient cases</span><span>. </span></p> \n<p><span>Medical experts frequently disagree with each other on diagnoses—which is why many patients seek a second opinion. Not surprisingly, this ambiguity&nbsp;can also affect the diagnostic labels that experts assign in&nbsp;training datasets.&nbsp;“For me, a red flag is not demonstrating deep knowledge of how your labels are defined,” Ghassemi says.</span></p> \n<p><span>Such training data can also reflect the cultural stereotypes and biases of the humans who labeled the data, says&nbsp;Narayanan at Princeton University. Like Ghassemi, he recommends&nbsp;taking a hard look at exactly what the&nbsp;AI has learned:&nbsp;</span>“A good way to start critically evaluating AI claims is by asking questions about the training data.”</p> \n<p>Another red flag is presenting an AI system’s performance through a single accuracy figure without much explanation, Narayanan says. Claiming that&nbsp;an AI model achieves “99 percent” accuracy doesn’t mean much without knowing the baseline for comparison—such as whether&nbsp;other systems have already achieved 99 percent accuracy—or how well that accuracy holds up in situations beyond the training dataset.</p> \n<p>Narayanan also emphasized the need to ask questions about an AI model’s false positive rate—the rate of making wrong&nbsp;predictions about&nbsp;the presence of a given condition. Even if the false positive rate of a hypothetical AI service is just one percent, that could have major consequences if that&nbsp;service ends up screening millions of people for cancer.</p> \n<p>Readers&nbsp;can also consider whether using AI in a given situation offers any meaningful improvement&nbsp;compared to traditional statistical methods, says&nbsp;<a href=\"https://claytonaldern.com/about/\">Clayton Aldern</a>, a data scientist and journalist who serves as managing director for Caldern LLC. He&nbsp;gave the hypothetical example of a “super-duper-fancy deep learning model” that achieves a prediction accuracy of 89 percent, compared to a&nbsp;“little polynomial regression model” that achieves 86 percent on the same dataset.</p> \n<p><span>“We're talking about a three-percentage-point increase on something that you learned about in&nbsp;Algebra 1,” Aldern says.&nbsp;“So is&nbsp;it worth the hype?”</span></p> \n<h2>Don’t Ignore the Drawbacks</h2> \n<p></p> \n<p>The hype surrounding AI isn’t just about the technical merits of services and products driven by machine learning. Overblown claims&nbsp;about the beneficial&nbsp; impacts of&nbsp;AI technology—or&nbsp;vague promises to address ethical issues related to deploying it—should also raise red flags.</p> \n<p><span id=\"docs-internal-guid-e3760253-7fff-8dde-1eb5-411c27a3a506\">“If a company promises to <a href=\"https://spectrum.ieee.org/the-institute/ieee-products-services/what-social-good-means-to-engineers\">use its tech ethically</a>, it is important to question if its business model aligns with that promise,” Narayanan says.&nbsp;“Even if employees have noble intentions, it is unrealistic to expect the company as a whole to resist financial imperatives.”</span></p> \n<p>One example&nbsp;might be a company with a business model that depends on leveraging customers’ personal data. Such companies “tend to make empty promises when it comes to privacy,” Narayanan says. And, if companies hire workers to produce training data, it’s also worth asking whether the companies <a href=\"https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-mechanical-turk-revisited-tktkt\">treat those workers ethically</a>.</p> \n<p>The&nbsp;transparency—or lack thereof—about any AI claim can also be telling. A company or research group&nbsp;can minimize concerns by&nbsp;publishing&nbsp;technical claims in peer-reviewed journals or allowing&nbsp;credible third parties to evaluate their AI without giving away big intellectual property secrets, Narayanan says. Excessive secrecy is a big red flag.</p> \n<p>With these strategies, you don’t need to be a computer engineer&nbsp;or data scientist to start thinking critically about AI claims. And,&nbsp;<span>Narayanan says,</span> the&nbsp;<span>world needs many people from different backgrounds for societies to&nbsp;fully&nbsp;consider the real-world&nbsp;</span><span>implications of AI.&nbsp;</span></p> \n<p><span><em>Editor’s Note: The original version of this story misspelled Clayton Aldern’s last name as Alderton.</em></span></p> \n<div></div> \n<p></p> \n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=iB-EZS5TQKE:nV6g5-qUnfo:qj6IDK7rITs\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=iB-EZS5TQKE:nV6g5-qUnfo:6W8y8wAjSf4\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=iB-EZS5TQKE:nV6g5-qUnfo:-BTjWOF_DHI\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=iB-EZS5TQKE:nV6g5-qUnfo:-BTjWOF_DHI\" border=\"0\" /></a> \n <a href=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?a=iB-EZS5TQKE:nV6g5-qUnfo:gIN9vFwOqvQ\"><img src=\"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=iB-EZS5TQKE:nV6g5-qUnfo:gIN9vFwOqvQ\" border=\"0\" /></a> \n</div> \n<img src=\"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/iB-EZS5TQKE\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 23 Aug 2019 16:03:00 +0000","feedId":8606,"bgimg":"https://spectrum.ieee.org/image/MzM2MTI5OQ.jpeg","linkMd5":"953ccece03a4def30b62f328e3f48f7e","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn86@2020_5/2020/08/25/10-37-58-885_74477b395248b833.webp","destWidth":1240,"destHeight":930,"sourceBytes":70508,"destBytes":27522,"author":"Jeremy Hsu","articleImgCdnMap":{"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=iB-EZS5TQKE:nV6g5-qUnfo:-BTjWOF_DHI":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn43@2020_2/2020/08/25/10-38-07-124_f54ea4e7da6c3b77.webp","http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=iB-EZS5TQKE:nV6g5-qUnfo:gIN9vFwOqvQ":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn87@2020_6/2020/08/25/10-38-05-919_3fad89d519351818.webp","http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/iB-EZS5TQKE":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn9@2020_3/2020/08/25/10-38-15-522_d775bf4d36a5c029.webp"},"publishedOrCreatedDate":1598351859735}],"record":{"createdTime":"2020-08-25 18:37:39","updatedTime":"2020-08-25 18:37:39","feedId":8606,"fetchDate":"Tue, 25 Aug 2020 10:37:39 +0000","fetchMs":572,"handleMs":1243,"totalMs":51364,"newArticles":0,"totalArticles":100,"status":1,"type":0,"ip":"d01126ad4678a8e169f9117eff8589f2","hostName":"us-002*","requestId":"4eb1aede96ab402d86776fa24ee18207_8606","contentType":"text/xml; charset=UTF-8","totalBytes":50361920,"bgimgsTotal":100,"bgimgsGithubTotal":100,"articlesImgsTotal":468,"articlesImgsGithubTotal":468,"successGithubMap":{"myreaderx8":20,"myreaderx14":20,"myreaderx7":20,"myreaderx15":18,"myreaderx6":20,"myreaderx16":20,"myreaderx10":20,"myreaderx4":19,"myreaderx32":20,"myreaderx3":20,"myreaderx11":19,"myreaderx33":20,"myreaderx2":19,"myreaderx12":20,"myreaderx13":19,"myreaderx1":20,"myreaderx30":19,"myreaderx31":19,"myreaderx18":19,"myreaderx19":20,"myreaderx":20,"myreaderx25":20,"myreaderx27":19,"myreaderx21":19,"myreaderx22":20,"myreaderx23":20,"myreaderx24":19,"myreaderx5oss":20,"myreaderx29":19},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:34:29","updatedTime":"2020-08-25 04:34:29","id":8606,"name":"IEEE Spectrum Robotics","url":"http://feeds.feedburner.com/IeeeSpectrumRobotics","subscriber":null,"website":null,"icon":"https://spectrum.ieee.org/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx61/cdn26@2020_1/2020/08/25/10-37-38-943_d6893d3dfd1314b0.ico","description":"IEEE Spectrum Robotics recent content","weekly":null,"link":"https://spectrum.ieee.org"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":50361920,"tmpBgImgCdnBytes":0,"extra4":{"start":1598351857819,"total":0,"statList":[{"spend":679,"msg":"获取xml内容"},{"spend":1243,"msg":"解释文章"},{"spend":5611,"msg":"上传封面图到cdn"},{"spend":1,"msg":"修正封面图上传失败重新上传"},{"spend":30620,"msg":"正文链接上传到cdn"}]},"extra5":468,"extra6":468,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://feedproxy.google.com/image/MzM1MzE3OA.jpeg","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":8606,"totalSpendMs":90,"convertSpendMs":0,"createdTime":"2020-08-25 18:37:58","host":"us-036*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://feedproxy.google.com/image/MzM1MzE3OA.jpeg","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":8606,"totalSpendMs":77,"convertSpendMs":0,"createdTime":"2020-08-25 18:37:58","host":"us-028*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":500,"msg":"Cannot run program \"/tmp/bin/cwebp\": error=13, Permission denied","isDone":false,"sourceBytes":0,"destBytes":0,"totalSpendMs":0,"convertSpendMs":0,"createdTime":"2020-08-25 18:38:00","sourceSize":"0","destSize":"0"},{"code":500,"msg":"Cannot run program \"/tmp/bin/gif2webp\": error=13, Permission denied","isDone":false,"sourceBytes":0,"destBytes":0,"totalSpendMs":0,"convertSpendMs":0,"createdTime":"2020-08-25 18:38:02","sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA4Nw.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"sourceBytes":1457172,"destBytes":187734,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":990,"convertSpendMs":121,"createdTime":"2020-08-25 18:38:10","host":"us-008*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn99/contents/2020/08/25/10-38-10-184_6fc7af4aa7b5da0a.webp","resp":{"code":409,"msg":"Conflict","body":"{\n  \"message\": \"is at 9ee11d98ba02e6c711a01df7930e947e3b607464 but expected a0c4a725cc6996f703bd103d41dbcdd51206a54d\",\n  \"documentation_url\": \"https://docs.github.com/rest/reference/repos#create-or-update-file-contents\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-length":["222"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Tue, 25 Aug 2020 10:38:10 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["409 Conflict"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":[""],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["A3B2:53E4:274E1F0:441A330:5F44EA11"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["5000"],"x-ratelimit-remaining":["4846"],"x-ratelimit-reset":["1598354280"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 409,url is : https://api.github.com/repos/myreaderx23/cdn99/contents/2020/08/25/10-38-10-184_6fc7af4aa7b5da0a.webp","historyStatusCode":[],"spendMs":815},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":409,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.4 MB","destSize":"183.3 KB","compressRate":"12.9%"}],"extra10_invalidATagHrefValue":{"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures":"https://spectrum.ieee.org/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures","https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health_#SS810777737":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS810777737","https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing_/tech-talk/computing/software/monster-machine-defeats-prominent-pro-player":"https://spectrum.ieee.org/tech-talk/computing/software/monster-machine-defeats-prominent-pro-player","https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet_/robotics/industrial-robots/rethink-robotics-baxter-robot-factory-worker":"https://spectrum.ieee.org/robotics/industrial-robots/rethink-robotics-baxter-robot-factory-worker","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt_/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things_/view-from-the-valley/telecom/internet/brewster-kahle-on-whats-next-for-the-decentralized-web-movement":"https://spectrum.ieee.org/view-from-the-valley/telecom/internet/brewster-kahle-on-whats-next-for-the-decentralized-web-movement","https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo_/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures":"https://spectrum.ieee.org/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics_/tag/Alan+Turing":"https://spectrum.ieee.org/tag/Alan+Turing","https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/automaton/robotics/industrial-robots/robotic-dreams-robotic-realities":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robotic-dreams-robotic-realities","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next":"https://spectrum.ieee.org/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas_/tag/AI+history":"https://spectrum.ieee.org/tag/AI+history","https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot_/robotics/humanoids/building-robots-that-can-go-where-we-go":"https://spectrum.ieee.org/robotics/humanoids/building-robots-that-can-go-where-we-go","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/sony-aibo-robot-dog-is-coming-to-america":"https://spectrum.ieee.org/automaton/robotics/home-robots/sony-aibo-robot-dog-is-coming-to-america","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation_/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly_/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas_/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek_/tech-talk/robotics/artificial-intelligence/mb":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/mb","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_#qaTopicFive":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicFive","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/computing/hardware/us-energy-department-is-first-customer-for-worlds-biggest-chip_/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things_/geek-life/profiles/ge-wang-the-iphones-music-man":"https://spectrum.ieee.org/geek-life/profiles/ge-wang-the-iphones-music-man","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/humanoids/how-robonaut-2-will-help-astronauts-in-space":"https://spectrum.ieee.org/automaton/robotics/humanoids/how-robonaut-2-will-help-astronauts-in-space","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now_/tag/quadruped+robots":"https://spectrum.ieee.org/tag/quadruped+robots","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space_/tag/astrobee":"https://spectrum.ieee.org/tag/astrobee","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt_/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now_/tag/iros+2019":"https://spectrum.ieee.org/tag/iros+2019","https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid_/view-from-the-valley/robotics/drones/drone-control-heres-how-amazon-thinks-drones-should-fit-into-us-airspace":"https://spectrum.ieee.org/view-from-the-valley/robotics/drones/drone-control-heres-how-amazon-thinks-drones-should-fit-into-us-airspace","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek_/tech-talk/robotics/artificial-intelligence/pictionary-playing-ai-sketches-the-future-of-human-machine-collaborations":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/pictionary-playing-ai-sketches-the-future-of-human-machine-collaborations","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator_/robotics/artificial-intelligence":"https://spectrum.ieee.org/robotics/artificial-intelligence","https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation_/tag/gravitational+waves":"https://spectrum.ieee.org/tag/gravitational+waves","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/the-institute/ieee-products-services/what-social-good-means-to-engineers":"https://spectrum.ieee.org/the-institute/ieee-products-services/what-social-good-means-to-engineers","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot_/tag/subt":"https://spectrum.ieee.org/tag/subt","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/aerospace/robotic-exploration/planetary-rovers-are-we-alone":"https://spectrum.ieee.org/aerospace/robotic-exploration/planetary-rovers-are-we-alone","https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week_/tag/robonaut":"https://spectrum.ieee.org/tag/robonaut","https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built_/tech-talk/robotics/artificial-intelligence/mb":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/mb","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence_/view-from-the-valley/robotics/industrial-robots/robotics-company-prepares-to-take-responsibility-for-displaced-workers":"https://spectrum.ieee.org/view-from-the-valley/robotics/industrial-robots/robotics-company-prepares-to-take-responsibility-for-displaced-workers","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/industrial-robots/boston-dynamics-spotminis":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spotminis","https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020_/aerospace/robotic-exploration/china-gears-up-for-ambitious-landing-on-the-moons-far-side":"https://spectrum.ieee.org/aerospace/robotic-exploration/china-gears-up-for-ambitious-landing-on-the-moons-far-side","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/robot-gift-guide-2016":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2016","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/robot-gift-guide-2017":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2017","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/robot-gift-guide-2018":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2018","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped_/automaton/robotics/humanoids/nabiros-makes-us-wonder-why-we-all-dont-walk-sideways":"https://spectrum.ieee.org/automaton/robotics/humanoids/nabiros-makes-us-wonder-why-we-all-dont-walk-sideways","https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever_/video/robotics/military-robots/darpa-tests-batterypowered-exoskeletons-on-real-soldiers":"https://spectrum.ieee.org/video/robotics/military-robots/darpa-tests-batterypowered-exoskeletons-on-real-soldiers","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/military-robots/atlas-drc-robot-is-75-percent-new-completely-unplugged":"https://spectrum.ieee.org/automaton/robotics/military-robots/atlas-drc-robot-is-75-percent-new-completely-unplugged","https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816_/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle_/automaton/robotics/robotics-hardware/juggling-robot-takes-on-two-balls-with-one-very-fast-hand":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/juggling-robot-takes-on-two-balls-with-one-very-fast-hand","https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes_/robotics/humanoids/darpas-rescuerobot-showdown":"https://spectrum.ieee.org/robotics/humanoids/darpas-rescuerobot-showdown","https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone_/aerospace/aviation/how-to-become-an-faaapproved-drone-pilot":"https://spectrum.ieee.org/aerospace/aviation/how-to-become-an-faaapproved-drone-pilot","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/military-robots/whoa-boston-dynamics-announces-new-wildcat-quadruped":"https://spectrum.ieee.org/automaton/robotics/military-robots/whoa-boston-dynamics-announces-new-wildcat-quadruped","https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/automaton/robotics/home-robots/jibo-is-probably-totally-dead-now":"https://spectrum.ieee.org/automaton/robotics/home-robots/jibo-is-probably-totally-dead-now","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes_/automaton/robotics/robotics-hardware/mit-cheetah-robot-running":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-cheetah-robot-running","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit_/automaton/robotics/robotics-software/coordinated-robotics-winner-nasa-space-robotics-challenge":"https://spectrum.ieee.org/automaton/robotics/robotics-software/coordinated-robotics-winner-nasa-space-robotics-challenge","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models_/tag/Claude+Shannon":"https://spectrum.ieee.org/tag/Claude+Shannon","https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot_/automaton/robotics/humanoids/ford-self-driving-vans-will-use-legged-robots-to-make-deliveries":"https://spectrum.ieee.org/automaton/robotics/humanoids/ford-self-driving-vans-will-use-legged-robots-to-make-deliveries","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light_/nanoclast/semiconductors/devices/nanoneurons-enable-neuromorphic-chips-for-voice-recognition":"https://spectrum.ieee.org/nanoclast/semiconductors/devices/nanoneurons-enable-neuromorphic-chips-for-voice-recognition","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/aerospace/space-flight/nasas-lunar-space-station-is-a-greatterrible-idea":"https://spectrum.ieee.org/aerospace/space-flight/nasas-lunar-space-station-is-a-greatterrible-idea","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/diy/2012-robot-gift-guide":"https://spectrum.ieee.org/automaton/robotics/diy/2012-robot-gift-guide","https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot_/automaton/robotics/robotics-software/052710-how-i-became-a-texai-robot-and-went-partying":"https://spectrum.ieee.org/automaton/robotics/robotics-software/052710-how-i-became-a-texai-robot-and-went-partying","https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable_/automaton/robotics/drones/skydio-r1-drone":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-r1-drone","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/space-robots/robonaut-has-been-broken-for-years-and-now-nasa-is-bringing-it-home":"https://spectrum.ieee.org/automaton/robotics/space-robots/robonaut-has-been-broken-for-years-and-now-nasa-is-bringing-it-home","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot_/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies_/the-human-os/biomedical/devices/elon-musks-150-million-augmented-brain-project-what-the-media-missed":"https://spectrum.ieee.org/the-human-os/biomedical/devices/elon-musks-150-million-augmented-brain-project-what-the-media-missed","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft":"https://spectrum.ieee.org/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot_/tag/history+of+natural+language+processing":"https://spectrum.ieee.org/tag/history+of+natural+language+processing","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/home-robots/amazon-home-robots":"https://spectrum.ieee.org/automaton/robotics/home-robots/amazon-home-robots","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator_/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/geek-life/tools-toys/ieee-spectrums-2019-gift-guide":"https://spectrum.ieee.org/geek-life/tools-toys/ieee-spectrums-2019-gift-guide","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt_/telecom/wireless/how-the-internet-of-things-could-fracture-wifi":"https://spectrum.ieee.org/telecom/wireless/how-the-internet-of-things-could-fracture-wifi","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation_/tag/history+of+natural+language+processing":"https://spectrum.ieee.org/tag/history+of+natural+language+processing","https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light_/tech-talk/computing/hardware/a-neural-net-based-on-light-could-best-digital-computers":"https://spectrum.ieee.org/tech-talk/computing/hardware/a-neural-net-based-on-light-could-best-digital-computers","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/microsofts-ai-research-draws-controversy-over-possible-disinformation-use_/tech-talk/robotics/artificial-intelligence/ai-learns-gender-and-racial-biases-from-language":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-learns-gender-and-racial-biases-from-language","https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly_/static/tech-expedition-east-africas-big-bet-on-drones":"https://spectrum.ieee.org/static/tech-expedition-east-africas-big-bet-on-drones","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_#qaTopicFour":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicFour","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-spot-robots-pull-a-truck-and-more":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-spot-robots-pull-a-truck-and-more","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/military-robots/boston-dynamics-sand-flea-demonstrates-astonishing-jumping-skills":"https://spectrum.ieee.org/automaton/robotics/military-robots/boston-dynamics-sand-flea-demonstrates-astonishing-jumping-skills","https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food_/automaton/robotics/industrial-robots/bosch-deepfield-robotics-weed-control":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/bosch-deepfield-robotics-weed-control","https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface_/tag/climate+change":"https://spectrum.ieee.org/tag/climate+change","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics_/tag/history+of+natural+language+processing":"https://spectrum.ieee.org/tag/history+of+natural+language+processing","https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home_/automaton/robotics/home-robots/irobot-sweeps-up-evolution-robotics-for-74-million":"https://spectrum.ieee.org/automaton/robotics/home-robots/irobot-sweeps-up-evolution-robotics-for-74-million","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot_/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons_/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications_/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence_/static/journal-watch":"https://spectrum.ieee.org/static/journal-watch","https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/automaton/robotics/home-robots/consumer-robotics-company-anki-abruptly-shuts-down":"https://spectrum.ieee.org/automaton/robotics/home-robots/consumer-robotics-company-anki-abruptly-shuts-down","https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid_/automaton/robotics/drones/wing-officially-launches-australian-drone-delivery-service":"https://spectrum.ieee.org/automaton/robotics/drones/wing-officially-launches-australian-drone-delivery-service","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/industrial-robots/boston-dynamics-warehouse-robots-acquires-kinema-systems":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-warehouse-robots-acquires-kinema-systems","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills_/automaton/robotics/robotics-hardware/video-friday-cmu-two-armed-ballbot":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-cmu-two-armed-ballbot","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tag/history+of+natural+language+processing":"https://spectrum.ieee.org/tag/history+of+natural+language+processing","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope_/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816_/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body_/static/journal-watch":"https://spectrum.ieee.org/static/journal-watch","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/automaton/robotics/home-robots/buddy-the-social-robot-is-somehow-not-dead-yet":"https://spectrum.ieee.org/automaton/robotics/home-robots/buddy-the-social-robot-is-somehow-not-dead-yet","https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things_/view-from-the-valley/telecom/internet/hbos-silicon-valley-joins-the-push-for-a-decentralized-web":"https://spectrum.ieee.org/view-from-the-valley/telecom/internet/hbos-silicon-valley-joins-the-push-for-a-decentralized-web","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_/tag/autonomous+weapons":"https://spectrum.ieee.org/tag/autonomous+weapons","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots","https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley_/automaton/robotics/humanoids/explain-the-uncanny-valley-in-less-than-1-minute-go":"https://spectrum.ieee.org/automaton/robotics/humanoids/explain-the-uncanny-valley-in-less-than-1-minute-go","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters_/automaton/robotics/humanoids/caltech-building-agile-humanoid-robot-by-combining-legs-with-thrusters":"https://spectrum.ieee.org/automaton/robotics/humanoids/caltech-building-agile-humanoid-robot-by-combining-legs-with-thrusters","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india_/automaton/robotics/robotics-hardware/robot-research-in-the-wild-water-transport-in-rural-india":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-research-in-the-wild-water-transport-in-rural-india","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot_/tag/AI+history":"https://spectrum.ieee.org/tag/AI+history","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/humanoids/new-r5-valkyrie-robots":"https://spectrum.ieee.org/automaton/robotics/humanoids/new-r5-valkyrie-robots","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tag/deepfakes":"https://spectrum.ieee.org/tag/deepfakes","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/automaton/robotics/industrial-robots/watch-iits-new-hyqreal-quadruped-robot-pull-an-airplane":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/watch-iits-new-hyqreal-quadruped-robot-pull-an-airplane","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics_/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-why-alan-turing-wanted-ai-to-make-mistakes":"https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-why-alan-turing-wanted-ai-to-make-mistakes","https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances_/static/tech-expedition-east-africas-big-bet-on-drones":"https://spectrum.ieee.org/static/tech-expedition-east-africas-big-bet-on-drones","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-spot-robots-pull-a-truck-and-more":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-spot-robots-pull-a-truck-and-more","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas_/tag/history+of+natural+language+processing":"https://spectrum.ieee.org/tag/history+of+natural+language+processing","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview_/automaton/robotics/robotics-hardware/darpa-subt-meet-the-first-nine-teams":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subt-meet-the-first-nine-teams","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body_/tag/icra+2019":"https://spectrum.ieee.org/tag/icra+2019","https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built_/view-from-the-valley/tech-history/silicon-revolution/remembering-gene-amdahl":"https://spectrum.ieee.org/view-from-the-valley/tech-history/silicon-revolution/remembering-gene-amdahl","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/piaggio-cargo-robot":"https://spectrum.ieee.org/automaton/robotics/home-robots/piaggio-cargo-robot","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/robotics-hardware/spotmini-can-dance-now":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/spotmini-can-dance-now","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_#qaTopicOne":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicOne","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station_/automaton/robotics/space-robots/robonaut-and-astrobee-to-will-work-together-on-iss":"https://spectrum.ieee.org/automaton/robotics/space-robots/robonaut-and-astrobee-to-will-work-together-on-iss","https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone_/automaton/robotics/drones/review-parrot-anafi-drone":"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/military-robots/boston-dynamics-now-belongs-to-google":"https://spectrum.ieee.org/automaton/robotics/military-robots/boston-dynamics-now-belongs-to-google","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/robotics/home-robots/how-aldebaran-robotics-built-its-friendly-humanoid-robot-pepper":"https://spectrum.ieee.org/robotics/home-robots/how-aldebaran-robotics-built-its-friendly-humanoid-robot-pepper","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-4-back":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-4-back","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/military-robots/boston-dynamics-now-belongs-to-google":"https://spectrum.ieee.org/automaton/robotics/military-robots/boston-dynamics-now-belongs-to-google","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics_/tag/AI+history":"https://spectrum.ieee.org/tag/AI+history","https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/tech-talk/robotics/home-robots/ces-2017-why-every-social-robot-at-ces-looks-alike":"https://spectrum.ieee.org/tech-talk/robotics/home-robots/ces-2017-why-every-social-robot-at-ces-looks-alike","https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week_/automaton/robotics/space-robots/gitai-partners-with-jaxa-to-send-telepresence-robots-to-space":"https://spectrum.ieee.org/automaton/robotics/space-robots/gitai-partners-with-jaxa-to-send-telepresence-robots-to-space","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/automaton/robotics/robotics-hardware/video-friday-816":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816","https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone_/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_#qaTopicTwo":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicTwo","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge_/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics","https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface_/static/special-report-blueprints-for-a-miracle":"https://spectrum.ieee.org/static/special-report-blueprints-for-a-miracle","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/robotics-hardware/irobot-acquires-root-robotics-to-boost-stem-education-for-kids":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/irobot-acquires-root-robotics-to-boost-stem-education-for-kids","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/searchContent?q=blockchain":"https://spectrum.ieee.org/searchContent?q=blockchain","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space_/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/automaton/robotics/industrial-robots/the-hunt-for-robot-unicorns":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/the-hunt-for-robot-unicorns","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/automaton/robotics/industrial-robots/north-sea-deployment-shows-how-quadruped-robots-can-be-commercially-useful":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/north-sea-deployment-shows-how-quadruped-robots-can-be-commercially-useful","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt_/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau_/automaton/robotics/humanoids/jet-powered-icub-could-be-the-first-flying-humanoid-robot":"https://spectrum.ieee.org/automaton/robotics/humanoids/jet-powered-icub-could-be-the-first-flying-humanoid-robot","https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station_/preview/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow":"https://spectrum.ieee.org/preview/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit_/tag/subt":"https://spectrum.ieee.org/tag/subt","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tag/reinforcement+learning":"https://spectrum.ieee.org/tag/reinforcement+learning","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley_/automaton/robotics/humanoids/the-uncanny-valley-revisited-a-tribute-to-masahiro-mori":"https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley-revisited-a-tribute-to-masahiro-mori","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream_/automaton/robotics/home-robots/roomba-inventor-joe-jones-on-weed-killing-robot":"https://spectrum.ieee.org/automaton/robotics/home-robots/roomba-inventor-joe-jones-on-weed-killing-robot","https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley_/automaton/robotics/humanoids/telenoid-r1-hiroshi-ishiguro-newest-and-strangest-android":"https://spectrum.ieee.org/automaton/robotics/humanoids/telenoid-r1-hiroshi-ishiguro-newest-and-strangest-android","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/2013-robot-gift-guide":"https://spectrum.ieee.org/automaton/robotics/home-robots/2013-robot-gift-guide","https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020_/automaton/aerospace/robotic-exploration/how-nasa-curiosity-rover-will-land-on-mars":"https://spectrum.ieee.org/automaton/aerospace/robotic-exploration/how-nasa-curiosity-rover-will-land-on-mars","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot_/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/mit-mini-cheetah-accelerate-research","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/diy/spheros-new-rvr-is-more-hackable-than-ever":"https://spectrum.ieee.org/automaton/robotics/diy/spheros-new-rvr-is-more-hackable-than-ever","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans_/automaton/robotics/industrial-robots/robots-do-construction-with-brick-and-concrete":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robots-do-construction-with-brick-and-concrete","https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week_/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models_/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation_/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/static/project-moon-base":"https://spectrum.ieee.org/static/project-moon-base","https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley_/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley":"https://spectrum.ieee.org/automaton/robotics/humanoids/an-uncanny-mind-masahiro-mori-on-the-uncanny-valley","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit_/automaton/robotics/drones/review-parrot-anafi-drone":"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone","https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot_/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances_/the-human-os/biomedical/devices/drone-vs-ambulance-drone-wins":"https://spectrum.ieee.org/the-human-os/biomedical/devices/drone-vs-ambulance-drone-wins","https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot_/automaton/robotics/home-robots/mayfield-robotics-cancels-kuri-social-home-robot":"https://spectrum.ieee.org/automaton/robotics/home-robots/mayfield-robotics-cancels-kuri-social-home-robot","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills_/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek_/robotics/artificial-intelligence/in-the-quest-for-general-intelligence-ais-are-chasing-chickens-in-minecraft":"https://spectrum.ieee.org/robotics/artificial-intelligence/in-the-quest-for-general-intelligence-ais-are-chasing-chickens-in-minecraft","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one_/tag/AI+history":"https://spectrum.ieee.org/tag/AI+history","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills_/automaton/robotics/humanoids/watch-the-worlds-first-giant-robot-fight":"https://spectrum.ieee.org/automaton/robotics/humanoids/watch-the-worlds-first-giant-robot-fight","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/sony-upgrading-aibo-with-new-home-security-features-api-access":"https://spectrum.ieee.org/automaton/robotics/home-robots/sony-upgrading-aibo-with-new-home-security-features-api-access","https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing_/at-work/innovation/this-looks-like-a-job-forsuperatoms":"https://spectrum.ieee.org/at-work/innovation/this-looks-like-a-job-forsuperatoms","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-5-back":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-5-back","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters_/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_#qaTopicThree":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#qaTopicThree","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body_/robotics/humanoids/meet-aquanaut-the-underwater-transformer":"https://spectrum.ieee.org/robotics/humanoids/meet-aquanaut-the-underwater-transformer","https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet_/automaton/robotics/industrial-robots/rethink-robotics-pioneer-of-collaborative-robots-shuts-down":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/rethink-robotics-pioneer-of-collaborative-robots-shuts-down","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit_/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/automaton/robotics/robotics-software/wizards-of-ros-willow-garage-and-the-making-of-the-robot-operating-system":"https://spectrum.ieee.org/automaton/robotics/robotics-software/wizards-of-ros-willow-garage-and-the-making-of-the-robot-operating-system","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-prospects-for-tomorrows-jobs_/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models_/tag/history+of+natural+language+processing":"https://spectrum.ieee.org/tag/history+of+natural+language+processing","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots_/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military_/robotics/drones/in-the-air-with-ziplines-medical-delivery-drones":"https://spectrum.ieee.org/robotics/drones/in-the-air-with-ziplines-medical-delivery-drones","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/automaton/robotics/industrial-robots/robots-getting-a-grip-on-general-manipulation":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robots-getting-a-grip-on-general-manipulation","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek_/tech-talk/robotics/artificial-intelligence/deepminds-ai-shows-itself-to-be-a-worldbeating-world-builder":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/deepminds-ai-shows-itself-to-be-a-worldbeating-world-builder","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves_/static/journal-watch":"https://spectrum.ieee.org/static/journal-watch","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle_/automaton/robotics/robotics-software/quadrotors-demonstrate-mad-cooperative-juggling-skills":"https://spectrum.ieee.org/automaton/robotics/robotics-software/quadrotors-demonstrate-mad-cooperative-juggling-skills","https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/trump-cto-addresses-ai-facial-recognition-immigration-tech-infrastructure-and-more_/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things":"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/space-robots/robonaut-and-astrobee-to-will-work-together-on-iss":"https://spectrum.ieee.org/automaton/robotics/space-robots/robonaut-and-astrobee-to-will-work-together-on-iss","https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable_/automaton/robotics/drones/skydio-announces-sdk-to-make-worlds-cleverest-drone-even-cleverer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-announces-sdk-to-make-worlds-cleverest-drone-even-cleverer","https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health_#SS656985622":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS656985622","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_/computing/software/many-experts-say-we-shouldnt-worry-about-superintelligent-ai-theyre-wrong":"https://spectrum.ieee.org/computing/software/many-experts-say-we-shouldnt-worry-about-superintelligent-ai-theyre-wrong","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-2":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-2","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body_/automaton/robotics/drones/spring-loaded-drone-collapses-midflight-to-zip-through-windows":"https://spectrum.ieee.org/automaton/robotics/drones/spring-loaded-drone-collapses-midflight-to-zip-through-windows","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-1":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-1","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-4":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-4","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models_/tech-talk/telecom/internet/bell-labs-looks-at-claude-shannon-legacy-future-of-information-age":"https://spectrum.ieee.org/tech-talk/telecom/internet/bell-labs-looks-at-claude-shannon-legacy-future-of-information-age","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-3":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-3","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/2015-robot-gift-guide":"https://spectrum.ieee.org/automaton/robotics/home-robots/2015-robot-gift-guide","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-3-back":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-3-back","https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever_/robotics/drones":"https://spectrum.ieee.org/robotics/drones","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-5":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-5","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/automaton/robotics/robotics-software/willow-garage-to-shut-down":"https://spectrum.ieee.org/automaton/robotics/robotics-software/willow-garage-to-shut-down","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek_/podcast/robotics/artificial-intelligence/nick-bostrom-says-we-should-trust-our-future-robot-overlords":"https://spectrum.ieee.org/podcast/robotics/artificial-intelligence/nick-bostrom-says-we-should-trust-our-future-robot-overlords","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next":"https://spectrum.ieee.org/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt_/automaton/robotics/humanoids/bipedal-robot-uses-jetpowered-feet-to-step-over-large-gaps":"https://spectrum.ieee.org/automaton/robotics/humanoids/bipedal-robot-uses-jetpowered-feet-to-step-over-large-gaps","https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable_/automaton/robotics/drones/review-parrot-anafi-drone":"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone","https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone_/automaton/robotics/drones/faa-task-force-drone-registration":"https://spectrum.ieee.org/automaton/robotics/drones/faa-task-force-drone-registration","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/humanoids/boston-dynamics-marc-raibert-on-nextgen-atlas":"https://spectrum.ieee.org/automaton/robotics/humanoids/boston-dynamics-marc-raibert-on-nextgen-atlas","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models_/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots_/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/biomedical/devices/too-many-health-wearables-and-apps-are-digital-snake-oil":"https://spectrum.ieee.org/biomedical/devices/too-many-health-wearables-and-apps-are-digital-snake-oil","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/automaton/robotics/industrial-robots/rethink-robotics-pioneer-of-collaborative-robots-shuts-down":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/rethink-robotics-pioneer-of-collaborative-robots-shuts-down","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_/tech-talk/robotics/artificial-intelligence/alphago-zero-goes-from-blank-slate-to-grandmaster-in-three-dayswithout-any-help-at-all":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/alphago-zero-goes-from-blank-slate-to-grandmaster-in-three-dayswithout-any-help-at-all","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/humanoids/boston-dynamics-atlas-robot-shows-off-parkour-skills":"https://spectrum.ieee.org/automaton/robotics/humanoids/boston-dynamics-atlas-robot-shows-off-parkour-skills","https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans_/automaton/robotics/industrial-robots/robotic-construction-platform-creates-large-buildings-on-demand":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robotic-construction-platform-creates-large-buildings-on-demand","https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot_/automaton/robotics/industrial-robots/ces-2018-suitable-tech-introduces-beampro-2-telepresence-platform":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/ces-2018-suitable-tech-introduces-beampro-2-telepresence-platform","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas_/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo_/automaton/robotics/home-robots/buddy-the-social-robot-is-somehow-not-dead-yet":"https://spectrum.ieee.org/automaton/robotics/home-robots/buddy-the-social-robot-is-somehow-not-dead-yet","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt_/automaton/robotics/military-robots/firstlook-irobot-new-throwable-baby-surveillance-bot":"https://spectrum.ieee.org/automaton/robotics/military-robots/firstlook-irobot-new-throwable-baby-surveillance-bot","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots_/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever_/automaton/robotics/home-robots/double-2-review-telepresence-robot":"https://spectrum.ieee.org/automaton/robotics/home-robots/double-2-review-telepresence-robot","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/drones/faa-updates-drone-rules-for-everyone":"https://spectrum.ieee.org/automaton/robotics/drones/faa-updates-drone-rules-for-everyone","https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid_/automaton/aerospace/aviation/dji-promises-to-add-airsense-to-its-new-drones":"https://spectrum.ieee.org/automaton/aerospace/aviation/dji-promises-to-add-airsense-to-its-new-drones","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models_/tag/AI+history":"https://spectrum.ieee.org/tag/AI+history","https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly_/automaton/robotics/drones/skydio-camera-drone-autonomous-flying":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-camera-drone-autonomous-flying","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot_/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning","https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances_/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/searchContent?q=machine+learning":"https://spectrum.ieee.org/searchContent?q=machine+learning","https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone_/automaton/robotics/drones/a-weird-time-for-drone-operators":"https://spectrum.ieee.org/automaton/robotics/drones/a-weird-time-for-drone-operators","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview_/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt_/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/subt-the-next-darpa-challenge-for-robotics","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-the-birth-of-machine-bias":"https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-the-birth-of-machine-bias","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving_/automaton/robotics/artificial-intelligence/openai-demonstrates-complex-manipulation-transfer-from-simulation-to-real-world":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/openai-demonstrates-complex-manipulation-transfer-from-simulation-to-real-world","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats_/automaton/robotics/diy/new-beetlecam-wildlife-robotic-camera":"https://spectrum.ieee.org/automaton/robotics/diy/new-beetlecam-wildlife-robotic-camera","https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley_/robotics/humanoids/hiroshi-ishiguro-the-man-who-made-a-copy-of-himself":"https://spectrum.ieee.org/robotics/humanoids/hiroshi-ishiguro-the-man-who-made-a-copy-of-himself","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/humanoids/next-generation-of-boston-dynamics-atlas-robot":"https://spectrum.ieee.org/automaton/robotics/humanoids/next-generation-of-boston-dynamics-atlas-robot","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/roomba-inventor-joe-jones-on-weed-killing-robot":"https://spectrum.ieee.org/automaton/robotics/home-robots/roomba-inventor-joe-jones-on-weed-killing-robot","https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet_/automaton/robotics/industrial-robots/collaborative-robots-innovation-growth-driver":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/collaborative-robots-innovation-growth-driver","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation_/tag/Microsoft":"https://spectrum.ieee.org/tag/Microsoft","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/preview/automaton/robotics/industrial-robots/north-sea-deployment-shows-how-quadruped-robots-can-be-commercially-useful":"https://spectrum.ieee.org/preview/automaton/robotics/industrial-robots/north-sea-deployment-shows-how-quadruped-robots-can-be-commercially-useful","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation_/tag/AI+history":"https://spectrum.ieee.org/tag/AI+history","https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station_/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-launching-astrobee-robots-to-iss-tomorrow","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit_/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next":"https://spectrum.ieee.org/automaton/robotics/humanoids/darpa-robotics-challenge-amazing-moments-lessons-learned-whats-next","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt_/tag/SubT":"https://spectrum.ieee.org/tag/SubT","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/searchContent?q=Internet+of+things":"https://spectrum.ieee.org/searchContent?q=Internet+of+things","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions_/automaton/robotics/robotics-hardware/robot-squid-and-robot-scallop-showcase-bioinspired-underwater-propulsion":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-squid-and-robot-scallop-showcase-bioinspired-underwater-propulsion","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/consumer-electronics/gadgets/the-consumer-electronics-hall-of-fame-tandyradioshack-trs80-model-1":"https://spectrum.ieee.org/consumer-electronics/gadgets/the-consumer-electronics-hall-of-fame-tandyradioshack-trs80-model-1","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/2014-robot-gift-guide":"https://spectrum.ieee.org/automaton/robotics/home-robots/2014-robot-gift-guide","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot_/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet_/automaton/robotics/industrial-robots/universal-robots":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots","https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing_/aerospace/satellites/overclock-my-satellite":"https://spectrum.ieee.org/aerospace/satellites/overclock-my-satellite","https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet_/automaton/robotics/industrial-robots/universal-robots-ur3-robotic-arm":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-ur3-robotic-arm","https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions_/automaton/robotics/robotics-hardware/squishy-pink-robot-makes-explosive-jumps":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/squishy-pink-robot-makes-explosive-jumps","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-2-back":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-2-back","https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions_/automaton/robotics/robotics-hardware/novel-wings-and-jet-thrusters-for-swimming-flying-robots":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/novel-wings-and-jet-thrusters-for-swimming-flying-robots","https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health_#SS221474766":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health#SS221474766","https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans_/tech-talk/robotics/artificial-intelligence/interactive-simulation-teaches-ai-about-real-world":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/interactive-simulation-teaches-ai-about-real-world","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous_#TopPageAnchor":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous#TopPageAnchor","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills_/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies_/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes_/robotics/humanoids/human-reflexes-help-mits-hermes-rescue-robot-keep-its-footing":"https://spectrum.ieee.org/robotics/humanoids/human-reflexes-help-mits-hermes-rescue-robot-keep-its-footing","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics_/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built_/tech-talk/semiconductors/processors/whats-better-than-40-gpubased-servers-a-server-with-40-gpus":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/whats-better-than-40-gpubased-servers-a-server-with-40-gpus","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/automaton/robotics/industrial-robots/robotic-dreams-robotic-realities":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/robotic-dreams-robotic-realities","https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020_/automaton/robotics/space-robots/robot-arm-mars-2020-rover":"https://spectrum.ieee.org/automaton/robotics/space-robots/robot-arm-mars-2020-rover","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots_/automaton/robotics/home-robots/robot-gift-guide-2019":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation_/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/irobot-completely-redesigns-its-floor-care-robots-with-new-m6-and-s9":"https://spectrum.ieee.org/automaton/robotics/home-robots/irobot-completely-redesigns-its-floor-care-robots-with-new-m6-and-s9","https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies_/automaton/robotics/artificial-intelligence/nvidia-brings-robot-simulation-closer-to-reality-by-making-humans-redundant":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/nvidia-brings-robot-simulation-closer-to-reality-by-making-humans-redundant","https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility_/automaton/robotics/military-robots/atlas-drc-robot-is-75-percent-new-completely-unplugged":"https://spectrum.ieee.org/automaton/robotics/military-robots/atlas-drc-robot-is-75-percent-new-completely-unplugged","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream_/automaton/robotics/drones/voliro-hexacopter-uses-rotating-nacelles-to-perform-versatile-acrobatics":"https://spectrum.ieee.org/automaton/robotics/drones/voliro-hexacopter-uses-rotating-nacelles-to-perform-versatile-acrobatics","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/military-robots/nasa-jsc-unveils-valkyrie-drc-robot":"https://spectrum.ieee.org/automaton/robotics/military-robots/nasa-jsc-unveils-valkyrie-drc-robot","https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution_/automaton/robotics/industrial-robots/amazon-acquires-kiva-systems-for-775-million":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/amazon-acquires-kiva-systems-for-775-million","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots_/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims_/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-mechanical-turk-revisited-tktkt":"https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-mechanical-turk-revisited-tktkt","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/aerospace/robotic-exploration/how-nasa-will-use-robots-to-create-rocket-fuel-from-martian-soil":"https://spectrum.ieee.org/aerospace/robotic-exploration/how-nasa-will-use-robots-to-create-rocket-fuel-from-martian-soil","https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley_/automaton/robotics/humanoids/the-uncanny-valley":"https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview_/automaton/robotics/industrial-robots/darpa-subterranean-challenge-qa-with-program-manager-timothy-chung":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/darpa-subterranean-challenge-qa-with-program-manager-timothy-chung","https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station_/automaton/robotics/space-robots/how-nasa-astrobee-robot-is-bringing-useful-autonomy-to-the-iss":"https://spectrum.ieee.org/automaton/robotics/space-robots/how-nasa-astrobee-robot-is-bringing-useful-autonomy-to-the-iss","https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly_/tag/skydio":"https://spectrum.ieee.org/tag/skydio","https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot_/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasas-robonaut-to-return-to-iss-with-legs-attached","https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons_/robotics/drones":"https://spectrum.ieee.org/robotics/drones","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now_/preview/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale":"https://spectrum.ieee.org/preview/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge_/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle_/automaton/robotics/robotics-hardware/cassie-on-hovershoes":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/cassie-on-hovershoes","https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly_/automaton/robotics/drones/review-parrot-anafi-drone":"https://spectrum.ieee.org/automaton/robotics/drones/review-parrot-anafi-drone","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robot%20video%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters_mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday":"mailto:e.guizzo@ieee.org; evan.ackerman@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday","https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped_/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots_/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot","https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale_/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft":"https://spectrum.ieee.org/automaton/robotics/humanoids/softbank-acquires-boston-dynamics-and-schaft","https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019_/automaton/robotics/home-robots/how-root-wants-to-bring-coding-to-every-classroom":"https://spectrum.ieee.org/automaton/robotics/home-robots/how-root-wants-to-bring-coding-to-every-classroom","https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail_/automaton/robotics/space-robots/gitai-partners-with-jaxa-to-send-telepresence-robots-to-space":"https://spectrum.ieee.org/automaton/robotics/space-robots/gitai-partners-with-jaxa-to-send-telepresence-robots-to-space","https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light_/tech-talk/robotics/artificial-intelligence/intels-neuromorphic-system-hits-8-million-neurons-100-million-coming-by-2020":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/intels-neuromorphic-system-hits-8-million-neurons-100-million-coming-by-2020","https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation_#footnote-1-back":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation#footnote-1-back"},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-037.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-007.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-52.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-011.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-020.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-025.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-54.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe69.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe-57.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe64.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe-22.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-013.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-039.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe-24.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-55.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe61.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-030.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-026.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://us-001.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://europe-25.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-031.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-014.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-027.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://europe-56.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://us-53.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe68.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe63.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-015.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe-60.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-024.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe70.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-038.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://europe-23.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-017.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://europe21.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-008.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-029.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-012.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-018.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://europe65.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-034.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe-58.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":5,"resultList":[200,200,200,200,200]},"http://us-035.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-022.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-006.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-019.herokuapp.com/":{"failCount":0,"successCount":6,"resultList":[200,200,200,200,200,200]},"http://us-010.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe-59.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://us-51.herokuapp.com/":{"failCount":0,"successCount":8,"resultList":[200,200,200,200,200,200,200,200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":9,"resultList":[200,200,200,200,200,200,200,200,200]},"http://us-023.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":7,"resultList":[200,200,200,200,200,200,200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://feedproxy.google.com/image/MzM1MzE3OA.jpeg","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":8606,"totalSpendMs":77,"convertSpendMs":0,"createdTime":"2020-08-25 18:37:58","host":"us-028*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NzM1MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn33@2020_4/2020/08/25/10-37-58-806_e317ffe2a5e8da89.webp","sourceBytes":113230,"destBytes":33396,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":975,"convertSpendMs":42,"createdTime":"2020-08-25 18:37:58","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india","linkMd5ListStr":"645d9089aa580f603b8e05528f7302e7","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"110.6 KB","destSize":"32.6 KB","compressRate":"29.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MDMzNw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn58@2020_2/2020/08/25/10-37-58-812_4c92b939334d08ef.webp","sourceBytes":99777,"destBytes":35694,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1057,"convertSpendMs":59,"createdTime":"2020-08-25 18:37:58","host":"us-024*","referer":"https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality","linkMd5ListStr":"522bcd2fb6d5cf0d6950cfc6b4675a85","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"97.4 KB","destSize":"34.9 KB","compressRate":"35.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2OTI5Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn94@2020_2/2020/08/25/10-37-58-811_edc7fa982b7c63be.webp","sourceBytes":371519,"destBytes":56100,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1067,"convertSpendMs":57,"createdTime":"2020-08-25 18:37:58","host":"us-54*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot","linkMd5ListStr":"fb732aaba2fa77f47db578dd8a2e2c4e","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"362.8 KB","destSize":"54.8 KB","compressRate":"15.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4OTg0NA.jpeg","sourceStatusCode":200,"destWidth":2048,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn14@2020_5/2020/08/25/10-37-58-876_612463ef1ba83719.webp","sourceBytes":162523,"destBytes":44522,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1101,"convertSpendMs":124,"createdTime":"2020-08-25 18:37:58","host":"us-028*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","linkMd5ListStr":"9778ce9d1c486affccf103eca762f153","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"158.7 KB","destSize":"43.5 KB","compressRate":"27.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MTI5OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn86@2020_5/2020/08/25/10-37-58-885_74477b395248b833.webp","sourceBytes":70508,"destBytes":27522,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1155,"convertSpendMs":53,"createdTime":"2020-08-25 18:37:58","host":"us-004*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","linkMd5ListStr":"953ccece03a4def30b62f328e3f48f7e","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"68.9 KB","destSize":"26.9 KB","compressRate":"39%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU2Nw.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":1021,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn26@2020_5/2020/08/25/10-37-58-851_888a23e5ea91e340.webp","sourceBytes":653920,"destBytes":76910,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1164,"convertSpendMs":94,"createdTime":"2020-08-25 18:37:58","host":"us-012*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"638.6 KB","destSize":"75.1 KB","compressRate":"11.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDMxNw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn74@2020_1/2020/08/25/10-37-58-843_39ec1a66da957d67.webp","sourceBytes":494948,"destBytes":70372,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1153,"convertSpendMs":60,"createdTime":"2020-08-25 18:37:58","host":"us-020*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"483.3 KB","destSize":"68.7 KB","compressRate":"14.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NzI2Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn70@2020_4/2020/08/25/10-37-58-829_a151a33ce4fcda1a.webp","sourceBytes":245207,"destBytes":133280,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1164,"convertSpendMs":53,"createdTime":"2020-08-25 18:37:58","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","linkMd5ListStr":"ba866f9c04fa639f4052fdd904ce7f83","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"239.5 KB","destSize":"130.2 KB","compressRate":"54.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzY4MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn38@2020_5/2020/08/25/10-37-58-813_729009bcf7a04300.webp","sourceBytes":189130,"destBytes":100316,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1170,"convertSpendMs":43,"createdTime":"2020-08-25 18:37:58","host":"us-020*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space","linkMd5ListStr":"98d9acef762105ed3deea1273ccf06c8","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"184.7 KB","destSize":"98 KB","compressRate":"53%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MDAxNA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn97@2020_5/2020/08/25/10-37-58-826_57e4fa566227a65e.webp","sourceBytes":191999,"destBytes":107250,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1179,"convertSpendMs":88,"createdTime":"2020-08-25 18:37:58","host":"us-012*","referer":"https://spectrum.ieee.org/tech-talk/computing/software/aipowered-robotic-system-stirs-up-molecules-from-scratch","linkMd5ListStr":"1fbe4fd76aaa4013dc648f063ffe877a","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"187.5 KB","destSize":"104.7 KB","compressRate":"55.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NDAxNA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn18@2020_5/2020/08/25/10-37-58-828_c81be9b4b4fbdfa3.webp","sourceBytes":209744,"destBytes":86444,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1190,"convertSpendMs":56,"createdTime":"2020-08-25 18:37:58","host":"us-016*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","linkMd5ListStr":"745523971937e3d2bbf0ca9e0d077655","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"204.8 KB","destSize":"84.4 KB","compressRate":"41.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4MzA0Mw.jpeg","sourceStatusCode":200,"destWidth":1072,"destHeight":739,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn29@2020_5/2020/08/25/10-37-58-837_830ff145b063714e.webp","sourceBytes":355765,"destBytes":42380,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1206,"convertSpendMs":69,"createdTime":"2020-08-25 18:37:58","host":"us-54*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever","linkMd5ListStr":"019269ec580d7ee2f12ac1c56db02f28","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"347.4 KB","destSize":"41.4 KB","compressRate":"11.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1MzQzNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn2@2020_4/2020/08/25/10-37-58-871_719094b708d94048.webp","sourceBytes":134397,"destBytes":57690,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1214,"convertSpendMs":48,"createdTime":"2020-08-25 18:37:58","host":"us-020*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"131.2 KB","destSize":"56.3 KB","compressRate":"42.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxODYyMw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn82@2020_2/2020/08/25/10-37-58-825_c16fd9ea21c1ae96.webp","sourceBytes":188578,"destBytes":98204,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1233,"convertSpendMs":63,"createdTime":"2020-08-25 18:37:58","host":"us-016*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","linkMd5ListStr":"fdeba8f58f9ca993017a817b623b747a","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"184.2 KB","destSize":"95.9 KB","compressRate":"52.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3MzAzNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":857,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn73@2020_5/2020/08/25/10-37-58-852_23982c065623be53.webp","sourceBytes":728043,"destBytes":108186,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1246,"convertSpendMs":50,"createdTime":"2020-08-25 18:37:58","host":"us-040*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions","linkMd5ListStr":"d38b2f1b0a483a0b8124b15d386f7c04","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"711 KB","destSize":"105.7 KB","compressRate":"14.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MjkwNw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":843,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn94@2020_3/2020/08/25/10-37-58-815_e72fabfa413eae06.webp","sourceBytes":365014,"destBytes":95172,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1261,"convertSpendMs":65,"createdTime":"2020-08-25 18:37:58","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home","linkMd5ListStr":"5f68fac164425c5c46c2a07001ed834c","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"356.5 KB","destSize":"92.9 KB","compressRate":"26.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwOTIzOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn21@2020_2/2020/08/25/10-37-58-901_0e9de44fe4aa15d8.webp","sourceBytes":190162,"destBytes":95572,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1271,"convertSpendMs":69,"createdTime":"2020-08-25 18:37:58","host":"us-004*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous","linkMd5ListStr":"6ab80d13744ce4a42ea20fe59304598b","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"185.7 KB","destSize":"93.3 KB","compressRate":"50.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyNDI4Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":704,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn98@2020_6/2020/08/25/10-37-58-833_e0bf7973e0ba05b6.webp","sourceBytes":446637,"destBytes":37192,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1277,"convertSpendMs":37,"createdTime":"2020-08-25 18:37:58","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves","linkMd5ListStr":"51766f8db6dedb5c20550ff3c3224e16","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"436.2 KB","destSize":"36.3 KB","compressRate":"8.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NDM5NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn18@2020_3/2020/08/25/10-37-58-883_1da9fa18b44fa5dd.webp","sourceBytes":291254,"destBytes":168448,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1286,"convertSpendMs":123,"createdTime":"2020-08-25 18:37:58","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"284.4 KB","destSize":"164.5 KB","compressRate":"57.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NTExMQ.png","sourceStatusCode":200,"destWidth":607,"destHeight":451,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn42@2020_2/2020/08/25/10-37-58-994_d72dce30983dcfe9.webp","sourceBytes":445022,"destBytes":20244,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1357,"convertSpendMs":59,"createdTime":"2020-08-25 18:37:58","host":"us-008*","referer":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","linkMd5ListStr":"4afdb44919c1840f63dbb5b7c83f76a7","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"434.6 KB","destSize":"19.8 KB","compressRate":"4.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NTkyMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn78@2020_2/2020/08/25/10-37-58-961_29426200f9ef07e4.webp","sourceBytes":156381,"destBytes":85164,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1370,"convertSpendMs":92,"createdTime":"2020-08-25 18:37:58","host":"us-008*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816","linkMd5ListStr":"a31b47238c2872d0b2da6a891bae5495","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"152.7 KB","destSize":"83.2 KB","compressRate":"54.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMzgyNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn62@2020_2/2020/08/25/10-37-58-978_b35f47022b8dde79.webp","sourceBytes":769631,"destBytes":136162,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1393,"convertSpendMs":164,"createdTime":"2020-08-25 18:37:58","host":"us-012*","referer":"https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances","linkMd5ListStr":"ecb83ca1fe85f5c86b44772662510c06","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"751.6 KB","destSize":"133 KB","compressRate":"17.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwOTE2NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn53@2020_6/2020/08/25/10-37-58-981_1a8e6c6cea3c5c38.webp","sourceBytes":258907,"destBytes":142882,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1410,"convertSpendMs":218,"createdTime":"2020-08-25 18:37:58","host":"us-036*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","linkMd5ListStr":"9ddfdd2b95e95bd2322570f7f5c5dbdb","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"252.8 KB","destSize":"139.5 KB","compressRate":"55.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3ODY1Ng.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn5@2020_2/2020/08/25/10-37-58-973_83c8f686fddb633f.webp","sourceBytes":207195,"destBytes":121966,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1383,"convertSpendMs":89,"createdTime":"2020-08-25 18:37:58","host":"us-008*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019","linkMd5ListStr":"942b7fea25553a3c98fd6f49daf1b0f2","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"202.3 KB","destSize":"119.1 KB","compressRate":"58.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDQzNA.jpeg","sourceStatusCode":200,"destWidth":2000,"destHeight":1298,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn10@2020_3/2020/08/25/10-37-58-935_390e87105eec721e.webp","sourceBytes":720739,"destBytes":285378,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1451,"convertSpendMs":132,"createdTime":"2020-08-25 18:37:58","host":"us-040*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","linkMd5ListStr":"de8494e008723f24ca95f4421202eaa2","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"703.8 KB","destSize":"278.7 KB","compressRate":"39.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5Mzc0Ng.jpeg","sourceStatusCode":200,"destWidth":2303,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn22@2020_2/2020/08/25/10-37-58-930_82134a75d4c96a90.webp","sourceBytes":585617,"destBytes":166066,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1502,"convertSpendMs":166,"createdTime":"2020-08-25 18:37:58","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","linkMd5ListStr":"184896aa81aa8ae49425e7c03d12d965","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"571.9 KB","destSize":"162.2 KB","compressRate":"28.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxODg1Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":845,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn45@2020_4/2020/08/25/10-37-58-979_ba283e78cf896002.webp","sourceBytes":119067,"destBytes":68286,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1400,"convertSpendMs":58,"createdTime":"2020-08-25 18:37:58","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope","linkMd5ListStr":"bacf594041d2950428c4810385b725b0","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"116.3 KB","destSize":"66.7 KB","compressRate":"57.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDI2OA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":698,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn82@2020_5/2020/08/25/10-37-58-985_8fd194d8e4e3ec58.webp","sourceBytes":166088,"destBytes":94580,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1477,"convertSpendMs":55,"createdTime":"2020-08-25 18:37:58","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills","linkMd5ListStr":"b050441ca360dcb3e9e825f306284a0b","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"162.2 KB","destSize":"92.4 KB","compressRate":"56.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDc5Ng.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn90@2020_1/2020/08/25/10-37-58-951_5f6efee0592c4318.webp","sourceBytes":84893,"destBytes":46474,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1457,"convertSpendMs":40,"createdTime":"2020-08-25 18:37:58","host":"europe66*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"82.9 KB","destSize":"45.4 KB","compressRate":"54.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3MDkwNw.png","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn2@2020_5/2020/08/25/10-37-59-098_c04232e391ce18d3.webp","sourceBytes":1339469,"destBytes":106406,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1646,"convertSpendMs":101,"createdTime":"2020-08-25 18:37:58","host":"europe-24*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid","linkMd5ListStr":"783ad2e174faa245c24d60ad66d167ad","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"103.9 KB","compressRate":"7.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NTAwMg.jpeg","sourceStatusCode":200,"destWidth":2217,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn77@2020_1/2020/08/25/10-37-58-920_09c0be81568c7be0.webp","sourceBytes":190795,"destBytes":87906,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1863,"convertSpendMs":161,"createdTime":"2020-08-25 18:37:58","host":"us-028*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet","linkMd5ListStr":"b92198dab0130e2ebf3c419ddb0d4903","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"186.3 KB","destSize":"85.8 KB","compressRate":"46.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTc2NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn65@2020_2/2020/08/25/10-37-59-026_934227c26d565a28.webp","sourceBytes":325864,"destBytes":307400,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1847,"convertSpendMs":76,"createdTime":"2020-08-25 18:37:58","host":"europe-24*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","linkMd5ListStr":"ec21b6293bbcb368dd9079b4df431fd8","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"318.2 KB","destSize":"300.2 KB","compressRate":"94.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjI4Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn67@2020_5/2020/08/25/10-37-59-985_95de98088918a975.webp","sourceBytes":178722,"destBytes":25574,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":981,"convertSpendMs":31,"createdTime":"2020-08-25 18:37:59","host":"us-023*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot","linkMd5ListStr":"3a355f69e17aa2bb77552519184504ec","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"174.5 KB","destSize":"25 KB","compressRate":"14.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxMjEwOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":938,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn35@2020_6/2020/08/25/10-37-59-877_3cdfdf5f06e20eee.webp","sourceBytes":231618,"destBytes":52674,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1207,"convertSpendMs":37,"createdTime":"2020-08-25 18:37:59","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","linkMd5ListStr":"8be8e5cf657dd68e13509f254295323b","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"226.2 KB","destSize":"51.4 KB","compressRate":"22.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MTY2OA.jpeg","sourceStatusCode":200,"destWidth":1614,"destHeight":1002,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn39@2020_2/2020/08/25/10-37-59-804_8adac4e031007f90.webp","sourceBytes":422678,"destBytes":123628,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1166,"convertSpendMs":74,"createdTime":"2020-08-25 18:37:59","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream","linkMd5ListStr":"727addd4da03ea6fd9f64c61ab6be805","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"412.8 KB","destSize":"120.7 KB","compressRate":"29.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwODg0NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn100@2020_4/2020/08/25/10-38-00-126_1100de68ae27ac02.webp","sourceBytes":293895,"destBytes":9260,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":959,"convertSpendMs":60,"createdTime":"2020-08-25 18:37:59","host":"us-005*","referer":"https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020","linkMd5ListStr":"538b0c13aaa64a59898970581b42f5fa","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"287 KB","destSize":"9 KB","compressRate":"3.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNDUyOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn43@2020_2/2020/08/25/10-37-59-814_988d77132adc0f25.webp","sourceBytes":250127,"destBytes":145944,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1180,"convertSpendMs":54,"createdTime":"2020-08-25 18:37:59","host":"us-021*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation","linkMd5ListStr":"026bc4b5af35d78854aaaae5d60ee79a","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"244.3 KB","destSize":"142.5 KB","compressRate":"58.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODIyOQ.png","sourceStatusCode":200,"destWidth":607,"destHeight":458,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn76@2020_6/2020/08/25/10-37-59-997_b6a7f1eca650a0f9.webp","sourceBytes":545084,"destBytes":39720,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1085,"convertSpendMs":21,"createdTime":"2020-08-25 18:37:59","host":"us-003*","referer":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","linkMd5ListStr":"c37b6ff6169ec7e6a055a9904de421c4","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"532.3 KB","destSize":"38.8 KB","compressRate":"7.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMzA0Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":927,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn47@2020_2/2020/08/25/10-37-59-992_91772e9d1772a86d.webp","sourceBytes":376942,"destBytes":104684,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1198,"convertSpendMs":68,"createdTime":"2020-08-25 18:37:59","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","linkMd5ListStr":"5c8fee7c8bef1dc43b41be2a29c47ddb","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"368.1 KB","destSize":"102.2 KB","compressRate":"27.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTY0NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn10@2020_6/2020/08/25/10-37-59-589_306487e62d64bd2f.webp","sourceBytes":70750,"destBytes":43490,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2256,"convertSpendMs":32,"createdTime":"2020-08-25 18:37:58","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"69.1 KB","destSize":"42.5 KB","compressRate":"61.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNDQ5NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":762,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn19@2020_1/2020/08/25/10-38-00-157_6fc6d4e291470b2f.webp","sourceBytes":97264,"destBytes":37356,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":927,"convertSpendMs":29,"createdTime":"2020-08-25 18:38:00","host":"us-017*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone","linkMd5ListStr":"a2d95510787b8bacb7f69ee68b845ae0","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"95 KB","destSize":"36.5 KB","compressRate":"38.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NzgxNw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn91@2020_1/2020/08/25/10-38-00-062_a7d8996d7c304bc2.webp","sourceBytes":139981,"destBytes":73212,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1215,"convertSpendMs":59,"createdTime":"2020-08-25 18:37:59","host":"us-025*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","linkMd5ListStr":"c7f356a3967b14c7066d4a7ae2801b32","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"136.7 KB","destSize":"71.5 KB","compressRate":"52.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MzA3Mw.jpeg","sourceStatusCode":200,"destWidth":2048,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn32@2020_6/2020/08/25/10-37-59-478_5b3935cc3f8b22c9.webp","sourceBytes":502810,"destBytes":245590,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2137,"convertSpendMs":214,"createdTime":"2020-08-25 18:37:58","host":"europe63*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","linkMd5ListStr":"bac274849de61b732281df992d98ec37","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"491 KB","destSize":"239.8 KB","compressRate":"48.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyNDczNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":698,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn26@2020_4/2020/08/25/10-38-00-179_de252d8a3fcbefad.webp","sourceBytes":161691,"destBytes":83998,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1084,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:00","host":"us-019*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots","linkMd5ListStr":"67ca3612e21a3b8090962da8d8d72cea","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"157.9 KB","destSize":"82 KB","compressRate":"51.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4MDY0OQ.jpeg","sourceStatusCode":200,"destWidth":2048,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn87@2020_2/2020/08/25/10-38-00-108_96f1d359e1e449c2.webp","sourceBytes":356816,"destBytes":133450,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1286,"convertSpendMs":127,"createdTime":"2020-08-25 18:37:59","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"348.5 KB","destSize":"130.3 KB","compressRate":"37.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1OTIwOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn96@2020_4/2020/08/25/10-38-00-127_ba903a7565541c8d.webp","sourceBytes":209245,"destBytes":126562,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1292,"convertSpendMs":61,"createdTime":"2020-08-25 18:37:59","host":"us-015*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","linkMd5ListStr":"f4602756dbd851432f68681e9d7b807e","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"204.3 KB","destSize":"123.6 KB","compressRate":"60.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDg5Ng.jpeg","sourceStatusCode":200,"destWidth":1085,"destHeight":691,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn16@2020_3/2020/08/25/10-38-00-261_6cb852da5b471c03.webp","sourceBytes":506419,"destBytes":108916,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1156,"convertSpendMs":46,"createdTime":"2020-08-25 18:38:00","host":"us-027*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"494.5 KB","destSize":"106.4 KB","compressRate":"21.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNDMwOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn69@2020_3/2020/08/25/10-37-59-701_03c9c0f0f0e67746.webp","sourceBytes":162147,"destBytes":69880,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2477,"convertSpendMs":38,"createdTime":"2020-08-25 18:37:58","host":"europe62*","referer":"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things","linkMd5ListStr":"daf121b367dccdb1c38a3b54e0c17b83","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"158.3 KB","destSize":"68.2 KB","compressRate":"43.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NDc1Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn59@2020_5/2020/08/25/10-38-00-270_01c86ef1c9aa25b4.webp","sourceBytes":187017,"destBytes":47898,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1341,"convertSpendMs":37,"createdTime":"2020-08-25 18:37:59","host":"us-55*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain","linkMd5ListStr":"4bdb7374dde6559b940d866d23415332","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"182.6 KB","destSize":"46.8 KB","compressRate":"25.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NzgzMg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn72@2020_1/2020/08/25/10-38-00-217_de117fd7fec19bf4.webp","sourceBytes":263572,"destBytes":152398,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1481,"convertSpendMs":210,"createdTime":"2020-08-25 18:37:59","host":"us-013*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/want-a-really-hard-machine-learning-problem-try-agriculture-say-john-deere-labs-leaders","linkMd5ListStr":"5f2d11cc893fe393f2105cffb5ccf143","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"257.4 KB","destSize":"148.8 KB","compressRate":"57.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNjMwNg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":900,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn11@2020_4/2020/08/25/10-38-00-315_ebabe405998644e7.webp","sourceBytes":223611,"destBytes":35112,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1293,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:00","host":"us-039*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","linkMd5ListStr":"2c1d20e7c96f50d4e36fa6c0bb86c46f","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"218.4 KB","destSize":"34.3 KB","compressRate":"15.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4ODc2MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn6@2020_6/2020/08/25/10-37-59-935_fddbd50ffa89c32b.webp","sourceBytes":202779,"destBytes":110400,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2634,"convertSpendMs":44,"createdTime":"2020-08-25 18:37:58","host":"europe62*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence","linkMd5ListStr":"b19d13bf54f71c1c40ce0f7d38f1a58b","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"198 KB","destSize":"107.8 KB","compressRate":"54.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NjkxNQ.jpeg","sourceStatusCode":200,"destWidth":2048,"destHeight":1431,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn63@2020_3/2020/08/25/10-38-00-247_d89b9a0963551bbc.webp","sourceBytes":661233,"destBytes":464868,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1388,"convertSpendMs":169,"createdTime":"2020-08-25 18:38:00","host":"us-008*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food","linkMd5ListStr":"d8a32c6887301ef3975606092a06d695","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"645.7 KB","destSize":"454 KB","compressRate":"70.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNDk4NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn30@2020_2/2020/08/25/10-38-00-289_3559881935b23433.webp","sourceBytes":529573,"destBytes":189916,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1370,"convertSpendMs":56,"createdTime":"2020-08-25 18:38:00","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","linkMd5ListStr":"70815399970675009fdaa91f1ed9af88","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"517.2 KB","destSize":"185.5 KB","compressRate":"35.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3MzQ2Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":692,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn46@2020_1/2020/08/25/10-38-00-456_8eed592579d0b55e.webp","sourceBytes":152683,"destBytes":127574,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1202,"convertSpendMs":90,"createdTime":"2020-08-25 18:38:00","host":"us-038*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt","linkMd5ListStr":"fddb2a608a827cba7cd0974ca59eeee3","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"149.1 KB","destSize":"124.6 KB","compressRate":"83.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNDYzMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":878,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn53@2020_5/2020/08/25/10-38-00-524_61b21a0e36df9d34.webp","sourceBytes":210916,"destBytes":57420,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1060,"convertSpendMs":50,"createdTime":"2020-08-25 18:38:00","host":"us-017*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle","linkMd5ListStr":"ed36754ceb3d54691c9985ca6f20cf97","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"206 KB","destSize":"56.1 KB","compressRate":"27.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjUwMg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":712,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn49@2020_1/2020/08/25/10-37-59-961_4a0ade2551e29f57.webp","sourceBytes":307062,"destBytes":136976,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2762,"convertSpendMs":42,"createdTime":"2020-08-25 18:37:58","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"299.9 KB","destSize":"133.8 KB","compressRate":"44.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5OTYyMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn49@2020_4/2020/08/25/10-38-00-453_417a3d5a740fc8e5.webp","sourceBytes":312864,"destBytes":189692,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1295,"convertSpendMs":102,"createdTime":"2020-08-25 18:38:00","host":"us-026*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","linkMd5ListStr":"4116b47e7bcae1aed0f1e1c85e917d83","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"305.5 KB","destSize":"185.2 KB","compressRate":"60.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMDIwMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":742,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn14@2020_2/2020/08/25/10-38-00-025_597022f229b3aa34.webp","sourceBytes":340700,"destBytes":115194,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2808,"convertSpendMs":48,"createdTime":"2020-08-25 18:37:58","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats","linkMd5ListStr":"f89d2d766c8d79193395fae8610e0f13","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"332.7 KB","destSize":"112.5 KB","compressRate":"33.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTcxOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn25@2020_3/2020/08/25/10-38-00-327_3dead94228bef8f0.webp","sourceBytes":747309,"destBytes":109932,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2820,"convertSpendMs":94,"createdTime":"2020-08-25 18:37:58","host":"europe66*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/trump-cto-addresses-ai-facial-recognition-immigration-tech-infrastructure-and-more","linkMd5ListStr":"0e1d29b76415f6a75c1bacb441cc9c0d","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"729.8 KB","destSize":"107.4 KB","compressRate":"14.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjIwOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":743,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn8@2020_5/2020/08/25/10-38-00-147_54a7d4c124eb46a6.webp","sourceBytes":390434,"destBytes":126208,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1552,"convertSpendMs":42,"createdTime":"2020-08-25 18:38:00","host":"europe-59*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"381.3 KB","destSize":"123.2 KB","compressRate":"32.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMDk5NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":846,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn79@2020_3/2020/08/25/10-38-00-229_4f4a51bf3c70886b.webp","sourceBytes":471517,"destBytes":46824,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1669,"convertSpendMs":52,"createdTime":"2020-08-25 18:37:59","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo","linkMd5ListStr":"9959391ccbf704954bde8fcaaf54f5ba","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"460.5 KB","destSize":"45.7 KB","compressRate":"9.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NjczOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn83@2020_3/2020/08/25/10-38-00-292_b36f4e23edcb5c83.webp","sourceBytes":207546,"destBytes":75038,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1702,"convertSpendMs":50,"createdTime":"2020-08-25 18:37:59","host":"europe-57*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/georgia-tech-smarticles-stochastic-robot-swarm","linkMd5ListStr":"3579544e8e9768028a4476e076f6e099","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"202.7 KB","destSize":"73.3 KB","compressRate":"36.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNjk2NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn34@2020_6/2020/08/25/10-38-00-309_ee6c587ae8395183.webp","sourceBytes":714662,"destBytes":89844,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1582,"convertSpendMs":63,"createdTime":"2020-08-25 18:38:00","host":"europe-25*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"697.9 KB","destSize":"87.7 KB","compressRate":"12.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTYzOQ.jpeg","sourceStatusCode":200,"destWidth":2000,"destHeight":1466,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn77@2020_5/2020/08/25/10-38-01-032_635a012d3c9fd410.webp","sourceBytes":54847,"destBytes":36744,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":930,"convertSpendMs":110,"createdTime":"2020-08-25 18:38:00","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"53.6 KB","destSize":"35.9 KB","compressRate":"67%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MDk2NA.jpeg","sourceStatusCode":200,"destWidth":1095,"destHeight":637,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn56@2020_4/2020/08/25/10-38-00-214_aad665bc80a92e1e.webp","sourceBytes":265276,"destBytes":209214,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1895,"convertSpendMs":47,"createdTime":"2020-08-25 18:37:59","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot","linkMd5ListStr":"19aafea32608c470cfdef56d8c5fc346","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"259.1 KB","destSize":"204.3 KB","compressRate":"78.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDU1OA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn73@2020_5/2020/08/25/10-38-01-000_9e8f4c5d4f76a3ef.webp","sourceBytes":392774,"destBytes":37070,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1018,"convertSpendMs":73,"createdTime":"2020-08-25 18:38:00","host":"us-035*","referer":"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies","linkMd5ListStr":"f06a1c762bc020e3eb9c056139e79fb8","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"383.6 KB","destSize":"36.2 KB","compressRate":"9.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NjQ0Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn97@2020_3/2020/08/25/10-38-01-127_febe66035ffb225f.webp","sourceBytes":593587,"destBytes":26086,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":958,"convertSpendMs":72,"createdTime":"2020-08-25 18:38:01","host":"us-53*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","linkMd5ListStr":"50bd446780e290d101b8680d1908ab28","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"579.7 KB","destSize":"25.5 KB","compressRate":"4.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2ODA2Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":948,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn81@2020_6/2020/08/25/10-38-01-054_365676f393d0bc09.webp","sourceBytes":965084,"destBytes":51076,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1123,"convertSpendMs":52,"createdTime":"2020-08-25 18:38:00","host":"us-015*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"942.5 KB","destSize":"49.9 KB","compressRate":"5.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxMTAzNA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn70@2020_5/2020/08/25/10-38-00-959_3e1672f9363a5fda.webp","sourceBytes":318686,"destBytes":205706,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1260,"convertSpendMs":81,"createdTime":"2020-08-25 18:38:00","host":"us-55*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","linkMd5ListStr":"ddd96fe231dfc465a52535e475a98b15","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"311.2 KB","destSize":"200.9 KB","compressRate":"64.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2Mzg1OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn4@2020_3/2020/08/25/10-38-00-606_020b802523e4b667.webp","sourceBytes":178085,"destBytes":105214,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2056,"convertSpendMs":64,"createdTime":"2020-08-25 18:38:00","host":"europe69*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light","linkMd5ListStr":"f1fb990643b446a2fb09abe6cb8c2668","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"173.9 KB","destSize":"102.7 KB","compressRate":"59.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3Nzg3NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":680,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn1@2020_6/2020/08/25/10-38-01-220_be87524870fa3a8c.webp","sourceBytes":515521,"destBytes":49396,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1022,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:01","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"503.4 KB","destSize":"48.2 KB","compressRate":"9.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NDQwNQ.jpeg","sourceStatusCode":200,"destWidth":1836,"destHeight":1306,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn14@2020_2/2020/08/25/10-38-01-310_8133afe0ba86868c.webp","sourceBytes":135356,"destBytes":48888,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1007,"convertSpendMs":78,"createdTime":"2020-08-25 18:38:01","host":"us-004*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot","linkMd5ListStr":"4542d0e2bf7583a7652c5511e14ef53d","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"132.2 KB","destSize":"47.7 KB","compressRate":"36.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxOTU0OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn38@2020_1/2020/08/25/10-38-00-356_a93f12e0e4bc5b59.webp","sourceBytes":409249,"destBytes":275892,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1955,"convertSpendMs":72,"createdTime":"2020-08-25 18:38:00","host":"europe66*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/help-rescuers-find-missing-persons-through-emergency-response-contest","linkMd5ListStr":"c70149e9a4334875ea4bbf98504cc029","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"399.7 KB","destSize":"269.4 KB","compressRate":"67.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4Nzk2NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":800,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn57@2020_1/2020/08/25/10-38-01-105_6270000a8ce60780.webp","sourceBytes":360000,"destBytes":110864,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1305,"convertSpendMs":60,"createdTime":"2020-08-25 18:38:00","host":"us-008*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters","linkMd5ListStr":"dd075dc0d056fad8777afa408a86be08","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"351.6 KB","destSize":"108.3 KB","compressRate":"30.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTYxOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn52@2020_1/2020/08/25/10-38-00-837_f26e88e257c2e145.webp","sourceBytes":383058,"destBytes":54404,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2294,"convertSpendMs":34,"createdTime":"2020-08-25 18:37:59","host":"europe-25*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"374.1 KB","destSize":"53.1 KB","compressRate":"14.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1MjU4Mw.jpeg","sourceStatusCode":200,"destWidth":1800,"destHeight":1200,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_6/2020/08/25/10-38-01-193_82711b7ee6c2607e.webp","sourceBytes":290474,"destBytes":104954,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1297,"convertSpendMs":82,"createdTime":"2020-08-25 18:38:00","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail","linkMd5ListStr":"2b5ea3f72dcd3a0879d0e273a51ddf91","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"283.7 KB","destSize":"102.5 KB","compressRate":"36.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTUwNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn6@2020_4/2020/08/25/10-38-01-261_fe79eadf32a07dbb.webp","sourceBytes":147129,"destBytes":82716,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1097,"convertSpendMs":41,"createdTime":"2020-08-25 18:38:01","host":"us-022*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/rigid-it-departments-jobhopping-data-scientists-implementing-machine-learning-isnt-easy","linkMd5ListStr":"110e6ac838f5a686ca9016e11c381c4e","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"143.7 KB","destSize":"80.8 KB","compressRate":"56.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTc2MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn10@2020_3/2020/08/25/10-38-01-354_d0a908ec9026c56b.webp","sourceBytes":406720,"destBytes":87488,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1189,"convertSpendMs":131,"createdTime":"2020-08-25 18:38:01","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"397.2 KB","destSize":"85.4 KB","compressRate":"21.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NTQ2NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn30@2020_1/2020/08/25/10-38-01-433_ded525da14056bcd.webp","sourceBytes":107064,"destBytes":63040,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1090,"convertSpendMs":43,"createdTime":"2020-08-25 18:38:01","host":"us-030*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","linkMd5ListStr":"ade9b8fe86da931a7835aee397c88af5","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"104.6 KB","destSize":"61.6 KB","compressRate":"58.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NTE5Ng.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn86@2020_6/2020/08/25/10-38-00-448_4376dd538513a1aa.webp","sourceBytes":696562,"destBytes":411318,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3628,"convertSpendMs":88,"createdTime":"2020-08-25 18:37:58","host":"europe-58*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/us-energy-department-is-first-customer-for-worlds-biggest-chip","linkMd5ListStr":"f98c1554d57da1ae3c3120fbc73c502e","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"680.2 KB","destSize":"401.7 KB","compressRate":"59%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjU0Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn61@2020_4/2020/08/25/10-38-00-970_c4dcfbc79c6fe996.webp","sourceBytes":178115,"destBytes":88388,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1659,"convertSpendMs":40,"createdTime":"2020-08-25 18:38:00","host":"europe-23*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever","linkMd5ListStr":"9811c27a8bfb20ca93a0926d39ae7e1e","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"173.9 KB","destSize":"86.3 KB","compressRate":"49.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjg5NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn93@2020_3/2020/08/25/10-38-01-195_c243fbbb43d6018e.webp","sourceBytes":131415,"destBytes":65548,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1272,"convertSpendMs":36,"createdTime":"2020-08-25 18:38:01","host":"europe62*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-prospects-for-tomorrows-jobs","linkMd5ListStr":"8e5ebdad5680d32249cb5e9715f5ba1f","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"128.3 KB","destSize":"64 KB","compressRate":"49.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3OTAxOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn33@2020_3/2020/08/25/10-38-01-419_9ce4826c1a7bd3b8.webp","sourceBytes":391323,"destBytes":225650,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1187,"convertSpendMs":63,"createdTime":"2020-08-25 18:38:01","host":"us-020*","referer":"https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface","linkMd5ListStr":"c521e6873fe26deaf6eefa6c6967cc9c","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"382.2 KB","destSize":"220.4 KB","compressRate":"57.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/EtQ_JVz0FMI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn78@2020_4/2020/08/25/10-38-01-674_1b5d89bcbe4796ee.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":867,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:01","host":"us-038*","referer":"https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality","linkMd5ListStr":"522bcd2fb6d5cf0d6950cfc6b4675a85","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MjE3Ng.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":868,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn50@2020_4/2020/08/25/10-38-01-575_d65d69e996dcea5d.webp","sourceBytes":297134,"destBytes":82122,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1079,"convertSpendMs":89,"createdTime":"2020-08-25 18:38:01","host":"us-038*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot","linkMd5ListStr":"507ca1aa50869b0a0bb43058028bc0d9","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"290.2 KB","destSize":"80.2 KB","compressRate":"27.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3MDI4OA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":957,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn38@2020_6/2020/08/25/10-38-01-661_52fb8c6dde7c5129.webp","sourceBytes":263945,"destBytes":79996,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1323,"convertSpendMs":54,"createdTime":"2020-08-25 18:38:01","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped","linkMd5ListStr":"6d47e21e002c0841e6518b91f4e6bdab","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"257.8 KB","destSize":"78.1 KB","compressRate":"30.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=wgEBanE3smE:EDJW7HFV0R8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn89@2020_4/2020/08/25/10-38-01-874_ca37d6cd52575bab.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":861,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:01","host":"us-001*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","linkMd5ListStr":"bf340835c615e553278f7841ec5e7cd7","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1MjQ1Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":794,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn66@2020_6/2020/08/25/10-38-01-552_713ff00546a33655.webp","sourceBytes":226702,"destBytes":41032,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1754,"convertSpendMs":30,"createdTime":"2020-08-25 18:38:00","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/disney-research-dynamic-robotic-characters","linkMd5ListStr":"de6c237869321c3674d763f513eace56","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"221.4 KB","destSize":"40.1 KB","compressRate":"18.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxODY5OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn25@2020_5/2020/08/25/10-38-01-465_7e2cc88753233054.webp","sourceBytes":144919,"destBytes":71344,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1480,"convertSpendMs":55,"createdTime":"2020-08-25 18:38:01","host":"us-51*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one","linkMd5ListStr":"eaa12b83fdd04d32713423e0cead6909","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"141.5 KB","destSize":"69.7 KB","compressRate":"49.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMTgxOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn90@2020_5/2020/08/25/10-38-01-197_e6826056637aa7a5.webp","sourceBytes":304914,"destBytes":186948,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1628,"convertSpendMs":61,"createdTime":"2020-08-25 18:38:01","host":"europe21*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","linkMd5ListStr":"7c7f2266a8977ac619b5378792fcd8f7","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"297.8 KB","destSize":"182.6 KB","compressRate":"61.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTMxNg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn42@2020_4/2020/08/25/10-38-01-655_9ade2f8a97dcc40d.webp","sourceBytes":238780,"destBytes":43780,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1376,"convertSpendMs":62,"createdTime":"2020-08-25 18:38:01","host":"us-001*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/microsofts-ai-research-draws-controversy-over-possible-disinformation-use","linkMd5ListStr":"5e83ca8ac583d87a19ee7990f07737a8","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"233.2 KB","destSize":"42.8 KB","compressRate":"18.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxMzkyMg.jpeg","sourceStatusCode":200,"destWidth":1280,"destHeight":720,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn42@2020_2/2020/08/25/10-38-01-172_4d40cf32504dad3c.webp","sourceBytes":229837,"destBytes":148672,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2484,"convertSpendMs":50,"createdTime":"2020-08-25 18:38:00","host":"europe-57*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots","linkMd5ListStr":"40500605e90a1fb25e13a0c4109cdc90","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"224.5 KB","destSize":"145.2 KB","compressRate":"64.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNjIxMw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1074,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn65@2020_4/2020/08/25/10-38-01-910_9ec07841a91a12b9.webp","sourceBytes":204914,"destBytes":50336,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1253,"convertSpendMs":41,"createdTime":"2020-08-25 18:38:01","host":"us-009*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot","linkMd5ListStr":"0c575b7593d53cada9ce6cb4a5dbad5f","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"200.1 KB","destSize":"49.2 KB","compressRate":"24.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MDcwMA.jpeg","sourceStatusCode":200,"destWidth":2304,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn52@2020_1/2020/08/25/10-38-01-648_dd2020d930153c78.webp","sourceBytes":715350,"destBytes":193966,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1390,"convertSpendMs":166,"createdTime":"2020-08-25 18:38:01","host":"us-039*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","linkMd5ListStr":"bf340835c615e553278f7841ec5e7cd7","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"698.6 KB","destSize":"189.4 KB","compressRate":"27.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=M6oPcVu-H6M:Ky6uSH84PmI:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn73@2020_2/2020/08/25/10-38-01-871_60065edb991267a7.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1106,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:01","host":"europe-60*","referer":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","linkMd5ListStr":"c37b6ff6169ec7e6a055a9904de421c4","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=af0ofRZ9M8M:kUGcuzMWe1A:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn1@2020_2/2020/08/25/10-38-01-963_ba4b2a09f5e52f35.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":967,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:01","host":"us-030*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots","linkMd5ListStr":"67ca3612e21a3b8090962da8d8d72cea","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA1MQ.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn82@2020_3/2020/08/25/10-38-01-865_b80bcb8b6379003b.webp","sourceBytes":1495128,"destBytes":159510,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1236,"convertSpendMs":161,"createdTime":"2020-08-25 18:38:01","host":"us-026*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.4 MB","destSize":"155.8 KB","compressRate":"10.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/LIToXQovbEs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn9@2020_5/2020/08/25/10-38-02-081_5451384509fafbd5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":848,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots","linkMd5ListStr":"40500605e90a1fb25e13a0c4109cdc90","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hHQ_Cl_97H4:rFENhMTkafk:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn5@2020_6/2020/08/25/10-38-02-092_2bf39581fb9da42a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":947,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:01","host":"us-018*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo","linkMd5ListStr":"9959391ccbf704954bde8fcaaf54f5ba","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NTE5Ng.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn58@2020_2/2020/08/25/10-38-01-826_0542b9d4493d8f0b.webp","sourceBytes":377148,"destBytes":223916,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1483,"convertSpendMs":88,"createdTime":"2020-08-25 18:38:01","host":"us-027*","referer":"https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever","linkMd5ListStr":"7798fbe18b39cdcbed2b1084de9fdb63","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"368.3 KB","destSize":"218.7 KB","compressRate":"59.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/jrUpLKvji58","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn32@2020_5/2020/08/25/10-38-02-268_488ef00ac7148f80.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":768,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"us-024*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","linkMd5ListStr":"fdeba8f58f9ca993017a817b623b747a","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NTcxMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn61@2020_3/2020/08/25/10-38-01-889_eceddb2223f6c649.webp","sourceBytes":255635,"destBytes":146912,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1525,"convertSpendMs":68,"createdTime":"2020-08-25 18:38:01","host":"us-018*","referer":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation","linkMd5ListStr":"8ba894e1b44b55f874fed65b911e3953","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"249.6 KB","destSize":"143.5 KB","compressRate":"57.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=LIToXQovbEs:w8wppRpFVAI:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn28@2020_2/2020/08/25/10-38-02-319_259d796e79ae6ca6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":857,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:02","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots","linkMd5ListStr":"40500605e90a1fb25e13a0c4109cdc90","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=AFnfw88Nq3o:6KgvyGl2jOE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn39@2020_4/2020/08/25/10-38-02-356_4a35676c031c9b67.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":883,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"us-001*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/help-rescuers-find-missing-persons-through-emergency-response-contest","linkMd5ListStr":"c70149e9a4334875ea4bbf98504cc029","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/yy1rq30DNiI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn56@2020_5/2020/08/25/10-38-02-400_c69e50949c11a6cd.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":810,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:02","host":"us-021*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","linkMd5ListStr":"ade9b8fe86da931a7835aee397c88af5","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNjMwMw.jpeg","sourceStatusCode":200,"destWidth":800,"destHeight":382,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn36@2020_4/2020/08/25/10-38-02-377_2a9ffa2671921c94.webp","sourceBytes":207707,"destBytes":31558,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":976,"convertSpendMs":50,"createdTime":"2020-08-25 18:38:02","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","linkMd5ListStr":"2c1d20e7c96f50d4e36fa6c0bb86c46f","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"202.8 KB","destSize":"30.8 KB","compressRate":"15.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WWCYX6DRQI0","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn22@2020_3/2020/08/25/10-38-02-347_25d4fcbac5c7ecb9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":886,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe-57*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/rigid-it-departments-jobhopping-data-scientists-implementing-machine-learning-isnt-easy","linkMd5ListStr":"110e6ac838f5a686ca9016e11c381c4e","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzM4MQ.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn85@2020_6/2020/08/25/10-38-02-069_c3c3f91fdfb4cdb3.webp","sourceBytes":1849729,"destBytes":264078,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1528,"convertSpendMs":143,"createdTime":"2020-08-25 18:38:01","host":"us-014*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.8 MB","destSize":"257.9 KB","compressRate":"14.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Bp2w1CN_Clw:Fcj0Rb0C_qQ:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn52@2020_5/2020/08/25/10-38-02-465_268c4c507d687dd8.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":881,"convertSpendMs":12,"createdTime":"2020-08-25 18:38:02","host":"us-032*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","linkMd5ListStr":"9ddfdd2b95e95bd2322570f7f5c5dbdb","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5_MvFCYKYnM:EayMvXTOEek:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn13@2020_4/2020/08/25/10-38-02-373_f29891d9aeea81b1.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1019,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe68*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever","linkMd5ListStr":"9811c27a8bfb20ca93a0926d39ae7e1e","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=1bD2VSOAgqs:Tu0j_hk2JoM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn18@2020_6/2020/08/25/10-38-02-430_d05d29cdf18daebd.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1023,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food","linkMd5ListStr":"d8a32c6887301ef3975606092a06d695","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMjkzMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":792,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn47@2020_2/2020/08/25/10-38-02-351_e78edf29f035936c.webp","sourceBytes":577637,"destBytes":116712,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1082,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:02","host":"us-54*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","linkMd5ListStr":"5c8fee7c8bef1dc43b41be2a29c47ddb","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"564.1 KB","destSize":"114 KB","compressRate":"20.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NTE5NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn69@2020_3/2020/08/25/10-38-02-162_bc8de0ce4759dabf.webp","sourceBytes":269887,"destBytes":72878,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1721,"convertSpendMs":47,"createdTime":"2020-08-25 18:38:01","host":"europe-24*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator","linkMd5ListStr":"7c80d1a412783e6b99d1b3dcd2b92262","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"263.6 KB","destSize":"71.2 KB","compressRate":"27%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/72K5f0p5Wz4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn72@2020_3/2020/08/25/10-38-02-550_f1e8a2ea642126fc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":905,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:02","host":"us-51*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot","linkMd5ListStr":"4542d0e2bf7583a7652c5511e14ef53d","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RNa1KqrBSZw:_EaQTOBiM-E:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn68@2020_1/2020/08/25/10-38-02-649_72ebbe547f3f23cc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1047,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe62*","referer":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation","linkMd5ListStr":"8ba894e1b44b55f874fed65b911e3953","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VszUWNQuPRA:_kdYL-vrJa4:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn83@2020_1/2020/08/25/10-38-02-758_00e4778bd80ba076.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":878,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:02","host":"us-007*","referer":"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things","linkMd5ListStr":"daf121b367dccdb1c38a3b54e0c17b83","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTU5Ng.jpeg","sourceStatusCode":200,"destWidth":2731,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn60@2020_3/2020/08/25/10-38-02-527_f64a89997e018552.webp","sourceBytes":390182,"destBytes":138954,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1209,"convertSpendMs":106,"createdTime":"2020-08-25 18:38:02","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"381 KB","destSize":"135.7 KB","compressRate":"35.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxODUyOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn18@2020_2/2020/08/25/10-38-01-983_725e65c2eccb3bc7.webp","sourceBytes":240257,"destBytes":127052,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2253,"convertSpendMs":77,"createdTime":"2020-08-25 18:38:01","host":"europe69*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence","linkMd5ListStr":"5cb62d3fcb55be58473c9653072f3b07","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"234.6 KB","destSize":"124.1 KB","compressRate":"52.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwODgyOA.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":630,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn96@2020_5/2020/08/25/10-38-02-911_531684463d4c315d.webp","sourceBytes":199445,"destBytes":6804,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":948,"convertSpendMs":64,"createdTime":"2020-08-25 18:38:02","host":"us-038*","referer":"https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020","linkMd5ListStr":"538b0c13aaa64a59898970581b42f5fa","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"194.8 KB","destSize":"6.6 KB","compressRate":"3.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=6emqUxmAG9w:BT7v-3tNUf0:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn8@2020_2/2020/08/25/10-38-02-948_17c1fd05640a2f2e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":911,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"us-004*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans","linkMd5ListStr":"05ca9649ab0849af31ee29f9a1f2318f","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WyumTS8jes0:bDgdbPvpRZA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn27@2020_4/2020/08/25/10-38-03-001_923e24f245237821.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":824,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:02","host":"us-012*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/microsofts-ai-research-draws-controversy-over-possible-disinformation-use","linkMd5ListStr":"5e83ca8ac583d87a19ee7990f07737a8","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/aO_Hjssz2eE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn92@2020_2/2020/08/25/10-38-02-881_d01f013c443c7f87.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":890,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe-59*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence","linkMd5ListStr":"b19d13bf54f71c1c40ce0f7d38f1a58b","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=2xbdHXuXRMk:1qI5zxyxn7c:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn19@2020_5/2020/08/25/10-38-02-966_901a08df68e35324.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":937,"convertSpendMs":7,"createdTime":"2020-08-25 18:38:02","host":"us-035*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/trump-cto-addresses-ai-facial-recognition-immigration-tech-infrastructure-and-more","linkMd5ListStr":"0e1d29b76415f6a75c1bacb441cc9c0d","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDUyNg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn75@2020_5/2020/08/25/10-38-02-813_23350128c35aae41.webp","sourceBytes":85600,"destBytes":47068,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1250,"convertSpendMs":62,"createdTime":"2020-08-25 18:38:02","host":"us-029*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"83.6 KB","destSize":"46 KB","compressRate":"55%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NzkyNw.jpeg","sourceStatusCode":200,"destWidth":680,"destHeight":336,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn23@2020_3/2020/08/25/10-38-02-920_080c779d14bd616e.webp","sourceBytes":68267,"destBytes":31782,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":933,"convertSpendMs":12,"createdTime":"2020-08-25 18:38:02","host":"us-023*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"66.7 KB","destSize":"31 KB","compressRate":"46.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDMxNg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn3@2020_3/2020/08/25/10-38-02-859_088f9438987a6a59.webp","sourceBytes":494948,"destBytes":70372,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1080,"convertSpendMs":52,"createdTime":"2020-08-25 18:38:02","host":"us-015*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"483.3 KB","destSize":"68.7 KB","compressRate":"14.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RignCTneE9M:v2UFZRjM9PY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn40@2020_3/2020/08/25/10-38-03-075_9e57bff312e42441.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":883,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","linkMd5ListStr":"8be8e5cf657dd68e13509f254295323b","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3MDYyOA.png","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn46@2020_6/2020/08/25/10-38-02-605_5f538270d3e935dc.webp","sourceBytes":575081,"destBytes":66822,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2336,"convertSpendMs":122,"createdTime":"2020-08-25 18:38:01","host":"europe67*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies","linkMd5ListStr":"d6ffcad3ca933b1d80d7759de6a31b95","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"561.6 KB","destSize":"65.3 KB","compressRate":"11.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=dR0py1h-AZM:ZBThNqRB3fg:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn51@2020_2/2020/08/25/10-38-03-186_d991dce8b784a7bc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":845,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:03","host":"us-009*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/want-a-really-hard-machine-learning-problem-try-agriculture-say-john-deere-labs-leaders","linkMd5ListStr":"5f2d11cc893fe393f2105cffb5ccf143","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=J1wRiTEvVSs:d30liMidOY4:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn48@2020_4/2020/08/25/10-38-03-199_dfb011f0f085d008.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":873,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"us-020*","referer":"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies","linkMd5ListStr":"f06a1c762bc020e3eb9c056139e79fb8","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTU5NQ.jpeg","sourceStatusCode":200,"destWidth":1713,"destHeight":1074,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn99@2020_3/2020/08/25/10-38-02-889_5a2813432fbbd90f.webp","sourceBytes":569297,"destBytes":171012,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1219,"convertSpendMs":88,"createdTime":"2020-08-25 18:38:02","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"556 KB","destSize":"167 KB","compressRate":"30%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDM1MQ.jpeg","sourceStatusCode":200,"destWidth":600,"destHeight":668,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn64@2020_1/2020/08/25/10-38-02-943_131953073c5caaa1.webp","sourceBytes":196202,"destBytes":26434,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1502,"convertSpendMs":23,"createdTime":"2020-08-25 18:38:02","host":"europe-23*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"191.6 KB","destSize":"25.8 KB","compressRate":"13.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjUyMQ.jpeg","sourceStatusCode":200,"destWidth":734,"destHeight":672,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn44@2020_5/2020/08/25/10-38-03-156_98aaa38d87450c96.webp","sourceBytes":65045,"destBytes":42500,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":987,"convertSpendMs":24,"createdTime":"2020-08-25 18:38:03","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"63.5 KB","destSize":"41.5 KB","compressRate":"65.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-xqOx5HkcOE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn68@2020_6/2020/08/25/10-38-03-277_91e696edddfa8b24.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":798,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:03","host":"us-028*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation","linkMd5ListStr":"026bc4b5af35d78854aaaae5d60ee79a","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4K0VX3LeIBo:fXrdncAkUXE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn72@2020_1/2020/08/25/10-38-03-312_0f700900ce17f2cf.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":819,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:03","host":"us-017*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever","linkMd5ListStr":"019269ec580d7ee2f12ac1c56db02f28","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=r2-HEkba0No:-emDvS734jg:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn36@2020_5/2020/08/25/10-38-03-193_1ca7d1ab61cc5266.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1049,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"europe64*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","linkMd5ListStr":"ddd96fe231dfc465a52535e475a98b15","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=72K5f0p5Wz4:Qf99AY9z4yM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn76@2020_5/2020/08/25/10-38-03-343_f850805fa0f173c5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":892,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot","linkMd5ListStr":"4542d0e2bf7583a7652c5511e14ef53d","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=oPPftINehQs:s-u5RQQjKAA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn12@2020_5/2020/08/25/10-38-03-071_89cbbcc28e12fbc9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1188,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator","linkMd5ListStr":"7c80d1a412783e6b99d1b3dcd2b92262","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTM3MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn22@2020_4/2020/08/25/10-38-02-502_e5f73080b15d2ff6.webp","sourceBytes":227598,"destBytes":118158,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2816,"convertSpendMs":52,"createdTime":"2020-08-25 18:38:01","host":"europe-60*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans","linkMd5ListStr":"05ca9649ab0849af31ee29f9a1f2318f","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"222.3 KB","destSize":"115.4 KB","compressRate":"51.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aU1T0j0-G7U:4jQqdmb0lgM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn15@2020_3/2020/08/25/10-38-03-086_f186c3d87c8d1ee7.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1189,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:02","host":"europe-56*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/us-energy-department-is-first-customer-for-worlds-biggest-chip","linkMd5ListStr":"f98c1554d57da1ae3c3120fbc73c502e","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=XQoU3ebUNAE:Lr6zffHQlz8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn95@2020_1/2020/08/25/10-38-03-521_4239620ca5cbbb6f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":818,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"us-014*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt","linkMd5ListStr":"fddb2a608a827cba7cd0974ca59eeee3","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=eWooJQd8zew:ZqKXuPKCKkE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn91@2020_2/2020/08/25/10-38-03-530_d46d14a8db8eb1c0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":830,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:03","host":"us-025*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NDMyMQ.jpeg","sourceStatusCode":200,"destWidth":4032,"destHeight":3024,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn98@2020_6/2020/08/25/10-38-02-922_a6e7819d981e22aa.webp","sourceBytes":956672,"destBytes":843012,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2542,"convertSpendMs":905,"createdTime":"2020-08-25 18:38:01","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"934.2 KB","destSize":"823.3 KB","compressRate":"88.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_Hi6JNHldFo:dQwWdXtGW-M:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn84@2020_3/2020/08/25/10-38-03-557_a81b563f31210f8b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1035,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","linkMd5ListStr":"184896aa81aa8ae49425e7c03d12d965","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=TYE4X4BWZag:Yijh8rC9x50:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn11@2020_4/2020/08/25/10-38-03-690_cecc7ca5eff8b34d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":867,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:03","host":"us-033*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","linkMd5ListStr":"ba866f9c04fa639f4052fdd904ce7f83","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-vdKHxTZE2E","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn80@2020_5/2020/08/25/10-38-03-453_97aa7a6681915bdf.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1042,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:03","host":"europe69*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle","linkMd5ListStr":"ed36754ceb3d54691c9985ca6f20cf97","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=3hDuhYQdo-A:oooi_DFqXNQ:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn15@2020_1/2020/08/25/10-38-03-748_eb25a2b5a8425bd6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":830,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"us-022*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one","linkMd5ListStr":"eaa12b83fdd04d32713423e0cead6909","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tbvl8qcvgVM:bcb_4nyhACk:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn39@2020_6/2020/08/25/10-38-03-889_bbd48ce76503c3c4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":816,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:03","host":"us-019*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions","linkMd5ListStr":"d38b2f1b0a483a0b8124b15d386f7c04","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNjMwNA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn32@2020_4/2020/08/25/10-38-03-405_b555a323edc2f2ac.webp","sourceBytes":209407,"destBytes":68834,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1615,"convertSpendMs":37,"createdTime":"2020-08-25 18:38:03","host":"europe-25*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","linkMd5ListStr":"2c1d20e7c96f50d4e36fa6c0bb86c46f","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"204.5 KB","destSize":"67.2 KB","compressRate":"32.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=v9Z7p-VLUvo:1XNgDJQjgWo:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn35@2020_4/2020/08/25/10-38-03-875_f1c86a481bbb7261.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":856,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"us-030*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-prospects-for-tomorrows-jobs","linkMd5ListStr":"8e5ebdad5680d32249cb5e9715f5ba1f","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ItofgVBt2wM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn31@2020_5/2020/08/25/10-38-03-866_f23c9c6705da6248.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":899,"convertSpendMs":19,"createdTime":"2020-08-25 18:38:03","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot","linkMd5ListStr":"0c575b7593d53cada9ce6cb4a5dbad5f","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTYxNg.jpeg","sourceStatusCode":200,"destWidth":2301,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn87@2020_6/2020/08/25/10-38-03-551_46907fe6d788edea.webp","sourceBytes":732653,"destBytes":188576,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1362,"convertSpendMs":146,"createdTime":"2020-08-25 18:38:03","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"715.5 KB","destSize":"184.2 KB","compressRate":"25.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/cfiHRefDfBg","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn56@2020_3/2020/08/25/10-38-03-978_5fa07270d8f9ca2f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":791,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:03","host":"us-039*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","linkMd5ListStr":"7c7f2266a8977ac619b5378792fcd8f7","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=yy1rq30DNiI:ZPKbepRAZAE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn44@2020_6/2020/08/25/10-38-03-952_1c208efb99ff9f91.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":886,"convertSpendMs":11,"createdTime":"2020-08-25 18:38:03","host":"us-008*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","linkMd5ListStr":"ade9b8fe86da931a7835aee397c88af5","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Du61YeV7EjU","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn24@2020_5/2020/08/25/10-38-03-882_99768cf861ea551e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":924,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"europe-24*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped","linkMd5ListStr":"6d47e21e002c0841e6518b91f4e6bdab","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTU5OA.jpeg","sourceStatusCode":200,"destWidth":2151,"destHeight":3294,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn99@2020_3/2020/08/25/10-38-03-817_f4443da59dd13854.webp","sourceBytes":461842,"destBytes":73202,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1241,"convertSpendMs":214,"createdTime":"2020-08-25 18:38:03","host":"us-003*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"451 KB","destSize":"71.5 KB","compressRate":"15.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4Nzg3Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn20@2020_2/2020/08/25/10-38-03-813_e58a5435a92fb223.webp","sourceBytes":198703,"destBytes":103152,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1072,"convertSpendMs":47,"createdTime":"2020-08-25 18:38:03","host":"us-011*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/want-a-really-hard-machine-learning-problem-try-agriculture-say-john-deere-labs-leaders","linkMd5ListStr":"5f2d11cc893fe393f2105cffb5ccf143","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"194 KB","destSize":"100.7 KB","compressRate":"51.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/hHQ_Cl_97H4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn27@2020_1/2020/08/25/10-38-03-911_1c48f5e419bbaf6c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":914,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"europe63*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo","linkMd5ListStr":"9959391ccbf704954bde8fcaaf54f5ba","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/lq8k4y7KjVg","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn80@2020_3/2020/08/25/10-38-04-108_43fb59dc1286696a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":788,"convertSpendMs":9,"createdTime":"2020-08-25 18:38:04","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019","linkMd5ListStr":"942b7fea25553a3c98fd6f49daf1b0f2","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=lq8k4y7KjVg:plxvSkQegxk:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn4@2020_4/2020/08/25/10-38-03-822_c409fb8261a2fe4a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1169,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:03","host":"europe66*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019","linkMd5ListStr":"942b7fea25553a3c98fd6f49daf1b0f2","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTY3Nw.png","sourceStatusCode":200,"destWidth":1080,"destHeight":2160,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn7@2020_2/2020/08/25/10-38-03-995_9df673b238a7d0ee.webp","sourceBytes":442682,"destBytes":88866,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1334,"convertSpendMs":87,"createdTime":"2020-08-25 18:38:03","host":"us-55*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"432.3 KB","destSize":"86.8 KB","compressRate":"20.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_fF0aLR9nWY:U1o9ocBtuGY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn92@2020_3/2020/08/25/10-38-04-221_67218181903ee6ed.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":800,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-001*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/9M4rEIftmus","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn88@2020_5/2020/08/25/10-38-04-194_15cdab8bf2ac071f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":810,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream","linkMd5ListStr":"727addd4da03ea6fd9f64c61ab6be805","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4MDc3OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn63@2020_5/2020/08/25/10-38-04-054_06170fef0befacb3.webp","sourceBytes":83554,"destBytes":43182,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":987,"convertSpendMs":35,"createdTime":"2020-08-25 18:38:04","host":"us-016*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"81.6 KB","destSize":"42.2 KB","compressRate":"51.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU4OQ.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":748,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn44@2020_6/2020/08/25/10-38-03-503_7e331c4f74e8189c.webp","sourceBytes":573791,"destBytes":85928,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2592,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:02","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"560.3 KB","destSize":"83.9 KB","compressRate":"15%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MDY4OA.jpeg","sourceStatusCode":200,"destWidth":458,"destHeight":322,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn84@2020_3/2020/08/25/10-38-04-191_f02f66c498f21354.webp","sourceBytes":26592,"destBytes":8820,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":827,"convertSpendMs":7,"createdTime":"2020-08-25 18:38:04","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","linkMd5ListStr":"bf340835c615e553278f7841ec5e7cd7","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"26 KB","destSize":"8.6 KB","compressRate":"33.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NzM0NA.jpeg","sourceStatusCode":200,"destWidth":768,"destHeight":1024,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn60@2020_4/2020/08/25/10-38-04-047_72b5cab7334e18c2.webp","sourceBytes":123519,"destBytes":108012,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1046,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:03","host":"us-027*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","linkMd5ListStr":"ba866f9c04fa639f4052fdd904ce7f83","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"120.6 KB","destSize":"105.5 KB","compressRate":"87.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WyumTS8jes0","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn3@2020_5/2020/08/25/10-38-04-292_c776c30bf23cbcec.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":792,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-032*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/microsofts-ai-research-draws-controversy-over-possible-disinformation-use","linkMd5ListStr":"5e83ca8ac583d87a19ee7990f07737a8","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-vdKHxTZE2E:fseR5PgFwgo:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn48@2020_1/2020/08/25/10-38-04-127_5a1d3cb023cf4207.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1034,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:03","host":"europe21*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle","linkMd5ListStr":"ed36754ceb3d54691c9985ca6f20cf97","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTQ4OA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":865,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn59@2020_5/2020/08/25/10-38-03-683_60b48fdfa57cbbfa.webp","sourceBytes":671197,"destBytes":96688,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1764,"convertSpendMs":83,"createdTime":"2020-08-25 18:38:03","host":"europe61*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"655.5 KB","destSize":"94.4 KB","compressRate":"14.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hoozvOaCyGQ:ZdVZchta5ec:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn7@2020_2/2020/08/25/10-38-04-353_d5013dc2e5c161f6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":851,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-021*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous","linkMd5ListStr":"6ab80d13744ce4a42ea20fe59304598b","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDcyMg.jpeg","sourceStatusCode":200,"destWidth":620,"destHeight":465,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn52@2020_1/2020/08/25/10-38-04-085_d48e08b85d2ef222.webp","sourceBytes":54912,"destBytes":33992,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1056,"convertSpendMs":15,"createdTime":"2020-08-25 18:38:04","host":"europe-60*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"53.6 KB","destSize":"33.2 KB","compressRate":"61.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/XQoU3ebUNAE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn12@2020_1/2020/08/25/10-38-04-443_47737e74d844bdeb.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":783,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt","linkMd5ListStr":"fddb2a608a827cba7cd0974ca59eeee3","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzc0MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn99@2020_6/2020/08/25/10-38-04-265_1a976b2f04dec70e.webp","sourceBytes":182565,"destBytes":100504,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1107,"convertSpendMs":47,"createdTime":"2020-08-25 18:38:04","host":"us-54*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space","linkMd5ListStr":"98d9acef762105ed3deea1273ccf06c8","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"178.3 KB","destSize":"98.1 KB","compressRate":"55.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=06nw6wEy23U:6leYwiyLyss:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn23@2020_1/2020/08/25/10-38-04-573_83efcba348e01ecd.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":825,"convertSpendMs":7,"createdTime":"2020-08-25 18:38:04","host":"us-51*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone","linkMd5ListStr":"a2d95510787b8bacb7f69ee68b845ae0","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=BZE40vAKpP8:C4a4qIgo22s:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn27@2020_1/2020/08/25/10-38-04-594_55e269a12b121f0a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":822,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-029*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_dQJoG0X58s:OA7yLaP9HCA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn96@2020_3/2020/08/25/10-38-04-413_79fa55f8ddaaabf6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1159,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=mMsm0tCvg3c:6NEoQDAwefA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn36@2020_5/2020/08/25/10-38-04-722_3623ed247c6282ab.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":842,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:04","host":"us-007*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","linkMd5ListStr":"50bd446780e290d101b8680d1908ab28","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rV5PxZEF_hs:AbMFTJ0QdHo:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn32@2020_1/2020/08/25/10-38-04-741_ef212499c794e3b2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":862,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:04","host":"us-018*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","linkMd5ListStr":"4116b47e7bcae1aed0f1e1c85e917d83","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=f4vRmWO59FQ:u8dJuNbBVJU:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn16@2020_5/2020/08/25/10-38-04-727_736eb8bda612cc0a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":996,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"europe-23*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","linkMd5ListStr":"de8494e008723f24ca95f4421202eaa2","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/qqJG5_zGl_w","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn60@2020_6/2020/08/25/10-38-04-843_7183d51123c04ca5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":826,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-004*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=KiyPhDu-vcA:sD8uwKJroUg:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn19@2020_6/2020/08/25/10-38-04-730_cfca997a8bd4560e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1028,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"europe62*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/FMSyH0YMiNw","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn39@2020_4/2020/08/25/10-38-04-828_e63ce3a861cd45e5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":881,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","linkMd5ListStr":"2c1d20e7c96f50d4e36fa6c0bb86c46f","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7B7ZQFeObi8:QLT2oDiNohE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn75@2020_1/2020/08/25/10-38-04-933_aaa7b2b8372dc7fe.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":839,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"us-023*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves","linkMd5ListStr":"51766f8db6dedb5c20550ff3c3224e16","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ho-KAV7HE7I:mSz7Bg_m1CE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn100@2020_1/2020/08/25/10-38-05-042_d07ee2c074838d7f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":783,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"us-020*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=bL0ZQdpPbV8:3_409Sqs-i8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn80@2020_5/2020/08/25/10-38-05-042_8ef3ec546ee12384.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":879,"convertSpendMs":20,"createdTime":"2020-08-25 18:38:04","host":"us-012*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816","linkMd5ListStr":"a31b47238c2872d0b2da6a891bae5495","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=jrUpLKvji58:Mv9REQr1UnA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn51@2020_5/2020/08/25/10-38-04-867_8b9c850b779e709a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1056,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"us-026*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","linkMd5ListStr":"fdeba8f58f9ca993017a817b623b747a","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNDYyNw.jpeg","sourceStatusCode":200,"destWidth":900,"destHeight":955,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn72@2020_1/2020/08/25/10-38-04-934_13d3e37a5014450b.webp","sourceBytes":103340,"destBytes":49478,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":977,"convertSpendMs":55,"createdTime":"2020-08-25 18:38:04","host":"us-035*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle","linkMd5ListStr":"ed36754ceb3d54691c9985ca6f20cf97","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"100.9 KB","destSize":"48.3 KB","compressRate":"47.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_SP9ysuBirA:u8fjM8CPmQg:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn4@2020_5/2020/08/25/10-38-05-086_18c47ecdf123c004.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":853,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-009*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/osC6qndPrDk","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn64@2020_4/2020/08/25/10-38-05-003_a32755e96e8b1608.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":928,"convertSpendMs":51,"createdTime":"2020-08-25 18:38:04","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills","linkMd5ListStr":"b050441ca360dcb3e9e825f306284a0b","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5urTYVBa2Lw:VJf9q2xFxfM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn15@2020_6/2020/08/25/10-38-05-161_9eafc6539bf586ee.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":829,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-040*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/georgia-tech-smarticles-stochastic-robot-swarm","linkMd5ListStr":"3579544e8e9768028a4476e076f6e099","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NDMxOQ.jpeg","sourceStatusCode":200,"destWidth":4032,"destHeight":3024,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn93@2020_3/2020/08/25/10-38-04-150_95fcf6efb19558ea.webp","sourceBytes":1243873,"destBytes":505882,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3916,"convertSpendMs":484,"createdTime":"2020-08-25 18:38:01","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"494 KB","compressRate":"40.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/7B7ZQFeObi8","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn28@2020_5/2020/08/25/10-38-05-166_3ccd3b37c0353e90.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":775,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves","linkMd5ListStr":"51766f8db6dedb5c20550ff3c3224e16","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Q6sSAseS4hE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn24@2020_4/2020/08/25/10-38-05-164_7d16dd000eb2eef4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":787,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:05","host":"us-017*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light","linkMd5ListStr":"f1fb990643b446a2fb09abe6cb8c2668","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNjk5Mg.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn47@2020_4/2020/08/25/10-38-04-984_d3eb14c5308181e4.webp","sourceBytes":1541917,"destBytes":178586,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1200,"convertSpendMs":172,"createdTime":"2020-08-25 18:38:04","host":"us-038*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.5 MB","destSize":"174.4 KB","compressRate":"11.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_fF0aLR9nWY:U1o9ocBtuGY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn96@2020_6/2020/08/25/10-38-05-085_2c907a499d4dcf46.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":954,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:04","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=605d7BLBLdY:xc88_2dnWUA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn19@2020_6/2020/08/25/10-38-05-193_6240f354af9b7380.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":849,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"us-028*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot","linkMd5ListStr":"3a355f69e17aa2bb77552519184504ec","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjA0NQ.png","sourceStatusCode":200,"destWidth":941,"destHeight":642,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn92@2020_2/2020/08/25/10-38-05-077_ebbd9d3a710e888b.webp","sourceBytes":804113,"destBytes":53088,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1063,"convertSpendMs":38,"createdTime":"2020-08-25 18:38:04","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"785.3 KB","destSize":"51.8 KB","compressRate":"6.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7B7ZQFeObi8:QLT2oDiNohE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn68@2020_4/2020/08/25/10-38-05-093_c78bb44aac8261da.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1192,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:04","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves","linkMd5ListStr":"51766f8db6dedb5c20550ff3c3224e16","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjA0Ng.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":841,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn71@2020_4/2020/08/25/10-38-04-465_3d1f21ab6e972710.webp","sourceBytes":727703,"destBytes":101806,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1985,"convertSpendMs":82,"createdTime":"2020-08-25 18:38:04","host":"europe68*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"710.6 KB","destSize":"99.4 KB","compressRate":"14%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/JTny_7tR99s","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn11@2020_2/2020/08/25/10-38-05-198_199abf13df2695f4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":991,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"europe61*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid","linkMd5ListStr":"783ad2e174faa245c24d60ad66d167ad","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/3hDuhYQdo-A","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn48@2020_5/2020/08/25/10-38-05-511_9e44c058eddf499b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":780,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-014*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one","linkMd5ListStr":"eaa12b83fdd04d32713423e0cead6909","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjc0Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":852,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn76@2020_5/2020/08/25/10-38-05-055_2b78e840f615f83c.webp","sourceBytes":636706,"destBytes":42392,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2093,"convertSpendMs":38,"createdTime":"2020-08-25 18:38:04","host":"europe-57*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"621.8 KB","destSize":"41.4 KB","compressRate":"6.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/QhC0HMCy6-k","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn51@2020_3/2020/08/25/10-38-05-546_a101d8249e0e884e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":774,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-003*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","linkMd5ListStr":"70815399970675009fdaa91f1ed9af88","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1MzQ3Nw.jpeg","sourceStatusCode":200,"destWidth":630,"destHeight":573,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn39@2020_5/2020/08/25/10-38-05-409_5d8b3640165b0650.webp","sourceBytes":274280,"destBytes":43102,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":947,"convertSpendMs":21,"createdTime":"2020-08-25 18:38:05","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"267.9 KB","destSize":"42.1 KB","compressRate":"15.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=FMSyH0YMiNw:MiUVkrzeNOc:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn44@2020_6/2020/08/25/10-38-05-456_0698a280ba446832.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":975,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-025*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","linkMd5ListStr":"2c1d20e7c96f50d4e36fa6c0bb86c46f","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzM4NA.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn87@2020_6/2020/08/25/10-38-04-590_3e24f562cb50673a.webp","sourceBytes":1274012,"destBytes":217204,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3637,"convertSpendMs":144,"createdTime":"2020-08-25 18:38:02","host":"europe70*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"212.1 KB","compressRate":"17%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=605d7BLBLdY:xc88_2dnWUA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn43@2020_6/2020/08/25/10-38-04-959_26d77631b6d9e169.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1622,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:04","host":"europe-59*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot","linkMd5ListStr":"3a355f69e17aa2bb77552519184504ec","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WYorWvFk5Jk:OlhVHrX5MDI:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn59@2020_2/2020/08/25/10-38-05-751_803ad8789614aec4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":866,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-55*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot","linkMd5ListStr":"507ca1aa50869b0a0bb43058028bc0d9","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=72K5f0p5Wz4:Qf99AY9z4yM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn68@2020_4/2020/08/25/10-38-05-748_7441bcec3a1243d0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":870,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-022*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/eth-surich-puppetmaster-robot","linkMd5ListStr":"4542d0e2bf7583a7652c5511e14ef53d","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/6emqUxmAG9w","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn63@2020_2/2020/08/25/10-38-05-695_59b7d2f41cd8b2d8.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":880,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"us-033*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans","linkMd5ListStr":"05ca9649ab0849af31ee29f9a1f2318f","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Za6B4U3ZdIk:TjeXp1c4B1Q:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn72@2020_5/2020/08/25/10-38-05-830_098f9bfa220ad51e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":872,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot","linkMd5ListStr":"fb732aaba2fa77f47db578dd8a2e2c4e","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cfiHRefDfBg:4Zg3ikJLBoQ:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn92@2020_3/2020/08/25/10-38-05-886_96e2924ba7492053.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":786,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"us-019*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","linkMd5ListStr":"7c7f2266a8977ac619b5378792fcd8f7","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDY5MA.gif","sourceStatusCode":200,"destWidth":620,"destHeight":622,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn56@2020_5/2020/08/25/10-38-05-504_0deaad3abe727cad.webp","sourceBytes":445451,"destBytes":181980,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1897,"convertSpendMs":525,"createdTime":"2020-08-25 18:38:04","host":"us-015*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"435 KB","destSize":"177.7 KB","compressRate":"40.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WYorWvFk5Jk","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn12@2020_2/2020/08/25/10-38-05-959_b49f5e644244be62.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":788,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-027*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot","linkMd5ListStr":"507ca1aa50869b0a0bb43058028bc0d9","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTY3MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn88@2020_2/2020/08/25/10-38-05-159_0e8510e32f0a3b92.webp","sourceBytes":191818,"destBytes":84852,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1625,"convertSpendMs":41,"createdTime":"2020-08-25 18:38:05","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"187.3 KB","destSize":"82.9 KB","compressRate":"44.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/C6zMyuDvd98","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn16@2020_5/2020/08/25/10-38-05-984_c66dfedfc5e4e694.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":813,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:05","host":"us-016*","referer":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","linkMd5ListStr":"4afdb44919c1840f63dbb5b7c83f76a7","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=A0x1pCFuojI:zBCg41xGM-k:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn55@2020_4/2020/08/25/10-38-05-868_9b22e440f26a540d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1109,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"europe66*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","linkMd5ListStr":"f4602756dbd851432f68681e9d7b807e","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=iB-EZS5TQKE:nV6g5-qUnfo:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn87@2020_6/2020/08/25/10-38-05-919_3fad89d519351818.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":995,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"us-030*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","linkMd5ListStr":"953ccece03a4def30b62f328e3f48f7e","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Bp2w1CN_Clw","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn76@2020_4/2020/08/25/10-38-05-863_23f4e0bc4fa63e78.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":977,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:05","host":"europe-24*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","linkMd5ListStr":"9ddfdd2b95e95bd2322570f7f5c5dbdb","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ZpLVW_108Mc","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn100@2020_6/2020/08/25/10-38-06-027_06e35845d0212194.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":911,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe21*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet","linkMd5ListStr":"b92198dab0130e2ebf3c419ddb0d4903","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-vdKHxTZE2E:fseR5PgFwgo:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn31@2020_5/2020/08/25/10-38-06-147_b420cb84ea9170fd.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":885,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:06","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/uc-berkeley-cassie-cal-robot-juggle","linkMd5ListStr":"ed36754ceb3d54691c9985ca6f20cf97","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4MzAwMg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":841,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn8@2020_5/2020/08/25/10-38-06-040_694582f0959001cb.webp","sourceBytes":508028,"destBytes":58168,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1077,"convertSpendMs":62,"createdTime":"2020-08-25 18:38:05","host":"us-039*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever","linkMd5ListStr":"019269ec580d7ee2f12ac1c56db02f28","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"496.1 KB","destSize":"56.8 KB","compressRate":"11.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=68kvbO8JlEQ:reuaV7O4PNQ:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn36@2020_1/2020/08/25/10-38-06-244_ce176dddb9090f83.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":850,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"us-024*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","linkMd5ListStr":"ec21b6293bbcb368dd9079b4df431fd8","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=8qXUW4b1PHc:ZGA2YPIR5uc:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn4@2020_4/2020/08/25/10-38-06-152_eb334d95d9d75ccf.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":978,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:06","host":"europe-60*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","linkMd5ListStr":"9778ce9d1c486affccf103eca762f153","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ukg9RZJ3fig","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn51@2020_3/2020/08/25/10-38-06-329_6155813a0a3f4ee0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":757,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:06","host":"us-54*","referer":"https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances","linkMd5ListStr":"ecb83ca1fe85f5c86b44772662510c06","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tD3Hqeyz2og:4-p-HsTg3jA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn43@2020_5/2020/08/25/10-38-06-324_6497e94b852709c2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":862,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:06","host":"us-001*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence","linkMd5ListStr":"5cb62d3fcb55be58473c9653072f3b07","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/QGOM8XaI22U","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn40@2020_4/2020/08/25/10-38-06-288_300b6f90e36e4055.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":894,"convertSpendMs":24,"createdTime":"2020-08-25 18:38:06","host":"us-013*","referer":"https://spectrum.ieee.org/tech-talk/computing/software/aipowered-robotic-system-stirs-up-molecules-from-scratch","linkMd5ListStr":"1fbe4fd76aaa4013dc648f063ffe877a","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/tbvl8qcvgVM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn56@2020_5/2020/08/25/10-38-06-326_3d1a49f95449594c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":772,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions","linkMd5ListStr":"d38b2f1b0a483a0b8124b15d386f7c04","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5Mzc2NQ.jpeg","sourceStatusCode":200,"destWidth":2383,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn84@2020_6/2020/08/25/10-38-06-152_32a08cbf2dc4cf2c.webp","sourceBytes":547769,"destBytes":86074,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1315,"convertSpendMs":173,"createdTime":"2020-08-25 18:38:05","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","linkMd5ListStr":"184896aa81aa8ae49425e7c03d12d965","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"534.9 KB","destSize":"84.1 KB","compressRate":"15.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=9M4rEIftmus:0mYUx3pzxPg:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn63@2020_5/2020/08/25/10-38-06-414_536ea9ab3e91a806.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":821,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:06","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream","linkMd5ListStr":"727addd4da03ea6fd9f64c61ab6be805","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NjQ5Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn95@2020_4/2020/08/25/10-38-06-170_e91eca728e214375.webp","sourceBytes":965598,"destBytes":221166,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1378,"convertSpendMs":121,"createdTime":"2020-08-25 18:38:05","host":"us-008*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","linkMd5ListStr":"50bd446780e290d101b8680d1908ab28","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"943 KB","destSize":"216 KB","compressRate":"22.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU4OA.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":900,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn60@2020_1/2020/08/25/10-38-06-408_9f8c4ea74c0716dc.webp","sourceBytes":268806,"destBytes":63892,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":995,"convertSpendMs":51,"createdTime":"2020-08-25 18:38:06","host":"us-021*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"262.5 KB","destSize":"62.4 KB","compressRate":"23.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/cPJEgwpqkv8","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn84@2020_1/2020/08/25/10-38-06-595_34e2723e743a8311.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":789,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:06","host":"us-018*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=X-v6WYhBbOk:6bxLlz5K92Q:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn21@2020_2/2020/08/25/10-38-06-228_7ba20b8739d6c111.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1256,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NTUxMQ.jpeg","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn27@2020_4/2020/08/25/10-38-06-124_5ef51988192fb6c4.webp","sourceBytes":70695,"destBytes":45550,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1268,"convertSpendMs":15,"createdTime":"2020-08-25 18:38:06","host":"europe-57*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","linkMd5ListStr":"ade9b8fe86da931a7835aee397c88af5","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"69 KB","destSize":"44.5 KB","compressRate":"64.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/oPPftINehQs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn88@2020_3/2020/08/25/10-38-06-632_a5eb04cc169b8079.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":765,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"us-007*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator","linkMd5ListStr":"7c80d1a412783e6b99d1b3dcd2b92262","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aO_Hjssz2eE:qUvcfntImLA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn80@2020_3/2020/08/25/10-38-06-652_d21e605947a48734.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":820,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"us-029*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence","linkMd5ListStr":"b19d13bf54f71c1c40ce0f7d38f1a58b","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNDUyNw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn80@2020_2/2020/08/25/10-38-05-974_76307efff0167b5d.webp","sourceBytes":124884,"destBytes":78322,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1455,"convertSpendMs":38,"createdTime":"2020-08-25 18:38:05","host":"europe63*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation","linkMd5ListStr":"026bc4b5af35d78854aaaae5d60ee79a","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"122 KB","destSize":"76.5 KB","compressRate":"62.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDM2NQ.octet-stream","sourceStatusCode":200,"destWidth":600,"destHeight":338,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn75@2020_4/2020/08/25/10-38-06-567_aef88a7029595157.webp","sourceBytes":151325,"destBytes":17642,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":894,"convertSpendMs":16,"createdTime":"2020-08-25 18:38:06","host":"us-51*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"147.8 KB","destSize":"17.2 KB","compressRate":"11.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjczOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn7@2020_1/2020/08/25/10-38-06-238_6bf21babdbb4e36e.webp","sourceBytes":726216,"destBytes":90066,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2370,"convertSpendMs":48,"createdTime":"2020-08-25 18:38:05","host":"europe-22*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"709.2 KB","destSize":"88 KB","compressRate":"12.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=b6oUZIfmp5k:VMbz5gRzLpY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn47@2020_2/2020/08/25/10-38-06-505_0f6b887fcbb6b0f5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1153,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:06","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","linkMd5ListStr":"bac274849de61b732281df992d98ec37","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU4Ng.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":831,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn31@2020_2/2020/08/25/10-38-06-233_1ab4c4f6c602e428.webp","sourceBytes":633205,"destBytes":82622,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2250,"convertSpendMs":74,"createdTime":"2020-08-25 18:38:05","host":"europe69*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"618.4 KB","destSize":"80.7 KB","compressRate":"13%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aO_Hjssz2eE:qUvcfntImLA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn12@2020_6/2020/08/25/10-38-06-839_294634c6ed5f7bf4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":829,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:06","host":"us-004*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence","linkMd5ListStr":"b19d13bf54f71c1c40ce0f7d38f1a58b","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=D6FhFJID_ho:_h5f38TA_es:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn99@2020_2/2020/08/25/10-38-06-782_f53686ae0b7ab372.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":889,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"us-038*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","linkMd5ListStr":"745523971937e3d2bbf0ca9e0d077655","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Q6sSAseS4hE:ZGUTfGSk7bY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn71@2020_5/2020/08/25/10-38-06-751_e887981c40b102d3.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1041,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe62*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light","linkMd5ListStr":"f1fb990643b446a2fb09abe6cb8c2668","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4Mg.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn64@2020_5/2020/08/25/10-38-06-061_fa1a100150a5f6f9.webp","sourceBytes":3630515,"destBytes":3232148,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":4467,"convertSpendMs":2744,"createdTime":"2020-08-25 18:38:03","host":"us-040*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.5 MB","destSize":"3.1 MB","compressRate":"89%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZpLVW_108Mc:8j9lIRcENNw:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn28@2020_5/2020/08/25/10-38-07-012_d72314201d25f438.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":797,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"us-023*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet","linkMd5ListStr":"b92198dab0130e2ebf3c419ddb0d4903","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=dR0py1h-AZM:ZBThNqRB3fg:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn96@2020_4/2020/08/25/10-38-06-928_7f61e53813db3ee0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":960,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe-59*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/want-a-really-hard-machine-learning-problem-try-agriculture-say-john-deere-labs-leaders","linkMd5ListStr":"5f2d11cc893fe393f2105cffb5ccf143","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4MDc0Mw.gif","sourceStatusCode":200,"destWidth":1200,"destHeight":741,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn79@2020_1/2020/08/25/10-38-06-501_8b043de815673445.webp","sourceBytes":1333050,"destBytes":660888,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":5264,"convertSpendMs":3906,"createdTime":"2020-08-25 18:38:02","host":"us-018*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"645.4 KB","compressRate":"49.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/JL1R_ghrql8","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn24@2020_6/2020/08/25/10-38-06-993_f53bd6f9894c6375.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":865,"convertSpendMs":10,"createdTime":"2020-08-25 18:38:06","host":"us-035*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/disney-research-dynamic-robotic-characters","linkMd5ListStr":"de6c237869321c3674d763f513eace56","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/MWyBQ98AJqA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn16@2020_4/2020/08/25/10-38-06-969_0c07dcada79623d6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":873,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","linkMd5ListStr":"c7f356a3967b14c7066d4a7ae2801b32","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=iB-EZS5TQKE:nV6g5-qUnfo:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn43@2020_2/2020/08/25/10-38-07-124_f54ea4e7da6c3b77.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":797,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-53*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","linkMd5ListStr":"953ccece03a4def30b62f328e3f48f7e","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/vedFlE8nubE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn48@2020_2/2020/08/25/10-38-07-103_bf8e4127f2922455.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":807,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzMyNDM5NA.jpeg","sourceStatusCode":200,"destWidth":1668,"destHeight":1158,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn8@2020_6/2020/08/25/10-38-06-923_3b99fd99d44a1542.webp","sourceBytes":211188,"destBytes":72278,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1123,"convertSpendMs":72,"createdTime":"2020-08-25 18:38:06","host":"us-015*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"206.2 KB","destSize":"70.6 KB","compressRate":"34.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjc0NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn31@2020_2/2020/08/25/10-38-07-106_e043c307ca172145.webp","sourceBytes":563645,"destBytes":32770,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":902,"convertSpendMs":43,"createdTime":"2020-08-25 18:38:07","host":"us-012*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"550.4 KB","destSize":"32 KB","compressRate":"5.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3Njc2OQ.jpeg","sourceStatusCode":200,"destWidth":1024,"destHeight":1024,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn3@2020_2/2020/08/25/10-38-06-845_6d2f003f94f36adf.webp","sourceBytes":183729,"destBytes":61822,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1169,"convertSpendMs":63,"createdTime":"2020-08-25 18:38:06","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/georgia-tech-smarticles-stochastic-robot-swarm","linkMd5ListStr":"3579544e8e9768028a4476e076f6e099","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"179.4 KB","destSize":"60.4 KB","compressRate":"33.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=wgEBanE3smE:EDJW7HFV0R8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn52@2020_4/2020/08/25/10-38-07-182_423bc79c009985fc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":837,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-020*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","linkMd5ListStr":"bf340835c615e553278f7841ec5e7cd7","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTQ4Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":743,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn19@2020_4/2020/08/25/10-38-06-629_25933dbbd8031aa6.webp","sourceBytes":386354,"destBytes":116696,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1894,"convertSpendMs":46,"createdTime":"2020-08-25 18:38:06","host":"europe68*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"377.3 KB","destSize":"114 KB","compressRate":"30.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDczOQ.jpeg","sourceStatusCode":200,"destWidth":620,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn55@2020_6/2020/08/25/10-38-07-141_1b16071b7a45541f.webp","sourceBytes":70156,"destBytes":41690,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":938,"convertSpendMs":20,"createdTime":"2020-08-25 18:38:07","host":"us-009*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"68.5 KB","destSize":"40.7 KB","compressRate":"59.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ysDZbFjjj5w:hYIYuBJN_yU:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn20@2020_1/2020/08/25/10-38-07-127_5dd5ee76a9c9ce74.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1044,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/AFnfw88Nq3o","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn68@2020_5/2020/08/25/10-38-07-296_d91e67c139ff3049.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":796,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:07","host":"us-040*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/help-rescuers-find-missing-persons-through-emergency-response-contest","linkMd5ListStr":"c70149e9a4334875ea4bbf98504cc029","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JTny_7tR99s:hOFx3iF1s6w:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn92@2020_2/2020/08/25/10-38-07-219_248dcc35caf86b97.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1323,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:06","host":"europe70*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid","linkMd5ListStr":"783ad2e174faa245c24d60ad66d167ad","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UofvhfDOHHU:aTec5z0MT0E:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn39@2020_4/2020/08/25/10-38-07-278_60377186350bad3c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":991,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"europe64*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VKQk2CUaQQk:wd4uXOXdla8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn96@2020_3/2020/08/25/10-38-07-453_2e7d02a20e263ccf.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":793,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-025*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot","linkMd5ListStr":"19aafea32608c470cfdef56d8c5fc346","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UuITyw6-YW0:nxysFLk59TA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn79@2020_2/2020/08/25/10-38-07-427_1704728b44474cdc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":823,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-006*","referer":"https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface","linkMd5ListStr":"c521e6873fe26deaf6eefa6c6967cc9c","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_SP9ysuBirA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn63@2020_6/2020/08/25/10-38-07-321_2869a6aa06834613.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":883,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"europe61*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NTYzMg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":715,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn83@2020_3/2020/08/25/10-38-06-639_58dbbb19ca518a25.webp","sourceBytes":1216839,"destBytes":194772,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3143,"convertSpendMs":118,"createdTime":"2020-08-25 18:38:05","host":"europe-25*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food","linkMd5ListStr":"d8a32c6887301ef3975606092a06d695","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"190.2 KB","compressRate":"16%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7rGryOunAtI:e8Vkj_SEwuY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn100@2020_6/2020/08/25/10-38-07-480_3dfd13c46a6df3e3.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":858,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-014*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies","linkMd5ListStr":"d6ffcad3ca933b1d80d7759de6a31b95","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=68kvbO8JlEQ:reuaV7O4PNQ:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn92@2020_2/2020/08/25/10-38-07-527_8c198de509188e7f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":904,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-037*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","linkMd5ListStr":"ec21b6293bbcb368dd9079b4df431fd8","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tD3Hqeyz2og:4-p-HsTg3jA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn75@2020_3/2020/08/25/10-38-07-621_e2592d42094b999a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":786,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:07","host":"us-017*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence","linkMd5ListStr":"5cb62d3fcb55be58473c9653072f3b07","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MjkyMjgxNg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":828,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn72@2020_4/2020/08/25/10-38-07-448_aa208f06292d1200.webp","sourceBytes":239546,"destBytes":48562,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1053,"convertSpendMs":36,"createdTime":"2020-08-25 18:38:07","host":"us-028*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"233.9 KB","destSize":"47.4 KB","compressRate":"20.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=qj6IDK7rITs","sourceStatusCode":200,"destWidth":113,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn4@2020_3/2020/08/25/10-38-07-600_af1c86a18a1486cf.webp","sourceBytes":1072,"destBytes":460,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":822,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:07","host":"us-003*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","linkMd5ListStr":"eaa12b83fdd04d32713423e0cead6909,110e6ac838f5a686ca9016e11c381c4e,c70149e9a4334875ea4bbf98504cc029,ac1ed2d177f3d00db61213d0a8b02970,4afdb44919c1840f63dbb5b7c83f76a7,3bfec1910aa88a37b6873b90b2190643,f4602756dbd851432f68681e9d7b807e,ed36754ceb3d54691c9985ca6f20cf97,daf121b367dccdb1c38a3b54e0c17b83,40500605e90a1fb25e13a0c4109cdc90,51766f8db6dedb5c20550ff3c3224e16,9811c27a8bfb20ca93a0926d39ae7e1e,5f2d11cc893fe393f2105cffb5ccf143,50bd446780e290d101b8680d1908ab28,3579544e8e9768028a4476e076f6e099,83dc60de229d0234a169f3b14521767c,5e83ca8ac583d87a19ee7990f07737a8,8be8e5cf657dd68e13509f254295323b,4bdb7374dde6559b940d866d23415332,645d9089aa580f603b8e05528f7302e7,ddd96fe231dfc465a52535e475a98b15,9778ce9d1c486affccf103eca762f153,f89d2d766c8d79193395fae8610e0f13,0e1d29b76415f6a75c1bacb441cc9c0d,c521e6873fe26deaf6eefa6c6967cc9c,c37b6ff6169ec7e6a055a9904de421c4,f1fb990643b446a2fb09abe6cb8c2668,522bcd2fb6d5cf0d6950cfc6b4675a85,b050441ca360dcb3e9e825f306284a0b,783ad2e174faa245c24d60ad66d167ad,da5f551943891d387e1f1c5f2532d096,b19d13bf54f71c1c40ce0f7d38f1a58b,d38b2f1b0a483a0b8124b15d386f7c04,d8a32c6887301ef3975606092a06d695,5f68fac164425c5c46c2a07001ed834c,f98c1554d57da1ae3c3120fbc73c502e,8651f5e29ba3de30c127e3e854f0dee5,975c3b1b81e23539106f318d7f3e28f7,4116b47e7bcae1aed0f1e1c85e917d83,affef744352a8c08e563bb642c76bc9c,6d47e21e002c0841e6518b91f4e6bdab,019269ec580d7ee2f12ac1c56db02f28,f42d642fe58f167022efac8e946848ef,7798fbe18b39cdcbed2b1084de9fdb63,026bc4b5af35d78854aaaae5d60ee79a,a2d95510787b8bacb7f69ee68b845ae0,2c1d20e7c96f50d4e36fa6c0bb86c46f,f06a1c762bc020e3eb9c056139e79fb8,fb732aaba2fa77f47db578dd8a2e2c4e,de6c237869321c3674d763f513eace56,5cb62d3fcb55be58473c9653072f3b07,1fbe4fd76aaa4013dc648f063ffe877a,507ca1aa50869b0a0bb43058028bc0d9,8e5ebdad5680d32249cb5e9715f5ba1f,cf05796d1ea712358de9c6a3708aa80c,9959391ccbf704954bde8fcaaf54f5ba,3a355f69e17aa2bb77552519184504ec,2b5ea3f72dcd3a0879d0e273a51ddf91,8ba894e1b44b55f874fed65b911e3953,ade9b8fe86da931a7835aee397c88af5,dd075dc0d056fad8777afa408a86be08,bc2b9a2906c1c252ead36f33d8597cd7,7c210ebadb5e65b08c2708471190bfd4,ecb83ca1fe85f5c86b44772662510c06,7c80d1a412783e6b99d1b3dcd2b92262,4542d0e2bf7583a7652c5511e14ef53d,6ab80d13744ce4a42ea20fe59304598b,bf340835c615e553278f7841ec5e7cd7,b4f4c90f36287bd417b7b4e5a43aa139,dac7321f3815f792354a3da3730527b3,98d9acef762105ed3deea1273ccf06c8,b92198dab0130e2ebf3c419ddb0d4903,70815399970675009fdaa91f1ed9af88,bacf594041d2950428c4810385b725b0,d6ffcad3ca933b1d80d7759de6a31b95,fddb2a608a827cba7cd0974ca59eeee3,fdeba8f58f9ca993017a817b623b747a,6d46fdc9cd73aab1f732a99137523da6,745523971937e3d2bbf0ca9e0d077655,a31b47238c2872d0b2da6a891bae5495,9ddfdd2b95e95bd2322570f7f5c5dbdb,04bd6282cc987a8acf3bef5de891d450,bac274849de61b732281df992d98ec37,a82857d2f8b930185848e80cae923b81,ba866f9c04fa639f4052fdd904ce7f83,7c7f2266a8977ac619b5378792fcd8f7,7cf67c6a00bfe23b83ee4f8fa82ccb7a,727addd4da03ea6fd9f64c61ab6be805,de8494e008723f24ca95f4421202eaa2,184896aa81aa8ae49425e7c03d12d965,67ca3612e21a3b8090962da8d8d72cea,5c8fee7c8bef1dc43b41be2a29c47ddb,538b0c13aaa64a59898970581b42f5fa,19aafea32608c470cfdef56d8c5fc346,942b7fea25553a3c98fd6f49daf1b0f2,05ca9649ab0849af31ee29f9a1f2318f,ec21b6293bbcb368dd9079b4df431fd8,0c575b7593d53cada9ce6cb4a5dbad5f,c7f356a3967b14c7066d4a7ae2801b32,953ccece03a4def30b62f328e3f48f7e","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1 KB","destSize":"460 B","compressRate":"42.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ePDcbxuJmvE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn87@2020_2/2020/08/25/10-38-07-512_bc1af74c0b1249f0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":887,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/UofvhfDOHHU","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn20@2020_6/2020/08/25/10-38-07-656_34a8da415654b004.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":807,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"us-022*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=r2-HEkba0No:-emDvS734jg:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn15@2020_4/2020/08/25/10-38-07-712_545b73adcbe745b9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":815,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:07","host":"us-033*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","linkMd5ListStr":"ddd96fe231dfc465a52535e475a98b15","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=8qXUW4b1PHc:ZGA2YPIR5uc:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn24@2020_4/2020/08/25/10-38-07-767_e33eb3365c620b07.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":811,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","linkMd5ListStr":"9778ce9d1c486affccf103eca762f153","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rgt5dqPWMbA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn7@2020_2/2020/08/25/10-38-07-705_f9bfd414f656657c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":869,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"europe66*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home","linkMd5ListStr":"5f68fac164425c5c46c2a07001ed834c","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/4K0VX3LeIBo","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn40@2020_1/2020/08/25/10-38-07-835_f520ca76362099ab.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":802,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"us-030*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever","linkMd5ListStr":"019269ec580d7ee2f12ac1c56db02f28","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=FMSyH0YMiNw:MiUVkrzeNOc:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn43@2020_1/2020/08/25/10-38-07-899_85a321625f09a1a6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":811,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"us-019*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/caltech-and-jpl-firing-quadrotors-out-of-cannons","linkMd5ListStr":"2c1d20e7c96f50d4e36fa6c0bb86c46f","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/-x2GxmjoVIQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn36@2020_3/2020/08/25/10-38-07-836_0a6fb2638bafb6bc.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":866,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:07","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","linkMd5ListStr":"5c8fee7c8bef1dc43b41be2a29c47ddb","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1OTI3NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn35@2020_6/2020/08/25/10-38-07-464_bc986b2e09b48dff.webp","sourceBytes":133375,"destBytes":65484,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1544,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:07","host":"europe-25*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","linkMd5ListStr":"f4602756dbd851432f68681e9d7b807e","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"130.2 KB","destSize":"63.9 KB","compressRate":"49.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Za6B4U3ZdIk","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn67@2020_6/2020/08/25/10-38-07-970_3a2f652279921085.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":794,"convertSpendMs":9,"createdTime":"2020-08-25 18:38:07","host":"us-016*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot","linkMd5ListStr":"fb732aaba2fa77f47db578dd8a2e2c4e","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JTny_7tR99s:hOFx3iF1s6w:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn64@2020_1/2020/08/25/10-38-07-985_da2a04d2969b5d82.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":817,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"us-027*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drone-remoteid","linkMd5ListStr":"783ad2e174faa245c24d60ad66d167ad","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QGOM8XaI22U:GfwBL-DH35Q:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn71@2020_3/2020/08/25/10-38-08-035_e6b411b852dc3d5f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":893,"convertSpendMs":12,"createdTime":"2020-08-25 18:38:07","host":"us-005*","referer":"https://spectrum.ieee.org/tech-talk/computing/software/aipowered-robotic-system-stirs-up-molecules-from-scratch","linkMd5ListStr":"1fbe4fd76aaa4013dc648f063ffe877a","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=aU1T0j0-G7U:4jQqdmb0lgM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn87@2020_4/2020/08/25/10-38-08-155_fc3e6a972230359e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":815,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:08","host":"us-024*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/us-energy-department-is-first-customer-for-worlds-biggest-chip","linkMd5ListStr":"f98c1554d57da1ae3c3120fbc73c502e","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=oPPftINehQs:s-u5RQQjKAA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn91@2020_3/2020/08/25/10-38-08-202_2c89dcd73bd4de3d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":874,"convertSpendMs":8,"createdTime":"2020-08-25 18:38:08","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-transferring-human-motion-to-a-mobile-robotic-manipulator","linkMd5ListStr":"7c80d1a412783e6b99d1b3dcd2b92262","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Kc0qsE87fHQ:RdO6Cd7FWsg:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn4@2020_4/2020/08/25/10-38-08-266_5037603819e8a098.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":830,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:08","host":"us-54*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope","linkMd5ListStr":"bacf594041d2950428c4810385b725b0","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Du61YeV7EjU:0OnRv_R-GvE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn47@2020_1/2020/08/25/10-38-08-258_1f65ce545d5e4de8.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1186,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:07","host":"us-008*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped","linkMd5ListStr":"6d47e21e002c0841e6518b91f4e6bdab","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTcwOA.png","sourceStatusCode":200,"destWidth":2310,"destHeight":1558,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn67@2020_2/2020/08/25/10-38-07-375_86aa1e1729154648.webp","sourceBytes":563695,"destBytes":325490,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2432,"convertSpendMs":726,"createdTime":"2020-08-25 18:38:06","host":"europe-23*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","linkMd5ListStr":"ec21b6293bbcb368dd9079b4df431fd8","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"550.5 KB","destSize":"317.9 KB","compressRate":"57.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Kc0qsE87fHQ:RdO6Cd7FWsg:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn8@2020_6/2020/08/25/10-38-08-303_7e39848d33f8d28a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":880,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:08","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope","linkMd5ListStr":"bacf594041d2950428c4810385b725b0","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=7rGryOunAtI:e8Vkj_SEwuY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn16@2020_6/2020/08/25/10-38-08-390_e0f482e8ca631ca5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":867,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:08","host":"us-010*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies","linkMd5ListStr":"d6ffcad3ca933b1d80d7759de6a31b95","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3Mjk5OQ.png","sourceStatusCode":200,"destWidth":1374,"destHeight":776,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn95@2020_4/2020/08/25/10-38-08-318_07ef09cd60357624.webp","sourceBytes":612117,"destBytes":95056,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1045,"convertSpendMs":72,"createdTime":"2020-08-25 18:38:08","host":"us-001*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions","linkMd5ListStr":"d38b2f1b0a483a0b8124b15d386f7c04","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"597.8 KB","destSize":"92.8 KB","compressRate":"15.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/5urTYVBa2Lw","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn40@2020_2/2020/08/25/10-38-08-490_e42d6f9dfe0c6706.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":799,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:08","host":"us-007*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/georgia-tech-smarticles-stochastic-robot-swarm","linkMd5ListStr":"3579544e8e9768028a4476e076f6e099","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cPJEgwpqkv8:6bcjwCgtYPw:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn80@2020_2/2020/08/25/10-38-08-383_8ce075a5fca283d8.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1097,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:08","host":"europe-57*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU1Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":640,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn12@2020_5/2020/08/25/10-38-08-314_d4fb690226a7401b.webp","sourceBytes":324820,"destBytes":124948,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1056,"convertSpendMs":40,"createdTime":"2020-08-25 18:38:08","host":"us-021*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"317.2 KB","destSize":"122 KB","compressRate":"38.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4lbED3n0y6U:GhJJ0usmqCA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn75@2020_4/2020/08/25/10-38-08-194_d0e78e9474fa480c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1242,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:08","host":"europe68*","referer":"https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020","linkMd5ListStr":"538b0c13aaa64a59898970581b42f5fa","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/TYE4X4BWZag","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn27@2020_6/2020/08/25/10-38-08-558_efd83fd3d257ab3e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":831,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:08","host":"us-51*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","linkMd5ListStr":"ba866f9c04fa639f4052fdd904ce7f83","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMjA2MQ.png","sourceStatusCode":200,"destWidth":607,"destHeight":451,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn36@2020_3/2020/08/25/10-38-08-500_6c427da6ffe5e547.webp","sourceBytes":445022,"destBytes":20244,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":941,"convertSpendMs":29,"createdTime":"2020-08-25 18:38:08","host":"us-018*","referer":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","linkMd5ListStr":"4afdb44919c1840f63dbb5b7c83f76a7","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"434.6 KB","destSize":"19.8 KB","compressRate":"4.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzM3OA.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn60@2020_6/2020/08/25/10-38-08-281_9642811ce33ff685.webp","sourceBytes":2182775,"destBytes":177464,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1450,"convertSpendMs":165,"createdTime":"2020-08-25 18:38:07","host":"us-039*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.1 MB","destSize":"173.3 KB","compressRate":"8.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDM2NA.octet-stream","sourceStatusCode":200,"destWidth":600,"destHeight":442,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn55@2020_2/2020/08/25/10-38-08-345_3f25f7447c40042c.webp","sourceBytes":93529,"destBytes":15796,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1375,"convertSpendMs":12,"createdTime":"2020-08-25 18:38:08","host":"europe-60*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"91.3 KB","destSize":"15.4 KB","compressRate":"16.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/J1wRiTEvVSs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn52@2020_6/2020/08/25/10-38-08-622_bce2d6ea67862f9d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":833,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:08","host":"us-038*","referer":"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies","linkMd5ListStr":"f06a1c762bc020e3eb9c056139e79fb8","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WYorWvFk5Jk:OlhVHrX5MDI:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn55@2020_4/2020/08/25/10-38-08-690_fd59797452acd1ef.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":815,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:08","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/agility-robotics-digit-v2-biped-robot","linkMd5ListStr":"507ca1aa50869b0a0bb43058028bc0d9","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4OTg4MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1445,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn84@2020_4/2020/08/25/10-38-08-418_2d4e9d3a0e61f7ba.webp","sourceBytes":528132,"destBytes":121708,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1390,"convertSpendMs":142,"createdTime":"2020-08-25 18:38:08","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","linkMd5ListStr":"9778ce9d1c486affccf103eca762f153","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"515.8 KB","destSize":"118.9 KB","compressRate":"23%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-x2GxmjoVIQ:dcG3leT_8II:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn24@2020_3/2020/08/25/10-38-08-602_83d1702aab4af228.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":971,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:08","host":"europe62*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","linkMd5ListStr":"5c8fee7c8bef1dc43b41be2a29c47ddb","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VszUWNQuPRA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn19@2020_1/2020/08/25/10-38-08-458_e15d2fe66f8344f4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1057,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:08","host":"europe-23*","referer":"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things","linkMd5ListStr":"daf121b367dccdb1c38a3b54e0c17b83","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDY2OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":868,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn32@2020_3/2020/08/25/10-38-08-536_3f66d2cdbfac2e93.webp","sourceBytes":575643,"destBytes":117416,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1130,"convertSpendMs":85,"createdTime":"2020-08-25 18:38:08","host":"us-029*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"562.2 KB","destSize":"114.7 KB","compressRate":"20.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Du61YeV7EjU:0OnRv_R-GvE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn48@2020_4/2020/08/25/10-38-08-742_20335e34f6198a63.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":984,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:08","host":"europe-59*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped","linkMd5ListStr":"6d47e21e002c0841e6518b91f4e6bdab","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA0NQ.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn99@2020_1/2020/08/25/10-38-08-476_05389d29e1abfbb1.webp","sourceBytes":975490,"destBytes":47170,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1355,"convertSpendMs":98,"createdTime":"2020-08-25 18:38:08","host":"europe65*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"952.6 KB","destSize":"46.1 KB","compressRate":"4.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cPJEgwpqkv8:6bcjwCgtYPw:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn83@2020_4/2020/08/25/10-38-08-987_fba4ce39c73b8c31.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":826,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:08","host":"us-012*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MDY4OQ.jpeg","sourceStatusCode":200,"destWidth":1536,"destHeight":1392,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn28@2020_6/2020/08/25/10-38-08-460_3809d5f337927f39.webp","sourceBytes":339608,"destBytes":92880,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1925,"convertSpendMs":73,"createdTime":"2020-08-25 18:38:07","host":"europe-24*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","linkMd5ListStr":"bf340835c615e553278f7841ec5e7cd7","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"331.6 KB","destSize":"90.7 KB","compressRate":"27.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDc3Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":698,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn76@2020_1/2020/08/25/10-38-08-868_b9ac266f1f46d0be.webp","sourceBytes":227359,"destBytes":76320,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1055,"convertSpendMs":56,"createdTime":"2020-08-25 18:38:08","host":"us-035*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"222 KB","destSize":"74.5 KB","compressRate":"33.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTQ4NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":881,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn12@2020_6/2020/08/25/10-38-08-933_46dd9d9b6a202245.webp","sourceBytes":253739,"destBytes":69630,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1202,"convertSpendMs":43,"createdTime":"2020-08-25 18:38:08","host":"us-55*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"247.8 KB","destSize":"68 KB","compressRate":"27.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA5Mw.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn59@2020_2/2020/08/25/10-38-08-862_a5011ea0d246d747.webp","sourceBytes":1571382,"destBytes":198766,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1240,"convertSpendMs":145,"createdTime":"2020-08-25 18:38:08","host":"us-015*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.5 MB","destSize":"194.1 KB","compressRate":"12.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTUxNw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":772,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn79@2020_5/2020/08/25/10-38-09-025_8b4ef8379f5be9d1.webp","sourceBytes":128702,"destBytes":97264,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1126,"convertSpendMs":36,"createdTime":"2020-08-25 18:38:08","host":"us-023*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"125.7 KB","destSize":"95 KB","compressRate":"75.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/mMsm0tCvg3c","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn7@2020_2/2020/08/25/10-38-09-216_b20b5ecdbc264217.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":793,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:09","host":"us-009*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","linkMd5ListStr":"50bd446780e290d101b8680d1908ab28","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-x2GxmjoVIQ:dcG3leT_8II:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn99@2020_4/2020/08/25/10-38-09-170_6f9b1cb59938eea4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":952,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","linkMd5ListStr":"5c8fee7c8bef1dc43b41be2a29c47ddb","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/aU1T0j0-G7U","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn87@2020_1/2020/08/25/10-38-09-121_bb441d1060cfc2a1.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":920,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:09","host":"europe-25*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/us-energy-department-is-first-customer-for-worlds-biggest-chip","linkMd5ListStr":"f98c1554d57da1ae3c3120fbc73c502e","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4OTg3OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1433,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn32@2020_3/2020/08/25/10-38-08-647_11ba8337a3ff5223.webp","sourceBytes":428524,"destBytes":146170,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2149,"convertSpendMs":84,"createdTime":"2020-08-25 18:38:07","host":"europe63*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","linkMd5ListStr":"9778ce9d1c486affccf103eca762f153","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"418.5 KB","destSize":"142.7 KB","compressRate":"34.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=XQoU3ebUNAE:Lr6zffHQlz8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn3@2020_6/2020/08/25/10-38-09-163_15be7438b0b59dab.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":973,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-020*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robots-explore-tunnels-darpa-subt","linkMd5ListStr":"fddb2a608a827cba7cd0974ca59eeee3","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzI0MTAwOQ.jpeg","sourceStatusCode":200,"destWidth":280,"destHeight":147,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn28@2020_5/2020/08/25/10-38-09-369_964adaa282b1aac4.webp","sourceBytes":47143,"destBytes":7240,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":807,"convertSpendMs":7,"createdTime":"2020-08-25 18:38:09","host":"us-017*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"51766f8db6dedb5c20550ff3c3224e16,b19d13bf54f71c1c40ce0f7d38f1a58b,affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"46 KB","destSize":"7.1 KB","compressRate":"15.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/F9OLArjw1as","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn92@2020_6/2020/08/25/10-38-09-258_2bf7410ac3e1cc46.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":961,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:09","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india","linkMd5ListStr":"645d9089aa580f603b8e05528f7302e7","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MDQ3Nw.jpeg","sourceStatusCode":200,"destWidth":1280,"destHeight":1021,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn96@2020_4/2020/08/25/10-38-09-159_27f4981a792ae2fa.webp","sourceBytes":421942,"destBytes":186550,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1153,"convertSpendMs":67,"createdTime":"2020-08-25 18:38:09","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","linkMd5ListStr":"de8494e008723f24ca95f4421202eaa2","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"412.1 KB","destSize":"182.2 KB","compressRate":"44.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ysDZbFjjj5w","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn44@2020_4/2020/08/25/10-38-09-463_55861e35f5b2bb2a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":769,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:09","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/68kvbO8JlEQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn48@2020_6/2020/08/25/10-38-09-482_faa2cef02239d5f7.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":795,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-025*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","linkMd5ListStr":"ec21b6293bbcb368dd9079b4df431fd8","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VKQk2CUaQQk:wd4uXOXdla8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn31@2020_2/2020/08/25/10-38-09-476_e7d6fb1532fa721c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":885,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot","linkMd5ListStr":"19aafea32608c470cfdef56d8c5fc346","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5urTYVBa2Lw:VJf9q2xFxfM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn52@2020_2/2020/08/25/10-38-09-523_5c02ad0d50b26af3.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":825,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:09","host":"us-014*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/georgia-tech-smarticles-stochastic-robot-swarm","linkMd5ListStr":"3579544e8e9768028a4476e076f6e099","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODE5Mw.png","sourceStatusCode":200,"destWidth":607,"destHeight":458,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn24@2020_1/2020/08/25/10-38-09-418_b918eb67e88bfb0c.webp","sourceBytes":545084,"destBytes":39720,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":977,"convertSpendMs":44,"createdTime":"2020-08-25 18:38:09","host":"us-028*","referer":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","linkMd5ListStr":"c37b6ff6169ec7e6a055a9904de421c4","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"532.3 KB","destSize":"38.8 KB","compressRate":"7.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NjUwMg.jpeg","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn72@2020_2/2020/08/25/10-38-09-268_1c4c760479549201.webp","sourceBytes":151030,"destBytes":22640,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1439,"convertSpendMs":35,"createdTime":"2020-08-25 18:38:08","host":"europe-56*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","linkMd5ListStr":"50bd446780e290d101b8680d1908ab28","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"147.5 KB","destSize":"22.1 KB","compressRate":"15%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ysDZbFjjj5w:hYIYuBJN_yU:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn67@2020_3/2020/08/25/10-38-09-605_b6f0b47e820ca7c2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":834,"convertSpendMs":8,"createdTime":"2020-08-25 18:38:09","host":"us-033*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NzkyNg.png","sourceStatusCode":200,"destWidth":680,"destHeight":778,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn56@2020_1/2020/08/25/10-38-09-583_c37dc9d9af101bfc.webp","sourceBytes":719330,"destBytes":24858,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":912,"convertSpendMs":36,"createdTime":"2020-08-25 18:38:09","host":"us-003*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"702.5 KB","destSize":"24.3 KB","compressRate":"3.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=3hDuhYQdo-A:oooi_DFqXNQ:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn64@2020_2/2020/08/25/10-38-09-708_0ae6703a0f3e7276.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":900,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-55*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/for-centuries-people-dreamed-of-a-machine-that-can-produce-language-then-openai-made-one","linkMd5ListStr":"eaa12b83fdd04d32713423e0cead6909","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjMwNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn20@2020_4/2020/08/25/10-38-09-387_591c634711ade6a3.webp","sourceBytes":247775,"destBytes":194404,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1155,"convertSpendMs":60,"createdTime":"2020-08-25 18:38:09","host":"us-040*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot","linkMd5ListStr":"3a355f69e17aa2bb77552519184504ec","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"242 KB","destSize":"189.8 KB","compressRate":"78.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WyumTS8jes0:bDgdbPvpRZA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn71@2020_1/2020/08/25/10-38-09-727_cde057126deb20d5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":820,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-022*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/microsofts-ai-research-draws-controversy-over-possible-disinformation-use","linkMd5ListStr":"5e83ca8ac583d87a19ee7990f07737a8","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ypq5vl5-FKM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn88@2020_2/2020/08/25/10-38-09-904_11b6339f09049b1e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":778,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain","linkMd5ListStr":"4bdb7374dde6559b940d866d23415332","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/eWooJQd8zew","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn95@2020_1/2020/08/25/10-38-09-967_2453bb1648f6fdb8.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":794,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"us-019*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/M6oPcVu-H6M","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn84@2020_5/2020/08/25/10-38-09-907_fcb64f89cedb2119.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":890,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:09","host":"europe63*","referer":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","linkMd5ListStr":"c37b6ff6169ec7e6a055a9904de421c4","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzIzMTcyMw.jpeg","sourceStatusCode":200,"destWidth":2182,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn83@2020_1/2020/08/25/10-38-08-589_6d88d0474b1a79dd.webp","sourceBytes":690607,"destBytes":360402,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3269,"convertSpendMs":175,"createdTime":"2020-08-25 18:38:07","host":"europe69*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"674.4 KB","destSize":"352 KB","compressRate":"52.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=eWooJQd8zew:ZqKXuPKCKkE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn60@2020_1/2020/08/25/10-38-09-721_5b3dd062474aa283.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1224,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:09","host":"europe66*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTU4Mg.jpeg","sourceStatusCode":200,"destWidth":2515,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn75@2020_4/2020/08/25/10-38-09-916_5bb9b729ac0b7ff8.webp","sourceBytes":731773,"destBytes":97278,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1128,"convertSpendMs":159,"createdTime":"2020-08-25 18:38:09","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"714.6 KB","destSize":"95 KB","compressRate":"13.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NTI1MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":893,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn51@2020_1/2020/08/25/10-38-09-135_57078d60b7c528e3.webp","sourceBytes":417515,"destBytes":254164,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2831,"convertSpendMs":64,"createdTime":"2020-08-25 18:38:07","host":"europe21*","referer":"https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever","linkMd5ListStr":"7798fbe18b39cdcbed2b1084de9fdb63","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"407.7 KB","destSize":"248.2 KB","compressRate":"60.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjc0MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":827,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn12@2020_2/2020/08/25/10-38-09-477_a3ad8f43ebbf4e70.webp","sourceBytes":671687,"destBytes":66336,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1493,"convertSpendMs":69,"createdTime":"2020-08-25 18:38:09","host":"europe-22*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"655.9 KB","destSize":"64.8 KB","compressRate":"9.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hoozvOaCyGQ:ZdVZchta5ec:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn11@2020_3/2020/08/25/10-38-10-160_0919362de61d7e6a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":818,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:10","host":"us-039*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous","linkMd5ListStr":"6ab80d13744ce4a42ea20fe59304598b","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=lq8k4y7KjVg:plxvSkQegxk:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn15@2020_1/2020/08/25/10-38-10-175_6404a8bba9844e4a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":871,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:10","host":"us-027*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-92019","linkMd5ListStr":"942b7fea25553a3c98fd6f49daf1b0f2","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/RignCTneE9M","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn24@2020_2/2020/08/25/10-38-10-178_493edee4740f344f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":809,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:10","host":"us-005*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","linkMd5ListStr":"8be8e5cf657dd68e13509f254295323b","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTM0Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn36@2020_6/2020/08/25/10-38-09-880_05f0c97595ff0d61.webp","sourceBytes":196589,"destBytes":26440,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1456,"convertSpendMs":32,"createdTime":"2020-08-25 18:38:09","host":"europe69*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"192 KB","destSize":"25.8 KB","compressRate":"13.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Q6sSAseS4hE:ZGUTfGSk7bY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn32@2020_5/2020/08/25/10-38-10-295_3e23c6af9521bcda.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":839,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:10","host":"us-035*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/optoelectronics/ai-at-speed-of-light","linkMd5ListStr":"f1fb990643b446a2fb09abe6cb8c2668","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_Hi6JNHldFo:dQwWdXtGW-M:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn79@2020_6/2020/08/25/10-38-10-177_70afbaa1fcbe159d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1191,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:09","host":"europe-24*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","linkMd5ListStr":"184896aa81aa8ae49425e7c03d12d965","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=f4vRmWO59FQ:u8dJuNbBVJU:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn34@2020_4/2020/08/25/10-38-10-319_db083f9a46ebf17e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":859,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:10","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","linkMd5ListStr":"de8494e008723f24ca95f4421202eaa2","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/D6FhFJID_ho","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn48@2020_1/2020/08/25/10-38-10-360_0c959971b9ff265a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":822,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:10","host":"us-001*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","linkMd5ListStr":"745523971937e3d2bbf0ca9e0d077655","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjczMQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":744,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn16@2020_6/2020/08/25/10-38-10-125_b1a318877e9dd308.webp","sourceBytes":477506,"destBytes":15028,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1832,"convertSpendMs":37,"createdTime":"2020-08-25 18:38:09","host":"europe61*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"466.3 KB","destSize":"14.7 KB","compressRate":"3.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3NTUzNA.jpeg","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn43@2020_5/2020/08/25/10-38-10-389_0a51c416d3b156bb.webp","sourceBytes":64237,"destBytes":41712,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":956,"convertSpendMs":38,"createdTime":"2020-08-25 18:38:10","host":"us-013*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","linkMd5ListStr":"ade9b8fe86da931a7835aee397c88af5","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"62.7 KB","destSize":"40.7 KB","compressRate":"64.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMjA2Mw.png","sourceStatusCode":200,"destWidth":296,"destHeight":221,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn64@2020_5/2020/08/25/10-38-10-447_5c6704998a2104ba.webp","sourceBytes":113319,"destBytes":7588,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":867,"convertSpendMs":12,"createdTime":"2020-08-25 18:38:10","host":"us-021*","referer":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","linkMd5ListStr":"4afdb44919c1840f63dbb5b7c83f76a7","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"110.7 KB","destSize":"7.4 KB","compressRate":"6.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VszUWNQuPRA:_kdYL-vrJa4:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn60@2020_4/2020/08/25/10-38-10-541_99616023c1828a68.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":918,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:10","host":"us-032*","referer":"https://spectrum.ieee.org/view-from-the-valley/at-work/innovation/is-it-time-for-tech-to-stop-moving-fast-and-breaking-things","linkMd5ListStr":"daf121b367dccdb1c38a3b54e0c17b83","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=9M4rEIftmus:0mYUx3pzxPg:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn28@2020_6/2020/08/25/10-38-10-438_547193ffd27274ce.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1038,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:10","host":"europe68*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-humanoid-robot-roboy-serving-ice-cream","linkMd5ListStr":"727addd4da03ea6fd9f64c61ab6be805","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=F9OLArjw1as:mj94lXbFh2A:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn7@2020_2/2020/08/25/10-38-10-324_7df99b838c68940b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1172,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:10","host":"europe-60*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india","linkMd5ListStr":"645d9089aa580f603b8e05528f7302e7","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjA0Mw.png","sourceStatusCode":200,"destWidth":1595,"destHeight":1057,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn40@2020_5/2020/08/25/10-38-10-450_99f2399b6164613e.webp","sourceBytes":1766211,"destBytes":148592,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1194,"convertSpendMs":115,"createdTime":"2020-08-25 18:38:10","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 MB","destSize":"145.1 KB","compressRate":"8.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rgt5dqPWMbA:TjT0erajUXU:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn68@2020_2/2020/08/25/10-38-10-584_d0b837a4d9c7122b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1024,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:10","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home","linkMd5ListStr":"5f68fac164425c5c46c2a07001ed834c","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UofvhfDOHHU:aTec5z0MT0E:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn79@2020_4/2020/08/25/10-38-10-817_62f86a8117407a0d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":892,"convertSpendMs":7,"createdTime":"2020-08-25 18:38:10","host":"us-51*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4lbED3n0y6U:GhJJ0usmqCA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn83@2020_2/2020/08/25/10-38-10-813_79278e042af349c2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":836,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:10","host":"us-029*","referer":"https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020","linkMd5ListStr":"538b0c13aaa64a59898970581b42f5fa","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwMjkxMg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1097,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn56@2020_4/2020/08/25/10-38-10-526_daf78723772b4086.webp","sourceBytes":432218,"destBytes":146002,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1279,"convertSpendMs":92,"createdTime":"2020-08-25 18:38:10","host":"us-54*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/mit-little-hermes","linkMd5ListStr":"5c8fee7c8bef1dc43b41be2a29c47ddb","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"422.1 KB","destSize":"142.6 KB","compressRate":"33.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QGOM8XaI22U:GfwBL-DH35Q:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn76@2020_3/2020/08/25/10-38-10-734_9eae0ce37e6e4e39.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":997,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:10","host":"europe62*","referer":"https://spectrum.ieee.org/tech-talk/computing/software/aipowered-robotic-system-stirs-up-molecules-from-scratch","linkMd5ListStr":"1fbe4fd76aaa4013dc648f063ffe877a","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RNa1KqrBSZw:_EaQTOBiM-E:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn51@2020_4/2020/08/25/10-38-10-628_b6e92550230580db.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1223,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:10","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation","linkMd5ListStr":"8ba894e1b44b55f874fed65b911e3953","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/wgEBanE3smE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn7@2020_1/2020/08/25/10-38-10-947_bceb434086136834.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":809,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:10","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/anybotics-introduces-sleek-new-anymal-c-quadruped","linkMd5ListStr":"bf340835c615e553278f7841ec5e7cd7","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZpLVW_108Mc:8j9lIRcENNw:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn72@2020_2/2020/08/25/10-38-10-752_95a880ad559f8b66.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1179,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:10","host":"europe-23*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/universal-robots-introduces-its-strongest-robotic-arm-yet","linkMd5ListStr":"b92198dab0130e2ebf3c419ddb0d4903","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDcwNg.jpeg","sourceStatusCode":200,"destWidth":620,"destHeight":465,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn87@2020_2/2020/08/25/10-38-10-884_ae2cec59a66b55be.webp","sourceBytes":50918,"destBytes":28266,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":919,"convertSpendMs":28,"createdTime":"2020-08-25 18:38:10","host":"us-018*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"49.7 KB","destSize":"27.6 KB","compressRate":"55.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2ODAxNQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":696,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn91@2020_3/2020/08/25/10-38-11-022_5a0600b42b75e7df.webp","sourceBytes":239801,"destBytes":33578,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":999,"convertSpendMs":54,"createdTime":"2020-08-25 18:38:10","host":"us-007*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"234.2 KB","destSize":"32.8 KB","compressRate":"14%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-xqOx5HkcOE:UY5txkCnYhQ:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn42@2020_5/2020/08/25/10-38-11-231_75338573ec4d6239.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":798,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:11","host":"us-001*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation","linkMd5ListStr":"026bc4b5af35d78854aaaae5d60ee79a","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/WQZUBYIDfRg","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn19@2020_3/2020/08/25/10-38-11-108_8829968c6d83b9b0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":877,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats","linkMd5ListStr":"f89d2d766c8d79193395fae8610e0f13","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=EtQ_JVz0FMI:8EAx1saPp9A:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn33@2020_2/2020/08/25/10-38-11-230_2c6efeb6dacbfbf7.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":875,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"us-024*","referer":"https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality","linkMd5ListStr":"522bcd2fb6d5cf0d6950cfc6b4675a85","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=J1wRiTEvVSs:d30liMidOY4:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn100@2020_1/2020/08/25/10-38-11-098_6506c6fb57832a6b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1000,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:10","host":"europe-59*","referer":"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies","linkMd5ListStr":"f06a1c762bc020e3eb9c056139e79fb8","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=RignCTneE9M:v2UFZRjM9PY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn96@2020_2/2020/08/25/10-38-11-101_46dab501fe2c8c72.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1003,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:10","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","linkMd5ListStr":"8be8e5cf657dd68e13509f254295323b","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTQ3Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":752,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn12@2020_4/2020/08/25/10-38-11-118_78a21b132349b7ee.webp","sourceBytes":732433,"destBytes":62420,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1167,"convertSpendMs":58,"createdTime":"2020-08-25 18:38:10","host":"us-015*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"715.3 KB","destSize":"61 KB","compressRate":"8.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=TYE4X4BWZag:Yijh8rC9x50:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn54@2020_1/2020/08/25/10-38-11-396_a2c14b8066331e8b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":830,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","linkMd5ListStr":"ba866f9c04fa639f4052fdd904ce7f83","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=4K0VX3LeIBo:fXrdncAkUXE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn38@2020_6/2020/08/25/10-38-11-250_073e45cdfcbeaaa0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1003,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:11","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/swappable-flying-batteries-keep-drones-aloft-almost-forever","linkMd5ListStr":"019269ec580d7ee2f12ac1c56db02f28","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=X-v6WYhBbOk:6bxLlz5K92Q:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn23@2020_2/2020/08/25/10-38-11-225_156e769a1a9b5269.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1036,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=hHQ_Cl_97H4:rFENhMTkafk:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn57@2020_1/2020/08/25/10-38-11-388_6ef947e6b0fa6c7e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":828,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"us-021*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/interview-blue-frog-robotics-ceo","linkMd5ListStr":"9959391ccbf704954bde8fcaaf54f5ba","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjczMg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":910,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn39@2020_1/2020/08/25/10-38-10-735_a799955177608a9b.webp","sourceBytes":939573,"destBytes":163624,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2656,"convertSpendMs":56,"createdTime":"2020-08-25 18:38:09","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"917.6 KB","destSize":"159.8 KB","compressRate":"17.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTcxNQ.png","sourceStatusCode":200,"destWidth":2339,"destHeight":1559,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn67@2020_2/2020/08/25/10-38-10-507_3d095a67f94d6e30.webp","sourceBytes":1859787,"destBytes":393168,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3448,"convertSpendMs":1580,"createdTime":"2020-08-25 18:38:08","host":"europe67*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/parrot-adds-folding-vr-goggles-to-anafi-drone-kit","linkMd5ListStr":"ec21b6293bbcb368dd9079b4df431fd8","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.8 MB","destSize":"384 KB","compressRate":"21.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU0OQ.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":802,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn91@2020_2/2020/08/25/10-38-11-421_df95ba2d3ac39686.webp","sourceBytes":478904,"destBytes":64108,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2428,"convertSpendMs":37,"createdTime":"2020-08-25 18:38:09","host":"us-030*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"467.7 KB","destSize":"62.6 KB","compressRate":"13.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NzQ0NA.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":750,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn19@2020_6/2020/08/25/10-38-10-177_6538b306c2d48f4c.webp","sourceBytes":130245,"destBytes":51554,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2221,"convertSpendMs":35,"createdTime":"2020-08-25 18:38:10","host":"us-016*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","linkMd5ListStr":"c7f356a3967b14c7066d4a7ae2801b32","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"127.2 KB","destSize":"50.3 KB","compressRate":"39.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTY1Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn29@2020_2/2020/08/25/10-38-11-301_55f16f28d8d398ce.webp","sourceBytes":353715,"destBytes":204028,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1239,"convertSpendMs":159,"createdTime":"2020-08-25 18:38:11","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"345.4 KB","destSize":"199.2 KB","compressRate":"57.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDc4MQ.jpeg","sourceStatusCode":200,"destWidth":1059,"destHeight":719,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn62@2020_5/2020/08/25/10-38-11-444_b3506b8c4a138d6d.webp","sourceBytes":401059,"destBytes":85186,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1010,"convertSpendMs":36,"createdTime":"2020-08-25 18:38:11","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"391.7 KB","destSize":"83.2 KB","compressRate":"21.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=UuITyw6-YW0:nxysFLk59TA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn50@2020_3/2020/08/25/10-38-11-405_c9729e2fe6d9fb31.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1107,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:11","host":"us-54*","referer":"https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface","linkMd5ListStr":"c521e6873fe26deaf6eefa6c6967cc9c","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNjE2MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn74@2020_4/2020/08/25/10-38-11-613_0cd3327ac488aef4.webp","sourceBytes":78215,"destBytes":39634,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":967,"convertSpendMs":48,"createdTime":"2020-08-25 18:38:11","host":"us-51*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"76.4 KB","destSize":"38.7 KB","compressRate":"50.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/af0ofRZ9M8M","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn82@2020_2/2020/08/25/10-38-11-673_0beb0879a8887890.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":786,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"us-018*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots","linkMd5ListStr":"67ca3612e21a3b8090962da8d8d72cea","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2ODA0Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1653,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn4@2020_6/2020/08/25/10-38-10-722_3c88df0461c594cb.webp","sourceBytes":426523,"destBytes":58122,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2423,"convertSpendMs":64,"createdTime":"2020-08-25 18:38:10","host":"europe21*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"416.5 KB","destSize":"56.8 KB","compressRate":"13.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU1Nw.gif","sourceStatusCode":200,"destWidth":480,"destHeight":270,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn56@2020_3/2020/08/25/10-38-10-230_77849562adec2344.webp","sourceBytes":3778707,"destBytes":1204432,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":9288,"convertSpendMs":2609,"createdTime":"2020-08-25 18:38:03","host":"europe-22*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.6 MB","destSize":"1.1 MB","compressRate":"31.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTU1OA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":786,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn78@2020_3/2020/08/25/10-38-11-773_55594d41db1a1d40.webp","sourceBytes":122042,"destBytes":29548,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1005,"convertSpendMs":43,"createdTime":"2020-08-25 18:38:11","host":"us-029*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"119.2 KB","destSize":"28.9 KB","compressRate":"24.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=D6FhFJID_ho:_h5f38TA_es:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn97@2020_3/2020/08/25/10-38-11-877_b0a49d267730b00e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":861,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:11","host":"us-038*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/darpa-subterranean-challenge-tunnel-circuit-preview","linkMd5ListStr":"745523971937e3d2bbf0ca9e0d077655","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/tD3Hqeyz2og","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn10@2020_3/2020/08/25/10-38-11-891_7e9ca800712766d2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":778,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:11","host":"us-004*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-economic-impact-of-artificial-intelligence","linkMd5ListStr":"5cb62d3fcb55be58473c9653072f3b07","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/R0msYVZ-Qm4","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_5/2020/08/25/10-38-11-812_a0b90310900b276a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":867,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:11","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail","linkMd5ListStr":"2b5ea3f72dcd3a0879d0e273a51ddf91","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Za6B4U3ZdIk:TjeXp1c4B1Q:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn46@2020_5/2020/08/25/10-38-11-516_d70ea0567d08111e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1298,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:11","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-software/video-friday-misty-robotics-shipping-programmable-personal-robot","linkMd5ListStr":"fb732aaba2fa77f47db578dd8a2e2c4e","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=cfiHRefDfBg:4Zg3ikJLBoQ:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn6@2020_5/2020/08/25/10-38-11-958_f6e108d778d975de.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":908,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:11","host":"us-015*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-the-17th-century-leibniz-dreamed-of-a-machine-that-could-calculate-ideas","linkMd5ListStr":"7c7f2266a8977ac619b5378792fcd8f7","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjU1OQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":746,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn86@2020_6/2020/08/25/10-38-11-816_e76f28574abc8862.webp","sourceBytes":174047,"destBytes":88952,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1143,"convertSpendMs":42,"createdTime":"2020-08-25 18:38:11","host":"us-007*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever","linkMd5ListStr":"9811c27a8bfb20ca93a0926d39ae7e1e","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"170 KB","destSize":"86.9 KB","compressRate":"51.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MzA2Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":917,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn65@2020_1/2020/08/25/10-38-11-583_bf12bfb64f637cbd.webp","sourceBytes":149617,"destBytes":43050,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1335,"convertSpendMs":67,"createdTime":"2020-08-25 18:38:11","host":"europe-23*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","linkMd5ListStr":"bac274849de61b732281df992d98ec37","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"146.1 KB","destSize":"42 KB","compressRate":"28.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Bp2w1CN_Clw:Fcj0Rb0C_qQ:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn26@2020_2/2020/08/25/10-38-12-142_865ba440a76e7d51.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":845,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"us-023*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/andrey-markov-and-claude-shannon-built-the-first-language-generation-models","linkMd5ListStr":"9ddfdd2b95e95bd2322570f7f5c5dbdb","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ypq5vl5-FKM:qbOc_szKWNw:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn93@2020_1/2020/08/25/10-38-11-997_6372b0c7abe23ab9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1095,"convertSpendMs":1,"createdTime":"2020-08-25 18:38:11","host":"europe-59*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain","linkMd5ListStr":"4bdb7374dde6559b940d866d23415332","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=b6oUZIfmp5k:VMbz5gRzLpY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn21@2020_3/2020/08/25/10-38-12-181_ee81055b7f6fb6cb.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":895,"convertSpendMs":13,"createdTime":"2020-08-25 18:38:12","host":"us-035*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","linkMd5ListStr":"bac274849de61b732281df992d98ec37","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/sZJo6qaI9cs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn50@2020_1/2020/08/25/10-38-12-209_bdbae0ce53514ede.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":748,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"us-020*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space","linkMd5ListStr":"98d9acef762105ed3deea1273ccf06c8","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=5_MvFCYKYnM:EayMvXTOEek:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn90@2020_4/2020/08/25/10-38-12-021_aa03ec4bdb47442d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1167,"convertSpendMs":10,"createdTime":"2020-08-25 18:38:11","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever","linkMd5ListStr":"9811c27a8bfb20ca93a0926d39ae7e1e","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDM2Mw.jpeg","sourceStatusCode":200,"destWidth":600,"destHeight":338,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn30@2020_4/2020/08/25/10-38-12-169_d51209399321cc51.webp","sourceBytes":207210,"destBytes":32798,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":945,"convertSpendMs":19,"createdTime":"2020-08-25 18:38:12","host":"us-012*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"202.4 KB","destSize":"32 KB","compressRate":"15.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ItofgVBt2wM:kSA5pRDC0WQ:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn41@2020_6/2020/08/25/10-38-12-230_50e1a9c616360ecb.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":834,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot","linkMd5ListStr":"0c575b7593d53cada9ce6cb4a5dbad5f","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4OA.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn44@2020_5/2020/08/25/10-38-10-758_2f206ae410a8096a.webp","sourceBytes":2235935,"destBytes":2095362,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":4418,"convertSpendMs":2077,"createdTime":"2020-08-25 18:38:08","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.1 MB","destSize":"2 MB","compressRate":"93.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ho-KAV7HE7I","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn17@2020_1/2020/08/25/10-38-12-142_56c298d9a64c854c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":916,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:12","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/2xbdHXuXRMk","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn69@2020_2/2020/08/25/10-38-12-406_e299dd90b9cfda73.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":769,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:12","host":"us-028*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/trump-cto-addresses-ai-facial-recognition-immigration-tech-infrastructure-and-more","linkMd5ListStr":"0e1d29b76415f6a75c1bacb441cc9c0d","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/5_MvFCYKYnM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn37@2020_5/2020/08/25/10-38-12-287_8b2a5106a9747295.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":899,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/new-double-3-robot-makes-telepresence-easier-than-ever","linkMd5ListStr":"9811c27a8bfb20ca93a0926d39ae7e1e","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=mMsm0tCvg3c:6NEoQDAwefA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn66@2020_2/2020/08/25/10-38-12-504_24716e5eb026b3f7.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":886,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:12","host":"us-040*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/zipline-successfully-deploys-medical-delivery-drones-with-us-military","linkMd5ListStr":"50bd446780e290d101b8680d1908ab28","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_Hi6JNHldFo","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn77@2020_2/2020/08/25/10-38-12-506_069b429e7fe5ab06.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":804,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:12","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","linkMd5ListStr":"184896aa81aa8ae49425e7c03d12d965","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=uCBEFfSmP2w:ztejN2Y3WNM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn33@2020_1/2020/08/25/10-38-12-372_d02f2b0c0ea2c768.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1128,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe-25*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=2xbdHXuXRMk:1qI5zxyxn7c:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn74@2020_4/2020/08/25/10-38-12-498_eaea85be62a1617f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":933,"convertSpendMs":11,"createdTime":"2020-08-25 18:38:12","host":"us-017*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/trump-cto-addresses-ai-facial-recognition-immigration-tech-infrastructure-and-more","linkMd5ListStr":"0e1d29b76415f6a75c1bacb441cc9c0d","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzM4Nw.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn70@2020_1/2020/08/25/10-38-11-788_ee701dd838a1d4f5.webp","sourceBytes":1203861,"destBytes":87736,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1793,"convertSpendMs":80,"createdTime":"2020-08-25 18:38:11","host":"europe62*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.1 MB","destSize":"85.7 KB","compressRate":"7.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/UuITyw6-YW0","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn98@2020_1/2020/08/25/10-38-12-683_e4fa77a16472962d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":783,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:12","host":"us-014*","referer":"https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface","linkMd5ListStr":"c521e6873fe26deaf6eefa6c6967cc9c","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/06nw6wEy23U","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn1@2020_4/2020/08/25/10-38-12-702_4c6b61e4f398b5ce.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":767,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"us-003*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone","linkMd5ListStr":"a2d95510787b8bacb7f69ee68b845ae0","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ho-KAV7HE7I:mSz7Bg_m1CE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn90@2020_2/2020/08/25/10-38-12-683_3f89acfc310cdf07.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":854,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:12","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/A0x1pCFuojI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn58@2020_3/2020/08/25/10-38-12-463_1f3051f83edc70e4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1005,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe-22*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","linkMd5ListStr":"f4602756dbd851432f68681e9d7b807e","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=KiyPhDu-vcA:sD8uwKJroUg:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn9@2020_4/2020/08/25/10-38-12-793_d89b99629fcf0399.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":881,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"us-55*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=bL0ZQdpPbV8:3_409Sqs-i8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn18@2020_5/2020/08/25/10-38-12-873_ddb0860d32112960.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":927,"convertSpendMs":48,"createdTime":"2020-08-25 18:38:12","host":"us-022*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816","linkMd5ListStr":"a31b47238c2872d0b2da6a891bae5495","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NTc1NA.gif","sourceStatusCode":200,"destWidth":310,"destHeight":187,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn3@2020_5/2020/08/25/10-38-12-367_57f4b01dea1d32b0.webp","sourceBytes":2367378,"destBytes":1180596,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2820,"convertSpendMs":1359,"createdTime":"2020-08-25 18:38:10","host":"us-038*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.3 MB","destSize":"1.1 MB","compressRate":"49.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rV5PxZEF_hs:AbMFTJ0QdHo:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn86@2020_4/2020/08/25/10-38-12-777_f2424c39eec89855.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1044,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe-58*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","linkMd5ListStr":"4116b47e7bcae1aed0f1e1c85e917d83","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ItofgVBt2wM:kSA5pRDC0WQ:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn33@2020_4/2020/08/25/10-38-13-047_3435be41f8c55f3d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":875,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:12","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/harvard-amphibious-urchinbot","linkMd5ListStr":"0c575b7593d53cada9ce6cb4a5dbad5f","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5OTY1MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn13@2020_1/2020/08/25/10-38-12-976_541edd09b114255b.webp","sourceBytes":405792,"destBytes":47582,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1124,"convertSpendMs":45,"createdTime":"2020-08-25 18:38:12","host":"us-033*","referer":"https://spectrum.ieee.org/robotics/artificial-intelligence/lets-build-robots-that-are-as-smart-as-babies","linkMd5ListStr":"f06a1c762bc020e3eb9c056139e79fb8","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"396.3 KB","destSize":"46.5 KB","compressRate":"11.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3OTA2MQ.jpeg","sourceStatusCode":200,"destWidth":745,"destHeight":745,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn94@2020_6/2020/08/25/10-38-12-930_3a92b8244b68a6ec.webp","sourceBytes":140736,"destBytes":86060,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1249,"convertSpendMs":30,"createdTime":"2020-08-25 18:38:12","host":"us-025*","referer":"https://spectrum.ieee.org/tech-talk/energy/environment/the-ultimate-optimization-problem-how-to-best-use-every-square-meter-of-the-earths-surface","linkMd5ListStr":"c521e6873fe26deaf6eefa6c6967cc9c","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"137.4 KB","destSize":"84 KB","compressRate":"61.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_SP9ysuBirA:u8fjM8CPmQg:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn5@2020_5/2020/08/25/10-38-12-968_6faee1bfcce30e00.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1128,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe66*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/8qXUW4b1PHc","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn49@2020_2/2020/08/25/10-38-13-130_60ad82f1e3fcea40.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":870,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:13","host":"europe21*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/from-mainframes-to-pcs-what-robot-startups-can-learn-from-the-computer-revolution","linkMd5ListStr":"9778ce9d1c486affccf103eca762f153","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=BZE40vAKpP8:C4a4qIgo22s:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn66@2020_3/2020/08/25/10-38-13-296_53d4e317b00d782b.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":820,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:13","host":"us-016*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyNDI4MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":401,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn22@2020_1/2020/08/25/10-38-12-912_09bd06bad6d0d8d3.webp","sourceBytes":222228,"destBytes":65440,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1177,"convertSpendMs":29,"createdTime":"2020-08-25 18:38:12","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/japanese-researchers-teaching-robots-to-repair-themselves","linkMd5ListStr":"51766f8db6dedb5c20550ff3c3224e16","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"217 KB","destSize":"63.9 KB","compressRate":"29.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/ZSXSV_e5cKI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn53@2020_3/2020/08/25/10-38-13-158_25722e3a33193ce4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":918,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:13","host":"europe-60*","referer":"https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever","linkMd5ListStr":"7798fbe18b39cdcbed2b1084de9fdb63","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=tbvl8qcvgVM:bcb_4nyhACk:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn29@2020_1/2020/08/25/10-38-13-168_8f47435f0f325d73.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1048,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:13","host":"europe63*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/water-jet-powered-drone-takes-off-with-explosions","linkMd5ListStr":"d38b2f1b0a483a0b8124b15d386f7c04","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=osC6qndPrDk:KfkAlR-pyo0:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn70@2020_6/2020/08/25/10-38-13-297_8f87e48e85665a43.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":874,"convertSpendMs":9,"createdTime":"2020-08-25 18:38:13","host":"us-005*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills","linkMd5ListStr":"b050441ca360dcb3e9e825f306284a0b","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4MDgxMQ.jpeg","sourceStatusCode":200,"destWidth":2226,"destHeight":1104,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn38@2020_5/2020/08/25/10-38-13-153_0002fa76d54781bc.webp","sourceBytes":420530,"destBytes":134040,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1117,"convertSpendMs":112,"createdTime":"2020-08-25 18:38:13","host":"us-030*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"410.7 KB","destSize":"130.9 KB","compressRate":"31.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDU4Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":615,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn45@2020_6/2020/08/25/10-38-13-143_98cc14093d5fea7b.webp","sourceBytes":93154,"destBytes":55360,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1098,"convertSpendMs":35,"createdTime":"2020-08-25 18:38:13","host":"us-008*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"91 KB","destSize":"54.1 KB","compressRate":"59.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDYzMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn62@2020_3/2020/08/25/10-38-13-161_ee22df69a34cc73f.webp","sourceBytes":131459,"destBytes":69016,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1037,"convertSpendMs":46,"createdTime":"2020-08-25 18:38:13","host":"us-027*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"128.4 KB","destSize":"67.4 KB","compressRate":"52.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/hoozvOaCyGQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn82@2020_3/2020/08/25/10-38-12-646_5e1f1d7f6b71149d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1545,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe69*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/blogger-behind-ai-weirdness-thinks-todays-ai-is-dumb-and-dangerous","linkMd5ListStr":"6ab80d13744ce4a42ea20fe59304598b","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=06nw6wEy23U:6leYwiyLyss:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn25@2020_2/2020/08/25/10-38-13-198_59ef8a619db59085.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1254,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:12","host":"europe-24*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/video-friday-dji-mavic-mini-palm-sized-foldable-drone","linkMd5ListStr":"a2d95510787b8bacb7f69ee68b845ae0","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rgt5dqPWMbA:TjT0erajUXU:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn68@2020_2/2020/08/25/10-38-13-393_a544e65fc7a4275f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":933,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:13","host":"us-005*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/labrador-systems-developing-affordable-assistive-robots-for-the-home","linkMd5ListStr":"5f68fac164425c5c46c2a07001ed834c","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDU0Nw.gif","sourceStatusCode":200,"destWidth":620,"destHeight":517,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn16@2020_6/2020/08/25/10-38-12-611_9075caed7e2a750d.webp","sourceBytes":1338456,"destBytes":894654,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3331,"convertSpendMs":1391,"createdTime":"2020-08-25 18:38:10","host":"us-004*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"873.7 KB","compressRate":"66.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNDQ5NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":762,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn86@2020_2/2020/08/25/10-38-13-483_13ca3e3e4055e567.webp","sourceBytes":97264,"destBytes":37356,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":899,"convertSpendMs":36,"createdTime":"2020-08-25 18:38:13","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"95 KB","destSize":"36.5 KB","compressRate":"38.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4NDc4Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1729,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn57@2020_6/2020/08/25/10-38-13-329_8660c4b5ca9cdd21.webp","sourceBytes":1190909,"destBytes":91990,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1257,"convertSpendMs":124,"createdTime":"2020-08-25 18:38:13","host":"us-039*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/nasa-hiring-engineers-to-develop-next-generation-humanoid-robot","linkMd5ListStr":"3bfec1910aa88a37b6873b90b2190643","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.1 MB","destSize":"89.8 KB","compressRate":"7.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=AFnfw88Nq3o:6KgvyGl2jOE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn2@2020_1/2020/08/25/10-38-13-551_f519a75b29c0b293.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":860,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:13","host":"us-54*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/help-rescuers-find-missing-persons-through-emergency-response-contest","linkMd5ListStr":"c70149e9a4334875ea4bbf98504cc029","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2ODAxNg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":696,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn42@2020_5/2020/08/25/10-38-13-032_93c5ca37062db292.webp","sourceBytes":204660,"destBytes":35266,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":883,"convertSpendMs":41,"createdTime":"2020-08-25 18:38:12","host":"us-019*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"199.9 KB","destSize":"34.4 KB","compressRate":"17.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VuRd4Y5PJ5k:_NkqpHdLR9M:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn6@2020_2/2020/08/25/10-38-13-659_f1ae286b4bcc8229.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":869,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:13","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5MzE0NQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":565,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn94@2020_5/2020/08/25/10-38-13-574_721882c02859602f.webp","sourceBytes":222525,"destBytes":71216,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":946,"convertSpendMs":47,"createdTime":"2020-08-25 18:38:13","host":"us-001*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","linkMd5ListStr":"bac274849de61b732281df992d98ec37","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"217.3 KB","destSize":"69.5 KB","compressRate":"32%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=A0x1pCFuojI:zBCg41xGM-k:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn78@2020_4/2020/08/25/10-38-13-525_e315c8fb22b2fc83.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1010,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:13","host":"europe-57*","referer":"https://spectrum.ieee.org/tech-talk/semiconductors/processors/4-things-to-know-about-the-biggest-chip-ever-built","linkMd5ListStr":"f4602756dbd851432f68681e9d7b807e","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxMjEyOQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":1196,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn10@2020_2/2020/08/25/10-38-13-749_9ae56a3f256c383e.webp","sourceBytes":809575,"destBytes":95660,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1054,"convertSpendMs":68,"createdTime":"2020-08-25 18:38:13","host":"us-021*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","linkMd5ListStr":"8be8e5cf657dd68e13509f254295323b","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"790.6 KB","destSize":"93.4 KB","compressRate":"11.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU0Nw.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":800,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn89@2020_5/2020/08/25/10-38-13-678_4fbb5ad65f01f922.webp","sourceBytes":659348,"destBytes":82282,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1224,"convertSpendMs":143,"createdTime":"2020-08-25 18:38:13","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"643.9 KB","destSize":"80.4 KB","compressRate":"12.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjc0MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":809,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn13@2020_1/2020/08/25/10-38-13-832_f3560897fb077d5c.webp","sourceBytes":636097,"destBytes":63838,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":970,"convertSpendMs":40,"createdTime":"2020-08-25 18:38:13","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"621.2 KB","destSize":"62.3 KB","compressRate":"10%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA4Nw.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn99@2020_1/2020/08/25/10-38-13-069_6fc7af4aa7b5da0a.webp","sourceBytes":1457172,"destBytes":187734,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3595,"convertSpendMs":99,"createdTime":"2020-08-25 18:38:11","host":"europe-58*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.4 MB","destSize":"183.3 KB","compressRate":"12.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/r2-HEkba0No","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn37@2020_1/2020/08/25/10-38-14-012_ed2a0f2c65c0e9f6.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":780,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:13","host":"us-007*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/why-people-demanded-privacy-to-confide-in-the-worlds-first-chatbot","linkMd5ListStr":"ddd96fe231dfc465a52535e475a98b15","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=jrUpLKvji58:Mv9REQr1UnA:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn30@2020_3/2020/08/25/10-38-14-016_4eccb746e8510091.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":850,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:13","host":"us-029*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation","linkMd5ListStr":"fdeba8f58f9ca993017a817b623b747a","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzQyNA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":559,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn34@2020_3/2020/08/25/10-38-14-028_1e90193745150884.webp","sourceBytes":256131,"destBytes":32984,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":914,"convertSpendMs":32,"createdTime":"2020-08-25 18:38:13","host":"us-018*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"250.1 KB","destSize":"32.2 KB","compressRate":"12.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjA0Mg.png","sourceStatusCode":200,"destWidth":1912,"destHeight":1061,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn81@2020_1/2020/08/25/10-38-13-762_eede1a730a144cb1.webp","sourceBytes":1319283,"destBytes":101478,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1532,"convertSpendMs":233,"createdTime":"2020-08-25 18:38:13","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"99.1 KB","compressRate":"7.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=sZJo6qaI9cs:NrxaeWv9MOM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn57@2020_5/2020/08/25/10-38-14-185_822d1be621502f77.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":820,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:14","host":"us-015*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space","linkMd5ListStr":"98d9acef762105ed3deea1273ccf06c8","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=VuRd4Y5PJ5k:_NkqpHdLR9M:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn54@2020_6/2020/08/25/10-38-14-190_d26749f9db18482e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":826,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:14","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTQ4NA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":914,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn98@2020_4/2020/08/25/10-38-13-878_f7c30171498f69fd.webp","sourceBytes":123650,"destBytes":60380,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1441,"convertSpendMs":59,"createdTime":"2020-08-25 18:38:13","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"120.8 KB","destSize":"59 KB","compressRate":"48.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTY4Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":800,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn25@2020_3/2020/08/25/10-38-14-142_4417feaf310a7edf.webp","sourceBytes":447207,"destBytes":68812,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1164,"convertSpendMs":142,"createdTime":"2020-08-25 18:38:13","host":"us-51*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"436.7 KB","destSize":"67.2 KB","compressRate":"15.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjczMw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":775,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn74@2020_4/2020/08/25/10-38-13-811_f9181d23f8005c98.webp","sourceBytes":168518,"destBytes":52850,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1655,"convertSpendMs":43,"createdTime":"2020-08-25 18:38:13","host":"europe68*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"164.6 KB","destSize":"51.6 KB","compressRate":"31.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=osC6qndPrDk:KfkAlR-pyo0:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn17@2020_4/2020/08/25/10-38-14-169_0f610a8a27030f59.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1232,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:13","host":"europe-23*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-boston-dynamics-atlas-humanoid-robot-new-gymnastics-skills","linkMd5ListStr":"b050441ca360dcb3e9e825f306284a0b","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=v9Z7p-VLUvo:1XNgDJQjgWo:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn21@2020_5/2020/08/25/10-38-14-197_c1be9a330ef6bc57.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1251,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:13","host":"europe62*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-prospects-for-tomorrows-jobs","linkMd5ListStr":"8e5ebdad5680d32249cb5e9715f5ba1f","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/605d7BLBLdY","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn98@2020_1/2020/08/25/10-38-14-399_1ef54d2d1d2d720d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":771,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:14","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/blue-ocean-robotics-acquires-suitable-technologies-beam-telepresence-robot","linkMd5ListStr":"3a355f69e17aa2bb77552519184504ec","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ukg9RZJ3fig:ey0KAE2wY48:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn61@2020_2/2020/08/25/10-38-14-245_7da5ab59dc6e94ac.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":989,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:14","host":"us-004*","referer":"https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances","linkMd5ListStr":"ecb83ca1fe85f5c86b44772662510c06","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Kc0qsE87fHQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn69@2020_3/2020/08/25/10-38-14-264_f47582ddc4a7f042.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":889,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:14","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-robotic-magnetic-endoscope","linkMd5ListStr":"bacf594041d2950428c4810385b725b0","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=6emqUxmAG9w:BT7v-3tNUf0:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn2@2020_2/2020/08/25/10-38-14-453_3f1bfb389f3c3a25.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":773,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:14","host":"us-020*","referer":"https://spectrum.ieee.org/tech-talk/robotics/robotics-software/construction-robots-learn-to-excavate-by-mimicking-humans","linkMd5ListStr":"05ca9649ab0849af31ee29f9a1f2318f","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_fF0aLR9nWY","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn93@2020_4/2020/08/25/10-38-14-399_931e22780bd2f18c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":796,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:14","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ypq5vl5-FKM:qbOc_szKWNw:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn45@2020_6/2020/08/25/10-38-14-283_451c5eb60f5108a1.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1001,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:14","host":"europe-59*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-robotic-thread-brain","linkMd5ListStr":"4bdb7374dde6559b940d866d23415332","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjUyMA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":834,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn50@2020_6/2020/08/25/10-38-14-232_7103681b1f79fcfa.webp","sourceBytes":385828,"destBytes":135688,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1186,"convertSpendMs":91,"createdTime":"2020-08-25 18:38:14","host":"us-038*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"376.8 KB","destSize":"132.5 KB","compressRate":"35.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=af0ofRZ9M8M:kUGcuzMWe1A:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn41@2020_1/2020/08/25/10-38-14-297_8a40389a3e6fbbd5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1263,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:14","host":"europe70*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-nimbro-robust-walking-robots","linkMd5ListStr":"67ca3612e21a3b8090962da8d8d72cea","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rV5PxZEF_hs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn10@2020_2/2020/08/25/10-38-14-537_69e9b213f0cc6a32.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":889,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:14","host":"europe-22*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/natural-language-processing-dates-back-to-kabbalist-mystics","linkMd5ListStr":"4116b47e7bcae1aed0f1e1c85e917d83","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMTU2OA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":698,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn82@2020_5/2020/08/25/10-38-14-462_bccbf54455ed4124.webp","sourceBytes":135587,"destBytes":126738,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1129,"convertSpendMs":52,"createdTime":"2020-08-25 18:38:14","host":"us-012*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"132.4 KB","destSize":"123.8 KB","compressRate":"93.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTY1MA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn18@2020_1/2020/08/25/10-38-14-577_a8781bce75271f55.webp","sourceBytes":125085,"destBytes":50008,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":981,"convertSpendMs":52,"createdTime":"2020-08-25 18:38:14","host":"us-040*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"122.2 KB","destSize":"48.8 KB","compressRate":"40%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?d=6W8y8wAjSf4","sourceStatusCode":200,"destWidth":48,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn14@2020_6/2020/08/25/10-38-14-611_41fabe3ba2bd8e9a.webp","sourceBytes":962,"destBytes":252,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":885,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:14","host":"europe61*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","linkMd5ListStr":"eaa12b83fdd04d32713423e0cead6909,110e6ac838f5a686ca9016e11c381c4e,c70149e9a4334875ea4bbf98504cc029,ac1ed2d177f3d00db61213d0a8b02970,4afdb44919c1840f63dbb5b7c83f76a7,3bfec1910aa88a37b6873b90b2190643,f4602756dbd851432f68681e9d7b807e,ed36754ceb3d54691c9985ca6f20cf97,daf121b367dccdb1c38a3b54e0c17b83,40500605e90a1fb25e13a0c4109cdc90,51766f8db6dedb5c20550ff3c3224e16,9811c27a8bfb20ca93a0926d39ae7e1e,5f2d11cc893fe393f2105cffb5ccf143,50bd446780e290d101b8680d1908ab28,3579544e8e9768028a4476e076f6e099,83dc60de229d0234a169f3b14521767c,5e83ca8ac583d87a19ee7990f07737a8,8be8e5cf657dd68e13509f254295323b,4bdb7374dde6559b940d866d23415332,645d9089aa580f603b8e05528f7302e7,ddd96fe231dfc465a52535e475a98b15,9778ce9d1c486affccf103eca762f153,f89d2d766c8d79193395fae8610e0f13,0e1d29b76415f6a75c1bacb441cc9c0d,c521e6873fe26deaf6eefa6c6967cc9c,c37b6ff6169ec7e6a055a9904de421c4,f1fb990643b446a2fb09abe6cb8c2668,522bcd2fb6d5cf0d6950cfc6b4675a85,b050441ca360dcb3e9e825f306284a0b,783ad2e174faa245c24d60ad66d167ad,da5f551943891d387e1f1c5f2532d096,b19d13bf54f71c1c40ce0f7d38f1a58b,d38b2f1b0a483a0b8124b15d386f7c04,d8a32c6887301ef3975606092a06d695,5f68fac164425c5c46c2a07001ed834c,f98c1554d57da1ae3c3120fbc73c502e,8651f5e29ba3de30c127e3e854f0dee5,975c3b1b81e23539106f318d7f3e28f7,4116b47e7bcae1aed0f1e1c85e917d83,affef744352a8c08e563bb642c76bc9c,6d47e21e002c0841e6518b91f4e6bdab,019269ec580d7ee2f12ac1c56db02f28,f42d642fe58f167022efac8e946848ef,7798fbe18b39cdcbed2b1084de9fdb63,026bc4b5af35d78854aaaae5d60ee79a,a2d95510787b8bacb7f69ee68b845ae0,2c1d20e7c96f50d4e36fa6c0bb86c46f,f06a1c762bc020e3eb9c056139e79fb8,fb732aaba2fa77f47db578dd8a2e2c4e,de6c237869321c3674d763f513eace56,5cb62d3fcb55be58473c9653072f3b07,1fbe4fd76aaa4013dc648f063ffe877a,507ca1aa50869b0a0bb43058028bc0d9,8e5ebdad5680d32249cb5e9715f5ba1f,cf05796d1ea712358de9c6a3708aa80c,9959391ccbf704954bde8fcaaf54f5ba,3a355f69e17aa2bb77552519184504ec,2b5ea3f72dcd3a0879d0e273a51ddf91,8ba894e1b44b55f874fed65b911e3953,ade9b8fe86da931a7835aee397c88af5,dd075dc0d056fad8777afa408a86be08,bc2b9a2906c1c252ead36f33d8597cd7,7c210ebadb5e65b08c2708471190bfd4,ecb83ca1fe85f5c86b44772662510c06,7c80d1a412783e6b99d1b3dcd2b92262,4542d0e2bf7583a7652c5511e14ef53d,6ab80d13744ce4a42ea20fe59304598b,bf340835c615e553278f7841ec5e7cd7,b4f4c90f36287bd417b7b4e5a43aa139,dac7321f3815f792354a3da3730527b3,98d9acef762105ed3deea1273ccf06c8,b92198dab0130e2ebf3c419ddb0d4903,70815399970675009fdaa91f1ed9af88,bacf594041d2950428c4810385b725b0,d6ffcad3ca933b1d80d7759de6a31b95,fddb2a608a827cba7cd0974ca59eeee3,fdeba8f58f9ca993017a817b623b747a,6d46fdc9cd73aab1f732a99137523da6,745523971937e3d2bbf0ca9e0d077655,a31b47238c2872d0b2da6a891bae5495,9ddfdd2b95e95bd2322570f7f5c5dbdb,04bd6282cc987a8acf3bef5de891d450,bac274849de61b732281df992d98ec37,a82857d2f8b930185848e80cae923b81,ba866f9c04fa639f4052fdd904ce7f83,7c7f2266a8977ac619b5378792fcd8f7,7cf67c6a00bfe23b83ee4f8fa82ccb7a,727addd4da03ea6fd9f64c61ab6be805,de8494e008723f24ca95f4421202eaa2,184896aa81aa8ae49425e7c03d12d965,67ca3612e21a3b8090962da8d8d72cea,5c8fee7c8bef1dc43b41be2a29c47ddb,538b0c13aaa64a59898970581b42f5fa,19aafea32608c470cfdef56d8c5fc346,942b7fea25553a3c98fd6f49daf1b0f2,05ca9649ab0849af31ee29f9a1f2318f,ec21b6293bbcb368dd9079b4df431fd8,0c575b7593d53cada9ce6cb4a5dbad5f,c7f356a3967b14c7066d4a7ae2801b32,953ccece03a4def30b62f328e3f48f7e","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"962 B","destSize":"252 B","compressRate":"26.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/f4vRmWO59FQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn23@2020_5/2020/08/25/10-38-14-783_7b254f75daebaec4.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":757,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:14","host":"us-016*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/watch-astrobees-first-autonomous-flight-on-the-international-space-station","linkMd5ListStr":"de8494e008723f24ca95f4421202eaa2","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4NQ.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn35@2020_5/2020/08/25/10-38-12-659_01ef4d6d0e69c2ad.webp","sourceBytes":4642987,"destBytes":3748948,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":10086,"convertSpendMs":1664,"createdTime":"2020-08-25 18:38:05","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.4 MB","destSize":"3.6 MB","compressRate":"80.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=MWyBQ98AJqA:ZR_Y6xjpoC8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn90@2020_2/2020/08/25/10-38-14-635_82e446c1ef17bf96.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1071,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:14","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","linkMd5ListStr":"c7f356a3967b14c7066d4a7ae2801b32","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/4lbED3n0y6U","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn22@2020_3/2020/08/25/10-38-14-785_4bcdd8f623c61cac.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":799,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:14","host":"us-017*","referer":"https://spectrum.ieee.org/tech-talk/aerospace/robotic-exploration/china-says-its-mars-landing-technology-is-ready-for-2020","linkMd5ListStr":"538b0c13aaa64a59898970581b42f5fa","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/1bD2VSOAgqs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn53@2020_4/2020/08/25/10-38-15-042_3146c91eae6a0cc9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":758,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"us-003*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food","linkMd5ListStr":"d8a32c6887301ef3975606092a06d695","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=EtQ_JVz0FMI:8EAx1saPp9A:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn49@2020_4/2020/08/25/10-38-15-043_6f8775a7b1300911.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":794,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:14","host":"us-014*","referer":"https://spectrum.ieee.org/view-from-the-valley/semiconductors/design/dramatic-changes-in-chip-design-will-be-necessary-to-make-ar-glasses-a-reality","linkMd5ListStr":"522bcd2fb6d5cf0d6950cfc6b4675a85","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4NA.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn54@2020_4/2020/08/25/10-38-14-383_0a001ac576480b59.webp","sourceBytes":3050687,"destBytes":2656120,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3662,"convertSpendMs":2082,"createdTime":"2020-08-25 18:38:12","host":"us-009*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.9 MB","destSize":"2.5 MB","compressRate":"87.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=qqJG5_zGl_w:vBCzePmXCC8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn37@2020_5/2020/08/25/10-38-14-972_bdaa53b18355098f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1047,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:14","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/Tm3JI8_WGfM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn69@2020_4/2020/08/25/10-38-15-171_740245bb8bb42ead.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":752,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:15","host":"us-022*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU0Ng.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":864,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn65@2020_2/2020/08/25/10-38-14-341_ff072f801b90edc5.webp","sourceBytes":747901,"destBytes":100582,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1726,"convertSpendMs":46,"createdTime":"2020-08-25 18:38:14","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"730.4 KB","destSize":"98.2 KB","compressRate":"13.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/rH4V7CL1C5g","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn96@2020_3/2020/08/25/10-38-15-273_a5989649c809c708.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":767,"convertSpendMs":7,"createdTime":"2020-08-25 18:38:15","host":"us-007*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters","linkMd5ListStr":"dd075dc0d056fad8777afa408a86be08","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=yy1rq30DNiI:ZPKbepRAZAE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn66@2020_6/2020/08/25/10-38-15-218_3a336f2190e535ba.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":898,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"us-033*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-agents-startle-researchers-with-unexpected-strategies-in-hideandseek","linkMd5ListStr":"ade9b8fe86da931a7835aee397c88af5","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA5MA.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn62@2020_5/2020/08/25/10-38-14-357_0141dc805ce2dac2.webp","sourceBytes":1349064,"destBytes":205644,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3525,"convertSpendMs":124,"createdTime":"2020-08-25 18:38:12","host":"europe61*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"200.8 KB","compressRate":"15.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=_dQJoG0X58s:OA7yLaP9HCA:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn73@2020_4/2020/08/25/10-38-15-245_bbe7db621635ea56.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":868,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:15","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDUwMw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":538,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn46@2020_6/2020/08/25/10-38-15-080_62adf895cfc7f7fc.webp","sourceBytes":114013,"destBytes":72856,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1118,"convertSpendMs":28,"createdTime":"2020-08-25 18:38:14","host":"us-025*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"111.3 KB","destSize":"71.1 KB","compressRate":"63.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=uCBEFfSmP2w:ztejN2Y3WNM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn86@2020_1/2020/08/25/10-38-15-269_6a8ebd491a9198ef.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":855,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:15","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU0OA.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":722,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn29@2020_3/2020/08/25/10-38-14-932_b42d644fa18b5e34.webp","sourceBytes":592895,"destBytes":80462,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1306,"convertSpendMs":43,"createdTime":"2020-08-25 18:38:14","host":"us-006*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"579 KB","destSize":"78.6 KB","compressRate":"13.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ePDcbxuJmvE:HmzLziNy2zI:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn57@2020_3/2020/08/25/10-38-15-257_5e348405e682ed1f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1020,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"europe66*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU5MA.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":1025,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn86@2020_4/2020/08/25/10-38-14-679_5fabe96c544de9ce.webp","sourceBytes":835112,"destBytes":111184,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1729,"convertSpendMs":81,"createdTime":"2020-08-25 18:38:14","host":"europe-25*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"815.5 KB","destSize":"108.6 KB","compressRate":"13.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=-xqOx5HkcOE:UY5txkCnYhQ:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn94@2020_2/2020/08/25/10-38-15-368_2ac08e67610b1823.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":940,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:15","host":"us-008*","referer":"https://spectrum.ieee.org/tech-talk/computing/hardware/cerebras-unveils-ai-supercomputer-argonne-national-lab-first-installation","linkMd5ListStr":"026bc4b5af35d78854aaaae5d60ee79a","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTg5OQ.png","sourceStatusCode":200,"destWidth":808,"destHeight":1404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn90@2020_4/2020/08/25/10-38-15-309_6d6ed66b3098be41.webp","sourceBytes":702181,"destBytes":51264,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1049,"convertSpendMs":61,"createdTime":"2020-08-25 18:38:15","host":"us-030*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"685.7 KB","destSize":"50.1 KB","compressRate":"7.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4OQ.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn59@2020_6/2020/08/25/10-38-13-950_2797176b67eee70b.webp","sourceBytes":3715475,"destBytes":3015402,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":9010,"convertSpendMs":2117,"createdTime":"2020-08-25 18:38:07","host":"europe-22*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.5 MB","destSize":"2.9 MB","compressRate":"81.2%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA5Ng.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn62@2020_3/2020/08/25/10-38-15-283_841e1fd89c9098cc.webp","sourceBytes":1753529,"destBytes":236450,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1207,"convertSpendMs":117,"createdTime":"2020-08-25 18:38:15","host":"us-55*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 MB","destSize":"230.9 KB","compressRate":"13.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/7rGryOunAtI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn22@2020_5/2020/08/25/10-38-15-579_b3c5208b11ea06f2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":835,"convertSpendMs":15,"createdTime":"2020-08-25 18:38:15","host":"us-005*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/certified-artificial-rates-the-ai-expertise-of-thought-leaders-and-companies","linkMd5ListStr":"d6ffcad3ca933b1d80d7759de6a31b95","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/iB-EZS5TQKE","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn9@2020_3/2020/08/25/10-38-15-522_d775bf4d36a5c029.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":902,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"us-039*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/learn-the-red-flags-of-overhyped-ai-claims","linkMd5ListStr":"953ccece03a4def30b62f328e3f48f7e","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/bL0ZQdpPbV8","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn37@2020_5/2020/08/25/10-38-15-645_ba50582444db1398.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":786,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"us-024*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-816","linkMd5ListStr":"a31b47238c2872d0b2da6a891bae5495","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QhC0HMCy6-k:kD0Zu80vLQY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn34@2020_1/2020/08/25/10-38-15-658_59eba00190735dd3.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":814,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:15","host":"us-036*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","linkMd5ListStr":"70815399970675009fdaa91f1ed9af88","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rH4V7CL1C5g:iaetgA_Jprc:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn1@2020_3/2020/08/25/10-38-15-514_098886294508d459.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1083,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"europe21*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters","linkMd5ListStr":"dd075dc0d056fad8777afa408a86be08","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NzM0OA.jpeg","sourceStatusCode":200,"destWidth":725,"destHeight":842,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn77@2020_2/2020/08/25/10-38-15-414_f6d622475181e356.webp","sourceBytes":47885,"destBytes":22616,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1159,"convertSpendMs":18,"createdTime":"2020-08-25 18:38:15","host":"europe-24*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india","linkMd5ListStr":"645d9089aa580f603b8e05528f7302e7","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"46.8 KB","destSize":"22.1 KB","compressRate":"47.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/b6oUZIfmp5k","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn41@2020_3/2020/08/25/10-38-15-651_35b46b8111f568e2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":922,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:15","host":"us-013*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/openai-demonstrates-sim2real-by-with-onehanded-rubiks-cube-solving","linkMd5ListStr":"bac274849de61b732281df992d98ec37","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VKQk2CUaQQk","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn45@2020_1/2020/08/25/10-38-15-910_8db0d8302efd835e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":786,"convertSpendMs":11,"createdTime":"2020-08-25 18:38:15","host":"us-001*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-unitree-robotics-aliengo-quadruped-robot","linkMd5ListStr":"19aafea32608c470cfdef56d8c5fc346","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ePDcbxuJmvE:HmzLziNy2zI:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn30@2020_4/2020/08/25/10-38-15-755_48f393d4612a70e1.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1012,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:15","host":"europe-57*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjA0NA.png","sourceStatusCode":200,"destWidth":1560,"destHeight":1068,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn14@2020_3/2020/08/25/10-38-15-715_68bd74a7874cdff0.webp","sourceBytes":2179886,"destBytes":131524,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1253,"convertSpendMs":126,"createdTime":"2020-08-25 18:38:15","host":"us-027*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.1 MB","destSize":"128.4 KB","compressRate":"6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzA0OA.jpeg","sourceStatusCode":200,"destWidth":1880,"destHeight":1410,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn18@2020_2/2020/08/25/10-38-15-745_3813f43f9c054e34.webp","sourceBytes":1241509,"destBytes":169108,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1251,"convertSpendMs":159,"createdTime":"2020-08-25 18:38:15","host":"us-016*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.2 MB","destSize":"165.1 KB","compressRate":"13.6%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU4NQ.jpeg","sourceStatusCode":200,"destWidth":1200,"destHeight":795,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn33@2020_5/2020/08/25/10-38-15-604_8b74043831186346.webp","sourceBytes":518440,"destBytes":47078,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1904,"convertSpendMs":66,"createdTime":"2020-08-25 18:38:14","host":"europe69*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"506.3 KB","destSize":"46 KB","compressRate":"9.1%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/RNa1KqrBSZw","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn81@2020_1/2020/08/25/10-38-15-286_adc0ba404c07968d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1572,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"europe63*","referer":"https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/a-path-towards-reasonable-autonomous-weapons-regulation","linkMd5ListStr":"8ba894e1b44b55f874fed65b911e3953","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WQZUBYIDfRg:y_chSDmQyhE:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn58@2020_4/2020/08/25/10-38-16-132_1868c01ff72dd64d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":915,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:15","host":"us-032*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats","linkMd5ListStr":"f89d2d766c8d79193395fae8610e0f13","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1MzE3OA.jpeg","sourceStatusCode":200,"destWidth":800,"destHeight":300,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn61@2020_6/2020/08/25/10-38-16-022_1b6a6cab8e40218d.webp","sourceBytes":411447,"destBytes":42936,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":931,"convertSpendMs":20,"createdTime":"2020-08-25 18:38:15","host":"us-021*","referer":"https://spectrum.ieee.org/whitepaper/five-routes-to-competitive-advantage-with-ai","linkMd5ListStr":"975c3b1b81e23539106f318d7f3e28f7","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"401.8 KB","destSize":"41.9 KB","compressRate":"10.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzE4Nzk5MQ.jpeg","sourceStatusCode":200,"destWidth":1024,"destHeight":622,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn65@2020_6/2020/08/25/10-38-16-097_4cb0d53df7f9d541.webp","sourceBytes":64896,"destBytes":28338,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":900,"convertSpendMs":21,"createdTime":"2020-08-25 18:38:16","host":"us-010*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"63.4 KB","destSize":"27.7 KB","compressRate":"43.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODE5Mg.png","sourceStatusCode":200,"destWidth":290,"destHeight":218,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn89@2020_4/2020/08/25/10-38-16-182_d5c2323f3f32169b.webp","sourceBytes":62399,"destBytes":7248,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":810,"convertSpendMs":8,"createdTime":"2020-08-25 18:38:16","host":"us-007*","referer":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","linkMd5ListStr":"c37b6ff6169ec7e6a055a9904de421c4","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"60.9 KB","destSize":"7.1 KB","compressRate":"11.6%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=1bD2VSOAgqs:Tu0j_hk2JoM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn85@2020_6/2020/08/25/10-38-16-208_5b4ba18e4f860c55.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":819,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:16","host":"us-018*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/farmwise-raises-145-million-to-teach-giant-robots-to-grow-our-food","linkMd5ListStr":"d8a32c6887301ef3975606092a06d695","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU1MQ.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":665,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn6@2020_5/2020/08/25/10-38-15-921_f8c86f9d3d76337e.webp","sourceBytes":344643,"destBytes":20174,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1565,"convertSpendMs":40,"createdTime":"2020-08-25 18:38:15","host":"europe-60*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"336.6 KB","destSize":"19.7 KB","compressRate":"5.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/_dQJoG0X58s","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn69@2020_2/2020/08/25/10-38-16-155_aa138e372d556f7e.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":908,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe-23*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3Nzk4Mw.jpeg","sourceStatusCode":200,"destWidth":680,"destHeight":436,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn6@2020_5/2020/08/25/10-38-16-285_0a5b3e836c487bb4.webp","sourceBytes":55776,"destBytes":19434,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":858,"convertSpendMs":11,"createdTime":"2020-08-25 18:38:16","host":"us-026*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"54.5 KB","destSize":"19 KB","compressRate":"34.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=F9OLArjw1as:mj94lXbFh2A:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn74@2020_4/2020/08/25/10-38-16-346_410891ccb3e6e535.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1060,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe62*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-teaches-kids-hand-washing-skills-in-rural-india","linkMd5ListStr":"645d9089aa580f603b8e05528f7302e7","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ukg9RZJ3fig:ey0KAE2wY48:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn93@2020_1/2020/08/25/10-38-16-338_604f19fb32cfb6cf.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":980,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe70*","referer":"https://spectrum.ieee.org/the-human-os/biomedical/devices/drones-beat-new-york-city-ambulances","linkMd5ListStr":"ecb83ca1fe85f5c86b44772662510c06","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WQZUBYIDfRg:y_chSDmQyhE:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn98@2020_3/2020/08/25/10-38-16-411_dbc87045d62f6c10.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1017,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:16","host":"europe-59*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-kuka-robutt-robot-new-car-seats","linkMd5ListStr":"f89d2d766c8d79193395fae8610e0f13","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WWCYX6DRQI0:9DgkI1lrnnM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn30@2020_5/2020/08/25/10-38-16-574_a4488da2ad10b037.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":878,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"us-023*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/rigid-it-departments-jobhopping-data-scientists-implementing-machine-learning-isnt-easy","linkMd5ListStr":"110e6ac838f5a686ca9016e11c381c4e","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZSXSV_e5cKI:BHPW6pvdnAY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn26@2020_5/2020/08/25/10-38-16-567_b22aeb0f76f151ec.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":931,"convertSpendMs":6,"createdTime":"2020-08-25 18:38:16","host":"us-035*","referer":"https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever","linkMd5ListStr":"7798fbe18b39cdcbed2b1084de9fdb63","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxMjEyOA.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":503,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn13@2020_5/2020/08/25/10-38-16-442_35999cf9baacff97.webp","sourceBytes":785931,"destBytes":98000,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1096,"convertSpendMs":51,"createdTime":"2020-08-25 18:38:16","host":"us-004*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/quadruped-robots-can-climb-ladders-now","linkMd5ListStr":"8be8e5cf657dd68e13509f254295323b","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"767.5 KB","destSize":"95.7 KB","compressRate":"12.5%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQwNTY1Mg.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn46@2020_1/2020/08/25/10-38-16-654_9a7704e40b5b6aca.webp","sourceBytes":47098,"destBytes":15526,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":851,"convertSpendMs":30,"createdTime":"2020-08-25 18:38:16","host":"us-53*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"46 KB","destSize":"15.2 KB","compressRate":"33%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDU3MA.jpeg","sourceStatusCode":200,"destWidth":620,"destHeight":779,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn2@2020_4/2020/08/25/10-38-16-426_19fc3a920e054033.webp","sourceBytes":115707,"destBytes":63414,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1242,"convertSpendMs":30,"createdTime":"2020-08-25 18:38:16","host":"us-038*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"113 KB","destSize":"61.9 KB","compressRate":"54.8%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=QhC0HMCy6-k:kD0Zu80vLQY:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn18@2020_5/2020/08/25/10-38-16-595_aea35596c1065c5f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1040,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/were-at-iros-2019-in-macau","linkMd5ListStr":"70815399970675009fdaa91f1ed9af88","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM4ODc1Mw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn34@2020_5/2020/08/25/10-38-16-602_8b71174fc7786a92.webp","sourceBytes":157676,"destBytes":58640,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1016,"convertSpendMs":76,"createdTime":"2020-08-25 18:38:16","host":"us-012*","referer":"https://spectrum.ieee.org/tech-talk/robotics/drones/drones-as-detectives-surveying-crime-scenes-for-evidence","linkMd5ListStr":"b19d13bf54f71c1c40ce0f7d38f1a58b","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"154 KB","destSize":"57.3 KB","compressRate":"37.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=vedFlE8nubE:lU7iMmkn8mI:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx8/cdn49@2020_4/2020/08/25/10-38-16-779_a839930022e4e739.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":795,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx8","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=MWyBQ98AJqA:ZR_Y6xjpoC8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn78@2020_5/2020/08/25/10-38-16-157_e1aff2c64bfc3588.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1426,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:16","host":"us-51*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-to-score-at-the-darpa-subterranean-challenge","linkMd5ListStr":"c7f356a3967b14c7066d4a7ae2801b32","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=qqJG5_zGl_w:vBCzePmXCC8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn21@2020_5/2020/08/25/10-38-16-617_c88b56f167f37ec3.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1017,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:16","host":"europe-56*","referer":"https://spectrum.ieee.org/automaton/robotics/industrial-robots/boston-dynamics-spot-robot-dog-goes-on-sale","linkMd5ListStr":"8651f5e29ba3de30c127e3e854f0dee5","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM3MDI4Ng.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":974,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn10@2020_2/2020/08/25/10-38-16-446_3e07b6fdaee77003.webp","sourceBytes":650919,"destBytes":223994,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1223,"convertSpendMs":73,"createdTime":"2020-08-25 18:38:16","host":"us-015*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/romela-newest-robot-is-a-curiously-symmetrical-dynamic-quadruped","linkMd5ListStr":"6d47e21e002c0841e6518b91f4e6bdab","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"635.7 KB","destSize":"218.7 KB","compressRate":"34.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=R0msYVZ-Qm4:fR7STip5Pt8:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx13/cdn42@2020_3/2020/08/25/10-38-16-704_765bcf9a5d6b6652.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":996,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe64*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail","linkMd5ListStr":"2b5ea3f72dcd3a0879d0e273a51ddf91","githubUser":"myreaderx13","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=C6zMyuDvd98:JCm27VifXB0:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn53@2020_5/2020/08/25/10-38-16-854_70db5c8e71680bc0.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":855,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"us-020*","referer":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","linkMd5ListStr":"4afdb44919c1840f63dbb5b7c83f76a7","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=rH4V7CL1C5g:iaetgA_Jprc:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn38@2020_1/2020/08/25/10-38-16-786_e30f5dd98d607b59.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1076,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe-25*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-caltech-leonardo-bipedal-robot-with-thrusters","linkMd5ListStr":"dd075dc0d056fad8777afa408a86be08","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/v9Z7p-VLUvo","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn57@2020_6/2020/08/25/10-38-16-854_9deb8ac3ff6a8e3f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":838,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"us-009*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/ai-and-the-future-of-work-the-prospects-for-tomorrows-jobs","linkMd5ListStr":"8e5ebdad5680d32249cb5e9715f5ba1f","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=WWCYX6DRQI0:9DgkI1lrnnM:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn70@2020_1/2020/08/25/10-38-16-974_105415ceaf344f1f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":809,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"us-040*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/rigid-it-departments-jobhopping-data-scientists-implementing-machine-learning-isnt-easy","linkMd5ListStr":"110e6ac838f5a686ca9016e11c381c4e","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/BZE40vAKpP8","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn77@2020_6/2020/08/25/10-38-16-986_a728f8c5d635cd3a.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":865,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:16","host":"us-017*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/soft-selfhealing-materials-for-robots-that-cannot-be-destroyed","linkMd5ListStr":"7cf67c6a00bfe23b83ee4f8fa82ccb7a","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4Ng.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn78@2020_1/2020/08/25/10-38-16-309_cb47e98820a78239.webp","sourceBytes":3202629,"destBytes":2606482,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3563,"convertSpendMs":1961,"createdTime":"2020-08-25 18:38:14","host":"us-023*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.1 MB","destSize":"2.5 MB","compressRate":"81.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/KiyPhDu-vcA","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn93@2020_6/2020/08/25/10-38-17-076_c8633878f9bdac57.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":813,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:17","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/humanoids/what-is-the-uncanny-valley","linkMd5ListStr":"cf05796d1ea712358de9c6a3708aa80c","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/X-v6WYhBbOk","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn63@2020_4/2020/08/25/10-38-17-110_2c0ce5b3e116b1a2.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":804,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:17","host":"us-004*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/kiki-social-home-robot","linkMd5ListStr":"83dc60de229d0234a169f3b14521767c","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NDMyMA.jpeg","sourceStatusCode":200,"destWidth":900,"destHeight":900,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn49@2020_5/2020/08/25/10-38-16-329_0872930ff356618f.webp","sourceBytes":234688,"destBytes":179632,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":2001,"convertSpendMs":52,"createdTime":"2020-08-25 18:38:15","host":"europe65*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-robotics-teams-are-getting-ready-for-darpas-subt-challenge","linkMd5ListStr":"f42d642fe58f167022efac8e946848ef","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"229.2 KB","destSize":"175.4 KB","compressRate":"76.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/VuRd4Y5PJ5k","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn1@2020_2/2020/08/25/10-38-17-195_69b9978aa326ee43.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":797,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:17","host":"us-014*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-new-drone-is-smaller-even-smarter-and-almost-affordable","linkMd5ListStr":"04bd6282cc987a8acf3bef5de891d450","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JL1R_ghrql8:CdEDgfmEzKU:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn97@2020_5/2020/08/25/10-38-17-215_67e557ab6c9afb0d.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":856,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:17","host":"us-025*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/disney-research-dynamic-robotic-characters","linkMd5ListStr":"de6c237869321c3674d763f513eace56","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5NDM2Ng.octet-stream","sourceStatusCode":200,"destWidth":600,"destHeight":450,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx23/cdn81@2020_6/2020/08/25/10-38-17-087_cdac9e00e6d647b6.webp","sourceBytes":157365,"destBytes":30632,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":970,"convertSpendMs":19,"createdTime":"2020-08-25 18:38:17","host":"us-006*","referer":"https://spectrum.ieee.org/consumer-electronics/gaming/quantum-computing-gets-a-boost-from-ai-and-crowdsourcing","linkMd5ListStr":"dac7321f3815f792354a3da3730527b3","githubUser":"myreaderx23","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"153.7 KB","destSize":"29.9 KB","compressRate":"19.5%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=ZSXSV_e5cKI:BHPW6pvdnAY:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx18/cdn90@2020_4/2020/08/25/10-38-17-213_e77cb42fc963bc22.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":958,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:17","host":"europe-58*","referer":"https://spectrum.ieee.org/automaton/robotics/medical-robots/soft-exosuit-makes-walking-and-running-easier-than-ever","linkMd5ListStr":"7798fbe18b39cdcbed2b1084de9fdb63","githubUser":"myreaderx18","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=JL1R_ghrql8:CdEDgfmEzKU:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn66@2020_5/2020/08/25/10-38-17-063_c896ed55cd7bdf58.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1112,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:16","host":"europe61*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/disney-research-dynamic-robotic-characters","linkMd5ListStr":"de6c237869321c3674d763f513eace56","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU5MA.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn6@2020_4/2020/08/25/10-38-16-371_d69b19b3096c2e64.webp","sourceBytes":3214826,"destBytes":2691116,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3700,"convertSpendMs":1842,"createdTime":"2020-08-25 18:38:14","host":"us-009*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.1 MB","destSize":"2.6 MB","compressRate":"83.7%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1MzQ4OQ.jpeg","sourceStatusCode":200,"destWidth":952,"destHeight":953,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn61@2020_4/2020/08/25/10-38-16-956_28ee415d5e45c174.webp","sourceBytes":134711,"destBytes":61556,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1247,"convertSpendMs":31,"createdTime":"2020-08-25 18:38:16","host":"europe-22*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/robot-made-of-clay-can-sculpt-its-own-body","linkMd5ListStr":"affef744352a8c08e563bb642c76bc9c","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"131.6 KB","destSize":"60.1 KB","compressRate":"45.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/dR0py1h-AZM","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn6@2020_1/2020/08/25/10-38-17-526_bdeff458413ec2b9.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":758,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:17","host":"us-003*","referer":"https://spectrum.ieee.org/view-from-the-valley/robotics/artificial-intelligence/want-a-really-hard-machine-learning-problem-try-agriculture-say-john-deere-labs-leaders","linkMd5ListStr":"5f2d11cc893fe393f2105cffb5ccf143","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4Mw.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn42@2020_5/2020/08/25/10-38-16-752_5790bb2d4b3f4f66.webp","sourceBytes":2633138,"destBytes":2423484,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3378,"convertSpendMs":1795,"createdTime":"2020-08-25 18:38:14","host":"us-037*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 MB","destSize":"2.3 MB","compressRate":"92%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/IeeeSpectrumRobotics/~4/uCBEFfSmP2w","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn73@2020_4/2020/08/25/10-38-16-965_98f4be5ba6c42988.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1372,"convertSpendMs":5,"createdTime":"2020-08-25 18:38:16","host":"us-028*","referer":"https://spectrum.ieee.org/automaton/robotics/home-robots/robot-gift-guide-2019","linkMd5ListStr":"b4f4c90f36287bd417b7b4e5a43aa139","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=M6oPcVu-H6M:Ky6uSH84PmI:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn22@2020_6/2020/08/25/10-38-17-590_9c31be7838ba8fe5.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":844,"convertSpendMs":46,"createdTime":"2020-08-25 18:38:17","host":"us-022*","referer":"https://spectrum.ieee.org/robotics/robotics-software/the-3-advantages-of-assembled-cables","linkMd5ListStr":"c37b6ff6169ec7e6a055a9904de421c4","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=LIToXQovbEs:w8wppRpFVAI:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn42@2020_4/2020/08/25/10-38-17-618_b4b657b6c5d3d42c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":784,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:17","host":"us-030*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-mit-mini-cheetah-robots","linkMd5ListStr":"40500605e90a1fb25e13a0c4109cdc90","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1NzM0Mw.jpeg","sourceStatusCode":200,"destWidth":506,"destHeight":675,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn25@2020_3/2020/08/25/10-38-17-595_4293987c1c48f544.webp","sourceBytes":63659,"destBytes":38260,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":979,"convertSpendMs":24,"createdTime":"2020-08-25 18:38:17","host":"us-011*","referer":"https://spectrum.ieee.org/automaton/robotics/space-robots/russian-humanoid-robot-to-pilot-soyuz-capsule-to-iss-this-week","linkMd5ListStr":"ba866f9c04fa639f4052fdd904ce7f83","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"62.2 KB","destSize":"37.4 KB","compressRate":"60.1%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQxNzQzNQ.jpeg","sourceStatusCode":200,"destWidth":600,"destHeight":973,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn46@2020_2/2020/08/25/10-38-17-627_6201d026e0e5c0c8.webp","sourceBytes":181920,"destBytes":27666,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":919,"convertSpendMs":23,"createdTime":"2020-08-25 18:38:17","host":"us-019*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"177.7 KB","destSize":"27 KB","compressRate":"15.2%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Tm3JI8_WGfM:-EJlx19W6kU:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn10@2020_5/2020/08/25/10-38-17-523_cb8a17ba0d7c1117.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1162,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:17","host":"europe66*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=C6zMyuDvd98:JCm27VifXB0:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn30@2020_2/2020/08/25/10-38-17-711_af17ff1c308f2c0c.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":977,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:17","host":"europe-24*","referer":"https://spectrum.ieee.org/robotics/drones/uavbased-lidar-can-measure-shallow-water-depth","linkMd5ListStr":"4afdb44919c1840f63dbb5b7c83f76a7","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2NTY3Mg.jpeg","sourceStatusCode":200,"destWidth":2000,"destHeight":1104,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn86@2020_2/2020/08/25/10-38-17-132_7d054e7be221498a.webp","sourceBytes":223086,"destBytes":247434,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1679,"convertSpendMs":96,"createdTime":"2020-08-25 18:38:17","host":"europe69*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"217.9 KB","destSize":"241.6 KB","compressRate":"110.9%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=sZJo6qaI9cs:NrxaeWv9MOM:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn33@2020_4/2020/08/25/10-38-17-859_ba8c7834a2fb0d50.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":1133,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:17","host":"europe63*","referer":"https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/predicting-the-future-with-ai-and-sound-starting-with-robots-in-space","linkMd5ListStr":"98d9acef762105ed3deea1273ccf06c8","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=Tm3JI8_WGfM:-EJlx19W6kU:-BTjWOF_DHI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn14@2020_5/2020/08/25/10-38-18-212_99d3cefec5ff4b66.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":788,"convertSpendMs":4,"createdTime":"2020-08-25 18:38:18","host":"us-55*","referer":"https://spectrum.ieee.org/robotics/drones/snotbot-drone-swoops-over-blowholes-to-track-whale-health","linkMd5ListStr":"a82857d2f8b930185848e80cae923b81","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM5Mzc2Ng.jpeg","sourceStatusCode":200,"destWidth":2607,"destHeight":1536,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn26@2020_2/2020/08/25/10-38-17-470_68249d2494cad465.webp","sourceBytes":1372632,"destBytes":88196,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":3413,"convertSpendMs":105,"createdTime":"2020-08-25 18:38:15","host":"europe68*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydios-dock-in-a-box-enables-longterm-autonomy-for-drone-applications","linkMd5ListStr":"184896aa81aa8ae49425e7c03d12d965","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.3 MB","destSize":"86.1 KB","compressRate":"6.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU2Nw.jpeg","sourceStatusCode":200,"destWidth":1240,"destHeight":930,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn45@2020_6/2020/08/25/10-38-19-329_c6e258bbfbb67f52.webp","sourceBytes":389806,"destBytes":50146,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":978,"convertSpendMs":40,"createdTime":"2020-08-25 18:38:19","host":"us-031*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"380.7 KB","destSize":"49 KB","compressRate":"12.9%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM1ODU5OA.gif","sourceStatusCode":200,"destWidth":480,"destHeight":270,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx14/cdn81@2020_3/2020/08/25/10-38-18-937_8243f70c6dbdc776.webp","sourceBytes":7982804,"destBytes":3018684,"targetWebpQuality":52,"feedId":8606,"totalSpendMs":4451,"convertSpendMs":2262,"createdTime":"2020-08-25 18:38:16","host":"us-029*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/how-teams-are-solving-the-biggest-challenge-at-darpa-subt","linkMd5ListStr":"6d46fdc9cd73aab1f732a99137523da6","githubUser":"myreaderx14","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"7.6 MB","destSize":"2.9 MB","compressRate":"37.8%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzM2MjA0Nw.gif","sourceStatusCode":200,"destWidth":480,"destHeight":270,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn54@2020_1/2020/08/25/10-38-19-728_1ce04c5326c99030.webp","sourceBytes":4562281,"destBytes":580506,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":5079,"convertSpendMs":3559,"createdTime":"2020-08-25 18:38:15","host":"us-54*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/all-of-the-winners-in-the-darpa-subterranean-challenge-tunnel-circuit","linkMd5ListStr":"bc2b9a2906c1c252ead36f33d8597cd7","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.4 MB","destSize":"566.9 KB","compressRate":"12.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=vedFlE8nubE:lU7iMmkn8mI:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn38@2020_3/2020/08/25/10-38-20-611_c54559fc5b456489.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":831,"convertSpendMs":2,"createdTime":"2020-08-25 18:38:20","host":"us-52*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/it-shouldnt-be-this-hard-to-responsibly-fly-a-drone","linkMd5ListStr":"da5f551943891d387e1f1c5f2532d096","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMDQ4Ng.gif","sourceStatusCode":200,"destWidth":1200,"destHeight":812,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn74@2020_6/2020/08/25/10-38-20-112_150412e8ccc80ea1.webp","sourceBytes":1556948,"destBytes":1172864,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":7371,"convertSpendMs":5799,"createdTime":"2020-08-25 18:38:14","host":"us-035*","referer":"https://spectrum.ieee.org/robotics/humanoids/how-boston-dynamics-is-redefining-robot-agility","linkMd5ListStr":"7c210ebadb5e65b08c2708471190bfd4","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.5 MB","destSize":"1.1 MB","compressRate":"75.3%"},{"code":1,"isDone":false,"source":"https://spectrum.ieee.org/image/MzQyMjU4Nw.gif","sourceStatusCode":200,"destWidth":620,"destHeight":350,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn13@2020_6/2020/08/25/10-38-19-916_39e14162ca05c864.webp","sourceBytes":4855098,"destBytes":4259604,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":10619,"convertSpendMs":1929,"createdTime":"2020-08-25 18:38:12","host":"europe67*","referer":"https://spectrum.ieee.org/automaton/robotics/drones/skydio-2-review-this-is-the-drone-you-want-to-fly","linkMd5ListStr":"ac1ed2d177f3d00db61213d0a8b02970","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.6 MB","destSize":"4.1 MB","compressRate":"87.7%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/IeeeSpectrumRobotics?i=R0msYVZ-Qm4:fR7STip5Pt8:gIN9vFwOqvQ","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn18@2020_5/2020/08/25/10-38-28-419_d722d42f60373eeb.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":8606,"totalSpendMs":822,"convertSpendMs":3,"createdTime":"2020-08-25 18:38:28","host":"us-033*","referer":"https://spectrum.ieee.org/automaton/robotics/robotics-hardware/video-friday-wearable-robot-tail","linkMd5ListStr":"2b5ea3f72dcd3a0879d0e273a51ddf91","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"}],"successGithubMap":{"myreaderx8":20,"myreaderx14":20,"myreaderx7":20,"myreaderx15":18,"myreaderx6":20,"myreaderx16":20,"myreaderx10":20,"myreaderx4":19,"myreaderx32":20,"myreaderx3":20,"myreaderx11":19,"myreaderx33":20,"myreaderx2":19,"myreaderx12":20,"myreaderx13":19,"myreaderx1":20,"myreaderx30":19,"myreaderx31":19,"myreaderx18":19,"myreaderx19":20,"myreaderx":20,"myreaderx25":20,"myreaderx27":19,"myreaderx21":19,"myreaderx22":20,"myreaderx23":20,"myreaderx24":19,"myreaderx5oss":20,"myreaderx29":19},"failGithubMap":{}}