{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-10-13 01:54:14","updatedTime":"2020-10-13 01:54:14","title":"Massively Large-Scale Distributed Reinforcement Learning with Menger","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","description":"<span class=\"byline-author\">Posted by Amir Yazdanbakhsh, Research Scientist, and Junchao Chen, Software Engineer, Google Research</span> \n<p>In the last decade, <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">reinforcement learning</a> (RL) has become one of the most promising research areas in machine learning and has demonstrated great potential for solving sophisticated real-world problems, such as <a href=\"https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html\">chip placement</a> and <a href=\"https://people.csail.mit.edu/alizadeh/papers/deeprm-hotnets16.pdf\">resource management</a>, and solving challenging games (e.g., <a href=\"https://www.nature.com/articles/nature16961\">Go</a>, <a href=\"https://openai.com/projects/five/\">Dota 2</a>, and <a href=\"https://openai.com/blog/emergent-tool-use/\">hide-and-seek</a>). In simplest terms, an RL infrastructure is a loop of data collection and training, where <em>actors</em> explore the environment and collect samples, which are then sent to the <em>learners</em> to train and update the model. Most current RL techniques require many iterations over batches of millions of samples from the environment to learn a target task (e.g., <a href=\"https://openai.com/projects/five/\">Dota 2</a> learns from batches of 2 million frames every 2 seconds). As such, an RL infrastructure should not only scale efficiently (e.g., increase the number of actors) and collect an immense number of samples, but also be able to swiftly iterate over these extensive amounts of samples during training. </p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s576/image1.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"384\" data-original-width=\"576\" src=\"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s320/image1.gif\" width=\"320\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">Overview of an RL system in which an actor sends trajectories (e.g., multiple samples) to a learner. The learner trains a model using the sampled data and pushes the updated model back to the actor (e.g. <a href=\"https://github.com/tensorflow/agents\">TF-Agents</a>, <a href=\"https://arxiv.org/pdf/1802.01561.pdf\">IMPALA</a>).</td>\n  </tr>\n </tbody>\n</table>\n<p> Today we introduce Menger<sup id=\"fnref1\"><a href=\"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html#fn1\" rel=\"footnote\"><small>1</small></a></sup>, a massive large-scale distributed RL infrastructure with localized inference that scales up to several thousand actors across multiple processing clusters (e.g., <a href=\"https://research.google/pubs/pub43438/#:~:text=Google's%20Borg%20system%20is%20a,tens%20of%20thousands%20of%20machines.\">Borg cells</a>), reducing the overall training time in the task of chip placement. In this post we describe how we implement Menger using <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-pods-break-ai-training-records\">Google TPU accelerators</a> for fast training iterations, and present its performance and scalability on the challenging task of <a href=\"https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html\">chip placement</a>. Menger reduces the training time by up to 8.6<em>x </em>compared to a baseline <a href=\"https://arxiv.org/abs/2004.10746\">implementation</a>. </p>\n<p><b>Menger System Design</b><br />There are various distributed RL systems, such as <a href=\"https://deepmind.com/research/publications/Acme\">Acme</a> and <a href=\"https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html\">SEED RL</a>, each of which focus on optimizing a single particular design point in the space of distributed reinforcement learning systems. For example, while Acme uses local inference on each actor with frequent model retrieval from the learner, SEED RL benefits from a centralized inference design by allocating a portion of TPU cores for performing batched calls. The tradeoffs between these design points are (1) paying the communication cost of sending/receiving observations and actions to/from a centralized inference server or paying the communication cost of model retrieval from a learner and (2) the cost of inference on actors (e.g., CPUs) compared to accelerators (e.g., TPUs/GPUs). Because of the requirements of our target application (e.g., size of observations, actions, and model size), Menger uses local inference in a manner similar to Acme, but pushes the scalability of actors to virtually an unbounded limit. The main challenges to achieving massive scalability and fast training on accelerators include: </p>\n<ol>\n <li>Servicing a large number of read requests from actors to a learner for model retrieval can easily throttle the learner and quickly become a major bottleneck (e.g., significantly increasing the convergence time) as the number of actors increases. </li>\n <li>The TPU performance is often limited by the efficiency of the input pipeline in feeding the training data to the TPU compute cores. As the number of TPU compute cores increases (e.g., <a href=\"https://cloud.google.com/tpu/docs/system-architecture#pod\">TPU Pod</a>), the performance of the input pipeline becomes even more critical for the overall training runtime. </li>\n</ol>\n<p><b>Efficient Model Retrieval</b><br />To address the first challenge, we introduce transparent and distributed caching components between the learner and the actors optimized in TensorFlow and backed by <a href=\"https://github.com/deepmind/reverb\">Reverb</a> (similar approach used in <a href=\"https://openai.com/projects/five/\">Dota</a>). The main responsibility of the caching components is to strike a balance between the large number of requests from actors and the learner job. Adding these caching components not only significantly reduces the pressure on the learner to service the read requests, but also further distributes the actors across multiple <a href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf\">Borg cells</a> with a marginal communication overhead. In our study, we show that for a 16 MB model with 512 actors, the introduced caching components reduce the average read latency by a factor of ~4.0<em>x</em> leading to faster training iterations, especially for on-policy algorithms such as <a href=\"https://arxiv.org/abs/1707.06347\">PPO</a>. </p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-bPEExECG-ec/X3dhMogFunI/AAAAAAAAGpg/SQzJ2HXkG6cqhUQ9-fwY8tC_1hU53hprACLcBGAsYHQ/s1280/image6.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"538\" data-original-width=\"1280\" height=\"268\" src=\"https://1.bp.blogspot.com/-bPEExECG-ec/X3dhMogFunI/AAAAAAAAGpg/SQzJ2HXkG6cqhUQ9-fwY8tC_1hU53hprACLcBGAsYHQ/w640-h268/image6.gif\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">Overview of a distributed RL system with multiple actors placed in different Borg cells. Servicing the frequent model update requests from a massive number of actors across different Borg cells throttles the learner and the communication network between learner and actors, which leads to a significant increase in the overall convergence time. The dashed lines represent <a href=\"https://en.wikipedia.org/wiki/GRPC\">gRPC</a> communication between different machines.</td>\n  </tr>\n </tbody>\n</table>\n<p></p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-UdKDH4Qp6bk/X3dpTm-Qe-I/AAAAAAAAGqU/cSyhh_m2KQ0D6bMLSQUUgLySFIl1nQc5QCLcBGAsYHQ/s1280/CachedLearner-Actor.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"535\" data-original-width=\"1280\" height=\"268\" src=\"https://1.bp.blogspot.com/-UdKDH4Qp6bk/X3dpTm-Qe-I/AAAAAAAAGqU/cSyhh_m2KQ0D6bMLSQUUgLySFIl1nQc5QCLcBGAsYHQ/w640-h268/CachedLearner-Actor.gif\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">Overview of a distributed RL system with multiple actors placed in different Borg cells with the introduced transparent and distributed caching service. The learner only sends the updated model to the distributed caching services. Each caching service handles the model request updates from the nearby actors (i.e., actors placed on the same Borg cells) and the caching service. The caching service not only reduces the load on the learner for servicing the model update requests, but also reduces the average read latency by the actors.</td>\n  </tr>\n </tbody>\n</table>\n<p><b>High Throughput Input Pipeline</b><br />To deliver a high throughput input data pipeline, Menger uses <a href=\"https://deepmind.com/research/open-source/Reverb\">Reverb</a>, a recently open-sourced data storage system designed for machine learning applications that provides an efficient and flexible platform to implement <a href=\"https://developers.google.com/machine-learning/glossary#experience-replay\">experience replay</a> in a variety of on-policy/off-policy algorithms. However, using a single Reverb replay buffer service does not currently scale well in a distributed RL setting with thousands of actors, and simply becomes inefficient in terms of write throughput from actors.</p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-kGP9Qv_8i3o/X3dqJWLJbxI/AAAAAAAAGqc/7vy9cJfa9w8XF78lD8SpnVvsWtLwUkMIwCLcBGAsYHQ/s1280/ReplayBuffer.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"704\" data-original-width=\"1280\" height=\"352\" src=\"https://1.bp.blogspot.com/-kGP9Qv_8i3o/X3dqJWLJbxI/AAAAAAAAGqc/7vy9cJfa9w8XF78lD8SpnVvsWtLwUkMIwCLcBGAsYHQ/w640-h352/ReplayBuffer.gif\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">A distributed RL system with a single replay buffer. Servicing a massive number of write requests from actors throttles the replay buffer and reduces its overall throughput. In addition, as we scale the learner to a setting with multiple compute engines (e.g., TPU Pod), feeding the data to these engines from a single replay buffer service becomes inefficient, which negatively impacts the overall convergence time.</td>\n  </tr>\n </tbody>\n</table>\n<p>To better understand the efficiency of the replay buffer in a distributed setting, we evaluate the average write latency for various payload sizes from 16 MB to 512 MB and a number of actors ranging from 16 to 2048. We repeat the experiment when the replay buffer and actors are placed on the same <a href=\"https://research.google/pubs/pub49065/\">Borg</a> cell. As the number of actors grows the average write latency also increases significantly. Expanding the number of actors from 16 to 2048, the average write latency increases by a factor of ~6.2<em>x</em> and ~18.9<em>x</em> for payload size 16 MB and 512 MB, respectively. This increase in the write latency negatively impacts the data collection time and leads to inefficiency in the overall training time. </p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-mRT4DampNnw/X3dshyC82_I/AAAAAAAAGq0/Alou3d_qhhkAX7tKnqPdx9QOaGVnWGPtACLcBGAsYHQ/s1999/image2%2B%25281%2529.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1229\" data-original-width=\"1999\" height=\"394\" src=\"https://1.bp.blogspot.com/-mRT4DampNnw/X3dshyC82_I/AAAAAAAAGq0/Alou3d_qhhkAX7tKnqPdx9QOaGVnWGPtACLcBGAsYHQ/w640-h394/image2%2B%25281%2529.png\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">The average write latency to a single <a href=\"https://github.com/deepmind/reverb\">Reverb</a> replay buffer for various payload sizes (16 MB - 512 MB) and various number of actors (16 to 2048) when the actors and replay buffer are placed on the same Borg cells.</td>\n  </tr>\n </tbody>\n</table>\n<p>To mitigate this, we use the <a href=\"https://en.wikipedia.org/wiki/Shard_(database_architecture)\">sharding</a> capability provided by <a href=\"https://github.com/deepmind/reverb#sharding\">Reverb</a> to increase the throughput between actors, learner, and replay buffer services. Sharding balances the write load from the large number of actors across multiple replay buffer servers, instead of throttling a single replay buffer server, and also minimizes the average write latency for each replay buffer server (as fewer actors share the same server). This enables Menger to scale efficiently to thousands of actors across multiple Borg cells. </p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-9JuO2zOma9Q/X3dq58RwdyI/AAAAAAAAGqo/yizULYz0ng81h1tzv37Ar1-aeXafczf4wCLcBGAsYHQ/s1280/ShardedReplay.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"558\" data-original-width=\"1280\" height=\"280\" src=\"https://1.bp.blogspot.com/-9JuO2zOma9Q/X3dq58RwdyI/AAAAAAAAGqo/yizULYz0ng81h1tzv37Ar1-aeXafczf4wCLcBGAsYHQ/w640-h280/ShardedReplay.gif\" width=\"640\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">A distributed RL system with <em>sharded</em> replay buffers. Each replay buffer service is a dedicated data storage for a collection of actors, generally located on the same Borg cells. In addition, the sharded replay buffer configuration provides a higher throughput input pipeline to the accelerator cores.</td>\n  </tr>\n </tbody>\n</table>\n<p><b>Case Study: Chip Placement </b><br />We studied the benefits of Menger in the complex task of <a href=\"https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html\">chip placement</a> for a large <a href=\"https://en.wikipedia.org/wiki/Netlist\">netlist</a>. Using 512 TPU cores, Menger achieves significant improvements in the training time (up to ~8.6<em>x</em>, reducing the training time from ~8.6 hours down to merely one hour in the fastest configuration) compared to a strong <a href=\"https://arxiv.org/abs/2004.10746\">baseline</a>. While Menger was optimized for TPUs, that the key factor for this performance gain is the architecture, and we would expect to see similar gains when tailored to use on GPUs. </p>\n<table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\">\n <tbody>\n  <tr>\n   <td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-JBn66BJc1h4/X3diIY8G_6I/AAAAAAAAGqE/3PvQniZNxpgURAReB6VZgpP22ZIlqUSIACLcBGAsYHQ/s1999/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"1164\" data-original-width=\"1999\" height=\"233\" src=\"https://1.bp.blogspot.com/-JBn66BJc1h4/X3diIY8G_6I/AAAAAAAAGqE/3PvQniZNxpgURAReB6VZgpP22ZIlqUSIACLcBGAsYHQ/w400-h233/image4.png\" width=\"400\" /></a></td>\n  </tr>\n  <tr>\n   <td class=\"tr-caption\" style=\"text-align: center;\">The improvement in training time using Menger with variable number of TPU cores compared to a baseline in the task of <a href=\"https://arxiv.org/abs/2004.10746\">chip placement</a>.</td>\n  </tr>\n </tbody>\n</table>\n<p>We believe that Menger infrastructure and its promising results in the intricate task of chip placement demonstrate an innovative path forward to further shorten the chip design cycle and has the potential to not only enable further innovations in the chip design process, but other challenging real-world tasks as well. </p>\n<p><b>Acknowledgments</b><br /><em>Most of the work was done by Amir Yazdanbakhsh, Junchao Chen, and Yu Zheng.<strong> </strong>We would like to also thank Robert Ormandi, Ebrahim Songhori, Shen Wang, TF-Agents team, Albin Cassirer, Aviral Kumar, James Laudon, John Wilkes, Joe Jiang, Milad Hashemi, Sat Chatterjee, Piotr Stanczyk, Sabela Ramos, Lasse Espeholt, Marcin Michalski, Sam Fishman, Ruoxin Sang, Azalia Mirhosseini, Anna Goldie, and Eric Johnson for their help and support.</em></p>\n<!--Footnotes themselves at the bottom.-->\n<hr width=\"100%\" />\n<p><span class=\"Apple-style-span\" style=\"font-size: small;\"><a name=\"fn1\"><b>1 </b></a>A <a href=\"https://en.wikipedia.org/wiki/Menger_sponge\">Menger</a> cube is a three-dimensional fractal curve, and the inspiration for the name of this system, given that the proposed infrastructure can virtually scale ad infinitum.<a href=\"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html#fnref1\"><sup>↩</sup></a></span></p>\n<div class=\"feedflare\"> \n <a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=JbcHbaO2TTI:SxYoJICBHrQ:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\" /></a> \n</div>\n<img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/JbcHbaO2TTI\" height=\"1\" width=\"1\" alt=\"\" />","descriptionType":"html","publishedDate":"Fri, 02 Oct 2020 18:14:00 +0000","feedId":4924,"bgimg":"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s72-c/image1.gif","linkMd5":"2d87a6f0690059aa71d2d0a722b39c82","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn82@2020_1/2020/10/12/17-54-16-619_e5205efb45964f2a.webp","destWidth":72,"destHeight":72,"sourceBytes":318243,"destBytes":180122,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s320/image1.gif":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn50@2020_2/2020/10/12/17-54-17-392_4c44b2d5eec43bd4.webp","https://1.bp.blogspot.com/-bPEExECG-ec/X3dhMogFunI/AAAAAAAAGpg/SQzJ2HXkG6cqhUQ9-fwY8tC_1hU53hprACLcBGAsYHQ/w640-h268/image6.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn64@2020_5/2020/10/12/17-54-20-950_0b092c00399d372f.webp","https://1.bp.blogspot.com/-UdKDH4Qp6bk/X3dpTm-Qe-I/AAAAAAAAGqU/cSyhh_m2KQ0D6bMLSQUUgLySFIl1nQc5QCLcBGAsYHQ/w640-h268/CachedLearner-Actor.gif":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn69@2020_5/2020/10/12/17-54-17-738_dcf4dba373ee9f7c.webp","https://1.bp.blogspot.com/-kGP9Qv_8i3o/X3dqJWLJbxI/AAAAAAAAGqc/7vy9cJfa9w8XF78lD8SpnVvsWtLwUkMIwCLcBGAsYHQ/w640-h352/ReplayBuffer.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn47@2020_5/2020/10/12/17-54-40-319_e9c66ee4e620a78a.webp","https://1.bp.blogspot.com/-mRT4DampNnw/X3dshyC82_I/AAAAAAAAGq0/Alou3d_qhhkAX7tKnqPdx9QOaGVnWGPtACLcBGAsYHQ/w640-h394/image2%2B%25281%2529.png":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn73@2020_6/2020/10/12/17-54-14-314_7f7969ae0cc57985.webp","https://1.bp.blogspot.com/-9JuO2zOma9Q/X3dq58RwdyI/AAAAAAAAGqo/yizULYz0ng81h1tzv37Ar1-aeXafczf4wCLcBGAsYHQ/w640-h280/ShardedReplay.gif":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn54@2020_3/2020/10/12/17-54-38-886_7a12b1997f921fd4.webp","https://1.bp.blogspot.com/-JBn66BJc1h4/X3diIY8G_6I/AAAAAAAAGqE/3PvQniZNxpgURAReB6VZgpP22ZIlqUSIACLcBGAsYHQ/w400-h233/image4.png":null,"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn43@2020_4/2020/10/12/17-54-14-207_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/JbcHbaO2TTI":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn55@2020_3/2020/10/12/17-54-14-169_0c679635e93e9005.webp"},"publishedOrCreatedDate":1602525254114}],"record":{"createdTime":"2020-10-13 01:54:14","updatedTime":"2020-10-13 01:54:14","feedId":4924,"fetchDate":"Mon, 12 Oct 2020 17:54:14 +0000","fetchMs":80,"handleMs":335,"totalMs":28159,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"0a87772537dbb011ef60166702d51dd6","hostName":"us-010*","requestId":"c3c4b76b8a664e8c8d30f97069073587_4924","contentType":"text/xml; charset=UTF-8","totalBytes":6853772,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":9,"articlesImgsGithubTotal":8,"successGithubMap":{"myreaderx15":1,"myreaderx7":1,"myreaderx16":1,"myreaderx32":1,"myreaderx11":1,"myreaderx22":1,"myreaderx24":1,"myreaderx30":1,"myreaderx31":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1}},"feed":{"createdTime":"2020-08-25 04:29:57","updatedTime":"2020-09-01 10:58:50","id":4924,"name":"Google AI Blog","url":"http://feeds.feedburner.com/blogspot/gJZg/","subscriber":null,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn41@2020_1/2020/09/01/02-58-50-993_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2020-10-13 01:54:41","updatedTime":"2020-10-13 01:54:41","id":null,"feedId":4924,"linkMd5":"2d87a6f0690059aa71d2d0a722b39c82"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":6673650,"tmpBgImgCdnBytes":180122,"extra4":{"start":1602525253681,"total":0,"statList":[{"spend":99,"msg":"获取xml内容"},{"spend":335,"msg":"解释文章"},{"spend":2063,"msg":"上传封面图到cdn"},{"spend":1973,"msg":"修正封面图上传失败重新上传"},{"spend":27718,"msg":"正文链接上传到cdn"}]},"extra5":9,"extra6":9,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-JBn66BJc1h4/X3diIY8G_6I/AAAAAAAAGqE/3PvQniZNxpgURAReB6VZgpP22ZIlqUSIACLcBGAsYHQ/w400-h233/image4.png","sourceStatusCode":200,"destWidth":400,"destHeight":233,"sourceBytes":21666,"destBytes":9526,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":100,"convertSpendMs":9,"createdTime":"2020-10-13 01:54:14","host":"us-005*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn68/contents/2020/10/12/17-54-14-184_ab80f482031ae6c0.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 12 Oct 2020 17:54:14 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D84C:71BB:22EC94C:3B45169:5F849838"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1602525859"],"x-ratelimit-used":["65"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn68/contents/2020/10/12/17-54-14-184_ab80f482031ae6c0.webp","historyStatusCode":[],"spendMs":66},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.2 KB","destSize":"9.3 KB","compressRate":"44%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-JBn66BJc1h4/X3diIY8G_6I/AAAAAAAAGqE/3PvQniZNxpgURAReB6VZgpP22ZIlqUSIACLcBGAsYHQ/w400-h233/image4.png","sourceStatusCode":200,"destWidth":400,"destHeight":233,"sourceBytes":21666,"destBytes":9526,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":76,"convertSpendMs":9,"createdTime":"2020-10-13 01:54:14","host":"us-005*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn68/contents/2020/10/12/17-54-14-282_ab80f482031ae6c0.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Mon, 12 Oct 2020 17:54:14 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["D84C:71BB:22EC95A:3B461D1:5F849846"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, read:packages, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1602525859"],"x-ratelimit-used":["65"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn68/contents/2020/10/12/17-54-14-282_ab80f482031ae6c0.webp","historyStatusCode":[],"spendMs":52},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"21.2 KB","destSize":"9.3 KB","compressRate":"44%"}],"extra10_invalidATagHrefValue":{"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html_#fnref1":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html#fnref1","http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html_#fn1":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html#fn1"},"extra111_proxyServerAndStatMap":{"http://us-001.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe69.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-53.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe68.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-031.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-51.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-52.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-027.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/JbcHbaO2TTI","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn55@2020_3/2020/10/12/17-54-14-169_0c679635e93e9005.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":821,"convertSpendMs":3,"createdTime":"2020-10-13 01:54:14","host":"us-51*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn43@2020_4/2020/10/12/17-54-14-207_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":960,"convertSpendMs":18,"createdTime":"2020-10-13 01:54:14","host":"us-027*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-mRT4DampNnw/X3dshyC82_I/AAAAAAAAGq0/Alou3d_qhhkAX7tKnqPdx9QOaGVnWGPtACLcBGAsYHQ/w640-h394/image2%2B%25281%2529.png","sourceStatusCode":200,"destWidth":640,"destHeight":394,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn73@2020_6/2020/10/12/17-54-14-314_7f7969ae0cc57985.webp","sourceBytes":40579,"destBytes":16948,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":1286,"convertSpendMs":17,"createdTime":"2020-10-13 01:54:14","host":"europe69*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"39.6 KB","destSize":"16.6 KB","compressRate":"41.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s72-c/image1.gif","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn40@2020_1/2020/10/12/17-54-15-067_e5205efb45964f2a.webp","sourceBytes":318243,"destBytes":180122,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":2004,"convertSpendMs":807,"createdTime":"2020-10-13 01:54:14","host":"us-039*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"310.8 KB","destSize":"175.9 KB","compressRate":"56.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s72-c/image1.gif","sourceStatusCode":200,"destWidth":72,"destHeight":72,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn82@2020_1/2020/10/12/17-54-16-619_e5205efb45964f2a.webp","sourceBytes":318243,"destBytes":180122,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":1814,"convertSpendMs":284,"createdTime":"2020-10-13 01:54:16","host":"europe-58*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"310.8 KB","destSize":"175.9 KB","compressRate":"56.6%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-kZCZGNEzu1w/X3dg3YW_BvI/AAAAAAAAGpY/9bMTDQKqLzwBxogATkKnm-NbQHu73vccwCLcBGAsYHQ/s320/image1.gif","sourceStatusCode":200,"destWidth":320,"destHeight":213,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn50@2020_2/2020/10/12/17-54-17-392_4c44b2d5eec43bd4.webp","sourceBytes":1801638,"destBytes":1197518,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":5318,"convertSpendMs":3147,"createdTime":"2020-10-13 01:54:14","host":"us-53*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.7 MB","destSize":"1.1 MB","compressRate":"66.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-UdKDH4Qp6bk/X3dpTm-Qe-I/AAAAAAAAGqU/cSyhh_m2KQ0D6bMLSQUUgLySFIl1nQc5QCLcBGAsYHQ/w640-h268/CachedLearner-Actor.gif","sourceStatusCode":200,"destWidth":640,"destHeight":268,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn69@2020_5/2020/10/12/17-54-17-738_dcf4dba373ee9f7c.webp","sourceBytes":1044370,"destBytes":1101666,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":5527,"convertSpendMs":3457,"createdTime":"2020-10-13 01:54:14","host":"europe68*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1,019.9 KB","destSize":"1.1 MB","compressRate":"105.5%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-bPEExECG-ec/X3dhMogFunI/AAAAAAAAGpg/SQzJ2HXkG6cqhUQ9-fwY8tC_1hU53hprACLcBGAsYHQ/w640-h268/image6.gif","sourceStatusCode":200,"destWidth":638,"destHeight":268,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn64@2020_5/2020/10/12/17-54-20-950_0b092c00399d372f.webp","sourceBytes":636500,"destBytes":702634,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":8153,"convertSpendMs":6713,"createdTime":"2020-10-13 01:54:14","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"621.6 KB","destSize":"686.2 KB","compressRate":"110.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-9JuO2zOma9Q/X3dq58RwdyI/AAAAAAAAGqo/yizULYz0ng81h1tzv37Ar1-aeXafczf4wCLcBGAsYHQ/w640-h280/ShardedReplay.gif","sourceStatusCode":200,"destWidth":640,"destHeight":279,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn54@2020_3/2020/10/12/17-54-38-886_7a12b1997f921fd4.webp","sourceBytes":796775,"destBytes":1209382,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":26109,"convertSpendMs":24361,"createdTime":"2020-10-13 01:54:14","host":"us-52*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"778.1 KB","destSize":"1.2 MB","compressRate":"151.8%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-kGP9Qv_8i3o/X3dqJWLJbxI/AAAAAAAAGqc/7vy9cJfa9w8XF78lD8SpnVvsWtLwUkMIwCLcBGAsYHQ/w640-h352/ReplayBuffer.gif","sourceStatusCode":200,"destWidth":640,"destHeight":352,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn47@2020_5/2020/10/12/17-54-40-319_e9c66ee4e620a78a.webp","sourceBytes":1629119,"destBytes":2445120,"targetWebpQuality":75,"feedId":4924,"totalSpendMs":27694,"convertSpendMs":26037,"createdTime":"2020-10-13 01:54:14","host":"us-001*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/JbcHbaO2TTI/massively-large-scale-distributed.html","linkMd5ListStr":"2d87a6f0690059aa71d2d0a722b39c82","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"1.6 MB","destSize":"2.3 MB","compressRate":"150.1%"}],"successGithubMap":{"myreaderx15":1,"myreaderx7":1,"myreaderx16":1,"myreaderx32":1,"myreaderx11":1,"myreaderx22":1,"myreaderx24":1,"myreaderx30":1,"myreaderx31":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1}}