{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-04-10 12:07:53","updatedTime":"2021-04-10 12:07:53","title":"OnionSearch - a Python3 script that scrapes urls on different \".onion\" search engines.","link":"https://hakin9.org/?p=212880","description":"<h3>Educational purposes only</h3>\n<p>OnionSearch is a Python3 script that scrapes urls on different \".onion\" search engines.</p>\n<p><img loading=\"lazy\" class=\"\" src=\"https://raw.githubusercontent.com/megadose/gif-demo/master/onionsearch.gif\" width=\"1271\" height=\"636\" /></p>\n<h2>Prerequisite</h2>\n<p><a href=\"https://www.python.org/download/releases/3.0/\" rel=\"nofollow\">Python 3</a></p>\n<h2><a id=\"user-content--currently-supported-search-engines\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#-currently-supported-search-engines\" aria-hidden=\"true\"></a><img loading=\"lazy\" class=\"emoji\" src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png\" alt=\"books\" width=\"20\" height=\"20\" /> Currently supported Search engines</h2>\n<ul>\n<li><span style=\"font-size: 12pt;\">ahmia</span></li>\n<li><span style=\"font-size: 12pt;\">darksearchio</span></li>\n<li><span style=\"font-size: 12pt;\">onionland</span></li>\n<li><span style=\"font-size: 12pt;\">notevil</span></li>\n<li><span style=\"font-size: 12pt;\">darksearchenginer</span></li>\n<li><span style=\"font-size: 12pt;\">phobos</span></li>\n<li><span style=\"font-size: 12pt;\">onionsearchserver</span></li>\n<li><span style=\"font-size: 12pt;\">torgle</span></li>\n<li><span style=\"font-size: 12pt;\">onionsearchengine</span></li>\n<li><span style=\"font-size: 12pt;\">tordex</span></li>\n<li><span style=\"font-size: 12pt;\">tor66</span></li>\n<li><span style=\"font-size: 12pt;\">tormax</span></li>\n<li><span style=\"font-size: 12pt;\">haystack</span></li>\n<li><span style=\"font-size: 12pt;\">multivac</span></li>\n<li><span style=\"font-size: 12pt;\">evosearch</span></li>\n<li><span style=\"font-size: 12pt;\">deeplink</span></li>\n</ul>\n<h2><a id=\"user-content-️-installation\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#%EF%B8%8F-installation\" aria-hidden=\"true\"></a><img loading=\"lazy\" class=\"emoji\" src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png\" alt=\"hammer_and_wrench\" width=\"20\" height=\"20\" /> Installation</h2>\n<h3><a id=\"user-content-with-pypi\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#with-pypi\" aria-hidden=\"true\"></a>With PyPI</h3>\n<p><code>pip3 install onionsearch</code></p>\n<h3><a id=\"user-content-with-github\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#with-github\" aria-hidden=\"true\"></a>With Github</h3>\n<div class=\"highlight highlight-source-shell\">\n<pre>git clone https://github.com/megadose/OnionSearch.git\n<span class=\"pl-c1\">cd</span> OnionSearch/\npython3 setup.py install</pre>\n</div>\n<h2><a id=\"user-content---usage\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#--usage\" aria-hidden=\"true\"></a><img loading=\"lazy\" class=\"emoji\" src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png\" alt=\"chart_with_upwards_trend\" width=\"20\" height=\"20\" /> Usage</h2>\n<p>Help:</p>\n<pre><code>usage: onionsearch [-h] [--proxy PROXY] [--output OUTPUT]\n                  [--continuous_write CONTINUOUS_WRITE] [--limit LIMIT]\n                  [--engines [ENGINES [ENGINES ...]]]\n                  [--exclude [EXCLUDE [EXCLUDE ...]]]\n                  [--fields [FIELDS [FIELDS ...]]]\n                  [--field_delimiter FIELD_DELIMITER] [--mp_units MP_UNITS]\n                  search\n\npositional arguments:\n  search                The search string or phrase\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --proxy PROXY         Set Tor proxy (default: 127.0.0.1:9050)\n  --output OUTPUT       Output File (default: output_$SEARCH_$DATE.txt), where $SEARCH is replaced by the first chars of the search string and $DATE is replaced by the datetime\n  --continuous_write CONTINUOUS_WRITE\n                        Write progressively to output file (default: False)\n  --limit LIMIT         Set a max number of pages per engine to load\n  --engines [ENGINES [ENGINES ...]]\n                        Engines to request (default: full list)\n  --exclude [EXCLUDE [EXCLUDE ...]]\n                        Engines to exclude (default: none)\n  --fields [FIELDS [FIELDS ...]]\n                        Fields to output to csv file (default: engine name link), available fields are shown below\n  --field_delimiter FIELD_DELIMITER\n                        Delimiter for the CSV fields\n  --mp_units MP_UNITS   Number of processing units (default: core number minus 1)\n\n[...]\n</code></pre>\n<h3><a id=\"user-content-multi-processing-behaviour\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#multi-processing-behaviour\" aria-hidden=\"true\"></a>Multi-processing behaviour</h3>\n<p>By default, the script will run with the parameter <code>mp_units = cpu_count() - 1</code>. It means if you have a machine with 4 cores, it will run 3 scraping functions in parallel. You can force <code>mp_units</code> to any value but it is recommended to leave to default. You may want to set it to 1 to run all requests sequentially (disabling multi-processing feature).</p>\n<p>Please note that continuous writing to csv file has not been <em>heavily</em> tested with multiprocessing feature and therefore may not work as expected.</p>\n<p>Please also note that the progress bars may not be properly displayed when <code>mp_units</code> is greater than 1. <strong>It does not affect the results</strong>, so don't worry.</p>\n<h3><a id=\"user-content-examples\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#examples\" aria-hidden=\"true\"></a>Examples</h3>\n<p>To request all the engines for the word \"computer\":</p>\n<pre><code>onionsearch \"computer\"\n</code></pre>\n<p>To request all the engines excepted \"Ahmia\" and \"Candle\" for the word \"computer\":</p>\n<pre><code>onionsearch \"computer\" --exclude ahmia candle\n</code></pre>\n<p>To request only \"Tor66\", \"DeepLink\" and \"Phobos\" for the word \"computer\":</p>\n<pre><code>onionsearch \"computer\" --engines tor66 deeplink phobos\n</code></pre>\n<p>The same as previously but limiting to 3 the number of pages to load per engine:</p>\n<pre><code>onionsearch \"computer\" --engines tor66 deeplink phobos --limit 3\n</code></pre>\n<p>Please kindly note that the list of supported engines (and their keys) is given in the script help (-h).</p>\n<h3><a id=\"user-content-output\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#output\" aria-hidden=\"true\"></a><strong>Output</strong></h3>\n<h4><a id=\"user-content-default-output\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#default-output\" aria-hidden=\"true\"></a><strong>Default output</strong></h4>\n<p>By default, the file is written at the end of the process. The file will be csv formatted, containing the following columns:</p>\n<pre><code>\"engine\",\"name of the link\",\"url\"\n</code></pre>\n<h4><a id=\"user-content-customizing-the-output-fields\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#customizing-the-output-fields\" aria-hidden=\"true\"></a><strong>Customizing the output fields</strong></h4>\n<p>You can customize what will be flush in the output file by using the parameters <code>--fields</code> and <code>--field_delimiter</code>.</p>\n<p><code>--fields</code> allows you to add, remove, re-order the output fields. The default mode is show just below. Instead, you can for instance choose to output:</p>\n<pre><code>\"engine\",\"name of the link\",\"url\",\"domain\"\n</code></pre>\n<p>by setting <code>--fields engine name link domain</code>.</p>\n<p>Or even, you can choose to output:</p>\n<pre><code>\"engine\",\"domain\"\n</code></pre>\n<p>by setting <code>--fields engine domain</code>.</p>\n<p>These are examples but there are many possibilities.</p>\n<p>Finally, you can also choose to modify the CSV delimiter (comma by default), for instance: <code>--field_delimiter \";\"</code>.</p>\n<h4><a id=\"user-content-changing-filename\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#changing-filename\" aria-hidden=\"true\"></a><strong>Changing filename</strong></h4>\n<p>The filename will be set by default to <code>output_$DATE_$SEARCH.txt</code>, where $DATE represents the current datetime and $SEARCH the first characters of the search string.</p>\n<p>You can modify this filename by using <code>--output</code> when running the script, for instance:</p>\n<pre><code>onionsearch \"computer\" --output \"\\$DATE.csv\"\nonionsearch \"computer\" --output output.txt\nonionsearch \"computer\" --output \"\\$DATE_\\$SEARCH.csv\"\n...\n</code></pre>\n<p>(Note that it might be necessary to escape the dollar character.)</p>\n<p>In the csv file produced, the name and url strings are sanitized as much as possible, but there might still be some problems...</p>\n<h4><a id=\"user-content-write-progressively\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#write-progressively\" aria-hidden=\"true\"></a><strong>Write progressively</strong></h4>\n<p>You can choose to progressively write to the output (instead of everything at the end, which would prevent losing the results if something goes wrong). To do so you have to use <code>--continuous_write True</code>, just as is:</p>\n<pre><code>onionsearch \"computer\" --continuous_write True\n</code></pre>\n<p>You can then use the <code>tail -f</code> (tail follow) Unix command to actively watch or monitor the results of the scraping.</p>\n<h2><a id=\"user-content-thank-you-to-gobarigo\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#thank-you-to-gobarigo\" aria-hidden=\"true\"></a>Thank you to <a href=\"https://github.com/Gobarigo\">Gobarigo</a></h2>\n<h2><a id=\"user-content-thank-you-mxrch-for-this-logo\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#thank-you-mxrch-for-this-logo\" aria-hidden=\"true\"></a>Thank you <a href=\"https://github.com/mxrch\">mxrch</a> for this logo</h2>\n<h2><a id=\"user-content--license\" class=\"anchor\" href=\"https://github.com/megadose/OnionSearch#-license\" aria-hidden=\"true\"></a><img loading=\"lazy\" class=\"emoji\" src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png\" alt=\"memo\" width=\"20\" height=\"20\" /> License</h2>\n<p><a href=\"https://www.gnu.org/licenses/gpl-3.0.fr.html\" rel=\"nofollow\">GNU General Public License v3.0</a></p>\n<p>&#160;</p>\n<p>Originally posted at: <a href=\"https://github.com/megadose/OnionSearch\">https://github.com/megadose/OnionSearch </a></p>\n<p>The post <a rel=\"nofollow\" href=\"https://hakin9.org/onionsearch/\">OnionSearch - a Python3 script that scrapes urls on different &#34;.onion&#34; search engines.</a> appeared first on <a rel=\"nofollow\" href=\"https://hakin9.org\">Hakin9 -  IT Security Magazine</a>.</p>\n","descriptionType":"html","publishedDate":"Fri, 09 Apr 2021 12:36:12 +0000","feedId":12805,"bgimg":"https://raw.githubusercontent.com/megadose/gif-demo/master/onionsearch.gif","linkMd5":"094eb68c0334d38d4778736703d55357","author":"Magdalena Jarzębska","articleImgCdnMap":{"https://raw.githubusercontent.com/megadose/gif-demo/master/onionsearch.gif":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn9@2020_5/2021/04/10/04-10-09-246_de8307f367219d1c.webp","https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn13@2020_2/2021/04/10/04-09-55-398_26907cb404d4edec.webp","https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn6@2020_2/2021/04/10/04-09-55-618_6bb5a24e9bcda5cd.webp","https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn18@2020_5/2021/04/10/04-09-55-404_7f5c24327c28c678.webp","https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_5/2021/04/10/04-09-55-417_8daf9fda1a21c756.webp"},"publishedOrCreatedDate":1618027673495}],"record":{"createdTime":"2021-04-10 12:07:53","updatedTime":"2021-04-10 12:07:53","feedId":12805,"fetchDate":"Sat, 10 Apr 2021 04:07:53 +0000","fetchMs":119,"handleMs":27,"totalMs":243071,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"1bc2e11d54d94d2e8b384096218a63f0","hostName":"us-001*","requestId":"e1b1e81464ce40429906bd5d5d104439_12805","contentType":"application/rss+xml; charset=UTF-8","totalBytes":1609226,"bgimgsTotal":1,"bgimgsGithubTotal":0,"articlesImgsTotal":5,"articlesImgsGithubTotal":5,"successGithubMap":{"myreaderx6":1,"myreaderx21":1,"myreaderx33":1,"myreaderx22":1,"myreaderx31":1},"failGithubMap":{}},"feed":{"createdTime":"2020-08-25 04:37:58","updatedTime":"2020-09-05 16:41:01","id":12805,"name":"Hakin9 –  IT Security Magazine","url":"http://hakin9.org/feed/","subscriber":242,"website":null,"icon":"https://hakin9.org/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn13@2020_6/2020/09/05/08-40-54-902_a04e3940efd7f478.jpg","description":"Hackers about hacking techniques in our IT Security Magazine","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-04-10 12:11:56","updatedTime":"2021-04-10 12:11:56","id":null,"feedId":12805,"linkMd5":"094eb68c0334d38d4778736703d55357"}],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":1609226,"tmpBgImgCdnBytes":0,"extra4":{"start":1618027673313,"total":0,"statList":[{"spend":155,"msg":"获取xml内容"},{"spend":27,"msg":"解释文章"},{"spend":17077,"msg":"正文链接上传到cdn"},{"spend":121090,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"}]},"extra5":5,"extra6":5,"extra7ImgCdnFailResultVector":[null,null,null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://github.githubassets.com/images/icons/emoji/unicode/1f4da.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn13@2020_2/2021/04/10/04-09-55-398_26907cb404d4edec.webp","sourceBytes":3778,"destBytes":1736,"targetWebpQuality":75,"feedId":12805,"totalSpendMs":304,"convertSpendMs":5,"createdTime":"2021-04-10 12:09:55","host":"us-038*","referer":"https://hakin9.org/?p=212880","linkMd5ListStr":"094eb68c0334d38d4778736703d55357","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"3.7 KB","destSize":"1.7 KB","compressRate":"46%"},{"code":1,"isDone":false,"source":"https://github.githubassets.com/images/icons/emoji/unicode/1f4dd.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_5/2021/04/10/04-09-55-417_8daf9fda1a21c756.webp","sourceBytes":2496,"destBytes":1362,"targetWebpQuality":75,"feedId":12805,"totalSpendMs":349,"convertSpendMs":5,"createdTime":"2021-04-10 12:09:55","host":"us-010*","referer":"https://hakin9.org/?p=212880","linkMd5ListStr":"094eb68c0334d38d4778736703d55357","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.4 KB","destSize":"1.3 KB","compressRate":"54.6%"},{"code":1,"isDone":false,"source":"https://github.githubassets.com/images/icons/emoji/unicode/1f4c8.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx21/cdn18@2020_5/2021/04/10/04-09-55-404_7f5c24327c28c678.webp","sourceBytes":2229,"destBytes":1218,"targetWebpQuality":75,"feedId":12805,"totalSpendMs":380,"convertSpendMs":13,"createdTime":"2021-04-10 12:09:55","host":"us-026*","referer":"https://hakin9.org/?p=212880","linkMd5ListStr":"094eb68c0334d38d4778736703d55357","githubUser":"myreaderx21","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.2 KB","destSize":"1.2 KB","compressRate":"54.6%"},{"code":1,"isDone":false,"source":"https://github.githubassets.com/images/icons/emoji/unicode/1f6e0.png","sourceStatusCode":200,"destWidth":64,"destHeight":64,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn6@2020_2/2021/04/10/04-09-55-618_6bb5a24e9bcda5cd.webp","sourceBytes":2579,"destBytes":1504,"targetWebpQuality":75,"feedId":12805,"totalSpendMs":519,"convertSpendMs":12,"createdTime":"2021-04-10 12:09:55","host":"europe-22*","referer":"https://hakin9.org/?p=212880","linkMd5ListStr":"094eb68c0334d38d4778736703d55357","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.5 KB","destSize":"1.5 KB","compressRate":"58.3%"},{"code":1,"isDone":false,"source":"https://raw.githubusercontent.com/megadose/gif-demo/master/onionsearch.gif","sourceStatusCode":200,"destWidth":1904,"destHeight":952,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx31/cdn9@2020_5/2021/04/10/04-10-09-246_de8307f367219d1c.webp","sourceBytes":624895,"destBytes":1603406,"targetWebpQuality":75,"feedId":12805,"totalSpendMs":16911,"convertSpendMs":13443,"createdTime":"2021-04-10 12:09:55","host":"europe-60*","referer":"https://hakin9.org/?p=212880","linkMd5ListStr":"094eb68c0334d38d4778736703d55357,094eb68c0334d38d4778736703d55357","githubUser":"myreaderx31","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"610.2 KB","destSize":"1.5 MB","compressRate":"256.6%"}],"successGithubMap":{"myreaderx6":1,"myreaderx21":1,"myreaderx33":1,"myreaderx22":1,"myreaderx31":1},"failGithubMap":{}}