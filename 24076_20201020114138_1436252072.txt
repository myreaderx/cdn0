{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-10-20 19:40:13","updatedTime":"2020-10-20 19:40:13","title":"Live-blogging SfN 2017","link":"http://blog.chrisbaldassano.com//2017/11/16/sfn","description":"<p>[I wrote these posts during the Society for Neuroscience 2017 meeting, as one of the Official Annual Meeting Bloggers. These blog posts originally appeared on SfN’s Neuronline platform.]</p> \n<h2 id=\"supereeg-ecog-data-breaks-free-from-electrodes\">SuperEEG: ECoG data breaks free from electrodes</h2> \n<p>The “gold standard” for measuring neural activity in human brains is ECoG (electrocorticography), using electrodes implanted directly onto the surface of the brain. Unlike methods that measure blood oxygenation (which have poor temporal resolution) or that measure signals on the scalp (which have poor spatial resolution), ECoG data has both high spatial and temporal precision. Most of the ECoG data that has been collected comes from patients who are being treated for epileptic seizures and have had electrodes implanted in order to determine where the seizures are starting.</p> \n<p>The big problem with ECoG data, however, is that each patient typically only has about 150 implanted electrodes, meaning that we can only measure brain activity in 150 spots (compared to about 100,000 spots for functional MRI). It would seem like there is no way around this - if you don’t measure activity from some part of the brain, then you can’t know anything about what is happening there, right?</p> \n<p>Actually, you can, or at least you can guess! <a href=\"http://www.context-lab.com/\">Lucy Owen, Andrew Heusser, and Jeremy Manning</a> have developed a new analysis tool called SuperEEG, based on the idea that measuring from one region of the brain can actually tell you a lot about another unmeasured region, if the two regions are highly correlated (or anti-correlated). By using many ECoG subjects to learn the correlation structure of the brain, we can extrapolate from measurements in a small set of electrodes to estimate neural activity across the whole brain.</p> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/SuperEEG.png\" alt=\"Super EEG\" /> Figure from <a href=\"https://app.box.com/s/j1w3985f3gc4t7pkb60rd9y5w7la210t\">their SfN poster</a></p> \n<p>This breaks ECoG data free from little islands of electrodes and allows us to carry out analyses across the brain. Not all brain regions can be well-estimated using this method (due to the typical placement locations of the electrodes and the correlation structure of brain activity), but it works surprisingly well for most of the cortex:</p> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/SuperEEG2.jpg\" alt=\"Super EEG2\" /></p> \n<p>This could also help with the original medical purpose of implanting these electrodes, by allowing doctors to track seizure activity in 3D as it spreads through the brain. It could even be used to help surgeons choose the locations where electrodes should be placed in new patients, to make sure that seizures can be tracked as broadly and accurately as possible.</p> \n<h2 id=\"hippocampal-subregions-growing-old-together\">Hippocampal subregions growing old together</h2> \n<p>To understand and remember our experiences, we need to think both big and small. We need to keep track of our spatial location at broad levels (“what town am I in?”) all the way down to precise levels (“what part of the room am I in?”). We need to keep track of time on scales from years to fractions of a second. We need to access our memories at both a coarse grain (“what do I usually bring to the beach?”) and a fine grain (“remember that time I forgot the sunscreen?”).</p> \n<p>Data from both rodents and humans has suggested that different parts of the hippocampus keep track of different levels of granularity, with posterior hippocampus focusing on the fine details and anterior hippocampus seeing the bigger picture. Iva Brunec and her co-authors recently <a href=\"https://www.biorxiv.org/content/early/2017/08/24/179655\">posted a preprint</a> showing that temporal and spatial correlations change along the long axis of the hippocampus - in anterior hippocampus all the voxels are similar to each other and change slowly over time, while in posterior hippocampus the voxels are more distinct from each other and change more quickly over time.</p> \n<p>In their latest work, they look at how these functional properties of the hippocampus change over the course of our lives. Surprisingly, this anterior-posterior distinction actually increases with age, becoming the most dramatic in the oldest subjects in their sample.</p> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/Iva1.png\" alt=\"Iva1\" /> The interaction between the two halves of the hippocampus also changes - while in young adults activity timecourses in the posterior and anterior hippocampus are uncorrelated, they start to become anti-correlated in older adults, perhaps suggesting that the complementary relationship between the two regions has started to break down. Also, their functional connectivity with the rest of the brain shifts over time, with posterior hippocampus decoupling from posterior medial regions and anterior hippocampus increasing its coupling to medial prefrontal regions.</p> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/Iva2.png\" alt=\"Iva2\" /> These results raise a number of intriguing questions about the cause of these shifts, and their impacts on cognition and memory throughout the lifespan. Is this shift toward greater coupling with regions that represent coarse-grained schematic information compensating for degeneration in regions that represent details? What is the “best” balance between coarse- and fine-timescale information for processing complex stimuli like movies and narratives, and at what age is it achieved? How do these regions mature before age 18, and how do their developmental trajectories vary across people? By following the analysis approach of Iva and her colleagues on new datasets, we should hopefully be able to answer many of these questions in future studies.</p> \n<h2 id=\"the-science-of-scientific-bias\">The Science of Scientific Bias</h2> \n<p>This year’s David Kopf lecture on Neuroethics was given by Dr. Jo Handelsman, entitled <a href=\"http://www.abstractsonline.com/pp8/#!/4376/presentation/1298\">“The Fallacy of Fairness: Diversity in Academic Science”</a>. Dr. Handelsman is a microbiologist who recently spent three years as the Associate Director for Science at the White House Office of Science and Technology Policy, and has also led some of the most well-known <a href=\"http://www.pnas.org/content/109/41/16474.abstract\">studies of gender bias in science</a>.</p> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/IMG_1255.JPG\" alt=\"IMG_1255.JPG\" /></p> \n<p>She began her talk by pointing out that increasing diversity in science is not only a moral obligation, but also has major potential benefits for scientific discovery. Diverse groups have been shown to produce more effective, innovative, and well-reasoned solutions to complex problems. I think this is especially true in psychology - if we are trying to create theories of how all humans think and act, we shouldn’t be building teams composed of a thin slice of humanity.</p> \n<p>Almost all scientists agree in principle that we should not be discriminating based on race or gender. However, the process of recruiting, mentoring, hiring, and promotion relies heavily on “gut feelings” and subtle social cues, which are highly susceptible to implicit bias. Dr. Handelsman covered a wide array of studies over the past several decades, ranging from observational analyses to randomized controlled trials of scientists making hiring decisions. I’ll just mention two of the studies she described which I found the most interesting:</p> \n<ul> \n <li> <p>How it is possible that people can be making biased decisions, but still think they were objective when they reflect on those decisions? A fascinating study by <a href=\"http://journals.sagepub.com/doi/abs/10.1111/j.0956-7976.2005.01559.x\">Uhlmann &amp; Cohen</a> showed that subjects rationalized biased hiring decisions after the fact by redefining their evaluation criteria. For example, when choosing whether to hire a male candidate or a female candidate, who both had (randomized) positive and negative aspects to their resumes, the subjects would decide that the positive aspects of the male candidate were the most important for the job and that he therefore deserved the position. This is interestingly similar to the way that <a href=\"https://fivethirtyeight.com/features/science-isnt-broken/\">p-hacking</a> distorts scientific results, and the solution to the problem may be the same. Just as <a href=\"https://cos.io/prereg/\">pre-registration</a> forces scientists to define their analyses ahead of time, Uhlmann &amp; Cohen showed that forcing subjects to commit to their importance criteria before seeing the applications eliminated the hiring bias.</p> </li> \n <li> <p>Even relatively simple training exercises can be effective in making people more aware of implicit bias. Dr. Handelsman and her colleagues created a set of short videos called <a href=\"https://academics.skidmore.edu/blogs/vids/\">VIDS (Video Interventions for Diversity in STEM)</a>, consisting of narrative films illustrating issues that have been studied in the implicit bias literature, along with expert videos describing the findings of these studies. They then ran <a href=\"https://academics.skidmore.edu/blogs/vids/files/2017/02/Pietrietal_2017.pdf\">multiple experiments</a> showing that these videos were effective at educating viewers, and made them more likely to notice biased behavior. I plan on making these videos required viewing in my lab, and would encourage everyone working in STEM to watch them as well (the narrative videos are only 30 minutes total).</p> </li> \n</ul> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/IMG_1261.JPG\" alt=\"IMG_1261.JPG\" /></p> \n<h2 id=\"drawing-out-visual-memories\">Drawing out visual memories</h2> \n<p>If you close your eyes and try to remember something you saw earlier today, what exactly do you see? Can you visualize the right things in the right places? Are there certain key objects that stand out the most? Are you misremembering things that weren’t really there?</p> \n<p>Visual memory for natural images has <a href=\"http://jov.arvojournals.org/article.aspx?articleid=2191865\">typically been studied</a> with recognition experiments, in which subjects have to recognize whether an image is one they have seen before or not. But recognition is quite different from freely recalling a memory (without being shown it again), and can involve different neural mechanisms. How can we study visual recall, testing whether the mental images people are recalling are correct?</p> \n<p>One way option is to have subjects give verbal descriptions of what they remember, but this might not capture all the details of their mental representation, such as the precise relationships between the objects or whether their imagined viewpoint of the scene is correct. Instead, NIMH researchers Elizabeth Hall, Wilma Bainbridge, and Chris Baker had subjects draw photographs from memory, and then analyzed the contents of those drawings.</p> \n<p><img src=\"http://blog.chrisbaldassano.com/blog/public/drawing.jpg\" alt=\"drawing.JPG\" /></p> \n<p>This is a creative but challenging approach, since it requires quantitatively characterizing how well the drawings (all 1,728!) match the original photographs. They crowdsource this task using Amazon Mechanical Turk, getting high-quality ratings that include: how well can the original photograph be identified based on the drawing, what objects were correctly drawn, what objects were falsely remembered as being in the image, and how close the objects were to their correct locations. There are also “control” drawings made by subjects with full information (that get to look at the image while they draw) or minimal information (just a category label) that were rated for comparison.</p> \n<p>The punchline is that subjects can remember many of the images, and produce surprisingly detailed drawings that are quite similar to those drawn by the control group that could look at the pictures. They reproduce the majority of the objects, place them in roughly the correct locations, and draw very few incorrect objects, making it very easy to match the drawings with the original photographs. The only systematic distortion is that the drawings depicted the scenes as being slightly farther away than they actually were, which nicely replicates previous results on <a href=\"http://www.scholarpedia.org/article/Boundary_extension\">boundary extension</a>.</p> \n<p>This is a neat task that subjects are remarkably good at (which is not always the case in memory experiments!), and could be a great tool for investigating the neural mechanisms of naturalistic perception and memory. Another <a href=\"http://www.abstractsonline.com/pp8/#!/4376/presentation/26803\">intriguing SfN presentation</a> showed that is possible to have subjects draw while in an fMRI scanner, allowing this paradigm to be used in neuroimaging experiments. I wonder if this approach could also be extended into drawing comic strips of remembered events that unfold over time, or to illustrate mental images based on stories told through audio or text.</p>","descriptionType":"html","feedId":24076,"bgimg":"http://blog.chrisbaldassano.com/blog/public/SuperEEG.png","linkMd5":"0f8ed2d3ecfeaaa16e75dfe2d746e211","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"author":"","articleImgCdnMap":{"http://blog.chrisbaldassano.com/blog/public/SuperEEG.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","http://blog.chrisbaldassano.com/blog/public/SuperEEG2.jpg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","http://blog.chrisbaldassano.com/blog/public/Iva1.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","http://blog.chrisbaldassano.com/blog/public/Iva2.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","http://blog.chrisbaldassano.com/blog/public/IMG_1255.JPG":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","http://blog.chrisbaldassano.com/blog/public/IMG_1261.JPG":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","http://blog.chrisbaldassano.com/blog/public/drawing.jpg":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg"},"publishedOrCreatedDate":1603194013784}],"record":{"createdTime":"2020-10-20 19:40:13","updatedTime":"2020-10-20 19:40:13","feedId":24076,"fetchDate":"Tue, 20 Oct 2020 11:40:13 +0000","fetchMs":476,"handleMs":120355,"totalMs":205725,"newArticles":0,"totalArticles":20,"status":1,"type":0,"ip":"8f63d61dafbb91e16a52716ac9e06b5a","hostName":"us-018*","requestId":"a3f3cfc9e0d34a76a841e30892c1e0d5_24076","contentType":"application/xml","totalBytes":0,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":7,"articlesImgsGithubTotal":1,"successGithubMap":{},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 02:32:11","updatedTime":"2020-09-07 02:32:11","id":24076,"name":"Rooting for the machines","url":"http://www.princeton.edu/~chrisb/blog/atom.xml","subscriber":137,"website":null,"icon":"http://blog.chrisbaldassano.com/favicon.ico","icon_jsdelivr":null,"description":"","weekly":null,"link":"http://blog.chrisbaldassano.com"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":0,"tmpBgImgCdnBytes":0,"extra4":{"start":1603193892949,"total":0,"statList":[{"spend":480,"msg":"获取xml内容"},{"spend":120355,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":60524,"msg":"正文链接上传到cdn"}]},"extra5":7,"extra6":7,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/SuperEEG.png","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":15236,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:22","host":"us-011*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211,0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/SuperEEG.png","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":662,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:37","host":"us-036*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211,0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/Iva1.png","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":502,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"europe67*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/IMG_1261.JPG","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":592,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"europe63*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/IMG_1261.JPG","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":271,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"us-006*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/IMG_1255.JPG","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":1227,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"us-003*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/IMG_1255.JPG","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":599,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:39","host":"europe-24*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/Iva1.png","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":1267,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"us-022*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/Iva2.png","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":15266,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"us-004*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/Iva2.png","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":3535,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:53","host":"europe-59*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},null,null,{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/drawing.jpg","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":212,"convertSpendMs":0,"createdTime":"2020-10-20 19:41:38","host":"us-007*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/SuperEEG2.jpg","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":314,"convertSpendMs":0,"createdTime":"2020-10-20 19:41:38","host":"us-026*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"}],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://europe-24.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-007.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://europe63.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-022.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-026.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-006.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-004.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://europe-59.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://europe67.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-003.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://us-51.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-029.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/SuperEEG.png","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":662,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:37","host":"us-036*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211,0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/IMG_1261.JPG","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":271,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"us-006*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/IMG_1255.JPG","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":599,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:39","host":"europe-24*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/Iva1.png","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":1267,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:38","host":"us-022*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/Iva2.png","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":3535,"convertSpendMs":0,"createdTime":"2020-10-20 19:40:53","host":"europe-59*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/drawing.jpg","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":212,"convertSpendMs":0,"createdTime":"2020-10-20 19:41:38","host":"us-007*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://blog.chrisbaldassano.com/blog/public/SuperEEG2.jpg","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":24076,"totalSpendMs":314,"convertSpendMs":0,"createdTime":"2020-10-20 19:41:38","host":"us-026*","referer":"http://blog.chrisbaldassano.com//2017/11/16/sfn","linkMd5ListStr":"0f8ed2d3ecfeaaa16e75dfe2d746e211","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"}],"successGithubMap":{},"failGithubMap":{}}