{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-11-16 19:11:50","updatedTime":"2020-11-16 19:11:50","title":"Estimating the Impact of Training Data with Reinforcement Learning","link":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","description":"<span class=\"byline-author\">Posted by Jinsung Yoon and Sercan O. Arik, Research Scientists, Cloud AI Team, Google Research</span> <p><a href=\"https://arxiv.org/pdf/1812.05159.pdf\">Recent work</a> suggests that not all data samples are equally useful for training, particularly for <a href=\"https://en.wikipedia.org/wiki/Deep_learning\">deep neural networks</a> (DNNs). Indeed, if a dataset contains low-quality or incorrectly labeled data, one can often <a href=\"https://ieeexplore.ieee.org/document/6685834\">improve performance</a> by removing a significant portion of training samples.  Moreover, in cases where there is a mismatch between the train and test datasets (e.g., due to difference in train and test location or time), one can also achieve higher performance by carefully <a href=\"https://arxiv.org/pdf/1908.11406.pdf\">restricting samples</a> in the training set to those most relevant for the test scenario.  Because of the ubiquity of these scenarios, accurately quantifying the values of training samples has great potential for improving model performance on real-world datasets.</p><div class=\"separator\" style=\"clear: both; text-align: center;\"></div><br /><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody>  <tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HP-q7XgPDpY/X5igB_g49dI/AAAAAAAAGuU/Hwgh1wcRQF8kqrj8AVKhn3T8-jAfn_ttACLcBGAsYHQ/s1500/image2.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"1228\" data-original-width=\"1500\" height=\"328\" src=\"https://1.bp.blogspot.com/-HP-q7XgPDpY/X5igB_g49dI/AAAAAAAAGuU/Hwgh1wcRQF8kqrj8AVKhn3T8-jAfn_ttACLcBGAsYHQ/w400-h328/image2.png\" width=\"400\" /></a></td></tr>  <tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-O4u8daEoaOM/X5igG8VgrjI/AAAAAAAAGuY/fO6iSSo4auk5e3rAV2wUTQb0mW_ootKaACLcBGAsYHQ/s1564/image4.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"764\" data-original-width=\"1564\" height=\"195\" src=\"https://1.bp.blogspot.com/-O4u8daEoaOM/X5igG8VgrjI/AAAAAAAAGuY/fO6iSSo4auk5e3rAV2wUTQb0mW_ootKaACLcBGAsYHQ/w400-h195/image4.png\" width=\"400\" /></a></td></tr>  <tr><td class=\"tr-caption\" style=\"text-align: center;\"><b>Top:</b> Examples of low-quality samples (noisy/crowd-sourced); <b>Bottom:</b> Examples of a train and test mismatch.</td></tr></tbody></table><p>  In addition to improving model performance, assigning a quality value to individual data can also enable new use cases. It can be used to suggest better practices for data collection, e.g., what kinds of additional data would benefit the most, and can be used to construct large-scale training datasets more efficiently, e.g., by web searching using the labels as keywords and filtering out less valuable data.</p><p>In “<a href=\"https://proceedings.icml.cc/static/paper_files/icml/2020/3003-Paper.pdf\">Data Valuation Using Deep Reinforcement Learning</a>”, accepted at <a href=\"https://icml.cc/Conferences/2020\">ICML 2020</a>, we address the challenge of quantifying the value of training data using a novel approach based on meta-learning. Our method integrates data valuation into the training procedure of a predictor model that learns to recognize samples that are more valuable for the given task, improving both predictor and data valuation performance. We have also launched four <a href=\"https://cloud.google.com/ai-hub\">AI Hub</a> Notebooks that exemplify the use cases of DVRL and are designed to be conveniently adapted to other tasks and datasets, such as&nbsp;<a href=\"https://aihub.cloud.google.com/u/0/p/products%2F41419f02-f231-4b9c-b4fb-23ed9d5adc73\">domain adaptation</a>,&nbsp;<a href=\"https://aihub.cloud.google.com/u/0/p/products%2Fcb6b588c-1582-4868-a944-dc70ebe61a36\">corrupted sample discovery and robust learning</a>,&nbsp;<a href=\"https://aihub.cloud.google.com/u/0/p/products%2Fe1866581-7bf2-4550-b25d-4e129bab62ff\">transfer learning on image data</a>&nbsp;and&nbsp;<a href=\"https://aihub.cloud.google.com/u/0/p/products%2F92a5f181-b079-4ee5-8203-1f031033eb58\">data valuation</a>.</p><p><b>Quantifying the Value of Data</b><br />Not all data are equal for a given ML model — some have greater relevance for the task at hand or are more rich in informative content than others. So how does one evaluate the value of a single datum? At the granularity of a full dataset, it is straightforward; one can simply train a model on the entire dataset and use its performance on a test set as its value. However, estimating the value of <em>a single datum</em> is far more difficult, especially for complex models that rely on large-scale datasets, because it is computationally infeasible to re-train and re-evaluate a model on all possible subsets.  </p><p>To tackle this, researchers have explored permutation-based methods (e.g., <a href=\"https://arxiv.org/pdf/1703.04730.pdf\">influence functions</a>), and game theory-based methods (e.g., <a href=\"https://arxiv.org/pdf/1904.02868.pdf\">data Shapley</a>). However, even the best current methods are far from being computationally feasible for large datasets and complex models, and their data valuation performance is limited. Concurrently, <a href=\"https://arxiv.org/pdf/1803.09050.pdf\">meta learning-based adaptive weight assignment approaches</a> have been developed to estimate the weight values using a meta-objective. But rather than prioritizing learning from high value data samples, their data value mapping is typically based on gradient descent learning or other heuristic approaches that alter the conventional predictor model training dynamics, which can result in performance changes that are unrelated to the value of individual data points. </p><p><b>Data Valuation Using Reinforcement Learning (DVRL)</b><br />To infer the data values, we propose a <em>data value estimator</em> (DVE) that estimates data values and selects the most valuable samples to train the predictor model. This selection operation is fundamentally non-differentiable and thus conventional <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">gradient descent</a>-based methods cannot be used. Instead, we propose to use <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">reinforcement learning</a> (RL) such that the supervision of the DVE is based on a reward that quantifies the predictor performance on a small (but clean) validation set. The reward guides the optimization of the policy towards the action of optimal data valuation, given the state and input samples. Here, we treat the predictor model learning and evaluation framework as the environment, a novel application scenario of RL-assisted machine learning.</p><table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-P-efmED-fLs/X5igoGhF8qI/AAAAAAAAGuk/BAeekfQuMYIF97yPaJbM2yxcc0Y9v6uaQCLcBGAsYHQ/s1600/image6.gif\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"522\" data-original-width=\"1600\" src=\"https://1.bp.blogspot.com/-P-efmED-fLs/X5igoGhF8qI/AAAAAAAAGuk/BAeekfQuMYIF97yPaJbM2yxcc0Y9v6uaQCLcBGAsYHQ/s16000/image6.gif\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Training with Data Value Estimation using Reinforcement Learning (DVRL). When training the data value estimator with an accuracy reward, the most valuable samples (denoted with green dots) are used more and more, whereas the least valuable samples (red dots) are used less frequently.</td></tr></tbody></table><p><b>Results</b><br />We evaluate the data value estimation quality of DVRL on multiple types of datasets and use cases.  </p><!--<ol type=\"i\">--><ul>  <li><em>Model performance after removing high/low value samples</em><br />Removing low value samples from the training dataset can improve the predictor model performance, especially in the cases where the training dataset contains corrupted samples. On the other hand, removing high value samples, especially if the dataset is small, decreases the performance significantly. Overall, the performance after removing high/low value samples is a strong indicator for the quality of data valuation. <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HhsQ5eM7nW4/X5iiKz96THI/AAAAAAAAGu0/tjwrtPXQFsQ-G0gU1FWefSvjTgQHatp5wCLcBGAsYHQ/s1317/DVRL.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"756\" data-original-width=\"1317\" src=\"https://1.bp.blogspot.com/-HhsQ5eM7nW4/X5iiKz96THI/AAAAAAAAGu0/tjwrtPXQFsQ-G0gU1FWefSvjTgQHatp5wCLcBGAsYHQ/s16000/DVRL.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Accuracy with the removal of most and least valuable samples, where 20% of the labels are noisy by design. By removing such noisy labels as the least valuable samples, a high-quality data valuation method achieves better accuracy. We demonstrate that DVRL outperforms other methods significantly from this perspective.</td></tr></tbody></table>DVRL shows the <em>fastest</em> performance degradation after removing the <em>most important</em> samples and the <em>slowest</em> performance degradation after removing the <em>least important</em> samples in most cases, underlining the superiority of DVRL in identifying noisy labels compared to competing methods (<a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation\">Leave-One-Out</a> and Data Shapley).<br /><br /></li><li><em>Robust learning with noisy labels</em><br />We consider how reliably DVRL can learn with noisy data in an end-to-end way, without removing the low-value samples. Ideally, noisy samples should get low data values as DVRL converges and a high performance model would be returned. <table align=\"center\" cellpadding=\"0\" cellspacing=\"0\" class=\"tr-caption-container\" style=\"margin-left: auto; margin-right: auto;\"><tbody><tr><td style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-4ddPjHJ_rTs/X5iiWn7sswI/AAAAAAAAGu4/KfJ4viQlBcsOAQWZ0grrOo1n8Fg1XIx8QCLcBGAsYHQ/s1372/image5.png\" style=\"margin-left: auto; margin-right: auto;\"><img border=\"0\" data-original-height=\"462\" data-original-width=\"1372\" src=\"https://1.bp.blogspot.com/-4ddPjHJ_rTs/X5iiWn7sswI/AAAAAAAAGu4/KfJ4viQlBcsOAQWZ0grrOo1n8Fg1XIx8QCLcBGAsYHQ/s16000/image5.png\" /></a></td></tr><tr><td class=\"tr-caption\" style=\"text-align: center;\">Robust learning with noisy labels. Test accuracy for ResNet-32 and <a href=\"https://arxiv.org/pdf/1605.07146.pdf\">WideResNet-28-10</a> on <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 and CIFAR-100</a> datasets with 40% of uniform random noise on labels. DVRL outperforms other popular methods that are based on meta-learning.</td></tr></tbody></table>We show state-of-the-art results with DVRL in <em>minimizing the impact of noisy labels</em>. These also demonstrate that DVRL can scale to complex models and large-scale datasets.<br /><br />  </li><li><em>Domain adaptation</em><br />We consider the scenario where the training dataset comes from a substantially different distribution from the validation and testing datasets. Data valuation is expected to be beneficial for this task by selecting the samples from the training dataset that best match the distribution of the validation dataset. We focus on the three cases: (1) a training set based on image search results (low-quality web-scraped) applied to the task of predicting skin lesion classification using <a href=\"https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000\">HAM 10000</a> data (high-quality medical); (2) an MNIST training set for a digit recognition task on <a href=\"https://ieeexplore.ieee.org/document/291440\">USPS data</a> (different visual domain); (3)  <a href=\"http://www2.aueb.gr/users/ion/docs/ceas2006_paper.pdf\">e-mail spam data</a> to detect spam applied to an <a href=\"https://www.researchgate.net/publication/258514273_Towards_SMS_Spam_Filtering_Results_under_a_New_Dataset\">SMS dataset</a> (different task). DVRL yields significant improvements for domain adaptation, by jointly optimizing the data valuator and corresponding predictor model. <div style=\"text-align: center;\"><a href=\"https://1.bp.blogspot.com/-HQ-9kwQgd4Q/X5iil8tf1UI/AAAAAAAAGvA/9iNreSnUg146--hU8KpzDxqvikm7typtgCLcBGAsYHQ/s1999/image3.png\"><img border=\"0\" data-original-height=\"319\" data-original-width=\"1999\" src=\"https://1.bp.blogspot.com/-HQ-9kwQgd4Q/X5iil8tf1UI/AAAAAAAAGvA/9iNreSnUg146--hU8KpzDxqvikm7typtgCLcBGAsYHQ/s16000/image3.png\" /></a></div>  </li></ul><!--</ol>--><p><b>Conclusions</b><br />We propose a novel meta learning framework for data valuation which determines how likely each training sample will be used in training of the predictor model. Unlike previous works, our method integrates data valuation into the training procedure of the predictor model, allowing the predictor and DVE to improve each other's performance. We model this data value estimation task using a DNN trained through RL with a reward obtained from a small validation set that represents the target task performance.  In a computationally-efficient way, DVRL can provide high quality ranking of training data that is useful for domain adaptation, corrupted sample discovery and robust learning.  We show that DVRL significantly outperforms alternative methods on diverse types of tasks and datasets. </p><p><b>Acknowledgements</b><br /><em>We gratefully acknowledge the contributions of Tomas Pfister.</em></p><div class=\"feedflare\">\n<a href=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?a=fUirh6EaOcs:GMDkqm7BvoA:yIl2AUoC8zA\"><img src=\"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA\" border=\"0\"></img></a>\n</div><img src=\"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fUirh6EaOcs\" height=\"1\" width=\"1\" alt=\"\"/>","descriptionType":"html","publishedDate":"Wed, 28 Oct 2020 19:38:00 +0000","feedId":16545,"bgimg":"https://1.bp.blogspot.com/-HP-q7XgPDpY/X5igB_g49dI/AAAAAAAAGuU/Hwgh1wcRQF8kqrj8AVKhn3T8-jAfn_ttACLcBGAsYHQ/w400-h328/image2.png","linkMd5":"48f3ac69921fde7c726a3bb02b869ccd","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn85@2020_5/2020/11/16/11-11-50-629_b146b9b1da65f6e0.webp","destWidth":400,"destHeight":327,"sourceBytes":156599,"destBytes":17080,"author":"Google AI","articleImgCdnMap":{"https://1.bp.blogspot.com/-HP-q7XgPDpY/X5igB_g49dI/AAAAAAAAGuU/Hwgh1wcRQF8kqrj8AVKhn3T8-jAfn_ttACLcBGAsYHQ/w400-h328/image2.png":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn85@2020_5/2020/11/16/11-11-50-629_b146b9b1da65f6e0.webp","https://1.bp.blogspot.com/-O4u8daEoaOM/X5igG8VgrjI/AAAAAAAAGuY/fO6iSSo4auk5e3rAV2wUTQb0mW_ootKaACLcBGAsYHQ/w400-h195/image4.png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn10@2020_3/2020/11/16/11-11-51-790_f02ce875d60ab4bd.webp","https://1.bp.blogspot.com/-P-efmED-fLs/X5igoGhF8qI/AAAAAAAAGuk/BAeekfQuMYIF97yPaJbM2yxcc0Y9v6uaQCLcBGAsYHQ/s16000/image6.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn12@2020_1/2020/11/16/11-12-45-979_f00edaee8912e7d7.webp","https://1.bp.blogspot.com/-HhsQ5eM7nW4/X5iiKz96THI/AAAAAAAAGu0/tjwrtPXQFsQ-G0gU1FWefSvjTgQHatp5wCLcBGAsYHQ/s16000/DVRL.png":"https://cdn.jsdelivr.net/gh/myreaderx/cdn94@2020_6/2020/11/16/11-11-51-843_a47d56fdb5f21a1b.webp","https://1.bp.blogspot.com/-4ddPjHJ_rTs/X5iiWn7sswI/AAAAAAAAGu4/KfJ4viQlBcsOAQWZ0grrOo1n8Fg1XIx8QCLcBGAsYHQ/s16000/image5.png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn5@2020_6/2020/11/16/11-11-51-880_0e33adcf54e85360.webp","https://1.bp.blogspot.com/-HQ-9kwQgd4Q/X5iil8tf1UI/AAAAAAAAGvA/9iNreSnUg146--hU8KpzDxqvikm7typtgCLcBGAsYHQ/s16000/image3.png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_5/2020/11/16/11-11-52-214_ac7b35d1075cefe9.webp","http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn89@2020_6/2020/11/16/11-11-51-795_483d6fcb94af4f84.webp","http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fUirh6EaOcs":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn98@2020_2/2020/11/16/11-11-51-771_20467bbaffe3f73f.webp"},"publishedOrCreatedDate":1605525110216}],"record":{"createdTime":"2020-11-16 19:11:50","updatedTime":"2020-11-16 19:11:50","feedId":16545,"fetchDate":"Mon, 16 Nov 2020 11:11:50 +0000","fetchMs":1243,"handleMs":43,"totalMs":59014,"newArticles":0,"totalArticles":25,"status":1,"type":0,"ip":"13ceeb0da42989d3de6751ce913d735f","hostName":"us-011*","requestId":"e5be40367c3e4f4383a5ebf370892bbf_16545","contentType":"text/xml; charset=UTF-8","totalBytes":1947570,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":8,"articlesImgsGithubTotal":8,"successGithubMap":{"myreaderx25":1,"myreaderx32":1,"myreaderx33":1,"myreaderx12":1,"myreaderx24":1,"myreaderx1":1,"myreaderx30":1,"myreaderx":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 02:13:05","updatedTime":"2020-09-07 12:57:04","id":16545,"name":"Google AI Blog","url":"http://googleaiblog.blogspot.com/atom.xml","subscriber":202,"website":null,"icon":"http://ai.googleblog.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx63/cdn58@2020_2/2020/09/07/04-56-49-674_40612c2a706c05a6.ico","description":"The latest news from Google AI.","weekly":null,"link":null},"noPictureArticleList":[],"tmpCommonImgCdnBytes":17080,"tmpBodyImgCdnBytes":1930490,"tmpBgImgCdnBytes":0,"extra4":{"start":1605525108904,"total":0,"statList":[{"spend":1274,"msg":"获取xml内容"},{"spend":43,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":56199,"msg":"正文链接上传到cdn"}]},"extra5":8,"extra6":8,"extra7ImgCdnFailResultVector":[],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-032.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-031.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe66.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-016.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HP-q7XgPDpY/X5igB_g49dI/AAAAAAAAGuU/Hwgh1wcRQF8kqrj8AVKhn3T8-jAfn_ttACLcBGAsYHQ/w400-h328/image2.png","sourceStatusCode":200,"destWidth":400,"destHeight":327,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn85@2020_5/2020/11/16/11-11-50-629_b146b9b1da65f6e0.webp","sourceBytes":156599,"destBytes":17080,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":1352,"convertSpendMs":11,"createdTime":"2020-11-16 19:11:50","host":"europe62*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd,48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"152.9 KB","destSize":"16.7 KB","compressRate":"10.9%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-O4u8daEoaOM/X5igG8VgrjI/AAAAAAAAGuY/fO6iSSo4auk5e3rAV2wUTQb0mW_ootKaACLcBGAsYHQ/w400-h195/image4.png","sourceStatusCode":200,"destWidth":400,"destHeight":195,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn10@2020_3/2020/11/16/11-11-51-790_f02ce875d60ab4bd.webp","sourceBytes":90813,"destBytes":17518,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":876,"convertSpendMs":8,"createdTime":"2020-11-16 19:11:51","host":"us-032*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"88.7 KB","destSize":"17.1 KB","compressRate":"19.3%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~r/blogspot/gJZg/~4/fUirh6EaOcs","sourceStatusCode":200,"destWidth":1,"destHeight":1,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx30/cdn98@2020_2/2020/11/16/11-11-51-771_20467bbaffe3f73f.webp","sourceBytes":43,"destBytes":72,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":891,"convertSpendMs":5,"createdTime":"2020-11-16 19:11:51","host":"us-016*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx30","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"43 B","destSize":"72 B","compressRate":"167.4%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-4ddPjHJ_rTs/X5iiWn7sswI/AAAAAAAAGu4/KfJ4viQlBcsOAQWZ0grrOo1n8Fg1XIx8QCLcBGAsYHQ/s16000/image5.png","sourceStatusCode":200,"destWidth":1372,"destHeight":462,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn5@2020_6/2020/11/16/11-11-51-880_0e33adcf54e85360.webp","sourceBytes":32194,"destBytes":24174,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":1013,"convertSpendMs":23,"createdTime":"2020-11-16 19:11:51","host":"europe66*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"31.4 KB","destSize":"23.6 KB","compressRate":"75.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HhsQ5eM7nW4/X5iiKz96THI/AAAAAAAAGu0/tjwrtPXQFsQ-G0gU1FWefSvjTgQHatp5wCLcBGAsYHQ/s16000/DVRL.png","sourceStatusCode":200,"destWidth":1317,"destHeight":756,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn94@2020_6/2020/11/16/11-11-51-843_a47d56fdb5f21a1b.webp","sourceBytes":298025,"destBytes":65584,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":1168,"convertSpendMs":52,"createdTime":"2020-11-16 19:11:51","host":"us-028*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"291 KB","destSize":"64 KB","compressRate":"22%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-HQ-9kwQgd4Q/X5iil8tf1UI/AAAAAAAAGvA/9iNreSnUg146--hU8KpzDxqvikm7typtgCLcBGAsYHQ/s16000/image3.png","sourceStatusCode":200,"destWidth":1999,"destHeight":319,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn1@2020_5/2020/11/16/11-11-52-214_ac7b35d1075cefe9.webp","sourceBytes":152322,"destBytes":50242,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":1229,"convertSpendMs":44,"createdTime":"2020-11-16 19:11:51","host":"us-003*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"148.8 KB","destSize":"49.1 KB","compressRate":"33%"},{"code":1,"isDone":false,"source":"http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA","sourceStatusCode":200,"destWidth":62,"destHeight":24,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn89@2020_6/2020/11/16/11-11-51-795_483d6fcb94af4f84.webp","sourceBytes":997,"destBytes":310,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":1552,"convertSpendMs":4,"createdTime":"2020-11-16 19:11:51","host":"us-040*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"997 B","destSize":"310 B","compressRate":"31.1%"},{"code":1,"isDone":false,"source":"https://1.bp.blogspot.com/-P-efmED-fLs/X5igoGhF8qI/AAAAAAAAGuk/BAeekfQuMYIF97yPaJbM2yxcc0Y9v6uaQCLcBGAsYHQ/s16000/image6.gif","sourceStatusCode":200,"destWidth":1600,"destHeight":522,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn12@2020_1/2020/11/16/11-12-45-979_f00edaee8912e7d7.webp","sourceBytes":4362168,"destBytes":1772590,"targetWebpQuality":75,"feedId":16545,"totalSpendMs":55703,"convertSpendMs":54007,"createdTime":"2020-11-16 19:11:51","host":"us-031*","referer":"http://feedproxy.google.com/~r/blogspot/gJZg/~3/fUirh6EaOcs/estimating-impact-of-training-data-with.html","linkMd5ListStr":"48f3ac69921fde7c726a3bb02b869ccd","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"4.2 MB","destSize":"1.7 MB","compressRate":"40.6%"}],"successGithubMap":{"myreaderx25":1,"myreaderx32":1,"myreaderx33":1,"myreaderx12":1,"myreaderx24":1,"myreaderx1":1,"myreaderx30":1,"myreaderx":1},"failGithubMap":{}}