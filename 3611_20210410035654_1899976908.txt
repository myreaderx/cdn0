{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-04-10 11:53:31","updatedTime":"2021-04-10 11:53:31","title":"Designer-centered reinforcement learning","link":"https://www.microsoft.com/en-us/research/?p=726529","description":"</p>\n<figure class=\"wp-block-image alignwide size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/1400x788_Athens_no_logo-2.gif\" alt=\"An animation titled “Adding style to RL agent navigation” plays through three versions of agent behavior in a navigation task. Each shows the agent, represented by a robot icon, moving around two walls to reach a goal, represented by a trophy icon. The first version applies only a task reward, and the agent path to the goal is too close to the wall. The second applies an excess shaping reward, and the path is too far from the wall. The third balances reward components, and the path is more central. \"/></figure>\n</p>\n<p>In video games, nonplayer characters, bots, and other game agents help bring a digital world and its story to life. They can help make the mission of saving humanity feel urgent, transform every turn of a corner into a gamer’s potential demise, and intensify the rush of driving behind the wheel of a super-fast race car. These agents are meticulously designed and preprogrammed to contribute to an immersive player experience.</p>\n</p>\n<table style=\"float: right; width: 50%; margin: 15px; text-align: center; border: 1px solid #000000; border-collapse: collapse; border-spacing: inherit;\">\n<tbody>\n<tr style=\"height: 24px;\">\n<td style=\"background-color: #000000; padding: 5px 30px; border: inherit; height: 24px;\"><span style=\"color: #ffffff;\"><strong>Join Us</strong></span></td>\n</tr>\n<tr style=\"height: 23px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 23px;\"> This work was undertaken during an internship at Microsoft Research Cambridge. If you’re interested in exploring similar real-world challenges and developing actionable user-focused solutions, visit the lab’s <a href=\"https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/internships/?\"> internship page</a> for details on internships in deep RL for games and other research areas. </td>\n</tr>\n<tr style=\"height: 23px;\">\n<td style=\"padding: 5px 30px; border: inherit; height: 23px;\">For insights into AI and gaming research, register for the <a href=\"https://www.microsoft.com/en-us/research/event/aiandgaming2020/\">Microsoft AI and Gaming Research Summit 2021 (February 23–24)</a>, and for career opportunities in RL, check out the <a href=\"https://www.microsoft.com/en-us/research/theme/machine-intelligence/#!opportunities\">open positions with the Machine Intelligence theme at Microsoft Research Cambridge</a> and other<a href=\"https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/#!opportunities\"> opportunities across Microsoft Research. </a></td>\n</tr>\n</tbody>\n</table>\n<p>Now, what if these same agents could learn to behave in lifelike and interesting ways <em>without </em>a developer having to hardcode every possible natural behavior in each scenario? Imagine agents in an action game learning a variety of offensive strategies to challenge a protagonist or agents in an adventure game learning how to support the player in unlocking information about an unfamiliar environment. Reinforcement learning (RL), in which agents learn how to act when they must sequentially take actions over time, provides a framework for achieving that. Through RL, agents can be trained to devise their <em>own </em>solutions to tasks, transforming the role of game designers from defining behavior to defining tasks and letting the agents learn. Such a shift has the potential to lead to surprising responses, possibly ones a game designer may not have even imagined, helping to create more engaging characters and worlds.</p>\n</p>\n<div class=\"annotations\" data-bi-area=\"margin-callout\">\n<ul class=\"annotations__list annotations__list--right\">\n<li class=\"annotations__list-item\">\n\t\t\t\t\t\t\t<a href=\"https://www.microsoft.com/en-us/research/project/project-paidia/\" data-bi-type=\"annotated-link\" data-bi-name=\"Project Paidia: a Microsoft Research & Ninja Theory Collaboration\" class=\"annotations__list-thumbnail\" ><br />\n\t\t\t\t\t<img width=\"300\" height=\"129\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450-300x129.png\" class=\"attachment-medium size-medium\" alt=\"Project Paidia - game intelligence round robot character\" loading=\"lazy\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450-300x129.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450-1024x441.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450-768x331.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450.png 1044w\" sizes=\"(max-width: 300px) 100vw, 300px\" />\t\t\t\t</a><br />\n\t\t\t\t\t\t\t<span class=\"annotations__type\">Project </span><br />\n\t\t\t<a href=\"https://www.microsoft.com/en-us/research/project/project-paidia/\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Project Paidia: a Microsoft Research & Ninja Theory Collaboration\"><br />\n\t\t\t\tProject Paidia: a Microsoft Research & Ninja Theory Collaboration\t\t\t</a><br />\n\t\t\t<span class=\"svg-icon icon-chevron-right-med-blue\"></span>\n\t\t\t\t\t</li>\n</ul>\n</div>\n</p>\n<p>Reinforcement learning is already showing promising results. For example, we’ve demonstrated <a href=\"https://www.youtube.com/watch?v=dcngdjfhGXI&feature=youtu.be\">agents’ ability to effectively collaborate with each other in the Ninja Theory game<em> Bleeding Edg</em>e</a> as part of the <a href=\"https://www.microsoft.com/en-us/research/project/project-paidia/\">Project Paidia</a> research collaboration, which ultimately seeks to enable teamwork between agents and human players (for an RL overview, visit our <a href=\"https://innovation.microsoft.com/en-us/exploring-project-paidia\">Project Paidia website and interactive experience</a>). At the same time, many experts feel the use of RL in the commercial game industry is still far below its ultimate potential. The reasons why are numerous, including the need for a certain level of expertise to execute the technology. From our previous <a href=\"https://www.microsoft.com/en-us/research/publication/its-unwieldy-and-it-takes-a-lot-of-time-challenges-and-opportunities-for-creating-agents-in-commercial-games/\">research into the experiences of game agent creators</a>, we’ve come to realize that for RL techniques to be used in the game industry, we need to design them with potential users and their existing workflows and requirements in mind. In recent work, we focus on three specific challenges:</p>\n</p>\n<ul>\n<li>exercising authorial control when it comes to specifying the aesthetic style of game agents</li>\n<li>balancing multiple design constraints, specifically task completion and behaving in a desired style</li>\n<li>developing RL tools and infrastructure that are more meaningful from a designer perspective, allowing designers to make desired changes without formal engineering training</li>\n</ul>\n</p>\n<p>In this work, we establish the first steps toward a <em>designer-centered approach to RL</em>, making it easier for designers to specify an agent’s style through preference learning, automatically, robustly combined reward signals that satisfy varied design constraints, and a contextually meaningful workflow.</p>\n</p>\n<p>We show our results in a navigation task, as navigation is one of the most fundamental agent capabilities. In our experiments, we start with an agent that&#8217;s rewarded for getting closer to a goal as quickly as possible—in our case, a blue circle behind two “walls.” This results in the agent learning to take the shortest path to the blue circle, leading the agent to bump into and drag along the walls on its way. In this exploration, we assume the role of a designer aiming for movements more reflective of how a human player might approach the challenge, by taking a more central path.</p>\n</p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"500\" height=\"500\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video1_RlGamingBlog.gif\" alt=\"A video in which an agent, represented by a red circle, moves around two rectangular walls to reach its goal, represented by a smaller blue circle. A black dotted line shows the agent’s path, which runs closely along the walls.\" class=\"wp-image-726562\"/><figcaption>Video 1: In a navigation task like the above, it’s common for agents to drag along the walls, especially if they’re rewarded for getting closer to the goal (the blue circle). We want to tune this agent so that it doesn’t hug the walls as much and behaves in a way that’s more reflective of how a human player might achieve the task, via a more central path.</figcaption></figure>\n</div>\n</p>\n<h2>Preference&#160;learning as a method to specify&#160;style&#160;rewards&#160;&#160;</h2>\n</p>\n<p>RL algorithms learn through a reward function. Unfortunately, it’s very difficult to computationally specify aesthetic style. If we’re building a stealth game, we might want our agents to creep near building edges, but if we’re making a game about cyborg warriors, we would much prefer them charging through a scene. It’s unclear, though, how one might go about writing an RL <em>style reward</em> for being “stealthy” or “boisterous.” Even if it were, the designers who decide and tweak a game’s aesthetic aspects are often separate from the engineers who implement the underlying AI behavior, requiring that designers become proficient enough in RL to adjust the AI codebase to achieve a desired style. Such an expectation is unrealistic in larger teams and impractical with most designer workflows used today.</p>\n</p>\n<figure class=\"wp-block-video alignwide\"><video controls src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/BleedingEdge-Composite-Shot.mp4\"></video><figcaption>Video 2: In this human-controlled demonstration (exaggerated to illustrate how an RL agent would behave), the character grinding along the wall while moving to its target in<em> Bleeding Edge</em> looks jarring. It’s not representative of how a human player would likely move, nor does this type of behavior make sense for the aesthetic style of the game. From the perspective of an RL agent, however, there wouldn’t be any problem! The agent would take the quickest path to a given goal. (This video is for demonstration purposes only. Does not represent the agents used in <em>Bleeding Edge </em>or Project Paidia. Not representative of final game gameplay or visuals.)</figcaption></figure>\n</p>\n<p>The traditional method to achieve our goal of reaching the blue circle as quickly as possible while taking a more central path would be to write an extension to the reward function, also known as reward shaping. We can punish the agent for being too close to the walls while still rewarding it for reaching the goal. However, even with extensive experimentation, it’s difficult to attain a behavior that’s exactly what we want, as what we really want isn’t simply “staying away from the walls” but a more nuanced style of movement that’s hard to capture mathematically.</p>\n</p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"500\" height=\"500\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video3_RLGamingBlog.gif\" alt=\"A video in which an agent, represented by a red circle, moves around two rectangular walls to reach its goal, represented by a smaller blue circle. A black dotted line shows the agent’s path, which runs far from the walls.  \" class=\"wp-image-726568\"/><figcaption>Video 3: To achieve a more central path with a traditional reward shaping approach, the agent receives a penalty for being too close to the walls in addition to the original task reward. However, this can result in another type of undesirable behavior if the shaping reward is weighted too heavily as in this video. Consequently, finding the right balance between task and style rewards isn’t a simple task.</figcaption></figure>\n</div>\n</p>\n<p>It&#160;would be&#160;much&#160;easier&#160;to&#160;recognize a desired style&#160;as opposed&#160;to&#160;describing it&#160;mathematically.&#160;Because of&#160;this,&#160;we implemented a&#160;<a href=\"https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">preference-based&#160;learning method</a>&#160;to allow designers to specify their desired style&#160;through a simple&#160;user&#160;interface—no coding required!&#160;</p>\n</p>\n<p>Our proposed&#160;method works as follows:&#160;</p>\n</p>\n<ol>\n<li>The policy, pretrained on the task reward,&#160;interacts with the environment and produces a set of trajectories.</li>\n<li>The designer is shown segments of these trajectories,&#160;and they pick which segment is closer to their desired style.&#160;</li>\n<li>A reward&#160;network tasked with capturing the style is updated&#160;according to the designer’s&#160;preferences.</li>\n<li>The reward network predicts how much a state exhibits the learned style. This predicted style reward plus the original task reward is used to optimize the original policy.</li>\n<li>A new set of trajectories is collected with the updated policy to get new preferences.</li>\n</ol>\n</p>\n<p>Because RL training takes time, we fine-tune an already competent agent, cutting the amount of iteration down drastically compared to training an agent from scratch; all we need to do is add style to it. Further, this makes it easier to apply different styles to the same base agent, allowing AI engineers to train a base model and then designers to fine-tune style preferences.</p>\n</p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"628\" height=\"428\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_Video4.gif\" alt=\"A video demonstration of the prototype user interface. Two agent trajectories appear side by side. In each, the agent, represented by a blue circle, displays different styles of movement and behavior in making its way to the target, represented by a smaller red circle. At the bottom right, a hypothetical designer, represented by the movement of a cursor, selects their preference from three choices: left, even, and right.  \" class=\"wp-image-726595\"/><figcaption>Video 4: Describing an aesthetic style mathematically is difficult. Our prototype user interface allows designers to choose the behavior that is closest to their desired style from a series of trajectories generated by the agent interacting with its environment.</figcaption></figure>\n</div>\n</p>\n<p>In a feedback efficiency study, we show we can reliably train a successful agent in our given task. For our task, training is only considered successful if the agent completes the task with a high enough style reward (measured by the mean distance from the nearest wall) and an acceptable task reward (an approximate equivalent to the time it takes to reach the goal). We reached our desired behavior in 50 comparisons. We expect the number of required preferences to increase as the complexity of the environment and of the desired style goes up.</p>\n</p>\n<figure class=\"wp-block-gallery columns-2 is-cropped\">\n<ul class=\"blocks-gallery-grid\">\n<li class=\"blocks-gallery-item\">\n<figure><img loading=\"lazy\" width=\"500\" height=\"500\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart1_6-602c25e76a82b.gif\" alt=\"Side-by-side videos in which an agent, represented by a red circle, moves around two rectangular walls to reach its goal, represented by a smaller blue circle. On the left, a black dotted line shows a path that is mostly far from the wall. On the right, a black dotted line shows a style of behavior that starts far from the wall initially and becomes progressively more centered.\" data-id=\"726604\" data-full-url=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart1_6-602c25e76a82b.gif\" data-link=\"https://www.microsoft.com/en-us/research/?attachment_id=726604\" class=\"wp-image-726604\"/></figure>\n</li>\n<li class=\"blocks-gallery-item\">\n<figure><img loading=\"lazy\" width=\"479\" height=\"479\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart2_6.gif\" alt=\"Side-by-side videos in which an agent, represented by a red circle, moves around two rectangular walls to reach its goal, represented by a smaller blue circle. On the left, a black dotted line shows a path that is mostly far from the wall. On the right, a black dotted line shows a style of behavior that starts far from the wall initially and becomes progressively more centered.\" data-id=\"726607\" data-full-url=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart2_6.gif\" data-link=\"https://www.microsoft.com/en-us/research/?attachment_id=726607\" class=\"wp-image-726607\"/></figure>\n</li>\n</ul><figcaption class=\"blocks-gallery-caption\">Video 5: In our navigation task, the agent on the left was fine-tuned to maximize distance to the walls, while the agent on the right was fine-tuned using preference learning. Through preference learning, we were able to achieve more nuanced behavior that better captured our intended style.</figcaption></figure>\n</p>\n<h2>Potential-based shaping to combine style and task rewards</h2>\n</p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"624\" height=\"197\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1.png\" alt=\"A screenshot with the equation total_reward = A * style_reward + B * task_reward.  \" class=\"wp-image-726610\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1.png 624w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1-300x95.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1-16x5.png 16w\" sizes=\"(max-width: 624px) 100vw, 624px\" /><figcaption>The above represents the common workflow when combining multiple sources of reward. The RL agent is trying to optimize the total reward. It’s up to the designer to decide on the correct ratio of style reward to task reward by specifying the weight of each, A and B here. This workflow requires a lot of iteration and offers little control.</figcaption></figure>\n</div>\n</p>\n<p>Specifying the reward that captures the desired style is only the first step; the style reward must then be integrated into the agent’s preexisting task reward. This is by no means a trivial undertaking. If the ratio of the style reward to the task reward is too high, the style reward overwhelms the task reward and navigation performance suffers. If the ratio is too low, then there’s no observable behavior change. The default approach to solving this problem is to iterate—tweak the ratio slightly and run another experiment. Since each RL training run can take hours, trying to tune the style and task reward ratios manually is laborious and mind-numbingly boring.</p>\n</p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingVideo6.gif\" alt=\"\"/><figcaption>Video 6: In the above video, the agent demonstrates &#8220;reward hacking&#8221; by moving to the point farthest away from the walls instead of reaching the goal. This happens when the combination of task and style rewards is misconfigured in a way that simply maximizing the style reward to avoid walls and ignoring the task reward to reach the goal yields the highest reward.</figcaption></figure>\n</div>\n</p>\n<p>When we&#160;first&#160;tried&#160;to fine-tune our agent with our new style,&#160;it failed&#160;to be successful in the initial task, as it gave too much weight to exhibiting the style!&#160;We&#160;used&#160;<a href=\"http://people.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf\" target=\"_blank\" rel=\"noreferrer noopener\">potential-based reward shaping&#160;(PBRS)</a>&#160;to solve this problem. PBRS ensures that when a shaping reward is introduced—that is, a reward&#160;encouraging behavior other than the&#160;initial&#160;task,&#160;like our style reward—the optimal policy&#160;for the&#160;initial&#160;task&#160;remains the same.&#160;&#160;</p>\n</p>\n<p>PBRS is a simple&#160;yet powerful&#160;technique where,&#160;at each step, we subtract the&#160;previous step’s&#160;style reward from the total reward&#160;(task reward plus style reward)&#160;of the current step.&#160;This means&#160;the agent is rewarded for being at a certain state and not&#160;moving into&#160;a certain state.&#160;The intuition behind PBRS can be expressed with the following&#160;example:&#160;imagine&#160;an agent is rewarded for crossing the finish line in a race.&#160;After crossing the finish line initially,&#160;the agent might be encouraged&#160;to&#160;step back over the finish line and forward again&#160;multiple times, effectively&#160;gaining&#160;infinite rewards. However,&#160;with&#160;PBRS, we only reward the agent for&#160;<em>being</em>&#160;at the finish line,&#160;not crossing it:&#160;whenever the agent&#160;steps back,&#160;we take away the reward we had given it, preventing&#160;it&#160;from&#160;accruing more&#160;reward&#160;by&#160;simply&#160;crossing&#160;back and forth.&#160;</p>\n</p>\n<p>\n</p>\n</p>\n<p>While the distinction is subtle, this prevents the agent from “reward hacking” to maximize rewards without accomplishing the initial task. When we started using PBRS to integrate our style reward into the task reward, the training was more successful in both keeping the original task rewards high and displaying the desired style. This alternative to the time-consuming task of fine-tuning the ratio of style reward to task reward manually means designers can explore many more style variations instead of spending their resources getting a single style to work properly.</p>\n</p>\n<figure class=\"wp-block-gallery columns-1 is-cropped\">\n<ul class=\"blocks-gallery-grid\">\n<li class=\"blocks-gallery-item\">\n<figure><img loading=\"lazy\" width=\"1024\" height=\"594\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-1024x594.png\" alt=\"chart, bar chart, histogram\" data-id=\"726616\" data-full-url=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511.png\" data-link=\"https://www.microsoft.com/en-us/research/?attachment_id=726616\" class=\"wp-image-726616\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-1024x594.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-300x174.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-768x446.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-1536x891.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-2048x1188.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-16x9.png 16w\" sizes=\"(max-width: 1024px) 100vw, 1024px\" /></figure>\n</li>\n</ul><figcaption class=\"blocks-gallery-caption\">Figure 1: Potential-based reward shaping (PBRS) makes integrating the style reward into the task reward easier, preventing the style reward from degrading task performance even at high style-reward-to-task-reward ratios. The above graph shows the percentage of successful trials with a particular reward ratio. In our navigation task, a successful trial was defined as the agent completing the task with a high enough style reward, measured by the mean distance from the nearest wall, and an acceptable task reward, an approximate equivalent to the time it takes to reach the goal. As shown, when PBRS isn’t used, the agent is only successful with a 0.1 style-reward-to-task-reward ratio. When PBRS is used, the agent is successful at ratios between 0.5 and 100.</figcaption></figure>\n</p>\n<h2>Automatic reward ratio adjustment to increase designer control</h2>\n</p>\n<p>Even though using PBRS made finding an acceptable ratio of style reward to task reward much easier, we’re still asking designers to fine-tune a behavior by changing an arbitrary numeric ratio. There’s no designer-interpretable meaning to “combining one parts task reward with four parts style reward.”</p>\n</p>\n<p>Rewards, especially when designed intentionally, can be meaningful from a design perspective. For example, if the agent is penalized by 1 point every second, we can see how fast the agent reaches the goal by simply looking at the final reward. A –15 task reward means the agent took 15 seconds to reach the goal. In scenarios where it’s possible to provide similar types of meaningful rewards, it would be much more efficient for a designer to specify a minimum performance that’s acceptable—a<em> lower bound reward threshold</em>—versus tweaking arbitrary numeric ratios.</p>\n</p>\n<p>Toward this end, we implemented an automatic reward ratio schedule that tries to maximize the style reward while respecting the designer-specified threshold. The automated scheduler increases the ratio of style reward to task reward while the task reward is higher than the designer-specified threshold and reduces the ratio when the task performance starts to degrade. To be more specific, we linearly scale the style reward ratio between a maximum number and 0—between the starting performance and the threshold performance. In the above example, if a designer wanted their agent to reach the goal in at most 15 seconds, the automated scheduler would increase the ratio of style reward to task reward until the agent started taking longer than the specified 15 seconds. At that point, the scheduler would then reduce the style reward weight until a performance time of 15 seconds was achieved. This automated schedule would continue over the course of training.</p>\n</p>\n<div class=\"wp-block-image\">\n<figure class=\"aligncenter size-large\"><img loading=\"lazy\" width=\"800\" height=\"649\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2.jpg\" alt=\"\" class=\"wp-image-726538\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2.jpg 800w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2-300x243.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2-768x623.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2-16x12.jpg 16w\" sizes=\"(max-width: 800px) 100vw, 800px\" /><figcaption>Figures 2a and 2b: The above figures demonstrate how designer-specified minimum task reward thresholds affect training. Figure 2a (top) plots the task rewards over total timesteps (length of an experiment) under five different minimum reward thresholds, from 150 to 110. The graph shows that the training starts from the same initial task reward of 150 but goes down to the minimum task threshold for each of the experiments. The automatic reward ratio scheduler is effective in keeping the task reward above the specified task threshold. Figure 2b (bottom) plots the mean distance from the walls (our proxy for the style reward) over total timesteps under the same five reward thresholds. The mean distance from the wall increases as the threshold is reduced from 150 to 110. Most notably, the agent fails to move away from the walls with a reward threshold of 150 since it has no slack to sacrifice task reward.</figcaption></figure>\n</div>\n</p>\n<p>Figure 2a shows the task reward with different designer-specified minimum task reward thresholds. The automated reward ratio adjustment is effective in keeping the task reward above the specified performance threshold.</p>\n</p>\n<p>Figure 2b shows the mean distance to the closest wall, a simple approximation of our target style, under different reward thresholds. When the minimum task reward threshold is very high (150), the change in behavior is rather small, as the agent is prioritizing the task reward over exhibiting the style. However, as the designer relaxes the constraints, more style behavior emerges.</p>\n</p>\n<p>This method of specifying a desired reward is much more meaningful than iteratively changing the numeric ratio to hit the desired target. We believe this workflow simplifies the job of the designer immensely.</p>\n</p>\n<h2>Open questions and continued collaboration</h2>\n</p>\n<p>While these results are encouraging, there are several open research questions. First, we need to validate our findings with a user study. While the high-level workflow is established, there’s more to learn regarding the specifics. In the context of our proposed solutions, do designers continuously monitor the training, or do they give feedback in batches? What information is shown to the designers for them to make accurate choices?</p>\n</p>\n<p>Another open question is exploring different methods of specifying a style. While preferences are useful, there are many other methods we can employ. Designers can demonstrate the desired style by taking control of the agent, or they can annotate individual states to guide the fine-tuning. It’s unclear which of these methods (or what combination) offers the most control to the designers.</p>\n</p>\n<div class=\"annotations\" data-bi-area=\"margin-callout\">\n<ul class=\"annotations__list annotations__list--right\">\n<li class=\"annotations__list-item\">\n\t\t\t\t\t\t\t<a href=\"https://note.microsoft.com/MSR-Webinar-Project-Malmo-Registration-Live.html?wt.mc_id=blog_MSR-WBNR_malmo_margin\" data-bi-type=\"annotated-link\" data-bi-name=\"Reinforcement learning in Minecraft: Challenges and opportunities in multiplayer games\" class=\"annotations__list-thumbnail\" ><br />\n\t\t\t\t\t<img width=\"300\" height=\"169\" src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-300x169.png\" class=\"attachment-medium size-medium\" alt=\"\" loading=\"lazy\" srcset=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-1536x865.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-2048x1153.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-16x9.png 16w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-1280x720.png 1280w, https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-1920x1080.png 1920w\" sizes=\"(max-width: 300px) 100vw, 300px\" />\t\t\t\t</a><br />\n\t\t\t\t\t\t\t<span class=\"annotations__type\">Event </span><br />\n\t\t\t<a href=\"https://note.microsoft.com/MSR-Webinar-Project-Malmo-Registration-Live.html?wt.mc_id=blog_MSR-WBNR_malmo_margin\" data-bi-type=\"annotated-link\" data-bi-area=\"margin-callout\" data-bi-name=\"Reinforcement learning in Minecraft: Challenges and opportunities in multiplayer games\"><br />\n\t\t\t\tReinforcement learning in Minecraft: Challenges and opportunities in multiplayer games\t\t\t</a><br />\n\t\t\t<span class=\"svg-icon icon-external-link\"></span>\n\t\t\t\t\t</li>\n</ul>\n</div>\n</p>\n<p>The journey toward RL that can be easily and organically incorporated into commercial game design is a long one. We feel taking a designer-centered approach, as demonstrated by the prototypes above, offers a promising means to achieving that goal, and we look forward to continuing to work with professionals in the game industry to deliver practical and empowering solutions.</p>\n</p>\n<h3>Additional resources and opportunities</h3>\n</p>\n<ul>\n<li><a href=\"https://www.microsoft.com/en-us/research/blog/research-collection-shall-we-play-a-game/\" target=\"_blank\" rel=\"noreferrer noopener\">Research Collection – Shall we play a game?</a>&#160;</li>\n<li><a href=\"https://note.microsoft.com/MSR-Webinar-Project-Malmo-Registration-Live.html?wt.mc_id=blog_MSR-WBNR_malmo_link\" target=\"_blank\" rel=\"noreferrer noopener\">“Reinforcement learning in Minecraft: Challenges and opportunities in multiplayer games” Webinar</a>&#160;</li>\n</ul>\n</p>\n<hr class=\"wp-block-separator is-style-dots\"/>\n</p>\n<p><em>This&#160;work was spearheaded by&#160;UC Santa Cruz PhD student&#160;<a href=\"https://nam06.safelinks.protection.outlook.com/?url=http%3A%2F%2Fbatuaytemiz.com%2F&data=04%7C01%7Cv-alhage%40microsoft.com%7C5807472333794332262e08d8d36eea52%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637491820638492875%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=B2MfF8jVQhHHFx1hljw%2BRAMYohjFzljoHWnGy0Yla%2Fk%3D&reserved=0\">Batu&#160;Aytemiz</a>&#160;during a <a href=\"https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Flab%2Fmicrosoft-research-cambridge%2Finternships%2F%3F&data=04%7C01%7Cv-alhage%40microsoft.com%7C5807472333794332262e08d8d36eea52%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637491820638502831%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=7B6lDNfYmijv8tEsi8YsHCG0Q7AXDmeasqR3u0JA9Ls%3D&reserved=0\">Microsoft Research&#160;Cambridge&#160;internship</a>. Team members&#160;<a href=\"https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fpeople%2Ft-mijaco%2F&data=04%7C01%7Cv-alhage%40microsoft.com%7C5807472333794332262e08d8d36eea52%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637491820638512798%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=UYlmXOuDtvV3SzSSUbEaYeF0WpcyX5bM1WdoZItTJLU%3D&reserved=0\">Mikhail Jacob</a>, <a href=\"https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fpeople%2Fsadevlin%2F&data=04%7C01%7Cv-alhage%40microsoft.com%7C5807472333794332262e08d8d36eea52%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637491820638522752%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=WWhWMFa6QP9vmU8B7qLVLuac64L%2FO4I2xLsrwvky%2F6g%3D&reserved=0\">Sam Devlin</a>, and&#160;<a href=\"https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fpeople%2Fkahofman%2F&data=04%7C01%7Cv-alhage%40microsoft.com%7C5807472333794332262e08d8d36eea52%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637491820638532708%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r9pnmvqyEnaw828oQwWAqz0kgGs%2BwZwxbRnMM28t%2F1A%3D&reserved=0\">Katja Hofmann</a>&#160;served as advisors on the work.</em>&#160;</p></p>\n<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/blog/designer-centered-reinforcement-learning/\">Designer-centered reinforcement learning</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n","descriptionType":"html","publishedDate":"Wed, 17 Feb 2021 18:23:31 +0000","feedId":3611,"bgimg":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/1400x788_Athens_no_logo-2.gif","linkMd5":"b3217467e147025ca452a022c17bbaf3","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn32@2020_1/2021/04/10/03-55-29-999_6d1e37d867003b9e.webp","destWidth":1920,"destHeight":1080,"sourceBytes":11437885,"destBytes":1530718,"author":"Alexis Hagen","enclosureType":"video/mp4","enclosureUrl":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/BleedingEdge-Composite-Shot.mp4","articleImgCdnMap":{"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/1400x788_Athens_no_logo-2.gif":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn32@2020_1/2021/04/10/03-55-29-999_6d1e37d867003b9e.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450-300x129.png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn4@2020_5/2021/04/10/03-55-31-486_27f01d80d8f4f679.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video1_RlGamingBlog.gif":null,"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video3_RLGamingBlog.gif":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn100@2020_2/2021/04/10/03-55-33-079_9abff9f8246f2c71.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_Video4.gif":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn96@2020_6/2021/04/10/03-56-54-147_a7064fb5875a75ec.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart1_6-602c25e76a82b.gif":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn91@2020_6/2021/04/10/03-55-31-706_098b03cc4953a381.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart2_6.gif":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn26@2020_5/2021/04/10/03-55-32-915_a96fec0dff049c47.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1.png":null,"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingVideo6.gif":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn89@2020_5/2021/04/10/03-55-31-734_9d751983462a9b88.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-1024x594.png":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn6@2020_6/2021/04/10/03-55-31-774_7aef0eaece8001a8.webp","https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2.jpg":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn86@2020_4/2021/04/10/03-55-31-579_bb385e707061f07c.webp","https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-300x169.png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn18@2020_3/2021/04/10/03-55-31-732_708eaad6d83f6241.webp"},"publishedOrCreatedDate":1618026811846}],"record":{"createdTime":"2021-04-10 11:53:31","updatedTime":"2021-04-10 11:53:31","feedId":3611,"fetchDate":"Sat, 10 Apr 2021 03:53:31 +0000","fetchMs":492,"handleMs":29,"totalMs":203393,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"d14e31baf87e33c96e7a5992ca4c5253","hostName":"europe70*","requestId":"8103d6b63f494d928fdf66f9cceace10_3611","contentType":"application/rss+xml; charset=UTF-8","totalBytes":2580368,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":12,"articlesImgsGithubTotal":10,"successGithubMap":{"myreaderx25":1,"myreaderx7":1,"myreaderx27":1,"myreaderx32":1,"myreaderx3":1,"myreaderx33":1,"myreaderx12":1,"myreaderx24":1,"myreaderx5oss":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}},"feed":{"createdTime":"2020-08-25 04:29:28","updatedTime":"2020-09-01 10:32:05","id":3611,"name":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/feed/","subscriber":null,"website":null,"icon":"https://www.microsoft.com/favicon.ico","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx62/cdn38@2020_5/2020/09/01/02-32-02-775_ddd9473b17dec87a.ico","description":"","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2021-04-10 11:56:54","updatedTime":"2021-04-10 11:56:54","id":null,"feedId":3611,"linkMd5":"b3217467e147025ca452a022c17bbaf3"}],"tmpCommonImgCdnBytes":1530718,"tmpBodyImgCdnBytes":1049650,"tmpBgImgCdnBytes":0,"extra4":{"start":1618026811230,"total":0,"statList":[{"spend":588,"msg":"获取xml内容"},{"spend":29,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":83417,"msg":"正文链接上传到cdn"}]},"extra5":12,"extra6":12,"extra7ImgCdnFailResultVector":[null,{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1.png","sourceStatusCode":200,"destWidth":624,"destHeight":197,"sourceBytes":48533,"destBytes":7774,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":467,"convertSpendMs":10,"createdTime":"2021-04-10 11:55:31","host":"europe67*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn14/contents/2021/04/10/03-55-31-536_c3ab539216773e38.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sat, 10 Apr 2021 03:55:31 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["C9CA:2B31:4080E36:417BB52:607121B3"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1618029933"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn14/contents/2021/04/10/03-55-31-536_c3ab539216773e38.webp","historyStatusCode":[],"spendMs":168},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"47.4 KB","destSize":"7.6 KB","compressRate":"16%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGaming_GIF1.png","sourceStatusCode":200,"destWidth":624,"destHeight":197,"sourceBytes":48533,"destBytes":7774,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":273,"convertSpendMs":8,"createdTime":"2021-04-10 11:55:31","host":"europe67*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn14/contents/2021/04/10/03-55-31-843_c3ab539216773e38.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sat, 10 Apr 2021 03:55:31 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["C9CA:2B31:4080EA4:417BBC4:607121B3"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1618029933"],"x-ratelimit-used":["60"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn14/contents/2021/04/10/03-55-31-843_c3ab539216773e38.webp","historyStatusCode":[],"spendMs":161},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"47.4 KB","destSize":"7.6 KB","compressRate":"16%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video1_RlGamingBlog.gif","sourceStatusCode":200,"destWidth":500,"destHeight":500,"sourceBytes":159648,"destBytes":39898,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":938,"convertSpendMs":782,"createdTime":"2021-04-10 11:55:31","host":"us-033*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn19/contents/2021/04/10/03-55-32-241_d9608e89e448a1f6.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sat, 10 Apr 2021 03:55:32 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["AB6E:0FEA:37D21F:A8483C:607121B4"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1618029919"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn19/contents/2021/04/10/03-55-32-241_d9608e89e448a1f6.webp","historyStatusCode":[],"spendMs":31},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"155.9 KB","destSize":"39 KB","compressRate":"25%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video1_RlGamingBlog.gif","sourceStatusCode":200,"destWidth":500,"destHeight":500,"sourceBytes":159648,"destBytes":39898,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":405,"convertSpendMs":248,"createdTime":"2021-04-10 11:55:32","host":"us-004*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx23/cdn19/contents/2021/04/10/03-55-32-728_d9608e89e448a1f6.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 69189253.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Sat, 10 Apr 2021 03:55:32 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["C8C6:2D06:B5D963:1CD1866:607121B4"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1618029919"],"x-ratelimit-used":["61"],"x-xss-protection":["0"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx23/cdn19/contents/2021/04/10/03-55-32-728_d9608e89e448a1f6.webp","historyStatusCode":[],"spendMs":34},"base64UserPassword":null,"token":"df0b9******************************93a6e"},"githubUser":"myreaderx23","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"155.9 KB","destSize":"39 KB","compressRate":"25%"},null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-032.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe62.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-021.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-004.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-033.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-005.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-028.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe67.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-003.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-040.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/1400x788_Athens_no_logo-2.gif","sourceStatusCode":200,"destWidth":1920,"destHeight":1080,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn32@2020_1/2021/04/10/03-55-29-999_6d1e37d867003b9e.webp","sourceBytes":11437885,"destBytes":1530718,"targetWebpQuality":30,"feedId":3611,"totalSpendMs":56458,"convertSpendMs":55200,"createdTime":"2021-04-10 11:54:33","host":"us-001*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3,b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"10.9 MB","destSize":"1.5 MB","compressRate":"13.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/ProjPaidia_AI_GameIntell-graphic_1044x450-300x129.png","sourceStatusCode":200,"destWidth":300,"destHeight":129,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn4@2020_5/2021/04/10/03-55-31-486_27f01d80d8f4f679.webp","sourceBytes":9326,"destBytes":1776,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":421,"convertSpendMs":10,"createdTime":"2021-04-10 11:55:31","host":"us-003*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.1 KB","destSize":"1.7 KB","compressRate":"19%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RL-Gaming-Chart2.jpg","sourceStatusCode":200,"destWidth":800,"destHeight":649,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx25/cdn86@2020_4/2021/04/10/03-55-31-579_bb385e707061f07c.webp","sourceBytes":62844,"destBytes":37456,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":463,"convertSpendMs":51,"createdTime":"2021-04-10 11:55:31","host":"us-013*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx25","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"61.4 KB","destSize":"36.6 KB","compressRate":"59.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart1_6-602c25e76a82b.gif","sourceStatusCode":200,"destWidth":500,"destHeight":500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx32/cdn91@2020_6/2021/04/10/03-55-31-706_098b03cc4953a381.webp","sourceBytes":174438,"destBytes":43110,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":634,"convertSpendMs":259,"createdTime":"2021-04-10 11:55:31","host":"us-004*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx32","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"170.3 KB","destSize":"42.1 KB","compressRate":"24.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Fig1_Potential-Based-Reward-Mixing-Edited89511-1024x594.png","sourceStatusCode":200,"destWidth":1024,"destHeight":594,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx5oss/cdn6@2020_6/2021/04/10/03-55-31-774_7aef0eaece8001a8.webp","sourceBytes":62238,"destBytes":57480,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":705,"convertSpendMs":245,"createdTime":"2021-04-10 11:55:31","host":"us-005*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx5oss","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"60.8 KB","destSize":"56.1 KB","compressRate":"92.4%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2020/12/1400x788_Webinar_Social_Asset_noicons-300x169.png","sourceStatusCode":200,"destWidth":300,"destHeight":169,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn18@2020_3/2021/04/10/03-55-31-732_708eaad6d83f6241.webp","sourceBytes":42297,"destBytes":7852,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":776,"convertSpendMs":35,"createdTime":"2021-04-10 11:55:31","host":"us-032*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"41.3 KB","destSize":"7.7 KB","compressRate":"18.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingVideo6.gif","sourceStatusCode":200,"destWidth":500,"destHeight":500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx24/cdn89@2020_5/2021/04/10/03-55-31-734_9d751983462a9b88.webp","sourceBytes":87726,"destBytes":16326,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":960,"convertSpendMs":234,"createdTime":"2021-04-10 11:55:31","host":"europe62*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx24","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"85.7 KB","destSize":"15.9 KB","compressRate":"18.6%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_GIFPart2_6.gif","sourceStatusCode":200,"destWidth":479,"destHeight":479,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx27/cdn26@2020_5/2021/04/10/03-55-32-915_a96fec0dff049c47.webp","sourceBytes":169455,"destBytes":37798,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1908,"convertSpendMs":1076,"createdTime":"2021-04-10 11:55:31","host":"us-021*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx27","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"165.5 KB","destSize":"36.9 KB","compressRate":"22.3%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/Video3_RLGamingBlog.gif","sourceStatusCode":200,"destWidth":500,"destHeight":500,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn100@2020_2/2021/04/10/03-55-33-079_9abff9f8246f2c71.webp","sourceBytes":174438,"destBytes":43110,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":1959,"convertSpendMs":1266,"createdTime":"2021-04-10 11:55:31","host":"us-028*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"170.3 KB","destSize":"42.1 KB","compressRate":"24.7%"},{"code":1,"isDone":false,"source":"https://www.microsoft.com/en-us/research/uploads/prod/2021/02/RLGamingBlog_Video4.gif","sourceStatusCode":200,"destWidth":628,"destHeight":428,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn96@2020_6/2021/04/10/03-56-54-147_a7064fb5875a75ec.webp","sourceBytes":2884032,"destBytes":804742,"targetWebpQuality":75,"feedId":3611,"totalSpendMs":21090,"convertSpendMs":20422,"createdTime":"2021-04-10 11:56:33","host":"us-021*","referer":"https://www.microsoft.com/en-us/research/?p=726529","linkMd5ListStr":"b3217467e147025ca452a022c17bbaf3","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"2.8 MB","destSize":"785.9 KB","compressRate":"27.9%"}],"successGithubMap":{"myreaderx25":1,"myreaderx7":1,"myreaderx27":1,"myreaderx32":1,"myreaderx3":1,"myreaderx33":1,"myreaderx12":1,"myreaderx24":1,"myreaderx5oss":1,"myreaderx29":1},"failGithubMap":{"myreaderx14":1,"myreaderx23":1}}