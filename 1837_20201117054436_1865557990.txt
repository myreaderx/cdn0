{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2020-11-17 13:42:32","updatedTime":"2020-11-17 13:42:32","title":"机器人是怎么知道如何抓握杯子的？","link":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","description":"<div><div><div id=\"media\" class=\"rich_media_thumb_wrp\">\n\n            <img class=\"rich_media_thumb\" src=\"http://contentg.sov5.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1xibhOzAG7bEuQfDsTt3TzgavmUqI0UbZFxF3r7JQRicvszLyU8phGNg?imageView2/1/w/600\">\n        </div>\n    \n\n    \n\n    <div class=\"rich_media_content\" id=\"js_content\">\n                    \n\n                    \n\n                    \n                    \n                    <section data-mpa-powered-by=\"yiban.io\" data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-style=\"max-width: 100%; font-variant-numeric: normal; font-variant-east-asian: normal; letter-spacing: 0.544px; line-height: 27.2px; white-space: normal; widows: 1; font-family: \" helvetica neue sans gb yahei arial sans-serif background-color: rgb box-sizing: border-box overflow-wrap: break-word class=\"js_darkmode__0\" style=\"max-width: 100%;font-variant-numeric: normal;font-variant-east-asian: normal;letter-spacing: 0.544px;white-space: normal;line-height: 27.2px;widows: 1;font-family: \" visible rgba><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-id=\"85660\" data-custom=\"rgb(117, 117, 118)\" data-color=\"rgb(117, 117, 118)\" data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-id=\"85660\" data-custom=\"rgb(117, 117, 118)\" data-color=\"rgb(117, 117, 118)\" data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" style=\"margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><p style=\"margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;font-family: inherit;border-width: initial;border-style: initial;border-color: currentcolor;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;background-color: rgb(117, 117, 118);color: rgb(255, 255, 255);font-size: 15px;font-family: inherit;text-decoration: inherit;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">机器之心分析师网络</span><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></p><section style=\"max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;color: rgb(136, 136, 136);visibility: visible;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgb(136, 136, 136)\" data-darkmode-original-color-15899528520055=\"rgb(136, 136, 136)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\">作者：<strong data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgb(136, 136, 136)\" data-darkmode-original-color-15899528520055=\"rgb(136, 136, 136)\" style=\"max-width: 100%;letter-spacing: 0.544px;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\">Yuanyuan Li</strong></strong></span></section><section style=\"max-width: 100%;min-height: 1em;text-align: center;visibility: visible;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;color: rgb(136, 136, 136);visibility: visible;font-size: 12px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgb(136, 136, 136)\" data-darkmode-original-color-15899528520055=\"rgb(136, 136, 136)\" style=\"max-width: 100%;visibility: visible;box-sizing: border-box !important;overflow-wrap: break-word !important;\">编辑：Joni</strong></span></section></section></section></section></section></section></section></section></section></section></section></section><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-style=\"max-width: 100%; font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif font-variant-numeric: normal font-variant-east-asian: letter-spacing: line-height: white-space: widows: background-color: rgb box-sizing: border-box overflow-wrap: break-word class=\"js_darkmode__1\" style=\"max-width: 100%;font-family: -apple-system-font, BlinkMacSystemFont, \"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgba(230, 230, 230, 0.498)\" data-darkmode-original-color-15899528520055=\"rgba(0, 0, 0, 0.498)\" style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgba(230, 230, 230, 0.498)\" data-darkmode-original-color-15899528520055=\"rgba(0, 0, 0, 0.498)\" style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section data-darkmode-bgcolor-15899528520055=\"rgb(36, 36, 36)\" data-darkmode-original-bgcolor-15899528520055=\"rgb(255, 255, 255)\" data-darkmode-color-15899528520055=\"rgba(230, 230, 230, 0.498)\" data-darkmode-original-color-15899528520055=\"rgba(0, 0, 0, 0.498)\" style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><blockquote data-type=\"2\" data-url=\"\" data-author-name=\"\" data-content-utf8-length=\"53\" data-source-title=\"\" style=\"color: rgba(0, 0, 0, 0.498);max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><section style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><p style=\"max-width: 100%;min-height: 1em;line-height: 1.75em;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\">如何推理一个物体的 Affordance 是机器人相关研究的一个重点关注方向。在具体的 Affordance 中，抓取（grasping）又是格外重要的一个功能。</span></p></section></blockquote></section></section></section></section></section><section style=\"line-height: 1.75em;text-align: center;\"><strong><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>前言</span></strong><br></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">作为最早提出 Affordance 这一概念的学者，James Gibson 在他的书 [1] 中正式定义了 Affordance：</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><blockquote class=\"js_blockquote_wrap\" data-type=\"2\" data-url=\"\" data-author-name=\"\" data-content-utf8-length=\"84\" data-source-title=\"\"><section class=\"js_blockquote_digest\"><section>Affordance 是环境所允许个人能实现的功能（Affordance is what the environment offers the individual）。</section></section></blockquote><p><br></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">不过 Affordance 所最为人知的定义应该是在他几年后出版的书 [2] 中的定义：</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><blockquote class=\"js_blockquote_wrap\" data-type=\"2\" data-url=\"\" data-author-name=\"\" data-content-utf8-length=\"114\" data-source-title=\"\"><section class=\"js_blockquote_digest\"><section>Afford 是在字典中存在的一个词，但 Affordance 不是。Affordance 是我个人创造出来的一个词。Affordance 指代环境为动物 / 人类提供的一种功能。我想用这个词来表达环境和动物 / 人类的互补性。</section></section></blockquote><p><br></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在中文语境下，Affordance 可以大致理解为物体 / 环境的直观功能。虽然语意上比较难以理解，Affordance 却是每个人在日常生活中下意识便会应用的技能。举例来说，当人类看到一个马克杯的时候，他 / 她立刻就能够理解杯子是可以用来盛物体的——不论是咖啡等液体还是固体——并且马克的杯柄可以被抓握。除此之外，假如人类看到茶杯、玻璃杯、酒杯等任何非马克杯的物体，也不会因此而无法推断该物体是否还能够盛物体。人类几乎天然就理解小型物体上的柄可以被抓握和应该从那个角度抓握。人类也可以轻易理解门上的把手是用来推或拉的，挂钩上的钩子是用来挂东西的、家用电器上的按钮是用来按（或者扭）的。设计师在设计产品时也必须将物体的 Affordance （直观功能）和如何引导用户理解物体的 Affordance 纳入考虑中。不信？请移步设计师 Katerina Kamprani 精心设计的「不舒服」的产品一览究竟。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.8285714285714286\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"700\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEK8ednibCzGbuiaZWHqCicFb9QtotvNBS2x7l0qG1TJbsDJ0pp8OB22WGw/640?wx_fmt=jpeg\"></p><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>设计师 Katerina Kamprani 设计的一系列让人感觉「不舒服」的产品，其实就是违反了一个物件应该有的 Affordance（</span></em></span><em style=\"color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>图源：https://www.theuncomfortable.com）</em></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">虽然 Affordance 是人类与生俱来的技能点，机器人却没有这样的「运气」。由于机器人本质上只是在运行人类开发的软件而不能进行真正的推理——至少目前还是这样——机器人对 Affordance 的理解取决于人类在这一领域的研究进展。实际上，机器人领域的研究已经证明了 Affordance 远远不止是只存在于书本中的心理学概念。具体来说，物体的抓取和操纵中，机器人需要通过视觉线索和经验中学习周围环境中物体的 Affordance，包括是否可以操纵物体、如何抓握物体以及学习操作对象以达到特定目标。 </span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">不难看出，如何推理一个物体的 Affordance 是相关研究的一个重点关注方向。在具体的 Affordance 中，抓取（grasping）又是格外重要的一个功能。这两点将是本文的讨论重点。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>推理</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">推理（reasoning）Affordance 很好理解，即推断一个物体的 Affordance —— 不论是通过视觉上的线索，还是通过过去的经验知识。机器人需要理解有柄的物体可以抓握，带有凹陷的物体可以盛物品。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">推理物体的 Affordance 可以简单分为推理单一物体的 Affordance 和推理多个物体的 Affordance。由于多个物体间可能存在互动 （interaction），Affordance 的推理很容易就会变得非常复杂。本文将集中于单一物体的 Affordance 推理。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.8085714285714286\" data-s=\"300,640\" data-type=\"jpeg\" data-w=\"700\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEzBCccZgCibU8v6cvVibJkxGSNlgzBIcibBL87XrWdAxbh3a6Nn5w2icEcw/640?wx_fmt=jpeg\"></p><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>两个物体之间可能有互动（interaction），从而改变了物体的 Affordance。（</span></em></span><em style=\"color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>图源：https://www.theuncomfortable.com）</span></em></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">具体来说，学习推理单一物体 Affordance 的方法可以分为三类：通过模拟（simulation）、通过视觉特征（visual features）、和通过构建知识图谱（knowledge graph）。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: justify;\"><span style=\"font-size: 15px;\"><strong>通过模拟（simulation）</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在 [3] 中，Hongtao Wu 和 Gregory S. Chirikjian 利用模拟物体落入容器中的物理过程来对开放式容器——即无盖的容器，杯子、碗、碟等——的容纳性 （containability affordance）进行推理。还是用水杯举例，当机器人面对一个水杯时，需要模拟从水杯上方倾倒物体的结果——物体是会落到桌面上还是会落到水杯内？假如机器人面对的是一个没有杯底的水杯，结果是什么？</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">整个实验中，作者使用了一个具有抓握功能的机器手，并在其上安装了 RGB-D 摄像机来对推理对象进行扫描。在 RGB-D 摄像机下有一个透明的平台，被推理的物体将会被放置在其上。具体设置如下图所示。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.6461538461538462\" data-s=\"300,640\" data-type=\"png\" data-w=\"910\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1zLts88cM29JS8CQocBP0zQoJwn22ahCVKMBhvANL0lHt3kxZFdEWQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>实验设计（图源：H. Wu, G. S. Chirikjian. (2020). Can I Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations. arXiv:2008.02321.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">实验过程中，不同的物体会被随机放置在透明平台上，机器手会依次移动到 24 个预先设定好的位置上，利用腕上安装的 RGB-D 摄像机捕获场景的深度图像。接下来作者使用  TSDF Fusion [12] 密集地重建场景。由于平台是透明的，相机的深度传感器不会捕捉到该平台，被建模物体的 3D 重建会比较简单 —— 只要在重建的场景中剪切（crop）即可。生成的 3D 模型会被用于接下来的模拟中，如下图中间所示，算法需要模拟类似于 M&amp;M 豆大小的灰色的小颗粒物体向被推理对象掉落的物理过程，并计算究竟有多少颗粒会进入到该物体内并被容纳以量化一个物体的容纳性，即判断一个物体是否是开放式容器。若模拟结果显示被推理物体内不保有任何颗粒，则该物体不是开放式容器。在下图例子中，算法对纸杯和一卷胶带分别进行了模拟，结果显示只有纸杯是开放式物体。若一个物体被判断为开放式容器，机器手还会再次进行模拟，以推断自己应该倒入的位置和方向，然后将之付诸于行动。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.32734375\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEZSVRsgoHibsjUCKUthbdOl6havmj6gUt7jvDdMksIc6Q9nDL6rkUibjg/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 12px;\"><em><span style=\"color: rgb(136, 136, 136);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>机器手对纸杯和胶带的容纳性进行推理。（</span></em></span><em style=\"font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif><span style=\"color: rgb(136, 136, 136);\">图源：H. Wu, G. S. Chirikjian. (2020). Can I Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations. arXiv:2008.02321.）</span></em></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">作者利用 11 个物体对整个模拟算法（以 Imagination 代指）进行了校准，然后将其与利用深度学习对 RGB 图像进行学习的 AffordanceNet [11] 进行了比较。测试集包含 51 个物体——23 个开放式容器和 28 个非开放式容器，测试表现用 accuracy 和 AUC （area under curve）进行衡量。下表中可以看出 AffordanceNet 和 作者所提出的模拟方法都在测试集上取得了非常好的表现。Imagination 的准确度稍差，主要是因为对被推理物体的 3D 建模不够准确，从而导致生成的 3D 模型上出现了轻微凹陷，而凹陷部位可以容纳小颗粒物体从而导致了物体被误判为开放式容器。另一些失败的情况则比较模棱两可，如汤匙，这种争议在人类标注者上也存在。另一方面，由于使用了 RGB-D 相机，Imagination 算法可以利用深度信息，这对推理和执行倾倒这个动作是有优势的。作者在之后进行的一些实验也证明了这一点。</span></section><section style=\"line-height: 1.75em;\"><br></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.40293040293040294\" data-s=\"300,640\" data-type=\"png\" data-w=\"546\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEEQicV1wooo8CYbw89CLRvKibks2U5u30ZRlYYAx9BNobFC2CZFKEOMRQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>利用模拟来对物体的 Affordance 进行学习的优势主要在于可解释度高、对未知物体的鲁棒性好，难点则在于嵌入式开发中的硬件设施、计算能力、模拟算法的准确性等。比如本文的实验中 （1）被推理物体需要一直处于深度传感器的测量范围内；（2）只能对物体的顶部和侧面进行建模，因为无法机器手无法从被推理物体的下方进行扫描；（3）模拟算法模拟的是离散的刚性颗粒，其他物体——比如水——则具有完全不同的物理特性；（4）Affordance 的推理局限于物体的容纳性，如果想要将该研究延伸到新的 Affordance 如物体的抓握性，则需要完全的不同的模拟算法。</span><br></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: justify;\"><span style=\"font-size: 15px;\"><strong>通过视觉特征（visual features）</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">由于人类主要通过视觉线索对物体的 Affordance 进行推理，利用 RGB 照片建模的研究并不少见，比如前文提到的 AffordanceNet。随着深度学习的流行，不少研究会训练卷积神经网络（CNN）来取代传统的特征工程。[4] 就是典型的一例。这篇研究有意思的地方在于作者特意选择了专家演示的视频组成数据集，利用人类理解物体 Affordance 的线索训练模型。如果 [4] 也使用的视频中也有水杯，那么 CNN 就需要对专家演示中将茶水倾倒到水杯中和抓握杯柄将水杯端起来的片段理解水杯的容纳功能和抓握功能。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">专家演示的视频来自互联网上充斥着的大量的产品评论视频 （product review videos），其中很多视频中会有一名「专家」——比如产品评论者——通过对产品对象的一系列操作来详细演示产品功能。除了为消费者们拔草种草外，这些视频还为作者提供了新思路——用这些视频组成能够为机器人提供有关 affordance 以及人们如何与产品交互的大规模、高质量数据。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.6151419558359621\" data-s=\"300,640\" data-type=\"png\" data-w=\"634\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEgSq3WXjDA3jUMkweIxjUokkLulCQhNyJq9H7prsI53QmqxHzyUIFcA/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>产品评论视频中往往有一名「专家」在对产品进行演示（图源：K. Fang, T. Wu, D. Yang, S. Savarese and J. J. Lim. (2018) Demo2Vec: Reasoning Object Affordances from Online Videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">这种方法虽然从逻辑上看非常可行，但却面临两个挑战：第一，这些视频中的产品和机器人要面对的产品在外观上可能有非常大的差异，如何保证机器人学到的 affordance 对产品外观是稳健的；第二，在视频中「专家」和产品的交互并不频繁，比如在上图例子中几乎只有第三帧中「专家」有对产品进行操作，还是在有大量的背景信息下进行的，机器人需要在其中辨别并学习真正有用的信息。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">作者提出的解决办法是将模型（Demo2Vec）分解为演示编码器（Demonstration encoder）和 affordance 预测器（affordance prediction），演示编码器负责将演示视频通过「演示嵌入」(demonstration embedding) 总结为了人类动作和被推理物体外观的低维向量，这里的需要解决的问题主要是如何提取关于人与物体交互的有用视觉提示，如前文提到的，这种交互在视频中比较稀疏 （「sparse」）， 且存在许多其他无关物体。作者提出用卷积 LSTM 网络（ConvLSTM）和 soft-attention 来组成演示编码器。卷积 LSTM 网络使用两组信息作为输入，一组是视频帧，即正常的 RGB 图像，另一组则是当前的视频帧和前一帧的差值（∆x_t = x_t -x_{t-1}）用以捕捉两帧之间的动态变化, 从而捕捉手部动作的变化讯息。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">接下来，卷积 LSTM 网络的两组输出（RGB 特征和动态特征）将会被输入到 soft attention 模块中，最终得到的注意力权重会与 RGB 特征相乘，并对所有帧求和，从而生成 demonstration embedding。利用 demonstration embedding，affordance 预测器 （predictor）将知识转移到目标图像上预测被推理物体的交互区域和动作标签。</span></section><section style=\"line-height: 1.75em;\"><br></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.4515625\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBERUpQGic7cRicSNxB7VZXpfg79OoyVVYa2q2OnXKBAuOkQfwYH9hSa0rg/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em>（a）模型概述。Demo2Vec 模型由一个演示编码器和一个 affordance 预测器组成。（b）演示编码器。演示编码器将输入的演示视频嵌入到低维向量，输入图像包括 RGB 图像和 运动图像 （motion modality），然后用 soft attention 将两部分信息融合起来。affordance 预测器然后利用嵌入向量来预测目标图像中展示的物体的 affordance 和热力图（heat map）。(<span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>图源：K. Fang, T. Wu, D. Yang, S. Savarese and J. J. Lim. (2018) Demo2Vec: Reasoning Object Affordances from Online Videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition.)</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">训练后的模型在面对同一个演示视频的不同时段，根据演示者的不同操作会对同一个物体推理出不同的 affordance。在下图给出的例子中，演示者在制作奶昔，并依次涉及到了四个 affordance：拿住（hold）、拿起（pick up）、推（push）、拿起（pick up）。该视频被分为 4 个短片（由不同颜色表示），模型能够正确的根据演示理解 affordance 并且识别每个 affordance 对应的部位。这和人类的表现更相似，能够让机器人更自然一些。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.2671875\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEl0DgFqpyKwEa2e7TzwHVV0iaDX0exCjiay9ibHiaPaXKBPhbjME8ZWF8JQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>不同的演示可以令模型对同一物体推理出不同的 affordance（图源：K. Fang, T. Wu, D. Yang, S. Savarese and J. J. Lim. (2018) Demo2Vec: Reasoning Object Affordances from Online Videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition.）</span><br></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">但是，不可避免地，当演示视频中出现许多杂物或其他和被推理物体十分相似的物体时，模型很容易受到误导。如下图所示，演示人站在摄像机前占据了大部分画面，并且遮挡了被推理物体，此时模型错误的将物体的 Affordance 预测为 Hold（如图中红色方框所示）， 而实际上应该是 Rotate（如图中绿色方框所示）。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.11424100156494522\" data-s=\"300,640\" data-type=\"png\" data-w=\"1278\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEoO4lvEHrR1dOgAX7X8ksk2vh6VnYhx5Ja90ia2f8ckAT7HZVX2eeribA/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 12px;\"><em><span style=\"color: rgb(136, 136, 136);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>当演示人遮挡了被推理物体时推理结果会出错（图源：K. Fang, T. Wu, D. Yang, S. Savarese and J. J. Lim. (2018) Demo2Vec: Reasoning Object Affordances from Online Videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">此外，每次对物体的 affordance 进行判断时，演示视频是必不可少的。而人类只需要对演示视频观看几次就能够直接对未知物体进行推理。从这一点看利用模拟（simulation）的 [3] 的泛化能力要更好一点。但利用视觉线索仍然是最接近人类推理 Affordance 的方法之一。算力方面，根据模型的不同——比如 [5] 直接用 RGB-D 图像 3D 建模——有可能对计算能力有很高的要求。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: justify;\"><span style=\"font-size: 15px;\"><strong>通过构建知识图谱（knowledge graph）</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">可以看出，利用视觉信息 + CNN 的方法主要是试图模拟人类学习 Affordance 的过程，但是由于 CNN 是黑箱模型，训练出来的模型可解释性差。除此之外，绝大多数训练好的 CNN 本质上仍是一个分类器，因而其能够推理的 Affordance 也局限于训练数据所包含的 Affordance。使用构建知识库的方法则不一样，由于物品的各项特征都被单独标记了出来用于最后的推理，模型在可解释性方面更有优势，使用基于知识的表示形式便于对学习范围进行扩展。如果利用知识图谱对水杯的 Affordance 进行学习，得到的规则可能是「有柄的物体可以被抓握，有开口并且有底的物体可以容纳」。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">[6] 利用图像和其他元数据源中获取物品的各种信息，然后使用马尔可夫逻辑网络（MLN）学习知识图谱。在对未知物品进行推理时只依赖于已习得的知识库而无需训练单独的分类器，包括 zero-shot affordance prediction。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">作者通过从图像以及诸如 Amazon 和 eBay 之类的在线文本源中提取信息来抽取物品的属性和 Affordance， 然后从中学习知识图谱。每一个物品都有三类属性：视觉属性（Visual attributes）、物理属性（Physical attributes）和分类属性（Categorical attributes）。视觉属性对应于从视觉感知中获得的信息，包括物品的形状和材质等；物理属性包括物体的重量和大小；分类属性则反映物体所属于的更抽象的类别，比如动物、机器、器械、电器等等。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">相应地，每个物品也有三类标签：Affordance 标签、人体姿势（Human poses）和人与物品的相对位置（Human-object relative locations）。后两者分别用于描述人体的姿势和人与物品交互过程中人与物体之间的空间关系。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在数据收集好之后，就可以利用马尔可夫逻辑网络（MLN）从中学习关系即通用规则的权重来构建知识图谱。下图可视化了作者所构建的知识图谱的一部分。在下图中，每个节点包含了某一类属性或标签，连接两个节点的线段则代表两个节点之间的逻辑公式——比如既是 vehicle 又是 animal，MLN 需要学习相应的权重，其中正权重表示两者可能同时出现，由绿色实线表示，负权重表示两者是负相关的，由红色虚线表示。在本例中 vehicle 和 animal 是由红色虚线连接的，即两者不大可能出现在同一物体上。</span></section><section style=\"line-height: 1.75em;\"><br></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.24921875\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEZQIPRn6Gqqfab1Z0wHsZ3sIGDBNoaQj5KRfppicC5GhhRCSHSLlfuVw/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"font-size: 12px;\"><em><span style=\"color: rgb(136, 136, 136);\">构造的 KB 的图形化显示。 （<span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>图源：Zhu, Y., Fathi, A., &amp; Fei-Fei, L. (2014). Reasoning about Object Affordances in a Knowledge Base Representation. ECCV.）</span></span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在执行推理时，模型首先根据提供的图像抽取物体的视觉属性，然后推测其物理和分类属性。利用这些属性模型可以在习得的知识图谱中对物体的 Affordance 进行查询。下图给出了  zero-shot affordance prediction 的例子。</span></section><p style=\"text-align: center;\"><br></p><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.18125\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEER0yx8912tu3gic3YbTia11zsBo6IDBcAW7eeaxSQ1q6rO0Kxq7Ct8RQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em>zero shot affordance prediction 的推理过程。给定一个未知对象的图像，模型通过 hierarchical model 估算对象属性。这些属性可作为知识图谱查询的线索，从而对 Affordance 进行预测，并估计人体姿势和人体的相对位置。（<span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>图源：Zhu, Y., Fathi, A., &amp; Fei-Fei, L. (2014). Reasoning about Object Affordances in a Knowledge Base Representation. ECCV.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">由于推理是运用多个线索综合完成的，模型的稳健性较好，不容易出现缺少某一个属性的信息就推理失败的情况。灵活性也好，可以比较容易的对模型进行扩展。不便之处则在于模型的质量很大程度上依靠于知识图谱的质量，而后者又依靠于数据集的质量。如果数据集中有很强的偏置（bias），比如红色的物体刚好都可以被抓握，所生成的模型表现也会受到影响。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">当然，许多研究会将上述的方法混合起来，比如 [9] 和 [10] 就使用了 CNN 抽取特征用于构建知识图谱。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>抓握（grasping）</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">抓握（grasping）是人类生活最常用到的动作之一，而机器人的任务就是根据已经学习过的物体推断未知物体的 grasping affordance，即一个物体是否能被抓握。抓握和推理任务在一定程度上有重合。在机器人领域内，有很多研究会将推断物体是否能被抓握和识别物体具体能够被抓握的位置放在一个学习任务中。另一方面，抓握也可以分为学习 simple-task affordance 和 task-specific affordance。本文的重点会更偏向于推理未知物体能否被抓握，借用 zero-shot learning 的概念——在上文中的知识图谱中也有简要提到——这一领域也被叫做 zero-shot （grasp） affordance。这也算是推理抓握功能的难点之一，其他难点还包括实时推理、数据收集等。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">早期的一些研究会利用本地特征抽取器（local feature extractors ）来学习一个物体是否能被抓握，如 [7]。虽然随着深度学习的流行手工设计的特征已经不再吃香，这篇文章还是在一定程度上解决了如何面对未知物体的问题：只寻找物体上是否有具有已知能够抓握的部位。[8] 则使用了神经网络来判断一个物体可能的 Affordance，包括正面吸取（suction down）、侧面吸取（suction side）、抓握（grasp down）和齐平抓握（flush grasp）。四种 Affordance 如下图所示，抓握和齐平抓握的区别主要是后者具有在目标对象和墙壁之间滑动一根手指的附加行为。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.346875\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE66icHU8Y8eXUcTV0iaKFaJYTtSPxLeIHqr2uW29kFdBtYHsNicVccpOibA/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>神经网络需要学习的四种 Affordance（图源：Zeng, A., Song, S., Yu, K.-T., Donlon, E., Hogan, F. R., Bauza, M., Ma, D., Taylor, O., Liu, M., Romo, E., Fazeli, N., Alet, F., Chavan Dafle, N., Holladay, R., Morona, I., Nair, P. Q., Green, D., Taylor, I., Liu, W., … Rodriguez, A. (2019). Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. The International Journal of Robotics Research.）</span></em></span><br></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在训练过程中，作者使用到两个全卷积残差网络（FCN） —— ResNet101 —— 中分别判断物体的吸取和抓握的 Affordance。为判断物体是否能够被吸取，下图中上排的 FCN 使用多视角 RGB-D 图像作为输入，然后对每个像素的 吸取 Affordance 进行预测，预测值越接近 1，则代表该部位越容易被吸取。随后，模型需要将所有视角的的预测汇集到 3D 点云（3D point cloud）上。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">图中下排的 FCN 则负责抓握 Affordance 的推理。作者假设一个物体可抓握与否取决于该物体是否有可抓握区域，并且可抓握区域可以从物体具备的几何形状和外观推断，即 [7] 的思想。首先，RGB-D 图像将被合并到场景的正交 RGB-D 高度图（orthographic RGB-D heightmap）中，来生成场景的高度图。图中的每个像素代表垂直方向——即重力方向——上的 2mm*2mm 的空间。FCN 需要对图中的每个像素的抓握 Affordance 进行判断（0-1 的概率），由于作者假设机器手的位置是与生成的高度图在垂直方向上平行的，生成的 Affordance heatmap 可以直接用于机器手在该方向上抓握该物体某一个部位的可能性。通过将高度旋转 16 次到不同的角度，并用 FCN 对其进行预测，则可以得出在不同方向上对该物体抓握的可能性，即预测结果直接包含了 16 种不同的自上而下的抓取角度的概率图。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">在后处理（post-processing）时，根据最佳抓握点在生成的 3D 点云中的位置，算法会计算机器手两根手指的最佳宽度。同时，如果最佳抓握点过于靠近墙壁，算法会推荐执行齐平抓握，否则执行一般的抓握。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.3515625\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEFO4fSBoECWNHfHxrLwKgkOe7YWjDDQxUfxt60MWVtAcTmfb5LiaX4bw/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>作者所提出的神经网络训练流程（图源：Zeng, A., Song, S., Yu, K.-T., Donlon, E., Hogan, F. R., Bauza, M., Ma, D., Taylor, O., Liu, M., Romo, E., Fazeli, N., Alet, F., Chavan Dafle, N., Holladay, R., Morona, I., Nair, P. Q., Green, D., Taylor, I., Liu, W., … Rodriguez, A. (2019). Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. The International Journal of Robotics Research.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">由于在进行推理时，训练好的神经网络有可能遇到未知的物体，作者提出了跨领域图像匹配（cross-domain image matching）模型来通过通过检索一组产品图像中的最佳匹配来解决此识别问题。该模型由两组 ConvNet （two-stream ConvNet）组成，一个用来对已知的图像计算 2048 维特征，另一个则为用于检索的图像——即未知物品的图像——计算 2048 维特征。在训练时作者从已知的物体中提供一系列匹配和不匹配的图像对来提供平衡的正例和反例，然后用 Triplet Loss 作为损失函数。这样可以有效地优化网络，从而最大程度地减小匹配对特征之间的 l2 距离，同时拉开不匹配对特征之间的 l2 距离。在测试过程中，已知对象和未知对象的图像都被映射到公共特征空间上，模型通过将观察到的图像映射到相同的特征空间并找到可能性最高的匹配来识别它们。本质上[8] 是把未知物体的推理简化成了搜索任务。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.26328125\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE8mkH9VUpaBOdicGaibIhRaL3tKdrJPZdo0en807PMLaJAObHs2LuyY0g/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>未知物体的识别框架（图源：Zeng, A., Song, S., Yu, K.-T., Donlon, E., Hogan, F. R., Bauza, M., Ma, D., Taylor, O., Liu, M., Romo, E., Fazeli, N., Alet, F., Chavan Dafle, N., Holladay, R., Morona, I., Nair, P. Q., Green, D., Taylor, I., Liu, W., … Rodriguez, A. (2019). Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. The International Journal of Robotics Research.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">[9] 和 [6] 有一定相似度，但 [9] 中的 CNN 是为了构建知识图谱然后用 MLN 进行学习而服务的。其提出的模型如下图所示，在学习阶段（紫色方框）根据已经给出的一系列物体属性、抓握功能 以及依次创造出的规则（rules），作者使用 MLN 来学习物品的属性、位置和抓握功能之间的语意关系。学习完成后得到白色方框内所示例的知识图谱。在推理阶段（蓝色方框），作者使用预训练卷积神经网络（CNN）从被推理的 RGB 图像中提取被推理物品的属性，即形状、纹理、材料、位置等。为了从训练好的知识图谱中查询具体的抓握功能，作者使用吉布斯抽样（Gibbs sampling），在计算量允许的情况下遍历尽可能多的可能性来生成后验样本。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.31953125\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBElLYFFQXJIG0WxpTZAKwmZOKr7v8sgQKzWUkOibzFkYaL4UHznjkkkcQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>[9]中的抓握功能推理框架，由学习（learn），查询（query）和映射（mapping）组成。学习的模型（由白色矩形标注）使用彩色线段对节点之间的关系进行编码（</span></em></span><em style=\"color: rgb(136, 136, 136);font-size: 12px;font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>图源：Ardón P., Pairet È. , Petrick R. P. A. , Ramamoorthy S. and Lohan K. S. (2019). Learning Grasp Affordance Reasoning Through Semantic Relations. IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 4571-4578.）</em></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">相对于 [6]， [9] 专注于解决抓握功能的推理，同时对其进行了很多细分。在此之后， [9] 将一组细分后的掌握功能与一个物体相关联。下图描绘了在不同室内场景中，不同形状的物体可能对应的抓握功能。三个箭头按照颜色代表物体上不同位置更可能或更不可能拥有的抓握功能。所有的可能性都已经经过了归一化处理，在（+1，-1）内分布，数值越高，则代表可能性越大。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">有趣的是，可以看到，在不同的抓握功能中，与性状最无关的功能是移交（hand over）。而其他功能，如盛纳（contain）则与物体的形状关联度很大。开放式容器更有可能具有盛纳功能而其他形状如螺丝刀等则最不可能具有盛纳功能。</span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\"><br></span></section><p style=\"text-align: center;\"><img class=\"rich_pages js_insertlocalimg\" data-ratio=\"0.5078125\" data-s=\"300,640\" data-type=\"png\" data-w=\"1280\" style=\"max-width: 600px\" src=\"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEu9OJickFHwrgQeASdqM71EmRaoYH0h1ibzmGWiap5EkLMZrkJ7RAIfiacQ/640?wx_fmt=png\"></p><section style=\"line-height: 1.75em;\"><span style=\"color: rgb(136, 136, 136);font-size: 12px;\"><em><span style=\"color: rgb(136, 136, 136);font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif>对物体的形状和抓握功能之间的可能性进行可视化（图源：Ardón P., Pairet È. , Petrick R. P. A. , Ramamoorthy S. and Lohan K. S. (2019). Learning Grasp Affordance Reasoning Through Semantic Relations. IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 4571-4578.）</span></em></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: center;\"><span style=\"font-size: 16px;\"><strong>结论</strong></span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">不难看出，Affordance 的研究重点之一是发展机器人的泛化能力。正如人类能够对形状明显不同但 Affordance 相同的物体——比如玻璃杯 vs 葡萄酒杯——成功进行推理，同时还不会混淆形状相同但 Affordance 不同的物体，比如杯子和蜡烛。我们离人类的水平目前还有一段距离，但得益于深度学习的发展，Affordance 的推理也取得了很多突破。特别是 CNN 结合知识图谱的路线，已经提供不少非常有希望的结果。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">但另一方面，硬件仍然是这个领域内的一个挑战。相较于普通的视觉任务（vision task），深度信息对于 Affordance 的学习至关重要。想象一下上文中两个杯子交缠的例子，若没有深度信息模型就无法正确推理。虽然这个例子比较极端，但当机器人需要和真实世界——一个有很多静止和非静止物体的世界——进行交互时，深度信息绝对是必不可少的。此外，如果模型不能被部署在云端上，算力也是对目前机器人身上的硬件的一个挑战。使用模拟或者使用神经网络，这两个方法对算力的要求都比较高，并且很多研究在推理并定位了 Affordance 还需要将结果投射到 3D 模型上，又进一步加大了硬件算力方面的挑战。</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 15px;\">总的来说，笔者对机器人这一领域的发展充满信心，期待能够尽快看到更多研究上的突破落实到实际生产中。也许在不远的将来，我们就能在海底捞看到机器人服务员为你端茶倒水、为你表演扯面了呢  :-）</span></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;\"><strong><span style=\"font-size: 15px;\">参考文献</span></strong></section><section style=\"line-height: 1.75em;\"><br></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[1] J. J. Gibson (1966). The Senses Considered as Perceptual Systems. Allen and Unwin, London.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[2] J. J. Gibson (1975). 'Affordances and behavior'. In E. S. Reed &amp; R. Jones (eds.), Reasons for Realism: Selected Essays of James J. Gibson, pp. 410-411. Lawrence Erlbaum, Hillsdale, NJ, 1 edn.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[3] H. Wu, G. S. Chirikjian. (2020). Can I Pour into It? Robot Imagining Open Containability Affordance of Previously Unseen Objects via Physical Simulations. arXiv:2008.02321.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[4] K. Fang, T. Wu, D. Yang, S. Savarese and J. J. Lim. (2018). Demo2Vec: Reasoning Object Affordances from Online Videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[5] Y. Zhu, C. Jiang, Y. Zhao, D. Terzopoulos, and S.-C. Zhu.(2016).  Inferring forces and learning human utilities from videos. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3823–3833.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[6] Zhu, Y., Fathi, A., &amp; Fei-Fei, L. (2014). Reasoning about Object Affordances in a Knowledge Base Representation. ECCV.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[7] Montesano L. and Lopes M. (2009).,Learning grasping affordances from local visual descriptors. 2009 IEEE 8th International Conference on Development and Learning</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[8] Zeng, A., Song, S., Yu, K.-T., Donlon, E., Hogan, F. R., Bauza, M., Ma, D., Taylor, O., Liu, M., Romo, E., Fazeli, N., Alet, F., Chavan Dafle, N., Holladay, R., Morona, I., Nair, P. Q., Green, D., Taylor, I., Liu, W., … Rodriguez, A. (2019). Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. The International Journal of Robotics Research.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[9] Ardón P., Pairet È. , Petrick R. P. A. , Ramamoorthy S. and Lohan K. S. (2019). Learning Grasp Affordance Reasoning Through Semantic Relations. IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 4571-4578.</span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[10] Ardón P., Pairet È., Petrick R., Ramamoorthy S., Lohan K. (2019) Reasoning on Grasp-Action Affordances. Towards Autonomous Robotic Systems. TAROS 2019. Lecture Notes in Computer Science, vol 11649. </span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[11] Do, T.-T., Anh N., and Ian R. (2018). Affordancenet: An end-to-end deep learning approach for object affordance detection. 2018 IEEE international conference on robotics and automation (ICRA). </span></section><section style=\"line-height: 1.75em;text-align: left;\"><span style=\"font-size: 14px;\">[12] B. Curless and M. Levoy. (1996) A volumetric method for building complex models from range images. Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pp. 303–312. </span></section><section style=\"line-height: 1.75em;\"><span style=\"font-size: 14px;\"><br></span></section><section style=\"max-width: 100%;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><strong style=\"max-width: 100%;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;font-size: 15px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\">分析师介绍</span><span style=\"max-width: 100%;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">：</span></span></strong><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></section><section style=\"max-width: 100%;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></section><section style=\"max-width: 100%;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal border-box break-word><f textcolor=\"#414141\" style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;font-size: 15px;letter-spacing: 0.544px;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"background-color: rgb(255, 255, 255);\">本文作者为</span><f textcolor=\"#414141\" style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"background-color: rgb(255, 255, 255);\">Yuanyuan Li。</span><span style=\"background-color: rgb(255, 255, 255);\">几次转行，本科国际贸易，研究生转向统计，毕业后留在比利时，从事农用机械研发工作，主要负责图像处理，实现计算机视觉算法的落地。</span><span style=\"background-color: rgb(255, 255, 255);\">欣赏一切简单、优雅但有效</span>的<span style=\"background-color: rgb(255, 255, 255);\">算法，试图在深度学习的簇拥者和怀疑者之间找到一个平衡。</span></f><span style=\"background-color: rgb(255, 255, 255);\"><f textcolor=\"#414141\" style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></f></span></span></f></section><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><strong style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"><span style=\"max-width: 100%;font-size: 15px;color: rgb(123, 12, 0);box-sizing: border-box !important;overflow-wrap: break-word !important;\">关于机器之心全球分析师网络 Synced Global Analyst Network</span></strong></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><span style=\"max-width: 100%;font-size: 15px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">机器之心全球分析师网络是由机器之心发起的全球性人工智能专业知识共享网络。在过去的四年里，已有数百名来自全球各地的 AI 领域专业学生学者、工程专家、业务专家，利用自己的学业工作之余的闲暇时间，通过线上分享、专栏解读、知识库构建、报告发布、评测及项目咨询等形式与全球 AI 社区共享自己的研究思路、工程经验及行业洞察等专业知识，并从中获得了自身的能力成长、经验积累及职业发展。</span></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><br style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\"></p><p style=\"max-width: 100%;min-height: 1em;font-family: -apple-system-font, BlinkMacSystemFont, \" helvetica neue sc sans gb yahei ui arial sans-serif normal rgb border-box break-word><span style=\"max-width: 100%;font-size: 15px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">感兴趣加入机器之心全球分析师网络？点击</span><span style=\"max-width: 100%;font-size: 15px;color: rgb(123, 12, 0);box-sizing: border-box !important;overflow-wrap: break-word !important;\"><strong style=\"max-width: 100%;box-sizing: border-box !important;overflow-wrap: break-word !important;\">阅读原文</strong></span><span style=\"max-width: 100%;font-size: 15px;box-sizing: border-box !important;overflow-wrap: break-word !important;\">，提交申请。</span></p>\n                </div>\n\n    \n        <br>\n        <div id=\"js_toobar3\" class=\"rich_media_tool\">\n            <a target=\"_blank\" href=\"http://jiqizhixin.mikecrm.com/rg2RY52\" id=\"js_view_source\" class=\"media_tool_meta meta_primary\">阅读原文</a>\n        </div>\n    \n    <br>\n\n    \n        <a target=\"_blank\" href=\"http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650802504&amp;idx=4&amp;sn=f7bad9bcf4ab0225eb88bf9a22539fdf&amp;chksm=84e5cd36b3924420413405a3e4070d9366594d9fce9d0dcf6e8640656666cd4b9c1b9aa36e94&amp;scene=0#rd\" style=\"color: blue\" class=\"media_tool_meta meta_primary\">原文</a>\n        <br>\n    \n\n    \n\n    <img alt=\"\" width=\"1px\" height=\"1px\" class=\"\" style=\"width:1px;height:1px;display:none\" src=\"http://www.jintiankansha.me/rss_static/24708/cFe6JfFDdv\"></div></div>","descriptionType":"html","publishedDate":"Mon, 16 Nov 2020 04:33:00 +0000","feedId":1837,"bgimg":"http://contentg.sov5.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1xibhOzAG7bEuQfDsTt3TzgavmUqI0UbZFxF3r7JQRicvszLyU8phGNg?imageView2/1/w/600","linkMd5":"a12f6169295fb78d005e2d481efabc34","bgimgJsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn35@2020_1/2020/11/17/05-42-33-802_ef68b3001f06c514.webp","destWidth":600,"destHeight":298,"sourceBytes":9648,"destBytes":6672,"author":"","articleImgCdnMap":{"http://contentg.sov5.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1xibhOzAG7bEuQfDsTt3TzgavmUqI0UbZFxF3r7JQRicvszLyU8phGNg?imageView2/1/w/600":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn35@2020_1/2020/11/17/05-42-33-802_ef68b3001f06c514.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEK8ednibCzGbuiaZWHqCicFb9QtotvNBS2x7l0qG1TJbsDJ0pp8OB22WGw/640?wx_fmt=jpeg":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEzBCccZgCibU8v6cvVibJkxGSNlgzBIcibBL87XrWdAxbh3a6Nn5w2icEcw/640?wx_fmt=jpeg":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn52@2020_5/2020/11/17/05-42-37-453_e8c2fcbf817ade3c.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1zLts88cM29JS8CQocBP0zQoJwn22ahCVKMBhvANL0lHt3kxZFdEWQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn44@2020_1/2020/11/17/05-43-42-125_8ff33733e5622b46.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEZSVRsgoHibsjUCKUthbdOl6havmj6gUt7jvDdMksIc6Q9nDL6rkUibjg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn49@2020_6/2020/11/17/05-42-55-092_290b82faa0eba538.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEEQicV1wooo8CYbw89CLRvKibks2U5u30ZRlYYAx9BNobFC2CZFKEOMRQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn88@2020_5/2020/11/17/05-42-35-268_61b8bfc086ad63d5.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEgSq3WXjDA3jUMkweIxjUokkLulCQhNyJq9H7prsI53QmqxHzyUIFcA/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBERUpQGic7cRicSNxB7VZXpfg79OoyVVYa2q2OnXKBAuOkQfwYH9hSa0rg/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn86@2020_1/2020/11/17/05-42-39-494_7230f858c427af20.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEl0DgFqpyKwEa2e7TzwHVV0iaDX0exCjiay9ibHiaPaXKBPhbjME8ZWF8JQ/640?wx_fmt=png":null,"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEoO4lvEHrR1dOgAX7X8ksk2vh6VnYhx5Ja90ia2f8ckAT7HZVX2eeribA/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn46@2020_2/2020/11/17/05-42-37-655_4d7c4f147e9bcb85.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEZQIPRn6Gqqfab1Z0wHsZ3sIGDBNoaQj5KRfppicC5GhhRCSHSLlfuVw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn57@2020_6/2020/11/17/05-42-38-874_0e3310bbee8cd237.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEER0yx8912tu3gic3YbTia11zsBo6IDBcAW7eeaxSQ1q6rO0Kxq7Ct8RQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn3@2020_3/2020/11/17/05-42-40-200_17862dedf51b3651.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE66icHU8Y8eXUcTV0iaKFaJYTtSPxLeIHqr2uW29kFdBtYHsNicVccpOibA/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn68@2020_2/2020/11/17/05-44-03-671_24f7dc3605102a2b.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEFO4fSBoECWNHfHxrLwKgkOe7YWjDDQxUfxt60MWVtAcTmfb5LiaX4bw/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn61@2020_5/2020/11/17/05-42-53-186_5ccb8ab40c25163b.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE8mkH9VUpaBOdicGaibIhRaL3tKdrJPZdo0en807PMLaJAObHs2LuyY0g/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn40@2020_4/2020/11/17/05-42-38-469_6dcc52a4d9921da0.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBElLYFFQXJIG0WxpTZAKwmZOKr7v8sgQKzWUkOibzFkYaL4UHznjkkkcQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn82@2020_3/2020/11/17/05-42-53-978_3930e4fd2c624416.webp","http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEu9OJickFHwrgQeASdqM71EmRaoYH0h1ibzmGWiap5EkLMZrkJ7RAIfiacQ/640?wx_fmt=png":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn77@2020_5/2020/11/17/05-42-39-081_04576839374817f8.webp","http://www.jintiankansha.me/rss_static/24708/cFe6JfFDdv":null},"publishedOrCreatedDate":1605591752234}],"record":{"createdTime":"2020-11-17 13:42:32","updatedTime":"2020-11-17 13:42:32","feedId":1837,"fetchDate":"Tue, 17 Nov 2020 05:42:32 +0000","fetchMs":1078,"handleMs":25,"totalMs":125234,"newArticles":0,"totalArticles":5,"status":1,"type":0,"ip":"ac489040e066f721b0d0c3774cf6c214","hostName":"us-004*","requestId":"671861ee72274f6b87747b2183b0a27e_1837","contentType":"application/rss+xml","totalBytes":393940,"bgimgsTotal":1,"bgimgsGithubTotal":1,"articlesImgsTotal":18,"articlesImgsGithubTotal":14,"successGithubMap":{"myreaderx7":1,"myreaderx15":1,"myreaderx16":1,"myreaderx10":1,"myreaderx4":1,"myreaderx22":1,"myreaderx11":1,"myreaderx33":1,"myreaderx3":1,"myreaderx12":1,"myreaderx2":1,"myreaderx1":1,"myreaderx29":1,"myreaderx19":1},"failGithubMap":{"myreaderx14":1}},"feed":{"createdTime":"2020-08-24 21:31:33","updatedTime":"2020-09-01 09:54:29","id":1837,"name":"机器之心","url":"http://feedmaker.kindle4rss.com/feeds/almosthuman2014.weixin.xml","subscriber":null,"website":null,"icon":"http://www.sogou.com/images/logo/new/favicon.ico?v=4","icon_jsdelivr":"https://cdn.jsdelivr.net/gh/myreaderx65/cdn87@2020_3/2020/09/01/01-54-30-263_d24121c9beed1de6.ico","description":"专业的人工智能媒体和产业服务平台","weekly":null,"link":null},"noPictureArticleList":[{"createdTime":"2020-11-17 13:44:36","updatedTime":"2020-11-17 13:44:36","id":null,"feedId":1837,"linkMd5":"a12f6169295fb78d005e2d481efabc34"}],"tmpCommonImgCdnBytes":6672,"tmpBodyImgCdnBytes":387268,"tmpBgImgCdnBytes":0,"extra4":{"start":1605591750977,"total":0,"statList":[{"spend":1233,"msg":"获取xml内容"},{"spend":25,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":121568,"msg":"正文链接上传到cdn"}]},"extra5":18,"extra6":16,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/24708/cFe6JfFDdv","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":406,"convertSpendMs":0,"createdTime":"2020-11-17 13:42:34","host":"us-007*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBERUpQGic7cRicSNxB7VZXpfg79OoyVVYa2q2OnXKBAuOkQfwYH9hSa0rg/640?wx_fmt=png","sourceStatusCode":403,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":1363,"convertSpendMs":0,"createdTime":"2020-11-17 13:42:34","host":"us-029*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","extra22GetBytesInfo":"2、Referer字段 ： http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","extra23historyStatusCode":[403,403],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://www.jintiankansha.me/rss_static/24708/cFe6JfFDdv","sourceStatusCode":405,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":823,"convertSpendMs":0,"createdTime":"2020-11-17 13:42:35","host":"europe68*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[405],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEu9OJickFHwrgQeASdqM71EmRaoYH0h1ibzmGWiap5EkLMZrkJ7RAIfiacQ/640?wx_fmt=png","sourceStatusCode":403,"sourceBytes":0,"destBytes":0,"feedId":1837,"totalSpendMs":1613,"convertSpendMs":0,"createdTime":"2020-11-17 13:42:34","host":"us-001*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","extra22GetBytesInfo":"2、Referer字段 ： http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","extra23historyStatusCode":[403,403],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEK8ednibCzGbuiaZWHqCicFb9QtotvNBS2x7l0qG1TJbsDJ0pp8OB22WGw/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":700,"destHeight":580,"sourceBytes":13850,"destBytes":8386,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":1960,"convertSpendMs":18,"createdTime":"2020-11-17 13:42:34","host":"us-018*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn93/contents/2020/11/17/05-42-36-578_f9377a1c22716032.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Tue, 17 Nov 2020 05:42:36 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["DF0A:2A2A:18A8614:37DD3F3:5FB362C6"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1605595118"],"x-ratelimit-used":["60"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn93/contents/2020/11/17/05-42-36-578_f9377a1c22716032.webp","historyStatusCode":[],"spendMs":41},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"13.5 KB","destSize":"8.2 KB","compressRate":"60.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEK8ednibCzGbuiaZWHqCicFb9QtotvNBS2x7l0qG1TJbsDJ0pp8OB22WGw/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":700,"destHeight":580,"sourceBytes":13850,"destBytes":8386,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":514,"convertSpendMs":17,"createdTime":"2020-11-17 13:42:36","host":"us-018*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","rawMap":{"githubUrl":"https://api.github.com/repos/myreaderx14/cdn93/contents/2020/11/17/05-42-37-103_f9377a1c22716032.webp","resp":{"code":403,"msg":"Forbidden","body":"{\n  \"message\": \"API rate limit exceeded for user ID 68584859.\",\n  \"documentation_url\": \"https://docs.github.com/rest/overview/resources-in-the-rest-api#rate-limiting\"\n}\n","headerMap":{"access-control-allow-origin":["*"],"access-control-expose-headers":["ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, Deprecation, Sunset"],"content-security-policy":["default-src 'none'"],"content-type":["application/json; charset=utf-8"],"date":["Tue, 17 Nov 2020 05:42:37 GMT"],"referrer-policy":["origin-when-cross-origin, strict-origin-when-cross-origin"],"server":["GitHub.com"],"status":["403 Forbidden"],"strict-transport-security":["max-age=31536000; includeSubdomains; preload"],"transfer-encoding":["chunked"],"vary":["Accept-Encoding, Accept, X-Requested-With"],"x-accepted-oauth-scopes":["repo"],"x-content-type-options":["nosniff"],"x-frame-options":["deny"],"x-github-media-type":["github.v3; format=json"],"x-github-request-id":["DF0A:2A2A:18A8646:37DD879:5FB362CC"],"x-oauth-scopes":["admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, delete:packages, delete_repo, gist, notifications, repo, user, workflow, write:discussion, write:packages"],"x-ratelimit-limit":["60"],"x-ratelimit-remaining":["0"],"x-ratelimit-reset":["1605595118"],"x-ratelimit-used":["60"],"x-xss-protection":["1; mode=block"]},"exceptionMsg":"Unexpected code 403,url is : https://api.github.com/repos/myreaderx14/cdn93/contents/2020/11/17/05-42-37-103_f9377a1c22716032.webp","historyStatusCode":[],"spendMs":44},"base64UserPassword":null,"token":"6b67d******************************91b08"},"githubUser":"myreaderx14","githubHttpCode":403,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"13.5 KB","destSize":"8.2 KB","compressRate":"60.5%"},null,null,null,null,null,null],"extra10_invalidATagHrefValue":{},"extra111_proxyServerAndStatMap":{"http://us-013.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-018.herokuapp.com/":{"failCount":1,"successCount":2,"resultList":[200,200,null]},"http://us-037.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe68.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]},"http://us-007.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[405]},"http://us-002.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-034.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://europe-58.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-030.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-009.herokuapp.com/":{"failCount":0,"successCount":2,"resultList":[200,200]},"http://us-025.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[null]},"http://us-001.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[403]},"http://europe69.herokuapp.com/":{"failCount":0,"successCount":3,"resultList":[200,200,200]},"http://us-036.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-019.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://europe64.herokuapp.com/":{"failCount":0,"successCount":1,"resultList":[200]},"http://us-014.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,null]},"http://us-029.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[403]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"http://contentg.sov5.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1xibhOzAG7bEuQfDsTt3TzgavmUqI0UbZFxF3r7JQRicvszLyU8phGNg?imageView2/1/w/600","sourceStatusCode":200,"destWidth":600,"destHeight":298,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx19/cdn35@2020_1/2020/11/17/05-42-33-802_ef68b3001f06c514.webp","sourceBytes":9648,"destBytes":6672,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":2372,"convertSpendMs":8,"createdTime":"2020-11-17 13:42:32","host":"us-007*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34,a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx19","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.4 KB","destSize":"6.5 KB","compressRate":"69.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEEQicV1wooo8CYbw89CLRvKibks2U5u30ZRlYYAx9BNobFC2CZFKEOMRQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":546,"destHeight":220,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx33/cdn88@2020_5/2020/11/17/05-42-35-268_61b8bfc086ad63d5.webp","sourceBytes":9866,"destBytes":14652,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":1949,"convertSpendMs":14,"createdTime":"2020-11-17 13:42:34","host":"us-030*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx33","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"9.6 KB","destSize":"14.3 KB","compressRate":"148.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEzBCccZgCibU8v6cvVibJkxGSNlgzBIcibBL87XrWdAxbh3a6Nn5w2icEcw/640?wx_fmt=jpeg","sourceStatusCode":200,"destWidth":700,"destHeight":566,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx16/cdn52@2020_5/2020/11/17/05-42-37-453_e8c2fcbf817ade3c.webp","sourceBytes":17322,"destBytes":11168,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":3551,"convertSpendMs":13,"createdTime":"2020-11-17 13:42:34","host":"us-009*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx16","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"16.9 KB","destSize":"10.9 KB","compressRate":"64.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEoO4lvEHrR1dOgAX7X8ksk2vh6VnYhx5Ja90ia2f8ckAT7HZVX2eeribA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":123,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx29/cdn46@2020_2/2020/11/17/05-42-37-655_4d7c4f147e9bcb85.webp","sourceBytes":104704,"destBytes":14104,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":3884,"convertSpendMs":21,"createdTime":"2020-11-17 13:42:34","host":"us-019*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx29","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"102.2 KB","destSize":"13.8 KB","compressRate":"13.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE8mkH9VUpaBOdicGaibIhRaL3tKdrJPZdo0en807PMLaJAObHs2LuyY0g/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":284,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx12/cdn40@2020_4/2020/11/17/05-42-38-469_6dcc52a4d9921da0.webp","sourceBytes":171357,"destBytes":31148,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":4782,"convertSpendMs":21,"createdTime":"2020-11-17 13:42:34","host":"europe69*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx12","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"167.3 KB","destSize":"30.4 KB","compressRate":"18.2%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEZQIPRn6Gqqfab1Z0wHsZ3sIGDBNoaQj5KRfppicC5GhhRCSHSLlfuVw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":269,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx2/cdn57@2020_6/2020/11/17/05-42-38-874_0e3310bbee8cd237.webp","sourceBytes":75733,"destBytes":20090,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":5104,"convertSpendMs":23,"createdTime":"2020-11-17 13:42:34","host":"us-036*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx2","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"74 KB","destSize":"19.6 KB","compressRate":"26.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEu9OJickFHwrgQeASdqM71EmRaoYH0h1ibzmGWiap5EkLMZrkJ7RAIfiacQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":548,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx10/cdn77@2020_5/2020/11/17/05-42-39-081_04576839374817f8.webp","sourceBytes":251806,"destBytes":59256,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":3723,"convertSpendMs":121,"createdTime":"2020-11-17 13:42:36","host":"us-002*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx10","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"245.9 KB","destSize":"57.9 KB","compressRate":"23.5%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBERUpQGic7cRicSNxB7VZXpfg79OoyVVYa2q2OnXKBAuOkQfwYH9hSa0rg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":488,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx4/cdn86@2020_1/2020/11/17/05-42-39-494_7230f858c427af20.webp","sourceBytes":155241,"destBytes":29200,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":4388,"convertSpendMs":29,"createdTime":"2020-11-17 13:42:36","host":"europe69*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx4","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"151.6 KB","destSize":"28.5 KB","compressRate":"18.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEER0yx8912tu3gic3YbTia11zsBo6IDBcAW7eeaxSQ1q6rO0Kxq7Ct8RQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":196,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx11/cdn3@2020_3/2020/11/17/05-42-40-200_17862dedf51b3651.webp","sourceBytes":115194,"destBytes":23372,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":6499,"convertSpendMs":18,"createdTime":"2020-11-17 13:42:34","host":"europe69*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx11","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"112.5 KB","destSize":"22.8 KB","compressRate":"20.3%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEFO4fSBoECWNHfHxrLwKgkOe7YWjDDQxUfxt60MWVtAcTmfb5LiaX4bw/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":380,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx3/cdn61@2020_5/2020/11/17/05-42-53-186_5ccb8ab40c25163b.webp","sourceBytes":359971,"destBytes":46230,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":19477,"convertSpendMs":50,"createdTime":"2020-11-17 13:42:34","host":"us-037*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx3","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"351.5 KB","destSize":"45.1 KB","compressRate":"12.8%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBElLYFFQXJIG0WxpTZAKwmZOKr7v8sgQKzWUkOibzFkYaL4UHznjkkkcQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":345,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx1/cdn82@2020_3/2020/11/17/05-42-53-978_3930e4fd2c624416.webp","sourceBytes":281202,"destBytes":36942,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":20334,"convertSpendMs":32,"createdTime":"2020-11-17 13:42:34","host":"europe64*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx1","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"274.6 KB","destSize":"36.1 KB","compressRate":"13.1%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBEZSVRsgoHibsjUCKUthbdOl6havmj6gUt7jvDdMksIc6Q9nDL6rkUibjg/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":354,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx7/cdn49@2020_6/2020/11/17/05-42-55-092_290b82faa0eba538.webp","sourceBytes":298142,"destBytes":35432,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":21279,"convertSpendMs":50,"createdTime":"2020-11-17 13:42:34","host":"us-014*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx7","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"291.2 KB","destSize":"34.6 KB","compressRate":"11.9%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE1zLts88cM29JS8CQocBP0zQoJwn22ahCVKMBhvANL0lHt3kxZFdEWQ/640?wx_fmt=png","sourceStatusCode":200,"destWidth":910,"destHeight":588,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx15/cdn44@2020_1/2020/11/17/05-43-42-125_8ff33733e5622b46.webp","sourceBytes":521643,"destBytes":41154,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":8310,"convertSpendMs":38,"createdTime":"2020-11-17 13:43:34","host":"us-030*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx15","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"509.4 KB","destSize":"40.2 KB","compressRate":"7.9%"},{"code":1,"isDone":false,"source":"http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibQicW2jLXX1pRrSWf4adZBE66icHU8Y8eXUcTV0iaKFaJYTtSPxLeIHqr2uW29kFdBtYHsNicVccpOibA/640?wx_fmt=png","sourceStatusCode":200,"destWidth":1080,"destHeight":375,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx22/cdn68@2020_2/2020/11/17/05-44-03-671_24f7dc3605102a2b.webp","sourceBytes":183185,"destBytes":24520,"targetWebpQuality":75,"feedId":1837,"totalSpendMs":29702,"convertSpendMs":24,"createdTime":"2020-11-17 13:43:34","host":"us-009*","referer":"http://weixin.sogou.com/weixin?type=2&query=%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83+%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%98%AF%E6%80%8E%E4%B9%88%E7%9F%A5%E9%81%93%E5%A6%82%E4%BD%95%E6%8A%93%E6%8F%A1%E6%9D%AF%E5%AD%90%E7%9A%84%EF%BC%9F","linkMd5ListStr":"a12f6169295fb78d005e2d481efabc34","githubUser":"myreaderx22","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"178.9 KB","destSize":"23.9 KB","compressRate":"13.4%"}],"successGithubMap":{"myreaderx7":1,"myreaderx15":1,"myreaderx16":1,"myreaderx10":1,"myreaderx4":1,"myreaderx22":1,"myreaderx11":1,"myreaderx33":1,"myreaderx3":1,"myreaderx12":1,"myreaderx2":1,"myreaderx1":1,"myreaderx29":1,"myreaderx19":1},"failGithubMap":{"myreaderx14":1}}