{"code":1,"isDone":false,"toInsertArticleList":[{"createdTime":"2021-04-26 02:03:55","updatedTime":"2021-04-26 02:03:55","title":"Seeing traffic through new eyes","link":"https://plus.maths.org/content/seeing-traffic-through-new-eyes","description":"<div class=\"field field-name-field-abs-img field-type-image field-label-hidden\"> \n <div class=\"field-items\"> \n  <div class=\"field-item even\"> \n   <img class=\"img-responsive\" loading=\"lazy\" src=\"https://plus.maths.org/content/sites/plus.maths.org/files/abstractpics/%5Buid%5D/%5Bsite-date%5D/integral_icon_0.jpg\" width=\"100\" height=\"100\" alt=\"\" /> \n  </div> \n </div> \n</div> \n<div class=\"field field-name-field-author field-type-text field-label-hidden\"> \n <div class=\"field-items\"> \n  <div class=\"field-item even\">\n    Marianne Freiberger \n  </div> \n </div> \n</div> \n<div class=\"field field-name-body field-type-text-with-summary field-label-hidden\"> \n <div class=\"field-items\"> \n  <div class=\"field-item even\"> \n   <p>Roads are arteries of modern life, but traffic comes with problems. It's bad for the environment and for our climate, and it can also be bad for our health. Road accidents cause deaths and injuries, pollution causes lung and respiratory diseases, as well as cancers, and the noise and stress of traffic can impact people's mental health. </p> \n   <p>Good planning and effective urban policies can help reduce these risks, but to make a good strategy you first need to know what kind of vehicles — bikes, cars, tuk tuks, trucks — are on the road, in what proportion, and how they tend to behave. Such data is difficult and expensive to collect. Especially in places where cities are expanding rapidly, such as India, very little data about traffic is available.</p> \n   <div class=\"rightimage\" style=\"max-width: 350px;\"> \n    <img alt=\"INTEGRAL team members\" width=\"400\" height=\"227\" class=\"b-lazy\" data-src=\"/content/sites/plus.maths.org/files/articles/2021/integral/integral2.png\" src=\"https://plus.maths.org/content/data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\" /> \n    <p>Some of the inter-disciplinary INTEGRAL team and partners. From top left: Carola-Bibiane Schönlieb, Rahul Goel, Kelly Kokka, Rihuan Ke, Angelica Aviles-Rivero, Sanjay Kumar, James Woodcock.</p> \n   </div> \n   <p>\"There is a growing awareness of the adverse impacts of motor vehicles on human health and the benefits that use of walking and cycling can achieve, however, data on usage of different modes of transport are rarely available for the cities,\" says public health expert <a href=\"https://www.cedar.iph.cam.ac.uk/people/leads/james-woodcock/\" target=\"blank\">James Woodcock</a> from the MRC Epidemiology Unit at the University of Cambridge. \"The lack of data is of greater concern in low-and-middle income countries that are witnessing an exponential growth in motor vehicles.\"</p> \n   <p> The challenge has inspired mathematicians and other scientists at the University of Cambridge, including Woodcock, to form <a href=\"https://sites.google.com/view/remote-sens-research-for-india/home?authuser=0\" target=\"blank\">INTEGRAL</a> which stands for INdia remoTE ImaGery anALysis. One of the aims of this project is to look at traffic in some of India's major cities through new eyes: to teach computers to understand photos and videos taken by Google Street View, Google Earth and traffic cameras. \"We want to develop an open-source tool that can help relevant stakeholders in India (and eventually world-wide) to quantify traffic volume and determine traffic type in cities,\" says INTEGRAL co-Head <a href=\"https://www.damtp.cam.ac.uk/user/cbs31/Home.html\" target=\"blank\">Carola-Bibiane Schönlieb</a>. </p> \n   <h3>Pooling expertise</h3> \n   <p>It's the artificial intelligence aspect of the project — getting computers to understand image data — that requires the input from mathematicians such as Schönlieb, and turns INTEGRAL into a truly interdisciplinary project. Along with Schönlieb and Woodcock it comprises scientists from the Cambridge Department of Plant Sciences and Cambridge Public Health. There are also a range of industrial partners, including <a href=\"https://www.c40.org/\" target=\"blank\">C40</a>, which connects major cities around the world to take action on climate change, the <a href=\"https://home.iitd.ac.in/\" target=\"blank\">Indian Institute of Technology Delhi</a>, <a href=\"https://www.gci.cam.ac.uk/\" target=\"blank\">Cambridge Global Challenges</a>, the <a href=\"https://www.dlr.de/EN/Home/home_node.html\" target=\"blank\">German Aerospace Centre</a>, Indian technology company <a href=\"https://kritikalsolutions.com/\" target=\"blank\">Kritikal Solutions</a> and the environmental advisory group <a href=\"http://ioraecological.com/\" target=\"blank\">IORA</a>.</p> \n   <p>\"I enjoy working at the interface of machine learning and public health,\" says INTEGRAL member <a href=\"https://www.mrc-epid.cam.ac.uk/people/kelly-kokka/\" target=\"blank\">Kelly Kokka</a>. \"Its global application and the potential to fill a major data gap is hugely satisfying! Also it's exciting to collaborate with such an amazing team and learn from each other.\"</p> \n   <p>Together with her colleagues, Kokka has conducted some proof of concept work, to show that the idea of using publicly available image data to assess traffic does work. \"From our earlier work using Google Street View images, we found that images of people on the road in different modes of transport are a strong predictor of overall traffic patterns in the cities,\" explains INTEGRAL member <a href=\"https://www.mrc-epid.cam.ac.uk/people/rahul-goel/\" target=\"blank\">Rahul Goel</a>. \"The ability to predict this information using globally available imagery data sources is a great step towards understanding travel patterns especially in places where surveys are not available.\"</p> \n   <h3>Learning to see</h3> \n   <p>The tricky part of the project will be to get machines to recognise different types of vehicles, cyclists and pedestrians in images and videos. We humans are very good at processing visual information. We don't have trouble telling apart two cars of the same colour, recognising a bus even when it's partly hidden by a tree, or noting that the cyclist that was here before we blinked has now moved over there. </p> \n   <p>A computer doesn't have these instinctive abilities. To a computer an image is just an array of numbers, each encoding the colour of the corresponding pixel. The INTEGRAL team are developing algorithms that will enable a computer to examine each pixel and its neighbours, and then allocate the pixel to one of a number of classes, such as \"car\", \"truck\", \"background\", \"road\", etc. The result will be an image in which pixels belonging to the same class are given the same colour, as shown below. Processing an image in this way is called <em> semantic segmentation</em>. Once an image, or video, has been segmented, it's an easy task to count the number and types of vehicles in it.</p> \n   <div class=\"centreimage\"> \n    <img alt=\"A segmented video\" width=\"512\" height=\"384\" class=\"b-lazy\" data-src=\"/content/sites/plus.maths.org/files/articles/2021/integral/combined.gif\" src=\"https://plus.maths.org/content/data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\" /> \n    <p style=\"max-width: 600px;\">This is an example of a video that has been segmented. The top shows the raw video footage and the bottom shows the segmented version, where features of the same kind are given the same colour. </p> \n   </div> \n   <p>To get a computer algorithm to correctly perform such semantic segmentation, the INTEGRAL team are pushing the boundaries of machine learning. This area of artificial intelligence involves a computer algorithm \"learning\" how to spot patterns within data that correspond to a particular feature, for example, to spot the patterns in the pixels of an image that mean the image depicts a car. </p> \n   <p>In its simplest guise, machine learning requires lots of <em>training data</em> — for example, to \"teach\" the algorithm to differentiate pictures from cars from pictures of trucks, you would first give it a large number of pictures of cars and trucks and also tell it which shows a car and which shows a truck. The algorithm will analyse the pixel values of each image and return the output \"it's a car\" or \"it's a truck\" based on the patterns it finds. It will then compare its output to the correct answer — and if it turns out that the output was wrong a lot of the time, it will tune some internal parameters to see if it gets a better result. Eventually, the parameters will be just right to get the correct answer almost all of the time. (To find out more about this type of machine learning, see <a href=\"https://plus.maths.org/content/what-machine-learning\" target=\"blank\">this article</a>).</p> \n   <h3>Learning with little teaching</h3> \n   <p> That an algorithm can learn at all is quite amazing, but it gets better still. This sort of <em>supervised learning</em> requires a lot of training images which all have to be labelled by a human. For the purpose of semantic segmentation this doesn't involve just looking at the image and saying \"it's a car\" or \"it's a truck\", but looking at each individual pixel and deciding what class it belongs to. What's more, when video footage is involved, a human needs to label a video frame by frame. That's far too time consuming and expensive a task.</p> \n   <p>This is why the INTEGRAL team are pursuing a <a href=\"https://arxiv.org/pdf/2012.00827.pdf\" target=\"blank\">new approach</a>, using something called semi-supervised learning. Here you hand-label only a very small collection of images (or video frames). A clever algorithm then attaches labels to a large number of previously unlabelled images, by squeezing as much statistical information as possible out of the set of images. The now much larger set of labelled images can then be used as training data.</p> \n   <p>\"Our framework is based on a holistic principle that leverages several semi-supervised learning techniques for a more meaningful prediction on the unlabelled part of the data,\" explains INTEGRAL member <a href=\"http://www.damtp.cam.ac.uk/person/rk621\" target=\"blank\">Rihuan Ke</a>. \"It significantly improves the accuracy of existing approaches and allows to learn from very limited manual annotations.\"</p> \n   <p>The work the INTEGRAL team have done so far is only the beginning. \"[Ultimately] the goal is to develop innovative, efficient, robust and generalisable tools for the analysis of complex urban level video scenes,\" says <a href=\"https://www.maths.cam.ac.uk/person/ai323\" target=\"blank\">Angelica Aviles-Rivero</a>, another member of INTEGRAL. \"These tools will provide an easier way to analyse vast amounts of data in an [incredibly] short period of time.\"</p> \n   <p>And this isn't all. While India's cities are expanding, the biodiversity of its forest is potentially in danger. Another strand of the INTEGRAL project will use the kind of methods developed for understanding footage of traffic to understand satellite images of forests. The purpose is to count different species of trees and thereby understand the state of India's biodiversity.</p> \n   <p> </p> \n   <p>Once a machine has learnt how to analyse images and videos reliably, there is almost no limit to the uses you can put it to.</p> \n   <hr /> \n   <h3>About this article</h3> \n   <p><a href=\"https://plus.maths.org/content/people/index.html#marianne\" target=\"blank\">Marianne Freiberger</a> is Editor of <em>Plus</em>. She spoke to the INTEGRAL team in December 2020.</p> \n  </div> \n </div> \n</div>","descriptionType":"text/html","publishedDate":"Thu, 22 Apr 2021 15:46:28 +0000","feedId":19160,"bgimg":"","linkMd5":"b9d70e5dab0bd9497ce336f76caf9acf","bgimgJsdelivr":"","metaImg":"","author":"Marianne","articleImgCdnMap":{"https://plus.maths.org/content/sites/plus.maths.org/files/abstractpics/%5Buid%5D/%5Bsite-date%5D/integral_icon_0.jpg":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn71@2020_6/2021/04/25/18-04-13-551_ff6d1af5002a7c30.webp","https://plus.maths.org/content/data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg"},"publishedOrCreatedDate":1619373835887}],"record":{"createdTime":"2021-04-26 02:03:55","updatedTime":"2021-04-26 02:03:55","feedId":19160,"fetchDate":"Sun, 25 Apr 2021 18:03:55 +0000","fetchMs":755,"handleMs":9581,"totalMs":33535,"newArticles":0,"totalArticles":10,"status":1,"type":0,"ip":"f306b40366a93aa712f78b685bea0f01","hostName":"us-004*","requestId":"4c1100ba153c475aaaf9280485f41df5_19160","contentType":"application/rss+xml; charset=utf-8","totalBytes":2268,"bgimgsTotal":0,"bgimgsGithubTotal":0,"articlesImgsTotal":2,"articlesImgsGithubTotal":2,"successGithubMap":{"myreaderx6":1},"failGithubMap":{}},"feed":{"createdTime":"2020-09-07 02:19:14","updatedTime":"2021-04-03 14:51:51","id":19160,"name":"plus.maths.org","url":"http://plus.maths.org/content/rss.xml","subscriber":173,"website":null,"icon":"https://plus.maths.org/favicon.ico","icon_jsdelivr":null,"description":"","weekly":null,"link":"https://plus.maths.org"},"noPictureArticleList":[],"tmpCommonImgCdnBytes":0,"tmpBodyImgCdnBytes":2268,"tmpBgImgCdnBytes":0,"extra4":{"start":1619373825465,"total":0,"statList":[{"spend":841,"msg":"获取xml内容"},{"spend":9581,"msg":"解释文章"},{"spend":0,"msg":"上传封面图到cdn"},{"spend":0,"msg":"修正封面图上传失败重新上传"},{"spend":5760,"msg":"正文链接上传到cdn"}]},"extra5":2,"extra6":2,"extra7ImgCdnFailResultVector":[{"code":1,"isDone":false,"source":"https://plus.maths.org/content/data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==","sourceStatusCode":404,"sourceBytes":0,"destBytes":0,"feedId":19160,"totalSpendMs":1943,"convertSpendMs":0,"createdTime":"2021-04-26 02:04:15","host":"europe63*","referer":"https://plus.maths.org/content/seeing-traffic-through-new-eyes","linkMd5ListStr":"b9d70e5dab0bd9497ce336f76caf9acf,b9d70e5dab0bd9497ce336f76caf9acf","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"},{"code":1,"isDone":false,"source":"https://plus.maths.org/content/data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":19160,"totalSpendMs":1600,"convertSpendMs":0,"createdTime":"2021-04-26 02:04:17","host":"europe64*","referer":"https://plus.maths.org/content/seeing-traffic-through-new-eyes","linkMd5ListStr":"b9d70e5dab0bd9497ce336f76caf9acf,b9d70e5dab0bd9497ce336f76caf9acf","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"}],"extra10_invalidATagHrefValue":{"https://plus.maths.org/content/seeing-traffic-through-new-eyes_/content/people/index.html#marianne":"https://plus.maths.org/content/people/index.html#marianne"},"extra111_proxyServerAndStatMap":{"http://europe63.herokuapp.com/":{"failCount":1,"successCount":0,"resultList":[404]},"http://europe64.herokuapp.com/":{"failCount":1,"successCount":1,"resultList":[200,404]}},"extra12ImgCdnSuccessResultVector":[{"code":1,"isDone":false,"source":"https://plus.maths.org/content/sites/plus.maths.org/files/abstractpics/%5Buid%5D/%5Bsite-date%5D/integral_icon_0.jpg","sourceStatusCode":200,"destWidth":100,"destHeight":100,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx6/cdn71@2020_6/2021/04/25/18-04-13-551_ff6d1af5002a7c30.webp","sourceBytes":24387,"destBytes":2268,"targetWebpQuality":75,"feedId":19160,"totalSpendMs":679,"convertSpendMs":4,"createdTime":"2021-04-26 02:04:13","host":"europe64*","referer":"https://plus.maths.org/content/seeing-traffic-through-new-eyes","linkMd5ListStr":"b9d70e5dab0bd9497ce336f76caf9acf","githubUser":"myreaderx6","githubHttpCode":201,"extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[200],"sourceSize":"23.8 KB","destSize":"2.2 KB","compressRate":"9.3%"},{"code":1,"isDone":false,"source":"https://plus.maths.org/content/data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==","sourceStatusCode":404,"fixedGithubDest":"https://cdn.jsdelivr.net/gh/myreaderx/cdn0@2020_1/404.jpg","sourceBytes":0,"destBytes":0,"feedId":19160,"totalSpendMs":1600,"convertSpendMs":0,"createdTime":"2021-04-26 02:04:17","host":"europe64*","referer":"https://plus.maths.org/content/seeing-traffic-through-new-eyes","linkMd5ListStr":"b9d70e5dab0bd9497ce336f76caf9acf,b9d70e5dab0bd9497ce336f76caf9acf","extra22GetBytesInfo":"1、没有Referer字段","extra23historyStatusCode":[404],"sourceSize":"0","destSize":"0"}],"successGithubMap":{"myreaderx6":1},"failGithubMap":{}}